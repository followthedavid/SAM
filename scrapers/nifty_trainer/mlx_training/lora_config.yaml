# MLX LoRA Fine-tuning Configuration for SAM Roleplay
# Target: Uncensored creative fiction roleplay

# Model to fine-tune (small, fast, good for Apple Silicon)
model: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# LoRA parameters - num_layers corresponds to lora_layers
num_layers: 16

# Training parameters (reduced for 8GB RAM)
batch_size: 1
learning_rate: 1e-4
iters: 1000
steps_per_eval: 100
save_every: 200
val_batches: 10

# Data directory containing train.jsonl, valid.jsonl
data: "/Volumes/David External/nifty_archive/training_data"

# Output
adapter_path: "/Volumes/David External/nifty_archive/models/sam-roleplay-nifty-lora"

# Sequence length (reduced for 8GB RAM)
max_seq_length: 1024

# Gradient checkpointing to save memory
grad_checkpoint: true

# Enable training
train: true
