{"conversations": [{"from": "human", "value": "Find the median of two sorted arrays"}, {"from": "gpt", "value": "O(log(min(m,n))) binary search solution:\n\n```python\ndef find_median_sorted_arrays(nums1: list, nums2: list) -> float:\n    # Ensure nums1 is shorter\n    if len(nums1) > len(nums2):\n        nums1, nums2 = nums2, nums1\n    \n    m, n = len(nums1), len(nums2)\n    low, high = 0, m\n    \n    while low <= high:\n        partition1 = (low + high) // 2\n        partition2 = (m + n + 1) // 2 - partition1\n        \n        left1 = float('-inf') if partition1 == 0 else nums1[partition1 - 1]\n        right1 = float('inf') if partition1 == m else nums1[partition1]\n        left2 = float('-inf') if partition2 == 0 else nums2[partition2 - 1]\n        right2 = float('inf') if partition2 == n else nums2[partition2]\n        \n        if left1 <= right2 and left2 <= right1:\n            if (m + n) % 2 == 0:\n                return (max(left1, left2) + min(right1, right2)) / 2\n            return max(left1, left2)\n        elif left1 > right2:\n            high = partition1 - 1\n        else:\n            low = partition1 + 1\n    \n    raise ValueError(\"Input arrays are not sorted\")\n```\n\nThe insight: we're partitioning both arrays such that all elements on the left are less than all elements on the right."}]}
{"conversations": [{"from": "human", "value": "Implement a rate limiter"}, {"from": "gpt", "value": "**Token bucket algorithm:**\n```python\nimport time\n\nclass TokenBucketRateLimiter:\n    def __init__(self, capacity: int, refill_rate: float):\n        self.capacity = capacity\n        self.tokens = capacity\n        self.refill_rate = refill_rate  # tokens per second\n        self.last_refill = time.time()\n    \n    def allow_request(self) -> bool:\n        self._refill()\n        if self.tokens >= 1:\n            self.tokens -= 1\n            return True\n        return False\n    \n    def _refill(self):\n        now = time.time()\n        elapsed = now - self.last_refill\n        self.tokens = min(self.capacity, self.tokens + elapsed * self.refill_rate)\n        self.last_refill = now\n```\n\n**Sliding window counter:**\n```python\nfrom collections import defaultdict\nimport time\n\nclass SlidingWindowRateLimiter:\n    def __init__(self, limit: int, window_seconds: int):\n        self.limit = limit\n        self.window = window_seconds\n        self.requests = defaultdict(list)\n    \n    def allow_request(self, user_id: str) -> bool:\n        now = time.time()\n        cutoff = now - self.window\n        \n        # Remove old requests\n        self.requests[user_id] = [\n            t for t in self.requests[user_id] if t > cutoff\n        ]\n        \n        if len(self.requests[user_id]) < self.limit:\n            self.requests[user_id].append(now)\n            return True\n        return False\n```\n\n**Distributed rate limiter (Redis-based concept):**\n```python\n# Pseudo-code for Redis sliding window\ndef is_allowed(redis, user_id, limit, window):\n    key = f\"rate_limit:{user_id}\"\n    now = time.time()\n    \n    pipe = redis.pipeline()\n    pipe.zremrangebyscore(key, 0, now - window)\n    pipe.zadd(key, {str(now): now})\n    pipe.zcard(key)\n    pipe.expire(key, window)\n    _, _, count, _ = pipe.execute()\n    \n    return count <= limit\n```"}]}
{"conversations": [{"from": "human", "value": "Implement a bloom filter"}, {"from": "gpt", "value": "```python\nimport mmh3  # pip install mmh3\nfrom bitarray import bitarray  # pip install bitarray\n\nclass BloomFilter:\n    def __init__(self, size: int, num_hashes: int):\n        self.size = size\n        self.num_hashes = num_hashes\n        self.bit_array = bitarray(size)\n        self.bit_array.setall(0)\n    \n    def add(self, item: str):\n        for seed in range(self.num_hashes):\n            index = mmh3.hash(item, seed) % self.size\n            self.bit_array[index] = 1\n    \n    def might_contain(self, item: str) -> bool:\n        for seed in range(self.num_hashes):\n            index = mmh3.hash(item, seed) % self.size\n            if not self.bit_array[index]:\n                return False  # Definitely not present\n        return True  # Might be present (could be false positive)\n\n# Without external dependencies:\nclass SimpleBloomFilter:\n    def __init__(self, size: int, num_hashes: int):\n        self.size = size\n        self.num_hashes = num_hashes\n        self.bit_array = [0] * size\n    \n    def _hashes(self, item: str):\n        h1 = hash(item)\n        h2 = hash(item + \"salt\")\n        for i in range(self.num_hashes):\n            yield (h1 + i * h2) % self.size\n    \n    def add(self, item: str):\n        for idx in self._hashes(item):\n            self.bit_array[idx] = 1\n    \n    def might_contain(self, item: str) -> bool:\n        return all(self.bit_array[idx] for idx in self._hashes(item))\n```\n\nUse cases:\n- Cache filtering (don't query DB if definitely not in cache)\n- Spell checkers\n- Malicious URL detection\n- Preventing duplicate content\n\nFalse positive rate ≈ (1 - e^(-kn/m))^k where k=hashes, n=items, m=size"}]}
{"conversations": [{"from": "human", "value": "Implement topological sort"}, {"from": "gpt", "value": "```python\nfrom collections import defaultdict, deque\n\ndef topological_sort_kahn(graph: dict, num_nodes: int) -> list:\n    \"\"\"Kahn's algorithm (BFS-based)\"\"\"\n    in_degree = defaultdict(int)\n    \n    for node in graph:\n        for neighbor in graph[node]:\n            in_degree[neighbor] += 1\n    \n    queue = deque([node for node in range(num_nodes) if in_degree[node] == 0])\n    result = []\n    \n    while queue:\n        node = queue.popleft()\n        result.append(node)\n        \n        for neighbor in graph[node]:\n            in_degree[neighbor] -= 1\n            if in_degree[neighbor] == 0:\n                queue.append(neighbor)\n    \n    if len(result) != num_nodes:\n        return []  # Cycle detected\n    return result\n\ndef topological_sort_dfs(graph: dict) -> list:\n    \"\"\"DFS-based (reverse post-order)\"\"\"\n    visited = set()\n    rec_stack = set()  # For cycle detection\n    result = []\n    \n    def dfs(node):\n        if node in rec_stack:\n            return False  # Cycle\n        if node in visited:\n            return True\n        \n        visited.add(node)\n        rec_stack.add(node)\n        \n        for neighbor in graph.get(node, []):\n            if not dfs(neighbor):\n                return False\n        \n        rec_stack.remove(node)\n        result.append(node)\n        return True\n    \n    for node in graph:\n        if node not in visited:\n            if not dfs(node):\n                return []  # Cycle detected\n    \n    return result[::-1]\n\n# Example: Course scheduling\ndef can_finish(num_courses: int, prerequisites: list) -> bool:\n    graph = defaultdict(list)\n    for course, prereq in prerequisites:\n        graph[prereq].append(course)\n    \n    return len(topological_sort_kahn(graph, num_courses)) == num_courses\n```\n\nUse cases: build systems, package dependencies, task scheduling, course prerequisites."}]}
{"conversations": [{"from": "human", "value": "Find the shortest path in a weighted graph (Dijkstra)"}, {"from": "gpt", "value": "```python\nimport heapq\nfrom collections import defaultdict\n\ndef dijkstra(graph: dict, start, end=None) -> dict:\n    \"\"\"\n    graph: {node: [(neighbor, weight), ...]}\n    Returns: distances from start to all nodes\n    \"\"\"\n    distances = {start: 0}\n    pq = [(0, start)]  # (distance, node)\n    visited = set()\n    \n    while pq:\n        dist, node = heapq.heappop(pq)\n        \n        if node in visited:\n            continue\n        visited.add(node)\n        \n        # Early exit if we only need path to specific node\n        if end and node == end:\n            return distances\n        \n        for neighbor, weight in graph.get(node, []):\n            if neighbor in visited:\n                continue\n            \n            new_dist = dist + weight\n            if neighbor not in distances or new_dist < distances[neighbor]:\n                distances[neighbor] = new_dist\n                heapq.heappush(pq, (new_dist, neighbor))\n    \n    return distances\n\ndef dijkstra_with_path(graph: dict, start, end) -> tuple:\n    \"\"\"Returns (distance, path)\"\"\"\n    distances = {start: 0}\n    previous = {start: None}\n    pq = [(0, start)]\n    visited = set()\n    \n    while pq:\n        dist, node = heapq.heappop(pq)\n        \n        if node in visited:\n            continue\n        visited.add(node)\n        \n        if node == end:\n            path = []\n            while node:\n                path.append(node)\n                node = previous[node]\n            return dist, path[::-1]\n        \n        for neighbor, weight in graph.get(node, []):\n            if neighbor in visited:\n                continue\n            new_dist = dist + weight\n            if neighbor not in distances or new_dist < distances[neighbor]:\n                distances[neighbor] = new_dist\n                previous[neighbor] = node\n                heapq.heappush(pq, (new_dist, neighbor))\n    \n    return float('inf'), []  # No path exists\n```\n\nTime: O((V + E) log V) with binary heap. For negative weights, use Bellman-Ford."}]}
{"conversations": [{"from": "human", "value": "Solve the N-Queens problem"}, {"from": "gpt", "value": "```python\ndef solve_n_queens(n: int) -> list:\n    solutions = []\n    \n    def is_safe(board, row, col):\n        # Check column\n        for i in range(row):\n            if board[i] == col:\n                return False\n        \n        # Check diagonals\n        for i in range(row):\n            if abs(board[i] - col) == abs(i - row):\n                return False\n        \n        return True\n    \n    def solve(row, board):\n        if row == n:\n            solutions.append(board[:])\n            return\n        \n        for col in range(n):\n            if is_safe(board, row, col):\n                board[row] = col\n                solve(row + 1, board)\n                board[row] = -1\n    \n    solve(0, [-1] * n)\n    return solutions\n\ndef format_solution(solution: list) -> list:\n    n = len(solution)\n    board = []\n    for col in solution:\n        row = '.' * col + 'Q' + '.' * (n - col - 1)\n        board.append(row)\n    return board\n\n# Optimized with bitsets:\ndef solve_n_queens_fast(n: int) -> int:\n    \"\"\"Returns count of solutions\"\"\"\n    count = 0\n    \n    def solve(row, cols, diag1, diag2):\n        nonlocal count\n        if row == n:\n            count += 1\n            return\n        \n        available = ((1 << n) - 1) & ~(cols | diag1 | diag2)\n        while available:\n            pos = available & -available  # Rightmost set bit\n            available ^= pos\n            solve(row + 1, cols | pos, (diag1 | pos) << 1, (diag2 | pos) >> 1)\n    \n    solve(0, 0, 0, 0)\n    return count\n\n# For n=8, there are 92 solutions\n```"}]}
{"conversations": [{"from": "human", "value": "Implement consistent hashing"}, {"from": "gpt", "value": "```python\nimport hashlib\nfrom bisect import bisect_left\n\nclass ConsistentHash:\n    def __init__(self, nodes=None, replicas=100):\n        self.replicas = replicas  # Virtual nodes per physical node\n        self.ring = []  # Sorted list of hash values\n        self.ring_to_node = {}  # Map hash value to node\n        \n        if nodes:\n            for node in nodes:\n                self.add_node(node)\n    \n    def _hash(self, key: str) -> int:\n        return int(hashlib.md5(key.encode()).hexdigest(), 16)\n    \n    def add_node(self, node: str):\n        for i in range(self.replicas):\n            virtual_key = f\"{node}:{i}\"\n            hash_val = self._hash(virtual_key)\n            self.ring.append(hash_val)\n            self.ring_to_node[hash_val] = node\n        self.ring.sort()\n    \n    def remove_node(self, node: str):\n        for i in range(self.replicas):\n            virtual_key = f\"{node}:{i}\"\n            hash_val = self._hash(virtual_key)\n            self.ring.remove(hash_val)\n            del self.ring_to_node[hash_val]\n    \n    def get_node(self, key: str) -> str:\n        if not self.ring:\n            return None\n        \n        hash_val = self._hash(key)\n        idx = bisect_left(self.ring, hash_val)\n        \n        # Wrap around\n        if idx == len(self.ring):\n            idx = 0\n        \n        return self.ring_to_node[self.ring[idx]]\n    \n    def get_nodes(self, key: str, n: int = 3) -> list:\n        \"\"\"Get n nodes for replication\"\"\"\n        if not self.ring or n <= 0:\n            return []\n        \n        hash_val = self._hash(key)\n        idx = bisect_left(self.ring, hash_val)\n        \n        nodes = []\n        seen = set()\n        \n        while len(nodes) < n and len(seen) < len(set(self.ring_to_node.values())):\n            node = self.ring_to_node[self.ring[idx % len(self.ring)]]\n            if node not in seen:\n                nodes.append(node)\n                seen.add(node)\n            idx += 1\n        \n        return nodes\n```\n\nUse cases: distributed caches, load balancers, sharding databases. Key benefit: when adding/removing nodes, only K/n keys need to be remapped (K=keys, n=nodes)."}]}
{"conversations": [{"from": "human", "value": "Find all anagrams in a string"}, {"from": "gpt", "value": "```python\nfrom collections import Counter\n\ndef find_anagrams(s: str, p: str) -> list:\n    \"\"\"Find all starting indices of p's anagrams in s\"\"\"\n    if len(p) > len(s):\n        return []\n    \n    p_count = Counter(p)\n    window_count = Counter(s[:len(p)])\n    result = []\n    \n    if window_count == p_count:\n        result.append(0)\n    \n    for i in range(len(p), len(s)):\n        # Add new character\n        window_count[s[i]] += 1\n        # Remove old character\n        old_char = s[i - len(p)]\n        window_count[old_char] -= 1\n        if window_count[old_char] == 0:\n            del window_count[old_char]\n        \n        if window_count == p_count:\n            result.append(i - len(p) + 1)\n    \n    return result\n\n# Optimized with matches counter:\ndef find_anagrams_fast(s: str, p: str) -> list:\n    if len(p) > len(s):\n        return []\n    \n    p_count = Counter(p)\n    window_count = Counter()\n    result = []\n    matches = 0\n    required = len(p_count)\n    \n    for i, char in enumerate(s):\n        window_count[char] += 1\n        \n        if window_count[char] == p_count[char]:\n            matches += 1\n        elif window_count[char] == p_count[char] + 1:\n            matches -= 1\n        \n        if i >= len(p):\n            old = s[i - len(p)]\n            if window_count[old] == p_count[old]:\n                matches -= 1\n            elif window_count[old] == p_count[old] + 1:\n                matches += 1\n            window_count[old] -= 1\n        \n        if matches == required:\n            result.append(i - len(p) + 1)\n    \n    return result\n```\n\nTime: O(n), Space: O(k) where k is alphabet size."}]}
{"conversations": [{"from": "human", "value": "Implement word break (DP)"}, {"from": "gpt", "value": "```python\ndef word_break(s: str, word_dict: list) -> bool:\n    \"\"\"Can s be segmented into words from dictionary?\"\"\"\n    word_set = set(word_dict)\n    n = len(s)\n    dp = [False] * (n + 1)\n    dp[0] = True\n    \n    for i in range(1, n + 1):\n        for j in range(i):\n            if dp[j] and s[j:i] in word_set:\n                dp[i] = True\n                break\n    \n    return dp[n]\n\n# With memoization (often cleaner):\ndef word_break_memo(s: str, word_dict: list) -> bool:\n    word_set = set(word_dict)\n    memo = {}\n    \n    def can_break(start):\n        if start == len(s):\n            return True\n        if start in memo:\n            return memo[start]\n        \n        for end in range(start + 1, len(s) + 1):\n            if s[start:end] in word_set and can_break(end):\n                memo[start] = True\n                return True\n        \n        memo[start] = False\n        return False\n    \n    return can_break(0)\n\n# Return all possible sentences:\ndef word_break_all(s: str, word_dict: list) -> list:\n    word_set = set(word_dict)\n    memo = {}\n    \n    def backtrack(start):\n        if start in memo:\n            return memo[start]\n        if start == len(s):\n            return [[]]\n        \n        results = []\n        for end in range(start + 1, len(s) + 1):\n            word = s[start:end]\n            if word in word_set:\n                for rest in backtrack(end):\n                    results.append([word] + rest)\n        \n        memo[start] = results\n        return results\n    \n    return [' '.join(words) for words in backtrack(0)]\n\n# Example:\n# word_break_all(\"catsanddog\", [\"cat\", \"cats\", \"and\", \"sand\", \"dog\"])\n# Returns: [\"cats and dog\", \"cat sand dog\"]\n```"}]}
{"conversations": [{"from": "human", "value": "Implement edit distance (Levenshtein)"}, {"from": "gpt", "value": "```python\ndef edit_distance(word1: str, word2: str) -> int:\n    m, n = len(word1), len(word2)\n    \n    # dp[i][j] = edit distance between word1[:i] and word2[:j]\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    # Base cases\n    for i in range(m + 1):\n        dp[i][0] = i  # Delete all chars from word1\n    for j in range(n + 1):\n        dp[0][j] = j  # Insert all chars from word2\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if word1[i-1] == word2[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                dp[i][j] = 1 + min(\n                    dp[i-1][j],    # Delete from word1\n                    dp[i][j-1],    # Insert into word1\n                    dp[i-1][j-1]   # Replace\n                )\n    \n    return dp[m][n]\n\n# Space-optimized O(n):\ndef edit_distance_optimized(word1: str, word2: str) -> int:\n    if len(word1) < len(word2):\n        word1, word2 = word2, word1\n    \n    prev = list(range(len(word2) + 1))\n    curr = [0] * (len(word2) + 1)\n    \n    for i in range(1, len(word1) + 1):\n        curr[0] = i\n        for j in range(1, len(word2) + 1):\n            if word1[i-1] == word2[j-1]:\n                curr[j] = prev[j-1]\n            else:\n                curr[j] = 1 + min(prev[j], curr[j-1], prev[j-1])\n        prev, curr = curr, prev\n    \n    return prev[len(word2)]\n\n# With operation tracking:\ndef edit_distance_with_ops(word1: str, word2: str) -> tuple:\n    m, n = len(word1), len(word2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    ops = [[None] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(m + 1):\n        dp[i][0] = i\n        ops[i][0] = 'delete' if i > 0 else None\n    for j in range(n + 1):\n        dp[0][j] = j\n        ops[0][j] = 'insert' if j > 0 else None\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if word1[i-1] == word2[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n                ops[i][j] = 'match'\n            else:\n                choices = [\n                    (dp[i-1][j] + 1, 'delete'),\n                    (dp[i][j-1] + 1, 'insert'),\n                    (dp[i-1][j-1] + 1, 'replace')\n                ]\n                dp[i][j], ops[i][j] = min(choices)\n    \n    return dp[m][n], ops\n```\n\nApplications: spell checking, DNA sequence alignment, plagiarism detection, fuzzy matching."}]}
{"conversations": [{"from": "human", "value": "Find bridges in a graph"}, {"from": "gpt", "value": "```python\ndef find_bridges(n: int, edges: list) -> list:\n    \"\"\"\n    Find all bridges (edges whose removal disconnects the graph)\n    Uses Tarjan's algorithm\n    \"\"\"\n    from collections import defaultdict\n    \n    graph = defaultdict(list)\n    for u, v in edges:\n        graph[u].append(v)\n        graph[v].append(u)\n    \n    visited = [False] * n\n    disc = [0] * n  # Discovery time\n    low = [0] * n   # Lowest reachable vertex\n    parent = [-1] * n\n    bridges = []\n    timer = [0]\n    \n    def dfs(u):\n        visited[u] = True\n        disc[u] = low[u] = timer[0]\n        timer[0] += 1\n        \n        for v in graph[u]:\n            if not visited[v]:\n                parent[v] = u\n                dfs(v)\n                low[u] = min(low[u], low[v])\n                \n                # If lowest vertex reachable from v is below u,\n                # then u-v is a bridge\n                if low[v] > disc[u]:\n                    bridges.append((u, v))\n            elif v != parent[u]:\n                low[u] = min(low[u], disc[v])\n    \n    for i in range(n):\n        if not visited[i]:\n            dfs(i)\n    \n    return bridges\n\n# Also find articulation points (cut vertices):\ndef find_articulation_points(n: int, edges: list) -> list:\n    from collections import defaultdict\n    \n    graph = defaultdict(list)\n    for u, v in edges:\n        graph[u].append(v)\n        graph[v].append(u)\n    \n    visited = [False] * n\n    disc = [0] * n\n    low = [0] * n\n    parent = [-1] * n\n    ap = set()\n    timer = [0]\n    \n    def dfs(u):\n        children = 0\n        visited[u] = True\n        disc[u] = low[u] = timer[0]\n        timer[0] += 1\n        \n        for v in graph[u]:\n            if not visited[v]:\n                children += 1\n                parent[v] = u\n                dfs(v)\n                low[u] = min(low[u], low[v])\n                \n                # u is AP if:\n                # 1. u is root and has 2+ children, or\n                # 2. u is not root and low[v] >= disc[u]\n                if parent[u] == -1 and children > 1:\n                    ap.add(u)\n                if parent[u] != -1 and low[v] >= disc[u]:\n                    ap.add(u)\n            elif v != parent[u]:\n                low[u] = min(low[u], disc[v])\n    \n    for i in range(n):\n        if not visited[i]:\n            dfs(i)\n    \n    return list(ap)\n```\n\nTime: O(V + E). Use cases: network reliability, identifying critical connections."}]}
{"conversations": [{"from": "human", "value": "Implement union-find (disjoint set)"}, {"from": "gpt", "value": "```python\nclass UnionFind:\n    def __init__(self, n: int):\n        self.parent = list(range(n))\n        self.rank = [0] * n\n        self.count = n  # Number of connected components\n    \n    def find(self, x: int) -> int:\n        # Path compression\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])\n        return self.parent[x]\n    \n    def union(self, x: int, y: int) -> bool:\n        px, py = self.find(x), self.find(y)\n        if px == py:\n            return False  # Already connected\n        \n        # Union by rank\n        if self.rank[px] < self.rank[py]:\n            px, py = py, px\n        self.parent[py] = px\n        if self.rank[px] == self.rank[py]:\n            self.rank[px] += 1\n        \n        self.count -= 1\n        return True\n    \n    def connected(self, x: int, y: int) -> bool:\n        return self.find(x) == self.find(y)\n\n# Example: Number of connected components\ndef count_components(n: int, edges: list) -> int:\n    uf = UnionFind(n)\n    for u, v in edges:\n        uf.union(u, v)\n    return uf.count\n\n# Example: Detect cycle in undirected graph\ndef has_cycle(n: int, edges: list) -> bool:\n    uf = UnionFind(n)\n    for u, v in edges:\n        if uf.connected(u, v):\n            return True\n        uf.union(u, v)\n    return False\n\n# Example: Kruskal's MST\ndef kruskal_mst(n: int, edges: list) -> list:\n    # edges: [(weight, u, v), ...]\n    uf = UnionFind(n)\n    edges.sort()\n    mst = []\n    \n    for weight, u, v in edges:\n        if uf.union(u, v):\n            mst.append((u, v, weight))\n            if len(mst) == n - 1:\n                break\n    \n    return mst\n```\n\nWith path compression and union by rank:\n- Find: O(α(n)) amortized (effectively O(1))\n- Union: O(α(n)) amortized\n\nα(n) is the inverse Ackermann function, grows incredibly slowly."}]}
{"conversations": [{"from": "human", "value": "Implement a skip list"}, {"from": "gpt", "value": "```python\nimport random\n\nclass SkipListNode:\n    def __init__(self, val=-1, level=0):\n        self.val = val\n        self.forward = [None] * (level + 1)\n\nclass SkipList:\n    def __init__(self, max_level=16, p=0.5):\n        self.max_level = max_level\n        self.p = p\n        self.level = 0\n        self.header = SkipListNode(-1, max_level)\n    \n    def random_level(self) -> int:\n        level = 0\n        while random.random() < self.p and level < self.max_level:\n            level += 1\n        return level\n    \n    def search(self, target: int) -> bool:\n        current = self.header\n        for i in range(self.level, -1, -1):\n            while current.forward[i] and current.forward[i].val < target:\n                current = current.forward[i]\n        current = current.forward[0]\n        return current is not None and current.val == target\n    \n    def insert(self, num: int) -> None:\n        update = [None] * (self.max_level + 1)\n        current = self.header\n        \n        for i in range(self.level, -1, -1):\n            while current.forward[i] and current.forward[i].val < num:\n                current = current.forward[i]\n            update[i] = current\n        \n        level = self.random_level()\n        if level > self.level:\n            for i in range(self.level + 1, level + 1):\n                update[i] = self.header\n            self.level = level\n        \n        new_node = SkipListNode(num, level)\n        for i in range(level + 1):\n            new_node.forward[i] = update[i].forward[i]\n            update[i].forward[i] = new_node\n    \n    def erase(self, num: int) -> bool:\n        update = [None] * (self.max_level + 1)\n        current = self.header\n        \n        for i in range(self.level, -1, -1):\n            while current.forward[i] and current.forward[i].val < num:\n                current = current.forward[i]\n            update[i] = current\n        \n        current = current.forward[0]\n        if current is None or current.val != num:\n            return False\n        \n        for i in range(self.level + 1):\n            if update[i].forward[i] != current:\n                break\n            update[i].forward[i] = current.forward[i]\n        \n        while self.level > 0 and self.header.forward[self.level] is None:\n            self.level -= 1\n        \n        return True\n```\n\nExpected time complexity: O(log n) for search, insert, delete.\nUsed in Redis sorted sets, LevelDB, and other systems where balanced trees are too complex."}]}
{"conversations": [{"from": "human", "value": "Solve the house robber problem"}, {"from": "gpt", "value": "**Basic version (can't rob adjacent houses):**\n```python\ndef rob(nums: list) -> int:\n    if not nums:\n        return 0\n    if len(nums) <= 2:\n        return max(nums)\n    \n    prev2, prev1 = nums[0], max(nums[0], nums[1])\n    \n    for i in range(2, len(nums)):\n        current = max(prev1, prev2 + nums[i])\n        prev2, prev1 = prev1, current\n    \n    return prev1\n```\n\n**Circular houses (first and last are adjacent):**\n```python\ndef rob_circular(nums: list) -> int:\n    if len(nums) == 1:\n        return nums[0]\n    \n    def rob_range(start, end):\n        prev2 = prev1 = 0\n        for i in range(start, end):\n            current = max(prev1, prev2 + nums[i])\n            prev2, prev1 = prev1, current\n        return prev1\n    \n    # Either skip first house or skip last house\n    return max(rob_range(0, len(nums) - 1), rob_range(1, len(nums)))\n```\n\n**Binary tree houses:**\n```python\ndef rob_tree(root) -> int:\n    def dfs(node):\n        if not node:\n            return (0, 0)  # (rob_this, skip_this)\n        \n        left = dfs(node.left)\n        right = dfs(node.right)\n        \n        # If we rob this node, we can't rob children\n        rob_this = node.val + left[1] + right[1]\n        # If we skip this node, we take max of robbing or skipping each child\n        skip_this = max(left) + max(right)\n        \n        return (rob_this, skip_this)\n    \n    return max(dfs(root))\n```\n\nThe key insight: at each position, you either take it (and skip previous) or skip it (and keep best so far)."}]}
{"conversations": [{"from": "human", "value": "Implement a segment tree"}, {"from": "gpt", "value": "```python\nclass SegmentTree:\n    \"\"\"Range sum queries with point updates\"\"\"\n    \n    def __init__(self, nums: list):\n        self.n = len(nums)\n        self.tree = [0] * (2 * self.n)\n        \n        # Build tree\n        for i in range(self.n):\n            self.tree[self.n + i] = nums[i]\n        for i in range(self.n - 1, 0, -1):\n            self.tree[i] = self.tree[2 * i] + self.tree[2 * i + 1]\n    \n    def update(self, i: int, val: int):\n        i += self.n\n        self.tree[i] = val\n        while i > 1:\n            i //= 2\n            self.tree[i] = self.tree[2 * i] + self.tree[2 * i + 1]\n    \n    def query(self, left: int, right: int) -> int:\n        \"\"\"Sum of range [left, right)\"\"\"\n        result = 0\n        left += self.n\n        right += self.n\n        \n        while left < right:\n            if left % 2 == 1:\n                result += self.tree[left]\n                left += 1\n            if right % 2 == 1:\n                right -= 1\n                result += self.tree[right]\n            left //= 2\n            right //= 2\n        \n        return result\n\n# With lazy propagation for range updates:\nclass LazySegmentTree:\n    def __init__(self, n: int):\n        self.n = n\n        self.tree = [0] * (4 * n)\n        self.lazy = [0] * (4 * n)\n    \n    def _push_down(self, node, start, end):\n        if self.lazy[node] != 0:\n            mid = (start + end) // 2\n            self.tree[2*node] += self.lazy[node] * (mid - start + 1)\n            self.tree[2*node+1] += self.lazy[node] * (end - mid)\n            self.lazy[2*node] += self.lazy[node]\n            self.lazy[2*node+1] += self.lazy[node]\n            self.lazy[node] = 0\n    \n    def range_update(self, node, start, end, l, r, val):\n        if l > end or r < start:\n            return\n        if l <= start and end <= r:\n            self.tree[node] += val * (end - start + 1)\n            self.lazy[node] += val\n            return\n        \n        self._push_down(node, start, end)\n        mid = (start + end) // 2\n        self.range_update(2*node, start, mid, l, r, val)\n        self.range_update(2*node+1, mid+1, end, l, r, val)\n        self.tree[node] = self.tree[2*node] + self.tree[2*node+1]\n    \n    def range_query(self, node, start, end, l, r):\n        if l > end or r < start:\n            return 0\n        if l <= start and end <= r:\n            return self.tree[node]\n        \n        self._push_down(node, start, end)\n        mid = (start + end) // 2\n        return (self.range_query(2*node, start, mid, l, r) +\n                self.range_query(2*node+1, mid+1, end, l, r))\n```\n\nTime: O(log n) for both update and query. Space: O(n)."}]}
{"conversations": [{"from": "human", "value": "Find the longest common subsequence"}, {"from": "gpt", "value": "```python\ndef longest_common_subsequence(text1: str, text2: str) -> int:\n    m, n = len(text1), len(text2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if text1[i-1] == text2[j-1]:\n                dp[i][j] = dp[i-1][j-1] + 1\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n    \n    return dp[m][n]\n\n# Space-optimized O(n):\ndef lcs_optimized(text1: str, text2: str) -> int:\n    if len(text1) < len(text2):\n        text1, text2 = text2, text1\n    \n    prev = [0] * (len(text2) + 1)\n    curr = [0] * (len(text2) + 1)\n    \n    for i in range(1, len(text1) + 1):\n        for j in range(1, len(text2) + 1):\n            if text1[i-1] == text2[j-1]:\n                curr[j] = prev[j-1] + 1\n            else:\n                curr[j] = max(prev[j], curr[j-1])\n        prev, curr = curr, prev\n    \n    return prev[len(text2)]\n\n# Get the actual subsequence:\ndef get_lcs(text1: str, text2: str) -> str:\n    m, n = len(text1), len(text2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if text1[i-1] == text2[j-1]:\n                dp[i][j] = dp[i-1][j-1] + 1\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n    \n    # Backtrack\n    result = []\n    i, j = m, n\n    while i > 0 and j > 0:\n        if text1[i-1] == text2[j-1]:\n            result.append(text1[i-1])\n            i -= 1\n            j -= 1\n        elif dp[i-1][j] > dp[i][j-1]:\n            i -= 1\n        else:\n            j -= 1\n    \n    return ''.join(reversed(result))\n```\n\nTime: O(m*n), Space: O(m*n) or O(n) optimized.\n\nRelated: Longest Common Substring (different - must be contiguous)."}]}
{"conversations": [{"from": "human", "value": "Implement a basic regex matcher"}, {"from": "gpt", "value": "```python\ndef is_match(s: str, p: str) -> bool:\n    \"\"\"\n    Supports . (any single char) and * (zero or more of preceding)\n    \"\"\"\n    m, n = len(s), len(p)\n    dp = [[False] * (n + 1) for _ in range(m + 1)]\n    dp[0][0] = True\n    \n    # Handle patterns like a*, a*b*, a*b*c*\n    for j in range(2, n + 1):\n        if p[j-1] == '*':\n            dp[0][j] = dp[0][j-2]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if p[j-1] == '*':\n                # Zero occurrences of preceding char\n                dp[i][j] = dp[i][j-2]\n                # One or more occurrences (if preceding matches)\n                if p[j-2] == '.' or p[j-2] == s[i-1]:\n                    dp[i][j] = dp[i][j] or dp[i-1][j]\n            elif p[j-1] == '.' or p[j-1] == s[i-1]:\n                dp[i][j] = dp[i-1][j-1]\n    \n    return dp[m][n]\n\n# Recursive with memoization (more intuitive):\ndef is_match_recursive(s: str, p: str) -> bool:\n    memo = {}\n    \n    def dp(i, j):\n        if (i, j) in memo:\n            return memo[(i, j)]\n        \n        if j == len(p):\n            return i == len(s)\n        \n        first_match = i < len(s) and (p[j] == s[i] or p[j] == '.')\n        \n        if j + 1 < len(p) and p[j + 1] == '*':\n            # Skip x* OR match current and continue with *\n            result = dp(i, j + 2) or (first_match and dp(i + 1, j))\n        else:\n            result = first_match and dp(i + 1, j + 1)\n        \n        memo[(i, j)] = result\n        return result\n    \n    return dp(0, 0)\n\n# Simple wildcard matching (? and *):\ndef wildcard_match(s: str, p: str) -> bool:\n    m, n = len(s), len(p)\n    dp = [[False] * (n + 1) for _ in range(m + 1)]\n    dp[0][0] = True\n    \n    for j in range(1, n + 1):\n        if p[j-1] == '*':\n            dp[0][j] = dp[0][j-1]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if p[j-1] == '*':\n                dp[i][j] = dp[i][j-1] or dp[i-1][j]\n            elif p[j-1] == '?' or p[j-1] == s[i-1]:\n                dp[i][j] = dp[i-1][j-1]\n    \n    return dp[m][n]\n```"}]}
