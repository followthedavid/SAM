{"conversations": [{"from": "human", "value": "Write a rate limiter in Python"}, {"from": "gpt", "value": "```python\nimport time\nfrom collections import deque\nfrom threading import Lock\n\nclass RateLimiter:\n    def __init__(self, max_requests: int, window_seconds: float):\n        self.max_requests = max_requests\n        self.window = window_seconds\n        self.requests = deque()\n        self.lock = Lock()\n    \n    def acquire(self) -> bool:\n        with self.lock:\n            now = time.time()\n            # Remove expired timestamps\n            while self.requests and self.requests[0] < now - self.window:\n                self.requests.popleft()\n            \n            if len(self.requests) < self.max_requests:\n                self.requests.append(now)\n                return True\n            return False\n    \n    def wait(self):\n        while not self.acquire():\n            time.sleep(0.1)\n\n# Usage\nlimiter = RateLimiter(max_requests=10, window_seconds=1.0)\n\nfor i in range(20):\n    limiter.wait()\n    print(f\"Request {i} at {time.time():.2f}\")\n```\n\nThis implements a sliding window rate limiter. Thread-safe, efficient O(1) amortized operations."}]}
{"conversations": [{"from": "human", "value": "How do I implement a LRU cache?"}, {"from": "gpt", "value": "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = OrderedDict()\n    \n    def get(self, key):\n        if key not in self.cache:\n            return None\n        self.cache.move_to_end(key)\n        return self.cache[key]\n    \n    def put(self, key, value):\n        if key in self.cache:\n            self.cache.move_to_end(key)\n        self.cache[key] = value\n        if len(self.cache) > self.capacity:\n            self.cache.popitem(first=True)\n\n# Or use functools for simple cases\nfrom functools import lru_cache\n\n@lru_cache(maxsize=128)\ndef expensive_function(n):\n    return sum(i**2 for i in range(n))\n```\n\nOrderedDict gives you O(1) operations. The stdlib `lru_cache` decorator handles most cases."}]}
{"conversations": [{"from": "human", "value": "Implement a pub/sub system"}, {"from": "gpt", "value": "```python\nfrom collections import defaultdict\nfrom typing import Callable, Any\nimport asyncio\n\nclass PubSub:\n    def __init__(self):\n        self.subscribers = defaultdict(list)\n    \n    def subscribe(self, topic: str, callback: Callable):\n        self.subscribers[topic].append(callback)\n        return lambda: self.subscribers[topic].remove(callback)\n    \n    def publish(self, topic: str, data: Any):\n        for callback in self.subscribers[topic]:\n            callback(data)\n\n# Async version\nclass AsyncPubSub:\n    def __init__(self):\n        self.subscribers = defaultdict(list)\n    \n    def subscribe(self, topic: str) -> asyncio.Queue:\n        queue = asyncio.Queue()\n        self.subscribers[topic].append(queue)\n        return queue\n    \n    async def publish(self, topic: str, data: Any):\n        for queue in self.subscribers[topic]:\n            await queue.put(data)\n\n# Usage\npubsub = PubSub()\nunsubscribe = pubsub.subscribe('users', lambda d: print(f'New user: {d}'))\npubsub.publish('users', {'name': 'Alice'})\nunsubscribe()  # Clean up\n```\n\nSimple but effective. Add persistence, filtering, or wildcards as needed."}]}
{"conversations": [{"from": "human", "value": "Write a connection pool"}, {"from": "gpt", "value": "```python\nimport queue\nimport threading\nfrom contextlib import contextmanager\nfrom typing import Callable, TypeVar\n\nT = TypeVar('T')\n\nclass ConnectionPool:\n    def __init__(self, create_conn: Callable[[], T], max_size: int = 10):\n        self.create_conn = create_conn\n        self.max_size = max_size\n        self.pool = queue.Queue(max_size)\n        self.size = 0\n        self.lock = threading.Lock()\n    \n    def get(self, timeout: float = None) -> T:\n        try:\n            return self.pool.get_nowait()\n        except queue.Empty:\n            with self.lock:\n                if self.size < self.max_size:\n                    self.size += 1\n                    return self.create_conn()\n            return self.pool.get(timeout=timeout)\n    \n    def put(self, conn: T):\n        try:\n            self.pool.put_nowait(conn)\n        except queue.Full:\n            pass  # Discard if pool is full\n    \n    @contextmanager\n    def connection(self):\n        conn = self.get()\n        try:\n            yield conn\n        finally:\n            self.put(conn)\n\n# Usage\nimport sqlite3\npool = ConnectionPool(lambda: sqlite3.connect('app.db'), max_size=5)\n\nwith pool.connection() as conn:\n    cursor = conn.execute('SELECT * FROM users')\n```\n\nThread-safe, lazy initialization, automatic return to pool."}]}
{"conversations": [{"from": "human", "value": "Build a simple task queue"}, {"from": "gpt", "value": "```python\nimport queue\nimport threading\nfrom typing import Callable, Any\nimport time\n\nclass TaskQueue:\n    def __init__(self, num_workers: int = 4):\n        self.queue = queue.Queue()\n        self.workers = []\n        self.running = True\n        \n        for _ in range(num_workers):\n            t = threading.Thread(target=self._worker, daemon=True)\n            t.start()\n            self.workers.append(t)\n    \n    def _worker(self):\n        while self.running:\n            try:\n                func, args, kwargs, result_queue = self.queue.get(timeout=1)\n                try:\n                    result = func(*args, **kwargs)\n                    if result_queue:\n                        result_queue.put(('success', result))\n                except Exception as e:\n                    if result_queue:\n                        result_queue.put(('error', e))\n                finally:\n                    self.queue.task_done()\n            except queue.Empty:\n                continue\n    \n    def submit(self, func: Callable, *args, **kwargs) -> queue.Queue:\n        result_queue = queue.Queue()\n        self.queue.put((func, args, kwargs, result_queue))\n        return result_queue\n    \n    def submit_nowait(self, func: Callable, *args, **kwargs):\n        self.queue.put((func, args, kwargs, None))\n    \n    def wait(self):\n        self.queue.join()\n    \n    def shutdown(self):\n        self.running = False\n        for w in self.workers:\n            w.join()\n\n# Usage\ntq = TaskQueue(num_workers=4)\n\ndef process(x):\n    time.sleep(0.1)\n    return x * 2\n\nresults = [tq.submit(process, i) for i in range(10)]\nfor r in results:\n    status, value = r.get()\n    print(value)\n```"}]}
{"conversations": [{"from": "human", "value": "Implement retry with exponential backoff"}, {"from": "gpt", "value": "```python\nimport time\nimport random\nfrom functools import wraps\nfrom typing import Tuple, Type\n\ndef retry(\n    max_attempts: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    jitter: bool = True,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,)\n):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            \n            for attempt in range(max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n                    \n                    if attempt == max_attempts - 1:\n                        raise\n                    \n                    delay = min(base_delay * (exponential_base ** attempt), max_delay)\n                    if jitter:\n                        delay *= (0.5 + random.random())\n                    \n                    print(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s\")\n                    time.sleep(delay)\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n# Usage\n@retry(max_attempts=5, exceptions=(ConnectionError, TimeoutError))\ndef fetch_data(url):\n    # ... make request\n    pass\n\n# Async version\nimport asyncio\n\ndef async_retry(**kwargs):\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kw):\n            # Same logic but with await asyncio.sleep(delay)\n            pass\n        return wrapper\n    return decorator\n```"}]}
{"conversations": [{"from": "human", "value": "Create a simple event loop"}, {"from": "gpt", "value": "```python\nimport heapq\nimport time\nfrom typing import Callable, Any\nfrom dataclasses import dataclass, field\n\n@dataclass(order=True)\nclass Event:\n    time: float\n    callback: Callable = field(compare=False)\n    args: tuple = field(default=(), compare=False)\n\nclass EventLoop:\n    def __init__(self):\n        self.events = []\n        self.running = False\n    \n    def call_later(self, delay: float, callback: Callable, *args):\n        event = Event(time.time() + delay, callback, args)\n        heapq.heappush(self.events, event)\n        return event\n    \n    def call_at(self, when: float, callback: Callable, *args):\n        event = Event(when, callback, args)\n        heapq.heappush(self.events, event)\n        return event\n    \n    def call_soon(self, callback: Callable, *args):\n        return self.call_later(0, callback, *args)\n    \n    def run_forever(self):\n        self.running = True\n        while self.running and self.events:\n            event = heapq.heappop(self.events)\n            wait_time = event.time - time.time()\n            if wait_time > 0:\n                time.sleep(wait_time)\n            event.callback(*event.args)\n    \n    def stop(self):\n        self.running = False\n\n# Usage\nloop = EventLoop()\n\ndef greet(name):\n    print(f\"Hello, {name}! Time: {time.time():.2f}\")\n\nloop.call_later(1, greet, \"Alice\")\nloop.call_later(2, greet, \"Bob\")\nloop.call_later(3, loop.stop)\n\nloop.run_forever()\n```\n\nSimplified version of what asyncio does under the hood."}]}
{"conversations": [{"from": "human", "value": "Implement a state machine"}, {"from": "gpt", "value": "```python\nfrom enum import Enum, auto\nfrom typing import Dict, Set, Callable, Optional\n\nclass StateMachine:\n    def __init__(self, initial_state):\n        self.state = initial_state\n        self.transitions: Dict[tuple, tuple] = {}\n        self.on_enter: Dict[any, Callable] = {}\n        self.on_exit: Dict[any, Callable] = {}\n    \n    def add_transition(self, from_state, event, to_state, action: Callable = None):\n        self.transitions[(from_state, event)] = (to_state, action)\n    \n    def on_enter_state(self, state, callback: Callable):\n        self.on_enter[state] = callback\n    \n    def on_exit_state(self, state, callback: Callable):\n        self.on_exit[state] = callback\n    \n    def trigger(self, event) -> bool:\n        key = (self.state, event)\n        if key not in self.transitions:\n            return False\n        \n        new_state, action = self.transitions[key]\n        \n        if self.state in self.on_exit:\n            self.on_exit[self.state]()\n        \n        old_state = self.state\n        self.state = new_state\n        \n        if action:\n            action(old_state, event, new_state)\n        \n        if new_state in self.on_enter:\n            self.on_enter[new_state]()\n        \n        return True\n\n# Example: Order state machine\nclass OrderState(Enum):\n    PENDING = auto()\n    PAID = auto()\n    SHIPPED = auto()\n    DELIVERED = auto()\n    CANCELLED = auto()\n\nsm = StateMachine(OrderState.PENDING)\nsm.add_transition(OrderState.PENDING, 'pay', OrderState.PAID)\nsm.add_transition(OrderState.PAID, 'ship', OrderState.SHIPPED)\nsm.add_transition(OrderState.SHIPPED, 'deliver', OrderState.DELIVERED)\nsm.add_transition(OrderState.PENDING, 'cancel', OrderState.CANCELLED)\n\nsm.trigger('pay')    # PENDING -> PAID\nsm.trigger('ship')   # PAID -> SHIPPED\n```"}]}
{"conversations": [{"from": "human", "value": "Write a dependency injection container"}, {"from": "gpt", "value": "```python\nfrom typing import Type, TypeVar, Dict, Callable, Any\nimport inspect\n\nT = TypeVar('T')\n\nclass Container:\n    def __init__(self):\n        self._bindings: Dict[Type, Callable] = {}\n        self._singletons: Dict[Type, Any] = {}\n        self._singleton_types: set = set()\n    \n    def bind(self, abstract: Type[T], concrete: Callable[..., T]):\n        self._bindings[abstract] = concrete\n    \n    def singleton(self, abstract: Type[T], concrete: Callable[..., T]):\n        self._bindings[abstract] = concrete\n        self._singleton_types.add(abstract)\n    \n    def resolve(self, abstract: Type[T]) -> T:\n        if abstract in self._singletons:\n            return self._singletons[abstract]\n        \n        if abstract in self._bindings:\n            concrete = self._bindings[abstract]\n        else:\n            concrete = abstract\n        \n        instance = self._build(concrete)\n        \n        if abstract in self._singleton_types:\n            self._singletons[abstract] = instance\n        \n        return instance\n    \n    def _build(self, concrete: Callable[..., T]) -> T:\n        sig = inspect.signature(concrete)\n        deps = {}\n        \n        for name, param in sig.parameters.items():\n            if param.annotation != inspect.Parameter.empty:\n                deps[name] = self.resolve(param.annotation)\n        \n        return concrete(**deps)\n\n# Usage\nclass Database:\n    def query(self, sql): pass\n\nclass UserRepository:\n    def __init__(self, db: Database):\n        self.db = db\n\nclass UserService:\n    def __init__(self, repo: UserRepository):\n        self.repo = repo\n\ncontainer = Container()\ncontainer.singleton(Database, Database)\ncontainer.bind(UserRepository, UserRepository)\ncontainer.bind(UserService, UserService)\n\nservice = container.resolve(UserService)  # Auto-wires all dependencies\n```"}]}
{"conversations": [{"from": "human", "value": "Implement a circuit breaker"}, {"from": "gpt", "value": "```python\nimport time\nfrom enum import Enum\nfrom functools import wraps\nfrom threading import Lock\n\nclass CircuitState(Enum):\n    CLOSED = 'closed'      # Normal operation\n    OPEN = 'open'          # Failing, reject requests\n    HALF_OPEN = 'half_open'  # Testing if service recovered\n\nclass CircuitBreaker:\n    def __init__(\n        self,\n        failure_threshold: int = 5,\n        recovery_timeout: float = 30.0,\n        expected_exceptions: tuple = (Exception,)\n    ):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.expected_exceptions = expected_exceptions\n        \n        self.state = CircuitState.CLOSED\n        self.failures = 0\n        self.last_failure_time = None\n        self.lock = Lock()\n    \n    def __call__(self, func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            return self.call(func, *args, **kwargs)\n        return wrapper\n    \n    def call(self, func, *args, **kwargs):\n        with self.lock:\n            if self.state == CircuitState.OPEN:\n                if time.time() - self.last_failure_time >= self.recovery_timeout:\n                    self.state = CircuitState.HALF_OPEN\n                else:\n                    raise CircuitOpenError(\"Circuit breaker is open\")\n        \n        try:\n            result = func(*args, **kwargs)\n            self._on_success()\n            return result\n        except self.expected_exceptions as e:\n            self._on_failure()\n            raise\n    \n    def _on_success(self):\n        with self.lock:\n            self.failures = 0\n            self.state = CircuitState.CLOSED\n    \n    def _on_failure(self):\n        with self.lock:\n            self.failures += 1\n            self.last_failure_time = time.time()\n            if self.failures >= self.failure_threshold:\n                self.state = CircuitState.OPEN\n\nclass CircuitOpenError(Exception):\n    pass\n\n# Usage\n@CircuitBreaker(failure_threshold=3, recovery_timeout=60)\ndef call_external_api():\n    # ... make request\n    pass\n```"}]}
{"conversations": [{"from": "human", "value": "Write a simple ORM"}, {"from": "gpt", "value": "```python\nimport sqlite3\nfrom typing import List, Optional, Type, TypeVar\nfrom dataclasses import dataclass, fields\n\nT = TypeVar('T')\n\nclass Database:\n    def __init__(self, path: str):\n        self.conn = sqlite3.connect(path)\n        self.conn.row_factory = sqlite3.Row\n    \n    def execute(self, sql: str, params: tuple = ()):\n        return self.conn.execute(sql, params)\n    \n    def commit(self):\n        self.conn.commit()\n\nclass Model:\n    _db: Database = None\n    _table: str = None\n    id: int = None\n    \n    @classmethod\n    def set_db(cls, db: Database):\n        cls._db = db\n    \n    @classmethod\n    def table_name(cls) -> str:\n        return cls._table or cls.__name__.lower() + 's'\n    \n    @classmethod\n    def create_table(cls):\n        cols = []\n        for f in fields(cls):\n            if f.name == 'id':\n                cols.append('id INTEGER PRIMARY KEY AUTOINCREMENT')\n            elif f.type == str:\n                cols.append(f'{f.name} TEXT')\n            elif f.type == int:\n                cols.append(f'{f.name} INTEGER')\n            elif f.type == float:\n                cols.append(f'{f.name} REAL')\n        \n        sql = f\"CREATE TABLE IF NOT EXISTS {cls.table_name()} ({', '.join(cols)})\"\n        cls._db.execute(sql)\n        cls._db.commit()\n    \n    def save(self):\n        fs = [f for f in fields(self) if f.name != 'id']\n        names = [f.name for f in fs]\n        values = [getattr(self, f.name) for f in fs]\n        placeholders = ','.join(['?' for _ in fs])\n        \n        if self.id is None:\n            sql = f\"INSERT INTO {self.table_name()} ({','.join(names)}) VALUES ({placeholders})\"\n            cursor = self._db.execute(sql, tuple(values))\n            self.id = cursor.lastrowid\n        else:\n            sets = ','.join([f\"{n}=?\" for n in names])\n            sql = f\"UPDATE {self.table_name()} SET {sets} WHERE id=?\"\n            self._db.execute(sql, tuple(values) + (self.id,))\n        \n        self._db.commit()\n    \n    @classmethod\n    def find(cls: Type[T], id: int) -> Optional[T]:\n        sql = f\"SELECT * FROM {cls.table_name()} WHERE id=?\"\n        row = cls._db.execute(sql, (id,)).fetchone()\n        return cls(**dict(row)) if row else None\n    \n    @classmethod\n    def all(cls: Type[T]) -> List[T]:\n        sql = f\"SELECT * FROM {cls.table_name()}\"\n        return [cls(**dict(row)) for row in cls._db.execute(sql)]\n\n# Usage\n@dataclass\nclass User(Model):\n    id: int = None\n    name: str = ''\n    email: str = ''\n\ndb = Database(':memory:')\nModel.set_db(db)\nUser.create_table()\n\nuser = User(name='Alice', email='alice@example.com')\nuser.save()\nprint(User.find(1))\n```"}]}
{"conversations": [{"from": "human", "value": "Implement async semaphore with timeout"}, {"from": "gpt", "value": "```python\nimport asyncio\nfrom typing import Optional\n\nclass AsyncSemaphore:\n    def __init__(self, value: int = 1):\n        self._value = value\n        self._waiters = []\n        self._lock = asyncio.Lock()\n    \n    async def acquire(self, timeout: Optional[float] = None) -> bool:\n        async with self._lock:\n            if self._value > 0:\n                self._value -= 1\n                return True\n        \n        event = asyncio.Event()\n        self._waiters.append(event)\n        \n        try:\n            await asyncio.wait_for(event.wait(), timeout=timeout)\n            return True\n        except asyncio.TimeoutError:\n            self._waiters.remove(event)\n            return False\n    \n    def release(self):\n        self._value += 1\n        if self._waiters:\n            event = self._waiters.pop(0)\n            event.set()\n            self._value -= 1\n    \n    async def __aenter__(self):\n        await self.acquire()\n        return self\n    \n    async def __aexit__(self, *args):\n        self.release()\n\n# Usage\nsem = AsyncSemaphore(3)  # Allow 3 concurrent operations\n\nasync def limited_task(n):\n    acquired = await sem.acquire(timeout=5.0)\n    if not acquired:\n        print(f\"Task {n} timed out waiting for semaphore\")\n        return\n    \n    try:\n        print(f\"Task {n} running\")\n        await asyncio.sleep(1)\n    finally:\n        sem.release()\n\nasync def main():\n    await asyncio.gather(*[limited_task(i) for i in range(10)])\n\nasyncio.run(main())\n```"}]}
{"conversations": [{"from": "human", "value": "Build a plugin system"}, {"from": "gpt", "value": "```python\nimport importlib\nimport importlib.util\nfrom pathlib import Path\nfrom typing import Dict, List, Type, Protocol\nfrom abc import ABC, abstractmethod\n\nclass Plugin(ABC):\n    name: str = \"unnamed\"\n    version: str = \"0.0.0\"\n    \n    @abstractmethod\n    def activate(self, app):\n        pass\n    \n    @abstractmethod\n    def deactivate(self):\n        pass\n\nclass PluginManager:\n    def __init__(self):\n        self.plugins: Dict[str, Plugin] = {}\n        self.loaded_modules = {}\n    \n    def discover(self, plugin_dir: str) -> List[str]:\n        discovered = []\n        path = Path(plugin_dir)\n        \n        for item in path.glob('*.py'):\n            if item.stem.startswith('_'):\n                continue\n            discovered.append(str(item))\n        \n        return discovered\n    \n    def load(self, plugin_path: str) -> Plugin:\n        path = Path(plugin_path)\n        spec = importlib.util.spec_from_file_location(path.stem, path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        \n        # Find Plugin subclass in module\n        for attr_name in dir(module):\n            attr = getattr(module, attr_name)\n            if (isinstance(attr, type) and \n                issubclass(attr, Plugin) and \n                attr is not Plugin):\n                plugin = attr()\n                self.plugins[plugin.name] = plugin\n                self.loaded_modules[plugin.name] = module\n                return plugin\n        \n        raise ValueError(f\"No Plugin class found in {plugin_path}\")\n    \n    def activate(self, name: str, app):\n        if name in self.plugins:\n            self.plugins[name].activate(app)\n    \n    def deactivate(self, name: str):\n        if name in self.plugins:\n            self.plugins[name].deactivate()\n    \n    def reload(self, name: str):\n        if name in self.loaded_modules:\n            module = self.loaded_modules[name]\n            plugin = self.plugins[name]\n            plugin.deactivate()\n            importlib.reload(module)\n\n# Example plugin file: plugins/hello_plugin.py\n'''\nfrom plugin_system import Plugin\n\nclass HelloPlugin(Plugin):\n    name = \"hello\"\n    version = \"1.0.0\"\n    \n    def activate(self, app):\n        print(\"Hello plugin activated!\")\n        app.register_command(\"hello\", self.hello)\n    \n    def deactivate(self):\n        print(\"Hello plugin deactivated\")\n    \n    def hello(self):\n        return \"Hello from plugin!\"\n'''\n```"}]}
{"conversations": [{"from": "human", "value": "Create a streaming JSON parser"}, {"from": "gpt", "value": "```python\nimport json\nfrom typing import Iterator, Any\n\nclass StreamingJSONParser:\n    \"\"\"Parse JSON objects from a stream, one at a time.\"\"\"\n    \n    def __init__(self):\n        self.buffer = \"\"\n        self.decoder = json.JSONDecoder()\n    \n    def feed(self, chunk: str) -> Iterator[Any]:\n        self.buffer += chunk\n        \n        while self.buffer:\n            self.buffer = self.buffer.lstrip()\n            if not self.buffer:\n                break\n            \n            try:\n                obj, idx = self.decoder.raw_decode(self.buffer)\n                self.buffer = self.buffer[idx:]\n                yield obj\n            except json.JSONDecodeError:\n                # Incomplete JSON, need more data\n                break\n\nclass JSONLinesParser:\n    \"\"\"Parse newline-delimited JSON.\"\"\"\n    \n    def __init__(self):\n        self.buffer = \"\"\n    \n    def feed(self, chunk: str) -> Iterator[Any]:\n        self.buffer += chunk\n        \n        while '\\n' in self.buffer:\n            line, self.buffer = self.buffer.split('\\n', 1)\n            line = line.strip()\n            if line:\n                yield json.loads(line)\n\n# Usage with streaming response\nimport requests\n\ndef stream_json_api(url: str) -> Iterator[dict]:\n    parser = JSONLinesParser()\n    \n    with requests.get(url, stream=True) as r:\n        for chunk in r.iter_content(chunk_size=1024, decode_unicode=True):\n            for obj in parser.feed(chunk):\n                yield obj\n\n# Example with Server-Sent Events\ndef parse_sse(response) -> Iterator[dict]:\n    for line in response.iter_lines(decode_unicode=True):\n        if line.startswith('data: '):\n            data = line[6:]\n            if data != '[DONE]':\n                yield json.loads(data)\n```"}]}
{"conversations": [{"from": "human", "value": "Implement a middleware chain"}, {"from": "gpt", "value": "```python\nfrom typing import Callable, Any, List\nfrom functools import reduce\n\nclass Request:\n    def __init__(self, data: dict):\n        self.data = data\n        self.context = {}\n\nclass Response:\n    def __init__(self, body: Any = None, status: int = 200):\n        self.body = body\n        self.status = status\n\nMiddleware = Callable[[Request, Callable], Response]\n\nclass MiddlewareChain:\n    def __init__(self):\n        self.middlewares: List[Middleware] = []\n    \n    def use(self, middleware: Middleware):\n        self.middlewares.append(middleware)\n        return self\n    \n    def execute(self, request: Request, handler: Callable[[Request], Response]) -> Response:\n        def create_next(middleware: Middleware, next_fn: Callable) -> Callable:\n            def wrapped(req: Request) -> Response:\n                return middleware(req, next_fn)\n            return wrapped\n        \n        chain = reduce(\n            lambda next_fn, mw: create_next(mw, next_fn),\n            reversed(self.middlewares),\n            handler\n        )\n        \n        return chain(request)\n\n# Example middlewares\ndef logging_middleware(request: Request, next_fn: Callable) -> Response:\n    print(f\"Request: {request.data}\")\n    response = next_fn(request)\n    print(f\"Response: {response.status}\")\n    return response\n\ndef auth_middleware(request: Request, next_fn: Callable) -> Response:\n    token = request.data.get('token')\n    if not token:\n        return Response({'error': 'Unauthorized'}, status=401)\n    request.context['user'] = 'authenticated_user'\n    return next_fn(request)\n\ndef timing_middleware(request: Request, next_fn: Callable) -> Response:\n    import time\n    start = time.time()\n    response = next_fn(request)\n    print(f\"Request took {time.time() - start:.3f}s\")\n    return response\n\n# Usage\nchain = MiddlewareChain()\nchain.use(logging_middleware)\nchain.use(timing_middleware)\nchain.use(auth_middleware)\n\ndef handler(request: Request) -> Response:\n    return Response({'message': f\"Hello {request.context.get('user')}\"}, 200)\n\nrequest = Request({'token': 'secret', 'action': 'greet'})\nresponse = chain.execute(request, handler)\n```"}]}
