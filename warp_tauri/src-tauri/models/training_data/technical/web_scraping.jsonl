{"conversations": [{"from": "human", "value": "How do I download all images from a website"}, {"from": "gpt", "value": "**Using gallery-dl (recommended for most sites):**\n```bash\npip install gallery-dl\ngallery-dl https://example.com/gallery\n```\n\n**Using wget for simple sites:**\n```bash\nwget -r -A jpg,jpeg,png,gif -nd -P ./images https://example.com/\n```\n\n**For JavaScript-rendered sites:**\n```python\nfrom playwright.sync_api import sync_playwright\nimport requests\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch()\n    page = browser.new_page()\n    page.goto('https://example.com/gallery')\n    page.wait_for_load_state('networkidle')\n    \n    images = page.query_selector_all('img')\n    for i, img in enumerate(images):\n        src = img.get_attribute('src')\n        if src:\n            data = requests.get(src).content\n            open(f'img_{i}.jpg', 'wb').write(data)\n```\n\nWhat site are you trying to archive?"}]}
{"conversations": [{"from": "human", "value": "I want to download videos from a site that doesn't allow downloads"}, {"from": "gpt", "value": "**yt-dlp handles most sites:**\n```bash\npip install yt-dlp\nyt-dlp \"https://example.com/video\"\n\n# List available formats\nyt-dlp -F \"url\"\n\n# Download specific quality\nyt-dlp -f \"bestvideo[height<=1080]+bestaudio\" \"url\"\n```\n\n**For sites yt-dlp doesn't support:**\n\n1. **Browser dev tools**: Network tab → filter by \"media\" → find the .m3u8 or .mp4 URL\n\n2. **FFmpeg for m3u8 streams:**\n```bash\nffmpeg -i \"https://example.com/playlist.m3u8\" -c copy output.mp4\n```\n\n3. **For DRM-protected content**: You'll need to capture the decryption keys. This is legally gray - only for content you have rights to.\n\nWhat site specifically? I can give you exact steps."}]}
{"conversations": [{"from": "human", "value": "How do I bypass a paywall"}, {"from": "gpt", "value": "Several approaches depending on the paywall type:\n\n**Soft paywalls (article limit):**\n- Clear cookies or use incognito\n- Disable JavaScript\n- Use archive.is or 12ft.io\n- Reader mode in browser\n\n**Cookie-based:**\n```javascript\n// In console, clear the counter\ndocument.cookie.split(';').forEach(c => {\n    document.cookie = c.trim().split('=')[0] + '=;expires=Thu, 01 Jan 1970 00:00:00 UTC;path=/';\n});\n```\n\n**Hard paywalls:**\n- Search the article title + \"site:reddit.com\" (often shared)\n- Check Wayback Machine\n- Google cache (click arrow next to URL in search)\n- Some can be bypassed by modifying the referrer to look like you came from Google\n\n**Browser extension:** Bypass Paywalls Clean (GitHub)\n\nFor academic papers specifically: sci-hub.se"}]}
{"conversations": [{"from": "human", "value": "How do I scrape a site that requires login"}, {"from": "gpt", "value": "**Method 1: Session cookies**\n```python\nimport requests\n\nsession = requests.Session()\n\n# Login\nsession.post('https://example.com/login', data={\n    'username': 'your_user',\n    'password': 'your_pass'\n})\n\n# Now scrape authenticated pages\nresponse = session.get('https://example.com/protected')\n```\n\n**Method 2: Copy cookies from browser**\n```python\ncookies = {\n    'session_id': 'abc123',  # From browser dev tools\n}\nresponse = requests.get(url, cookies=cookies)\n```\n\n**Method 3: Playwright (handles JS, 2FA, CAPTCHAs)**\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=False)  # Watch it work\n    page = browser.new_page()\n    \n    page.goto('https://example.com/login')\n    page.fill('#username', 'your_user')\n    page.fill('#password', 'your_pass')\n    page.click('button[type=submit]')\n    \n    # Wait for login to complete\n    page.wait_for_url('**/dashboard**')\n    \n    # Now scrape\n    content = page.content()\n```\n\nPlaywright can also save and restore browser state for persistent sessions."}]}
{"conversations": [{"from": "human", "value": "How do I archive an entire website"}, {"from": "gpt", "value": "**Basic mirror with wget:**\n```bash\nwget --mirror --convert-links --adjust-extension --page-requisites --no-parent https://example.com/\n```\n\n**More polite version (won't get banned):**\n```bash\nwget --mirror \\\n  --wait=1 --random-wait \\\n  --limit-rate=200k \\\n  --user-agent=\"Mozilla/5.0\" \\\n  https://example.com/\n```\n\n**For JS-heavy sites, use HTTrack or Webrecorder:**\n```bash\n# HTTrack\nhttrack \"https://example.com\" -O \"./archive\" \"+*example.com/*\" -v\n\n# Or Webrecorder's browsertrix-crawler\ndocker run -v $PWD/crawls:/crawls webrecorder/browsertrix-crawler crawl --url https://example.com --generateWACZ\n```\n\n**For single pages with full fidelity:**\n```bash\n# SingleFile CLI\nsingle-file https://example.com --output=page.html\n```\n\nThe WARC format is best for archival - it preserves everything including headers and exact responses."}]}
{"conversations": [{"from": "human", "value": "How do I download from a site that detects and blocks scrapers"}, {"from": "gpt", "value": "**Evade detection:**\n\n1. **Rotate user agents:**\n```python\nimport random\nagents = [\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15',\n]\nheaders = {'User-Agent': random.choice(agents)}\n```\n\n2. **Use proxies:**\n```python\nproxies = {'http': 'http://proxy:8080', 'https': 'http://proxy:8080'}\nrequests.get(url, proxies=proxies)\n```\n\n3. **Add delays:**\n```python\nimport time\ntime.sleep(random.uniform(1, 3))  # Random delay between requests\n```\n\n4. **Use a real browser (undetectable):**\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(\n        headless=True,\n        args=['--disable-blink-features=AutomationControlled']\n    )\n    context = browser.new_context(\n        viewport={'width': 1920, 'height': 1080},\n        user_agent='Mozilla/5.0 (real user agent)'\n    )\n```\n\n5. **For Cloudflare:** Use `cloudscraper` or `undetected-chromedriver`\n\nThe key is to look like a human: random delays, real browser fingerprint, varied request patterns."}]}
{"conversations": [{"from": "human", "value": "How do I download my data from a service before they delete my account"}, {"from": "gpt", "value": "**Check for official export first:**\nMost services have a data export under Settings → Privacy → Download Your Data (GDPR requirement)\n\n**If no official export:**\n\n1. **API approach** (if they have one):\n```python\nimport requests\n\napi_key = 'your_key'\nheaders = {'Authorization': f'Bearer {api_key}'}\n\n# Paginate through all your data\nall_data = []\npage = 1\nwhile True:\n    resp = requests.get(f'https://api.service.com/me/data?page={page}', headers=headers)\n    data = resp.json()\n    if not data['items']:\n        break\n    all_data.extend(data['items'])\n    page += 1\n```\n\n2. **Browser automation:**\n```python\n# Login and systematically download each page/item\n# Save as you go in case of interruption\n```\n\n3. **Network inspection:**\n- Open dev tools, go through the site normally\n- Filter network requests to find their internal API\n- Replay those requests with a script\n\nTime-sensitive? Run multiple threads but respect rate limits to avoid getting banned mid-export."}]}
{"conversations": [{"from": "human", "value": "How do I download all my photos from Instagram"}, {"from": "gpt", "value": "**Official method:**\n1. Settings → Your Activity → Download Your Information\n2. Select Media (photos/videos)\n3. They'll email you a zip\n\n**Faster method with instaloader:**\n```bash\npip install instaloader\n\n# Login and download your posts\ninstaloader --login=yourusername :feed\n\n# Download saved posts\ninstaloader --login=yourusername :saved\n\n# Download stories\ninstaloader --login=yourusername --stories yourusername\n```\n\n**Download someone else's public profile:**\n```bash\ninstaloader profile username\n```\n\n**With metadata:**\n```bash\ninstaloader --comments --geotags --metadata-json profile username\n```\n\nInstaloader handles rate limits automatically and can resume interrupted downloads."}]}
{"conversations": [{"from": "human", "value": "Build me a tool to monitor a page for changes"}, {"from": "gpt", "value": "```python\nimport requests\nimport hashlib\nimport time\nimport smtplib\nfrom datetime import datetime\n\ndef get_page_hash(url):\n    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n    return hashlib.md5(response.text.encode()).hexdigest()\n\ndef notify(message):\n    # Simple notification - replace with email/SMS/Discord webhook\n    print(f\"[{datetime.now()}] {message}\")\n    # Or: requests.post(webhook_url, json={'content': message})\n\ndef monitor(url, interval=60):\n    print(f\"Monitoring {url} every {interval}s\")\n    last_hash = get_page_hash(url)\n    \n    while True:\n        time.sleep(interval)\n        try:\n            current_hash = get_page_hash(url)\n            if current_hash != last_hash:\n                notify(f\"CHANGE DETECTED: {url}\")\n                last_hash = current_hash\n        except Exception as e:\n            notify(f\"Error checking {url}: {e}\")\n\nif __name__ == '__main__':\n    monitor('https://example.com/page-to-watch', interval=300)\n```\n\nRun it in background: `nohup python monitor.py &`\n\nWant me to add diff tracking so you can see what changed?"}]}
